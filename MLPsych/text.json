"{\"577240211\":\"Introduction In today's fast-paced, uncertain, and highly competitive business environment, organizations' human capital is increasingly recognized as a key source for a sustainable competitive advantage. As such, for organizations to unlock the full potential of their employees, they need to keep them healthy and motivated even in trying times. One promising approach to achieving these aims is highlighted in the concept of positive organizational scholarship (POS) . Based on positive psychology, the purpose of POS is to investigate virtuousness, positive outcomes, positive practices, and positive properties of organizations and their members. As employees in leading positions are often described as role models and social \\\"climate engineers\\\" that impact the well-being of their employees, examining the positive practices emanating from managers is particularly vital. While the literature has developed a variety of leadership theories that follow a positive approach, scholars have increasingly raised concerns about positive leadership (PL) research. For instance, Alvesson and Einola (2019) argue that PL is overly ideological and focuses too much on the person of the leader rather than on the experience of the follower. Moreover, PL has been criticized for its flawed theoretical foundations and for being too abstract to be effectively applied in real business practice. A more empirical and pragmatic research approach along with a stronger emphasis on individuals' experiences, as emphasized in POS , presents a possible avenue to address these concerns. However, conceptions of positive and virtuous leadership as they emerge from POS are yet not fully established and suffer from incompleteness and inconsistency. Only a few studies in this body of literature exist that examine positive practices and virtuousness emanating from employees in leading positions , disagreeing about the practices and virtues that should be included in an overall construct. Additionally, capturing information on (management) behavior has been criticized for relying too heavily on self-reporting questionnaires that do not capture what employees really feel, think, or see, because of framing effects or desirability biases. Consequently, no construct or standardized measurement currently captures and contextualizes positive practices across managers' full scope of roles and responsibilities. To fill this lacuna, this study aims to establish a framework of positive management practices (PMP) that holistically and coherently captures management practices that lead to satisfaction among employees. Since there is neither a holistic construct nor a uniform measurement method for PMP, I take a novel approach to studying positive practices by applying text mining and unsupervised machine learning methodologies on a large body of unstructured data. In particular, the data is retrieved from reviews by corporate employees that appeared on the company-rating platform kununu.com. This exploratory empirical approach enables to investigate managers across their full scope of roles and responsibilities, resulting in a holistic perspective on their workplace practices without being limited to the shortcomings of questionnaires or the ideological perspective and incomplete scope of single PL theories. The results of this exploratory approach reveal that PMP manifest in six dimensions, namely respectful interaction, open communication, interpersonal support, intellectual support, reasonable instructions, and managerial competence. Moreover, while POS focuses on human well-being at its core, scholars emphasize the side effect of positivity on organizational performance with the idea of \\\"doing well by doing good\\\" . Therefore, I conduct a subsequent quantitative analysis to investigate whether there is an association between PMP and firm profitability and which of the six identified dimensions are the most important drivers of this relationship. The richness of qualitative information on management practices that can be gleaned from employees' reviews on kununu.com, in combination with the applied text mining approach, enables this study to be conducted at the level of large German capital market-oriented companies between the years 2011 and 2019. The findings indicate that a higher level of PMP positively relates to firm profitability. This study offers numerous contributions to the contemporary literature of POS and PL. The explorative part extends the academic knowledge on positive practices and virtuousness in organizations. Although constructs for positive practices and organizational virtuousness exist , scholars disagree on which virtues to include in an overall construct. For instance, while Cameron et al. (2011) include in their definition of virtuousness the practices caring, compassion, forgiveness, meaning, inspiration, and respect, other authors consider virtues such as responsibility, thankfulness, humility, and prudence. Hence, as Meyer (2018) summarizes, \\\"the POS movement leaves it up to future research to determine exactly the virtues to be included in the concept of organizational virtuousness [...]\\\" (p. 259). Furthermore, as POS pursues a behavioristic approach that connote virtuousness to positive behavior, it has been criticized for its ignorance of the philosophical perspective that considers virtues as personal character traits. Meyer (2018) emphasizes the responsibility of future researchers to find ways to reconcile the POS idea of virtuousness and the Aristotelian notion of virtues. The current study addresses the aforementioned challenges by providing a comprehensive list of virtues that should be considered in the concept of leadership virtuousness. Moreover, the framework of PMP provides space to situate character-based virtues in their respective context and delivers key behavioral indicators that reflect those virtues , making a first step in harmonizing the behavioristic and the philosophical perspective of virtuousness. The quantitative analysis establishes the link between PMP and firm profitability using hard (objective) performance measures. Although the value of POS does not depend on how it impacts organizational outcomes, such findings may nonetheless increase organizations' readiness to implement positive practices. In the existing leadership literature, few studies have examined the relation between PL and firm profitability using hard performance measures. Moreover, the studies establishing this link either investigated branches or departments within one company  or focused solely on top management. However, since employees typically interact with supervisors rather than top managers (if at all), and since supervisor behavior has been proven to have a major influence on employee well-being , examining the impacts of supervising managers' practices is particularly vital. While there exists evidence of the various positive outcomes coming from healthy supervisors-followers relationships, such as increased follower satisfaction, productivity, and OCB , no study, to the best of my knowledge, has examined a relationship between supervising managers' positive practices and firm profitability at the organizational level. However, this jigsaw piece could be a reason for companies to recognize the importance of lowerlevel management practices. The ability of the applied text mining approach to analyze vast amounts of unstructured data allows for addressing this research gap and finds that \\\"doing well by doing good\\\" may improve profitability even at the level of large German organizations. Next, the paper makes methodological contributions to POS, and the organizational behavior literature in general. First, it introduces an alternative method for capturing practices in organizations without being influenced by the way researchers design surveys or participants' social desirability biases. Used in a complementary way to traditional research, text mining and unsupervised machine learning applied on unstructured data can enrich the literature to find novel, undiscovered patterns and to validate the generalizability or Zeitgeist relevance of established theories. Finally, the paper contributes to the literature of strategic human resource (HR) management, and in particular, to the emerging literature of workforce analytics (or HR analytics) . Marler and Boudreau (2017) describe workforce analytics as the use of more sophisticated analysis of HR-related data to support strategic decision-makers by linking HR decisions to organizational outcomes. Scholars agree that the relationship between HR measures and business performance needs to be further explored in quantitative and large-scale studies in order for HR analytics to gain more attention in practice. By linking PMP and firm profitability via text mining, this study not only introduces an avenue for analyzing a company's workforce well-being, but it also emphasizes the importance of investing in tools that help to improve employees' positive emotions, thus ensuring a productive workforce. The remainder of the study is organized as follows. The next section provides a definition of PMP and develops the study's hypothesis on why PMP could enhance the profitability of organizations. Thereafter, in the explorative part of this study, the text mining approach is introduced, followed by a presentation of the concept of PMP. Subsequently, in the quantitative part of this study, I investigate the relationship between PMP and firm profitability. Finally, the last section offers a general discussion, future directions, and practical implications. Theoretical Background Positive Management Practices The understanding of the term \\\"positive\\\" in this study is based on the concepts of positive psychology and POS. Positive psychology studies positive emotions, which manifest as flow and happiness in the present, hope and optimism for the future, and well-being and satisfaction in the past. The study of POS examines organizational practices through a positive lens ; it applies positivist methods to find good and thriving practices, rather than those that are bad and failing. Cameron et al. (2011) define positive practices as behaviors, techniques, and routines that are (i) positive deviant, (ii) have an affirmative bias, and (iii) reflect the ideas of virtuousness and eudemonism. The first connotation, positive deviance, describes exceptional actions that go beyond the usual performance. Second, the research on positive practices has an affirmative bias, in that it refers to strengths and capabilities rather than weaknesses and problems. It emphasizes what elevates organizations and its members, what goes right in organizations, and what is experienced as good. The third connotation refers to virtuousness and eudemonism, concepts that focus on fostering humankind's goodness. Eudemonism, which can be traced back to Aristotle in his Nicomachean Ethics, is a perspective of happiness and the worthiness of human life. Also inspired by the doctrines of Aristotle, virtuousness is outlined as a quality that represents moral excellence that is \\\"inherently valuable, apart from any benefit that may accrue\\\" (;, p. 269) . Examples of Aristotelian individual virtues are prudence, courage, justice, truthfulness, and friendliness. In contrast, virtuousness in the concept of Cameron et al. (2011) is reflected by the positive practices of caring, compassionate support, forgiveness, inspiration, and meaning, as well as respect, integrity, and gratitude. In line with the ideas of positive psychology and POS, I define PMP as behaviors, techniques, and routines of managers that denote virtuousness and lead to positive emotions among employees. Hence, the definition comprises the connotations of affirmative bias, expressed by the focus on employees' positive emotions resulting from PMP, and virtuousness, as an aggregate of individual virtues, which manifests in positive practices. Following Meyer (2018), however, I exclude the connotation of positive deviance because leadership practices do not have to be exceptional in order to induce positive emotions in subordinates. The term \\\"management\\\" is defined as a group of organizational members that have leading and supervising functions. This includes all levels of the hierarchy, from the CEO of a company to immediate supervisors and team managers, provided any leader-follower relationship exists. In recent years, the idea of \\\"positivity\\\" has also largely influenced the leadership literature. Leadership theories that follow a positive approach include virtuous leadership, authentic leadership (AL), PL, ethical leadership, servant leadership, transformational leadership, and charismatic leadership. However, these theories are grounded in different conceptional constructs and schools of moral philosophy. First, while leaderships styles such as transformational leadership place a strong focus on employee effectiveness , the main outcome of the PMP concept is employee well-being as induced by managers through the amplifying effect of virtuous behavior , with employee effectiveness being only a consequence of employees' positive emotions. Moreover, in contrast to the ethical leadership theory, the conceptualization of PMP does not explicitly expect subordinates to embrace norms and ethical values, and in contrast to servant leadership, the PMP approach is not solely focused on meeting subordinates' needs. Further, although the concept acknowledges virtuousness as part of positive practices, it also differs from virtuous leadership, which is solely grounded in virtue ethics. Rather, the PMP concept can be viewed as an empirically grounded approach to virtuousness, yet providing room for philosophically grounded virtues to be considered in specific contexts. Finally, although it is based on the same grounding of positive psychology, the concept of PMP differs from AL  and Cameron's (2012) concept of PL. While AL centers on the leaders themselves and their respective fidelity to their personal values, their self-awareness, and their ability of self-reflection , PMP focus on manager-subordinate relationships. Cameron's concept of PL stronger emphasizes extraordinary practices, whereas PMP more comprehensively incorporate practices that evoke positive emotions. Moreover, both AL and Cameron's PL are more generic in nature and build on attributes from other leadership theories; for instance, an authentic leader is described as one who acts in accordance with deep personal values, and a positive leader supplements other leadership theories with an emphasis on positive energy. In contrast, given its empirical origins, PMP is a contemporary and pragmatic approach that can be readily adopted by practitioners seeking to positively influence the psychological state of their employees. The Link Between Positive Management Practices and Firm Profitability I argue that PMP positively influence subordinates, leading to positive outcomes at the individual level. In order to corroborate this argument, I refer to theoretical concepts and empirical findings from positive psychology and social exchange theory (SET). Both concepts are interrelated in that supervisor support as part of the SET has been found to predict well-being and positive emotions among employees , while, on the other hand, positive emotions have been linked to better interpersonal relationships. For each line of the two concepts, I then establish the link between positive outcomes at the individual level and positive outcomes at the firm level, and, eventually, the profitability of a firm. I illustrate these relationships in Figure 1. Social Exchange Theory. The SET has been an influential concept for understanding workplace behavior. Social exchange is described by Emerson (1976) as a frame of reference in which the flow of valued items follows certain exchange rules. In organizational behavior, research often focuses on reciprocity as an exchange rule. Reciprocity means that an action of one party leads to a reaction by another party. In this context, Eisenberger et al. (1986) introduce the concept of perceived organizational support. Based on the reciprocity norm, the authors argue that an employee who feels supported by the organization feels, in turn, obligated to care about the organization's welfare, which leads to greater employee efforts to achieve the organization's goals. Indeed, empirical work finds evidence for a relationship between perceived organizational support and organizational citizenship behavior (OCB), such as interpersonal help, loyalty, or performance beyond duty. A further aspect of perceived organizational support is perceived supervisor support (PSS), which focuses on the relationship between supervisors and subordinates. Similarly, this relationship is at the focus of the leader-member exchange theory (LMX) . Both PSS and LMX are understood to be interrelated with perceived organizational support, in that PSS\\/LMX are often referred to as an antecedent of perceived organizational support. Empirical studies have found LMX and PSS to be positively related to job performance, job satisfaction, job retention, and positive emotions among employees, as well as negatively related to counterproductive work behavior. Taken together, I conjecture that PMP lead to increased perceived organizational support and PSS, which result in positive individual outcomes such as higher job satisfaction, higher employee retention, increased engagement in OCB, and decreased counterproductive work behavior. These individual outcomes can again be linked to positive outcomes at the firm level. For instance, a company with satisfied employees who highly engage in OCB and avoid counterproductive work behavior will have lower agency costs that may arise due to higher monitoring costs and disengagement in working toward organizational goals. Such costs could negatively impact a company's profit and loss statement and, hence, its financial performance. The relationship between improved OCB and organizational performance has also been supported by empirical evidence. Further, Huang et al. (2015) and Melian-Gonzalez et al. (2015) investigated the relationship between employee satisfaction and firm performance, reporting significant associations between employee satisfaction and various hard performance measures. Positive Psychology. Positive psychology is the study of finding ways to foster strengths, virtues, well-being, and resilience in individuals and groups in order to enhance happiness and life satisfaction. At the heart of positive psychology lie positive emotions that manifest themselves in individuals' well-being and satisfaction. As this study's conception of PMP is empirically linked to employee satisfaction, I expect that PMP enhance employees' positive emotions. Moreover, emotionally intelligent leaders are frequently cited as one of the main sources of positive emotions among employees. For instance, Avey et al. (2011) found evidence in an experimental study that leaders' positive psychological capital - efficacy, hope, optimism, and resiliency - is positively related to the positivity of their followers. Rego et al. (2014) showed that AL is positively linked to followers' hope and positive affect. Finally, certain virtues of leaders, such as humility and compassion, have been shown to increase positive emotions among followers. The literature has linked positive emotions at work to several individual outcomes, such as resilience, health, creativity, attention, and work engagement. As described in the broaden-and-build model introduced by Fredrickson (1998), positive emotions can build permanent personal resilience resources, which function as reserves to enable individuals and organizations to deal with negative events in the future. Cameron and colleagues refer to that as a \\\"buffering effect\\\", as positivity strengthens resilience, which buffers organizations from negative events. Further, Seligman and Csikszentmihalyi (2000) emphasize the implications of positive psychology for mental and physical health. They argue that optimistic beliefs can affect the immune system, protect people from illness, and prevent depression and anxiety. More recent studies even found that positive emotions are associated with a lower risk of heart disease and lower functional limitations. Second, positive emotions have been linked to creativity and attention. Fredrickson (1998) argued that individuals who experience positive emotions have a broader field of momentary thoughts. That is, they tend to have an expanded range of ideas that come to their minds. Related to this, Ellsworth and Smith (1988) examined the different patterns of pleasant emotional experience. In their study, they found a positive relationship between interest and attention as well as between interest and pleasantness. Another study by Avey et al. (2011) concluded that participants with a higher level of positive capital provide more solutions and a higher level of original solutions than participants who report a lower level of positive capital. More recently, Rego et al. (2012) and Rego et al. (2014) linked employees' positive emotions, such as hope and optimism, to their creativity. Third, positive emotions have been linked to work engagement. Schaufeli et al. (2002) define work engagement as a \\\"positive, fulfilling, work-related state of mind that is characterized by vigor, dedication, and absorption\\\" (;, p. 74). Here, vigor refers to a high level of energy, effort, and persistence with which employees perform their tasks, dedication refers to employees experiencing a sense of meaning, inspiration, and challenges in their work, and absorption describes a state of \\\"flow\\\" that employees experience while working. Fourth and finally, positive emotions are linked to improved interpersonal behavior, which leads to better teamwork and cooperation between coworkers. In summary, the literature suggests that PMP lead to higher creativity, attention, health, work engagement, and better mood among employees, yielding more productivity and better performance outcomes both at the individual and the team levels. Transferred to the corporate level, I conjecture that if a company's workforce is more attentive, fewer errors will occur, which leads to higher productivity and better quality of products and services. Moreover, a company with happy and healthy employees is more resilient to negative events and tends to have lower health expenses, less employee absenteeism, and decreased employee turnover, all of which impacts the profit and loss statement, and, hence, the profitability of a company. Given the theories and empirical evidence from research on positive psychology and social exchange, I suggest that the more employees experience PMP, the more positive the emotions of the workforce and the better the social exchange relationship between supervisors and followers, eventually being reflected in the profitability of the firm. Hence, I posit the study's hypothesis: Hypothesis: A greater prevalence of PMP in organizations is positively related to firm profitability. A Framework of Positive Management Practices Data Collection Given the increasing amount of text data on social media platforms, I gathered employees' opinions and perceptions about their supervisors from kununu, a leading European employer rating platform where employees can rate their employers on a 5-point Likert scale, with five stars representing the highest rating and one star the lowest rating (kununu.com). In addition to a quantitative assessment, employees can express their individual opinions and thoughts on various topics in a free-text format. To obtain information about PMP, I gathered reviews on the topic \\\"supervisor practices\\\" in which employees across all levels of hierarchy describe and evaluate the practices of their supervising manager(s). Moreover, to observe positive practices, reviews were collected that are rated with four or five stars, indicating that the employee is satisfied with the manager, which implies positive emotions. Example quotes are provided in the Appendix. I collected the data from large German capital market-oriented companies (annual sales of at least 50 million euros) between the years 2009 and 2019. Thereof, the data contains 14,722 reviews on supervisor behavior, with 5,650 (38%) rated positively (>3) and 7,361 (50%) rated negatively (<3) (12% are neutral ratings). Table 1 summarizes the underlying selection criteria. Methodology The text mining procedure followed a bag-of-words assumption and consisted of standard text preprocessing approaches in natural language processing such as tokenization, part-of-speech tagging, lemmatization, and removal of stop words. Text preprocessing is essential to transform the unstructured text data into a machine-readable format. I provide a detailed description of the preprocessing approach in the Appendix. Subsequently, I used unsupervised machine learning to cluster words that frequently occur together in reviews. To find a comprehensible and reliable solution for such word clusters, I used a meta-model approach  by combining four unsupervised machine learning techniques: hierarchical clustering (average linkage and Ward's method), non-hierarchical clustering (k-means clustering), and network analysis (Newman-Girvan algorithm) . The overall text mining and unsupervised machine learning procedures are illustrated in Figure 2. Further details are provided in the Appendix. The results reveal a six-cluster solution. In a final step, I compared the individual words that the different methods assigned to a cluster. The results of all four methods agree for 80% of the allocated words. Another 15% of words are found to agree by a majority vote, while the algorithms disagree for 5% of words. I accepted a word in the corresponding cluster if there is a full agreement or a majority vote (i.e., three out of four methods agree). Figure 3 summarizes the final cluster solution. Presentation and Interpretation of Positive Management Practices The six clusters identified in the previous section present the different contexts in which PMP manifest, which I refer to as \\\"dimensions of PMP\\\". The dimensions of PMP and their short definitions are presented in Table 2. To gain a better understanding of each dimension, I present additional words that correlate significantly with words in the respective dimension in the Appendix (Figure A 7--Figure A 12). In the first dimension, the majority of words are directly associated with the word to treat. This dimension is about how managers treat their employees and interact with them. Employees are satisfied with their treatment if their managers treat them in a friendly way, respectfully, appreciatively, and give them the feeling of meeting them at eye level. In addition, relationships between managers and employees are described as loyal, open, and honest (see Figure A 7). I label this dimension respectful interaction. This dimension reflects the virtues respect, friendliness, gratitude, trustworthiness, and truthfulness and is largely consistent with Cameron's (2011) positive practice dimensions respect, integrity, and gratitude. However, above that, the dimension respectful interaction particularly emphasizes the importance of treating employees as equals and avoiding arrogance, which points to the relevance of the virtue humility. The second dimension contains the words conversation, feedback, regularly, constructive, open, direct, and honest. This cluster suggests that employees value open, honest, and straightforward communication with their managers. Similarly, employees appreciate when they receive honest and constructive feedback on a regular basis. Hence, I label this dimension open communication. This dimension is less comparable to one of Cameron's (2011) positive practices, but it does cover the practice caring, in that managers who give frequent and constructive feedback seem to be interested in their employees, and the practice integrity, in that employees value an open and honest way of communicating. I also find the words feedback and constructive to be correlated with the word mistake, showing that open communication includes addressing errors and failures, but in a constructive manner (see Figure A 8). Although this dimension is connected to the first dimension (as illustrated by a dashed line in Figure 3), in that employees appreciate a respectful and equal way of communicating, the findings also indicate that the practice open communication largely concerns business task-related matters. Overall, this dimension comprises the virtues truthfulness (reflected by open and honest communication), courage (reflected by honest and straightforward communication), and humility and forgiveness (reflected by a respectful way of communicating, even in the event of negative feedback). In the third dimension, the three most often occurring words are to listen, problem, and responsive. Managers in this dimension are responsive, care for their employees' concerns and needs, offer advice, and find solutions (see Figure A 9). Hence, I label this dimension interpersonal support. In addition, this dimension frequently contains the words conflict and solution. Conflicts should be resolved diplomatically, objectively, and jointly, and discussed thoroughly (see Figure A 9). This dimension includes elements from Cameron's (2011) positive practice dimensions caring, and compassionate support, which emphasize helpful behavior and high levels of empathy and responsiveness. Hence, the dimension interpersonal support primarily reflects the virtues caring, compassion, and friendliness. However, in order to diplomatically resolve conflicts or to offer useful advice, the virtue prudence also plays an important role in this dimension. The fourth dimension concerns decision-making and goal setting, particularly how instructions are conveyed in the processes. I label this dimension reasonable instructions. Employees perceive it as positive if their managers' decisions are comprehensible, clear, fast, and transparent. It is also appreciated if decisions are well explained, justified, discussed, and if employees get involved (see Figure A 10). Likewise, the possibility of speaking up and the precise communication of information are important prerequisites in the conception of organizational justice, especially in the dimension of procedural justice. Regarding goal setting, I find that positive reviews often contain words such as realistic, comprehensible, and clearly defined. Employees seem to appreciate understanding their goals and expectations. These findings are similar to concepts of goal-setting theory, which states that managers should set specific goals. Furthermore, the word goal correlates with the words ambitious, demanding, challenging, and achievable (see Figure A 10). Overall, the dimension reasonable instructions is more a business task-related management practice. It shows analogies to Cameron's (2011) positive practices meaning and inspiration, by the motivating nature of these two practices, and addresses the virtues justice, prudence, courage, meaning, and inspiration. The fifth dimension includes the words to foster, to support, task, idea, and space. Employees appreciate managers who support their development, foster their strengths, and are interested in their ideas and proposals but, at the same time, leave room for discretion and responsibility. Their tasks are described as interesting, motivating, and diversified, tasks in which they can grow (see Figure A 11). Hence, I label this dimension intellectual support. This dimension comprises elements from Cameron's (2011) positive practices dimensions inspiration and meaning but also from compassionate support and caring, albeit the latter at more of a professional level. For instance, managers are interested in and promote the professional development of their employees and mentor their employees on business-related tasks. To conclude, this dimension comprises the virtues prudence (as a necessary prerequisite for providing intellectual support), caring, inspiration, and meaning. The last dimension yields a more general view of the competencies of a manager. Managers in this dimension are described as competent in terms of both professional and as social skills. I label this dimension managerial competence. The term professional expertise frequently appears with terms such as methodical and objectively. The dimension is also tightly linked to the dimensions respectful interaction and interpersonal support (see Figure 3). Managers seem to maintain a friendly and professional relationship with their employees and make them feel like they are taken seriously. Further, the word human is correlated with words such as pressure and courage, indicating that managers react with humanity in situations of pressure and even prove their courage in hard times (see Figure A 12). In addition, the words human, professional expertise and courage bring to mind the virtues friendliness, prudence (particularly knowledge of business), and courage, respectively. Finally, as illustrated in Figure 3 the individual dimensions exist on an interrelated continuum. For instance, the dimension open communication is closely connected to the dimension reasonable instructions, particularly in regard to goal setting. Employees appreciate clear and comprehensive goals, but they also value regular and constructive feedback on their task performance. Moreover, how feedback and instructions are conveyed plays an important role, which is evident from the connection to the dimension respectful interaction. Positive Management Practices and Firm Profitability Research Design In the following section, I conduct a regression analysis using panel data to investigate the potential relationship between PMP and organizational profitability. I obtained data on supervisor practices from kununu.com. The data includes all reviews that report supervisor practices. Financial and supplemental data is retrieved from the Compustat Global Database. Similar to previous studies that used employer rating platforms to asses employee satisfaction , I excluded firm-year observations with fewer than ten reviews. I collected data from large German capital market-oriented companies between the years 2011 and 2019, with 2011 being the first year with ten or more reviews per firm-year, resulting in a final sample of 799 firm-year observations from 182 companies. PMP Measures. In order to determine the measures for PMP and their dimensions at the firm level, I aggregated employees' individual reviews as follows: First, to determine if a review contains information on any of the dimensions, words that were identified as keywords in a respective dimension (see Figure 3) were used. Each time a review contained one of the words from a respective dimension, it was classified as such. A review that provided information on different dimensions was classified for each dimension. For an overall measure of PMP, I incorporated the words from all dimensions combined. Next, the reviews were labeled as \\\"1\\\", \\\"0\\\", or \\\"-1\\\" if they appeared in a positive, a neutral, or a negative context, respectively. Sentiments were measured by the corresponding 5-point Likert scale measure for supervisor practices. I then aggregated the reviews by summing up the pre-labeled reviews for each company in a given period t and divided them by the respective total number of reviews to obtain a relative frequency. I determined one variable for each dimension and one overall variable for PMP. These variables can be interpreted as follows: The more positive (negative) the value, the more employees agree on a positive (negative) sentiment toward their managers' practices. A value of zero indicates that employees either disagree in their opinions or do not address the management practices of a particular dimension at all. Firm Profitability Measure. In leadership and organizational research, the profit margin, net income divided by sales, is commonly used to measure firm profitability. However, in line with more recent literature , I use operating profit as the numerator since I am interested in the profitability of a company in view of its actual operational purpose, instead of the net income, which also includes non-operating costs (i.e., taxes and interest). Hence, I define the operating profit margin (OPM) as the operating profit, incorporated as earnings before interest and taxes (EBIT), divided by sales. Regression Models. In the first model, I examine the link between firm profitability and PMP (overall). In the second model, I investigate the relationship between firm profitability and the dimensions of PMP. I use both models without controls (baseline) and with controls (full). The full regression models are summarized in equations (1) and (2), Regression Model 1 OPMi,j = a+ b*PMPi,j + th* firmsizei,j + i*industryk + t*indexd + k*yearj + ei,j (1) Regression Model 2 OPMi,j = a+ b*reasonablei,j + g*openi,j + d*respectfuli,j + z*intellectuali,j + e*interpersonali,j + th*competencei,j + th* firmsizei,j + i*industryk + t*indexd + k*yearj + ei,j (2) where the indices i, j, k, and d correspond to firm, year, industry, and index, respectively. OPM is the OPM of a firm in a given year. The variables reasonable, open, respectful, intellectual, interpersonal, and competence are the relative frequencies obtained by the aforementioned aggregation process and correspond to the six dimensions reasonable instructions, open communication, respectful interaction, intellectual support, interpersonal support, andmanagerial competence, respectively.PMP is an overallmeasure of PMP. The vector firmsize contains thevariables ln(assets) and ln(sales), thenatural logarithmsofassets and sales. I use these covariates to control for firm size. The dummy variables industry, index, and year control for industry, index, and yearfixed effects, respectively. The industry is classified by the three-digitNAICS (NorthAmerican IndustryClassificationSystem) subsector code. The variable index describes the index of theGerman stockmarket inwhich the company is listed, i.e., DAX, MDAX, SDAX, or no listing. Results Main Analysis. Columns (1a) and (1b) of Table 3 shows the results from the first model. The coefficient of PMP is 0.275 and 0.122 for the model without and with control variables, respectively. Both coefficients are significant at the 1% level, indicating that PMP are positively associated with OPM. Further, I test whether additional information could be obtained by examining the six dimensions of PMP. I report the results from the second model in column (2a) and column (2b) of Table 3. I find a significant coefficient for the variables reasonable, respectful, and competence in in the model without control variables (column (2a)) but only a significant coefficient for the variable respectful in the model with control variables (column (2b)). Hence, the more often employees report PMP regarding the dimension respectful interaction in their reviews, the higher the OPM is on average. Additional Analyses. I employ additional analyses to investigate the impact of (i) selection bias, (ii) reversed causality, and (iii) model modifications on the regression results. The results are summarized in the Appendix (A.). First, by only incorporating companies with sufficient reviews on supervisor behavior, I risk a biased sample selection, as there may be specific reasons that employees of a particular company rate their supervisors. To mitigate that concern, I use a two-stage Heckman correction. Incorporating the inverse mills ratio into the regression models does not affect the results. Second, as higher company performance might lead to increased subsequent PMP, I add a time-lagged independent variable of return on assets (ROAt-1)  to control for past performance. Moreover, I add a lagged operating profit margin (OPMt+1) as the dependent variable to examine the direction of causality. The coefficient of PMP stays significant and positive when controlling for past performance. However, the coefficient narrowly misses the significance threshold when OPMt+1 is used as the dependent variable (p-value= 0.134). In terms of model 2, the variable respectful stays significant and positive in both modifications. Moreover, the coefficients of the variables intellectual and interpersonal become significantly negative in the modification with lagged OPM , potentially explaining the insignificant coefficient of PMP. Overall, the findings indicate that PMP and respectful have an effect on OPM rather than vice versa. Third and finally, I test the robustness of the main results by adding an alternative performance measure (EBITDAsales ) to mitigate potential earnings management within the balance sheet items of depreciations and amortization, and by creating a subsample that includes only observations with at least thirty reviews per firm-year. The results are robust for both models when incorporating an alternative performance measure. However, in the subsample with thirty or more observations, only the variable respectful stays significant positive. To conclude, the variable respectful stays significant and positive throughout all five additional tests, while the variable PMP is significant and positive in three additional tests. General Discussion Theoretical Implications The results of the present study have several implications for the field of POS. This study's findings reveal that PMP place a stronger emphasis on professional relationships compared to positive workplace practices, as was done in Cameron et al. (2011). In particular, giving task feedback, promoting employees' professional development, and showing justice and prudence in decision-making and goal setting are emerging components in PMP. Moreover, PMP at both the interpersonal and professional levels emphasize the importance of humility (as opposed to arrogance) for thriving relationships. Moreover, given that positive practices and virtues are tightly connected, the PMP framework has implications for the concept of organizational virtuousness by demonstrating (i) which virtues to include in a concept of virtuous leadership, (ii) that different virtues are important in different contexts, and (iii) that the same virtues may reflect in different practices depending on the respective context. In regard to (i), the results of this study indicate that out of the 59 virtues that Hackett and Wang (2012) summarized from the PL literature, 14 of these virtues frequently occur in the analyzed employee reviews. While Wang and Hackett (2016) incorporate only the six \\\"cardinal values\\\" in their construct of virtuousness leadership, the conception of PMP represents a more holistic, less abstract framework, yet the contextual classification still provides cohesiveness, indicating that the construct of leadership virtuousness can be easily applied. Regarding (ii) and (iii), the findings suggest that employees in different contexts value different virtues and that the same virtue of a manager may be reflected differently in their practices depending on the context. For instance, the virtue of courage appears to be important in the contexts of communication, instructions, and managerial competence. While courage is valued in the context of communication in the sense that managers communicate straightforwardly, courage is valued in the context of instructions in the sense that managers should be able to make a final decision in the event of disagreement. Another example is the virtue of prudence, which occurs in five of the PMP dimensions. In the context of communication, e.g., prudence is valued in the sense that it is needed to provide constructive feedback. In the context of instructions, prudence is important for making reasonable decisions and choosing the right means to achieve the desired goals. Moreover, the fact that prudence is the most frequently occurring virtue among all dimensions empirically proves the importance of this virtue, which has been frequently referred to as \\\"master virtue\\\" in a construct of leadership virtuousness. The second part of the study examines the link between PMP and firm profitability. Thus, this study adds to the body of literature that examines the relationship between PL and objectively measured organizational financial performance. However, the studies mentioned above focus on CEOs and top managers as the object of inquiry, explaining the link to firm profitability through role modeling and inspiration to influence follower effectiveness. In contrast, the current study establishes the link between PMP and firm profitability through managers' impact on employees' positive emotions, acknowledging that frequent exposure to supervisors' practices is a major determinant that affects employees'mental well-being. While the literature has linked positive supervisor behavior and positive leader-member exchanges to a number of individual- or team-level outcomes, such as strengthened resilience , increased work engagement , and reduced counterproductive work behavior , the question of whether and how PL (focusing on supervisors) relates to firm profitability has remained largely unsolved. In this regard, Hartnell et al. (2020) demonstrate that a supportive climate and increased OCB induced by PL can even negatively impact a branch's financial performance. The current study sheds new light on this relationship by demonstrating that the more employees in management positions engage in positive practices, and particularly in respectful interactions, the higher the company's profitability. This underscores the importance of the way supervising managers treat their employees in fostering employees' positive emotions and the resulting positive effects in organizations, which eventually impact the company's profitability. Regarding the remaining dimensions, the analyses yield mixed results. For the dimension intellectual support and interpersonal support, I find positive (but insignificant) coefficients in the main analysis and a significant negative coefficient in one of the additional analyses. One possible explanation for these mixed results could be that firm profitability can be affected in a negative way if managers give their employees too much freedom in their tasks by setting generalized goals. This argumentation is supported by the goalsetting theory. On the other hand, findings from research on shared leadership indicate that less clear instructions are positively related to team performance , indicating that there might be a non-linear relationship between intellectual support and firm profitability. Moreover, a negative effect of the dimensions intellectual support and in particular interpersonal support might be explained by the argument of resource allocation since supervisors who spend much time helping others have less time for their own task completion. However, as coefficients in the present study are insignificant in most tests, a final conclusion cannot be drawn. A more depth analysis between the dimensions of PMP and firm profitability and the underlying mechanisms of this relationship remain to be explored in future research. Finally, the study yields implications for the field of organizational change. The inherent uncertainty associated with change processes has the potential to trigger negative emotions (such as anger, anxiety, or rejection) among a company's workforce. Scholars emphasize the potential of leadership in positively influencing employees' emotions during change processes. As PMP and positive emotions are highly interwoven, I propose that PMP are particularly valuable mechanisms for fostering employees' positive emotions during organizational change processes. Moreover, as employees often view their supervisors as representatives of the organization , the positive social exchanges that result from PMP may spill over to employees' perceptions of their company and may lead them to trust and accept the company's change decisions. Similarly, Gigliotti et al. (2019) demonstrate a positive effect of employees' perceived support by their employers on the level of trust and readiness to change. In a different study, Vardaman et al. (2012) find a positive relationship between the number of positive relationships individuals form in their working environment and the extent to which they interpret the change as controllable, which in turn, has been linked to the success of the change process. Vardaman and colleagues argue that both friendship and advice-seeking relationships are associated with increased access to information and social support, leading individuals to view change issues as more controllable. Given the positive effect of PMP on positive emotions and healthy relationships, the findings of these studies underline the importance of a high prevalence of PMP in organizations to support change processes. I leave it to future research to further investigate this proposal. Similar to this study, unstructured data from employee reviews combined with text mining could be used in an event study to examine PMP, positive emotions, virtuous behavior, or other leadership practices and their effects on organizational change success. Limitations and Future Directions This study faces several limitations. First, I investigate PMP perceived by German employees. Dahlsgaard et al. (2005) identify the six virtues wisdom, courage, humanity, justice, temperance, and transcendence as recurring virtues across many cultures in the world, most of which become evident in the results of the present study. However, there might still be cultural differences in how employees perceive positive practices. Future research could extend the study's framework of PMP by investigating what employees of different cultures perceive as positive (or negative) management practices. Moreover, the explorative approach applied in this study requires some subjective choices by the researcher. Although I combined four different clustering methods in this study to reduce subjectivity and strengthen the robustness of the results, confirming the proposed six dimensions of PMP in a confirmatory factor analysis would be an important contribution for future research. In addition, I encourage future researchers to create a questionnaire for practical application. Finally, this study's methodology is not designed to prove a causal relationship between PMP and firm financial performance. Even though I addressed potential biases due to endogeneity, there are too many external influences that the study approach cannot control. Determining causal relationships requires conducting experiments, quasi-experiments, or event studies. Nonetheless, the study design has proven valuable in uncovering new patterns and relationships that could not have been uncovered without using these techniques. For future research, combining text mining with more traditional approaches can be fruitful to exploit the complementary strengths of different methods. This is applicable in mutual directions: Text mining and online media content could be used to generalize findings from the lab and to validate the local, temporal, or Zeitgeist relevance of established theories, while, at the same time, novel patterns and correlations identified with the help of text mining (as in this study) could be validated using more traditional approaches such as experiments and confirmatory factor analyses. Practical Implications This study also yields practical and social implications. Examining employee reviews with text mining approaches helped to identify practices that are valued by employees in real business practice and that lead to positive emotions and satisfaction among employees. In addition, organizations with a higher degree of PMP, particularly respectful interactions, are associated with higher profitability. At the same time, the PMP presented have triggered employees to publish a favorable rating on the employer rating platform kununu. Such platforms are becoming increasingly important for potential employees in their decision to work for a certain employer. Hence, teaching PMP may additionally lead to better evaluations on employer rating platforms, eventually helping companies to attract new talent. The findings are particularly important for fields of strategic human resource management (SHRM) and strategic management accounting (SMA), which play an important role in hiring employees and managing their behavior. SHRM and SMA practitioners could use both the text mining methods and the findings presented in this study to develop tools for measuring and implementing PMP in their organization. For instance, practitioners can apply the presented text mining approach to measure a company's level of PMP, or selected dimensions of interest, by simply counting reviews containing the respective words in positive and negative contexts, subtracting them from each other, and dividing them by the total number of reviews. Compared to a benchmark or looking at the ratio over time, the resulting ratio can indicate useful information about the average sentiment toward this management practice in the company. Integrated into a strategic performance measurement system, e.g., the balanced scorecard, such metrics can serve as decision support and for strategy control regarding the firm's desired management behavior. Further, based on the findings of this study, practitioners can design questionnaires to measure the PMP in specific (smaller) departments. For example, regarding the dimension respectful interactions, a questionnaire could ask employees to rate the following statements: \\\"my supervisor treats me and my co-workers with respect\\\", \\\"my supervisor shows appreciation for our work\\\", \\\"my supervisor is honest and acts with integrity\\\", and \\\"my supervisor is humble and meets me and my co-workers at eye level\\\". Moreover, the virtues identified within the PMP dimensions, i.e., prudence, respect, friendliness, gratitude, trustworthiness, humility, courage, forgiveness, caring, compassion, justice, meaning, inspiration, and truthfulness, can be integrated into the corporate values to increase the overall presence of such virtues. Finally, PMP can be taught in training courses for employees in management positions. In doing so, the training can be guided by the findings of this study and cover all six dimensions of PMP. Overall, I advise companies to ensure that employees in leadership positions understand and adopt PMP. Incorporating PMP measurements into strategic management accounting tools, reinforcing the virtues in the company's core values, or teaching them in management training are possible paths of implementation. Conclusion The concepts of positive psychology and POS emphasize the importance of employees' positive emotions and satisfaction for organizations to flourish. The current study provides a framework that holistically and coherently captures management practices that lead to satisfaction among employees. Moreover, this study finds a positive association between PMP at the organizational level and firm profitability. This association is particularly driven by the dimension of respectful interaction. In conclusion, PMP not only elicit positive emotions and satisfaction among employees, but also have a positive effect on firm profitability, even in large German organizations. While this study was not designed to prove a causal relationship between PMP and corporate financial performance, the results indicate that it is still worthwhile to consider the concept of PMP in future research and to integrate it into corporate identities.\",\"577240225\":\"Introduction Starting with the work by Conger (1990), leadership scholars have been increasingly focused on the destructive or negative side of leadership. People who score high on these so-called dark triad traits (narcissism, Machiavellianism, and psychopathy) tend to enact destructive styles of leadership. Although three dark traits have been identified, scholars have tended to concentrate on narcissism when studying destructive leadership. Narcissists are people whose decisions and goals are driven by unrelenting arrogance and self-absorption. Such individuals lack empathy, have fragile self-esteem, and are hostile to others who threaten their positive self-regard. To date, the majority of the published studies have used a leader-centric perspective for understanding the consequences of leader narcissism. A leader-centric perspective means that researchers only examine the leader's characteristics and attributes to understand the consequences of narcissism. For example, researchers have examined whether leader personality traits, such as extraversion or neuroticism , interact with leader narcissism to determine the consequences of this dark trait. Unfortunately, using a leader-centric perspective only provides an incomplete image of leadership. In contrast to this perspective, researchers are increasingly emphasizing the role of followers in the leadership process. Indeed, in their discussion of the toxic triangle, Padilla et al. (2007) argue that destructive leadership will occur when the right kind of leaders are matched with the right kind of followers in the right context. When followers are given their rightful attention, leadership switches from a focus on individual-level characteristics of a leader to a dynamic and emergent phenomenon that operates at a dyadic or group level of analysis. In the current study, we take a relational-based perspective to understand the consequences of narcissistic leadership. This perspective emphasizes that the leadership process is the joint, co-creation between leaders and followers. We focus on the traits of both leaders and followers to understand the consequences of leader narcissism. We maintain that follower attraction to certain leaders as well as the success of followers engaging with these leaders is a function of both leader and follower traits. When followers' traits are mismatched with the leader, followers either transfer to other groups or eventually become part of the leaders' out-group. On one hand, we believe that putting followers back in the leadership equation adds to the literature on destructive leadership. We argue that certain types of leaders and followers combine to form destructive leadership if the environment (in this case social media) is conducive. In this article, we attempt to identify the exact set of followers' personality characteristics most susceptible to the influence of such leaders, namely, narcissistic leaders. On the other hand, this article also clarifies some of the inconsistent findings concerning narcissistic leadership. For example, studies have found that narcissistic leadership can result in positive and negative consequences. The relational-based perspective answers recent calls for incorporating followers into this literature and has important implications for managers, executives, and organizations. Another contribution of this article is its setting, namely, the study of leader-follower interactions on social media. Traditional leadership research is commonly based on leaders (i.e., those who have and use their control of resources and organizational authority to influence others) and followers (i.e., those who follow leaders because of the degree of control of resources leaders within the same organization can exert upon them) within clearly defined social groups (i.e., organizations). However, much less is known about the reach of leadership beyond organizational boundaries through using tools such as social media. In the present work, we study the interactions between leaders (i.e., those who influence others, not just organizational followers but also the public, based on their communications online) and followers (i.e., those who follow and interact with leaders not because they have to, but because they choose to follow them) beyond traditional organizational boundaries. Therefore, in addition to providing a relational view of leadership, the present work is a portrayal of how leaders can use social media to expand their reach beyond organizational boundaries and influence the public at large. As such, the power that leaders can wield on social media in influencing those that lie beyond organizational boundaries (i.e., the public) must not be discounted. In sum, the present study takes a relational-based perspective to explore the leader narcissism-follower outcome relationship. Specifically, we test the moderating role of follower personality on the relationship between leader narcissism and follower engagement. We also examined leader gender to understand whether our obtained results operate differently for males and females. Our study is unique in that we tested our hypotheses in informal and leader-follower relationships on an online micro-blog platform (i.e., Twitter) using machine learning (ML) prediction models  to predict narcissism and Big Five personality traits. The information systems literature has found that text statements can be used to accurately predict the personality traits of individuals. Using the ML approach to estimate individual characteristics allowed us to test our hypotheses in a large sample of leaders and followers, including a multiverse analysis of the available data. However, before discussing our specific hypotheses and the ML algorithm, we provide an overview of existing narcissism and leadership literature. Narcissistic Leadership--A Relational Perspective Personality traits are dispositions that represent individuals' tendencies to think, feel, and behave in a particular manner, and individuals who score high on the personality trait narcissism are often referred to as narcissists. Narcissistic individuals often engage in self-enhancing and self-promoting behavior, which can result in initially being perceived as charming. However, these positive attributions tend to be short-lived, and longterm interactions with such individuals usually result in describing such individuals as untrustworthy and uncaring about others. These changed perceptions over time are due to narcissists being driven by their needs for power and admiration and being less concerned with the concerns of others or their organizations.2 The existing literature on narcissistic leadership has adopted a leader-centric perspective. This literature has documented that narcissistic leaders are overly concerned with their own reputation and interpret information in a biased and self-serving fashion. Such leaders manipulate conservations toward their own \\\"interests and accomplishments, and arrogantly fantasizing grandiose dreams\\\" . Consistent with evolutionary psychologists, narcissists tend to be identified as leaders by others. These positive consequences for narcissists have been documented in business organizations and at the national level. For example, Deluga (1997) found a positive relationship between narcissistic entitlement, self-sufficiency, and perceptions of the charisma of U.S. Presidents. Also, narcissists more often emerge as leaders in social contexts requiring agentic behavior  and in highly visible, more public tasks. Hence, narcissistic traits seem to be beneficial to leaders in certain contexts. A few studies have examined whether other leader characteristics (e.g., personality) might moderate the effectiveness of narcissistic leaders. For example, empirical findings suggest that narcissism is positively correlated with leader extraversion  and negatively correlated with leader agreeableness and conscientiousness. Unfortunately, prior research has focused on leader traits and contexts. The follower's role in narcissistic leadership has been ignored to date. As noted in their article on the toxic triangle, Padilla et al. (2007) argued that destructive leadership occurs when a particular set of leadership characteristics attract a particular kind of followers in a conducive or \\\"toxic\\\" environment. Our relational-based perspective suggests that follower characteristics are important for the success of these narcissistic leaders. The traits of some followers might make them susceptible and attracted to such leaders, and others less so. In the next section, we examine the Big Five personality characteristics and hypothesize how these personality factors for followers could moderate the leader narcissismfollower engagement relationship. Leader Narcissism and Follower Personality Follower openness. Individuals who score high on openness to experience tend to be curious, visionary, and imaginative. Given that narcissistic leaders usually exhibit great enthusiasm and express enticing visions of the future, it may appear at first that such followers would be drawn to narcissistic leaders. However, the literature shows that followers who score high on openness frequently have many interests and thus are likely to interact with many others. This attraction to and connection with multiple others allows followers to easily switch between leaders and possibly even interact with several leaders at once. Previous studies confirm that followers who are open to experience often express less commitment to their organization and teams. Hence, followers who score high on openness to experience might be less susceptible to narcissistic leaders than their counterparts who score low on openness to experience: Hypothesis 1 (H1): The leader narcissism-follower engagement relationship will be moderated by follower openness such that the magnitude of this relationship will be negatively related to followers' openness to experience. Follower agreeableness. Agreeable individuals tend to be trusting, cooperative, modest, compliant, and tender-minded. Highly agreeable followers are likely to avoid interpersonal conflict with others, experience higher levels of stress when there is a conflict , and can come across as \\\"too nice\\\" to others. These followers may be attracted to and influenced by narcissistic leaders because such leaders appear to be powerful and confident enough to handle future interpersonal conflicts. We, therefore, predict that agreeable followers are more likely to be drawn to and engaged with narcissistic leaders. We expect the attraction and engagement with narcissistic leaders will diminish as agreeableness decreases: Hypothesis 2 (H2): The leader narcissism-follower engagement relationship will be moderated by follower agreeableness such that the magnitude of this relationship will be positively related to follower agreeableness. Follower neuroticism. Neurotic individuals experience a high degree of anxiety and worry about the future and are more prone to impulsivity and vulnerability. However, as the majority of previous studies have focused almost exclusively on leader neuroticism , little is known about how follower neuroticism interacts with leader narcissism. One exception is a recent study by Gruda, Karanatsiou, et al. (2021) who found that narcissistic leaders seem to attract highly trait-anxious followers. They argue that traitanxious individuals tend to be more vigilant toward potential threats and therefore tend to worry more than individuals who score low on trait anxiety. Given that anxiety and neuroticism are closely related , we hypothesize that neurotic followers likely are drawn to narcissistic leaders because such leaders take control  and exhibit confidence , which reduces perceived environmental uncertainty and chaos : Hypothesis 3 (H3): The leader narcissism-follower engagement relationship will be moderated by follower neuroticism such that the magnitude of this relationship will be positively related to follower neuroticism. Follower extraversion and conscientiousness. Extraverts are assertive, excitement-seeking, and like to engage in social activities. Not surprisingly, extraverts are more effective and exhibit greater confidence in social interactions. Unfortunately, the lack of research on extraversion in followers makes it unclear whether such individuals would be more or less likely to engage with narcissistic leaders. Conscientious individuals are achievement-oriented, disciplined, responsible, and prefer orderliness. Such individuals tend to be less adaptable to change and might also be obsessive-compulsive. In traditional organizational settings, highly conscientious followers are likely to follow the instructions of their superiors and less likely to engage in uncivil behavior. Yet, it is questionable whether highly conscientious followers are more or less likely to engage in interactions with narcissistic leaders, precisely because of their affinity to follow rules and order. Due to the lack of existing research on follower extraversion and conscientiousness, hypotheses regarding these two follower personality traits acting as moderators of the leader narcissism-follower engagement relationship are more ambiguous. Thus, while we examine these possible interactions, we do so solely in an exploratory manner. Follower narcissism. The Big Five personality traits are not the only follower characteristics that can interact with leader narcissism to affect follower engagement. Followers can be narcissistic just like their leaders. The attraction similarity hypothesis predicts that similar people initially are more strongly attracted to one another and their relationships are more positive and longlasting. Empirical research has supported this hypothesis. Hence, it seems that at least initially, narcissistic followers seek out similar leaders in an attempt to find, relate to, and admire others for their status, \\\"power, beauty, intelligence or moral stature\\\" (;, p. 679). However, the long-term moderating effect of follower narcissism on the leader narcissism-follower engagement relationship has not been explored. We expect that over time any positive effects of narcissistic leaders will be short-lived with narcissistic followers as well. Therefore, we hypothesized, Hypothesis 4 (H4): The leader narcissism-follower engagement relationship will be moderated by follower narcissism such that the magnitude of this relationship will be negatively related to follower narcissism. The Moderating Role of Leader Gender Followers perceive female leaders differently from male leaders. Due to perceptual biases and the consequences of organizational glass-ceiling\\/ glass-labyrinth , females have to overcome more obstacles and challenges than males to be perceived as leaders. From the leader's perspective, when a female breaks through these barriers and becomes a successful leader, other barriers arise such as potential backlash from peers. Put differently, to be successful, female leaders must integrate their identity as a female with their identity as a leader. The extent to which one's gender and leader identities are perceived as compatible with each other is referred to as \\\"gender-leader identity integration\\\" (; p. 339). Leaders are expected to behave both agentically and determinedly. From the follower's perspective, expectations and positive attributions for agentic behavior are not extended to female leaders. While males can be considered a leader if they are only agentic, females must display both genderbased communal and leader-based agentic behavior to be accepted by their followers. We argue that narcissism might be beneficial to both females and males in terms of leader emergence. However, beneficial effects probably occur through different mechanisms. Male narcissists likely emerge as leaders due to their tendencies for self-promotion and high confidence , whereas female narcissists emerge as strong leaders due to their disregard for societal gender expectations. Although female narcissists benefit from their disregard for gender expectations, male narcissists are more likely to be perceived as stronger leaders because of the congruence of stereotypic male behavior and expected agentic behavior in leaders: Hypothesis 5 (H5): The leader narcissism-follower engagement relationship will be moderated by leader gender such that the magnitude of this relationship will be larger for males than for females. Although not explicitly hypothesized, we also tested whether leader gender moderated any of the previously hypothesized moderation of the leader narcissism-follower personality interactions. We do not make specific expectations about the direction of these three-way interactions; rather, we treated these analyses as exploratory. In general, however, we expect male narcissistic leaders to benefit from the positive tendencies of their narcissistic disposition while expecting that this relationship will be less evident in the case of female narcissistic leaders. Context: Online Environment As indicated earlier, destructive leadership emerges when there is a match between leader and follower traits in an environment conducive to unethical behavior. We expect that the attraction between followers and leaders is stronger in uncertain and chaotic contexts. In the current study, we study the leader-follower process in the online social media environment. The social media environment can be described as dynamic (i.e., updates continuously throughout the day), uncertain (e.g., personal attacks can be made against the original poster), and chaotic (e.g., the chain of logic might deteriorate into name-calling or a meaningful exchange of ideas). In other words, the social media environment is conducive to destructive leadership. In the present study, we examine data derived from the social media platform Twitter. Twitter has more than 330 million monthly active users (statista.com) and is the world's most popular microblogging platform. On average 6,000 tweets (up to 280-character text messages) per second or 500 million tweets per day are posted on the platform. Twitter users also can subscribe to other users' posts, who are known as followers. Twitter users' news feed captures a user's thoughts, feelings, and conversations at any moment in time as microblogs are quick, short, and mostly capture what is going on at any particular moment in users' lives. We recognize that interactions on Twitter between individuals, namely, \\\"leaders\\\" and their followers, may initially appear to be conceptually different than more traditional interactions between organizational leaders and their followers. We contend that when social media leadership is examined in more detail, the generalizability of social media leadership to more traditional settings becomes apparent for multiple reasons. First, we define leaders as individuals who are followed by others on Twitter. Just like informal leaders in traditional work environments, a regular employee (i.e., someone without managerial responsibilities and team oversight in their usual workplace) oftentimes can be perceived as a leader on social media platforms such as Twitter. Indeed, one could argue that similar to more traditional organizational leaders, Twitter leaders try to influence followers' communication content and patterns and can yield large referent and expert power. For example, Twitter leaders who are an expert in some field (e.g., a scientist, technician, craftsman, or journalist) and\\/or whose followers can identify with, often have a greater number of loyal online followers than those not considered an expert or likable. This might be the case, albeit these same leaders might not have managerial responsibility and supervisory oversight of others at their actual workplace. And similar to more traditional leaders, Twitter leaders are trying to influence followers' communication content and patterns. Second, the bases of power  used by traditional leaders and social media leaders are substantially the same. Although there is no formal authority structure that can be used to influence followers in the social media world, Twitter leaders can use both expert and referent power to influence their followers. Furthermore, these Twitter leaders can also use reward and coercive power. Twitter leaders can reward a follower by retweeting their comments or the leader can use coercive tactics by ridiculing or posting harsh criticism of the follower. Thus, although there is a lack of a formal structure providing legitimate power, social media leaders do indeed use many of the same bases of power as other more traditional organizational leaders. Third, the traditional literature repeatedly highlights that leaders influence their followers in several ways (Hanges et al., in press). Previous literature highlights the central role of leader communication and that literature views communication as a critical and fundamental component of leadership. Communication quality, quantity, and content (e.g., vision statements) are positively related to perceived leadership performance , overall team performance , and the effectiveness of transformational and charismatic leaders. Leader communication provides a frame of reference  due to activated schemas that facilitate followers' sense-making of their environment. Although essential for face-to-face teams, the importance of leaders' communication effectiveness is enhanced in a virtual context. And with the sudden and discontinuous changes to the work environment caused by the COVID-19 pandemic  highlighting the importance of leader-follower research in virtual environments, examining leadership in a social media context may provide the perfect laboratory for understanding this work in the future. Finally, studying leader-follower relationships in an online context removes several possible exogenous factors, such as switching costs, which might play a role in shaping followers' perceptions of their leaders. Prior research oftentimes equates followership with subordinates of a specific organizational leader. Various macro-economic factors, such as the state of the economy, industry growth, competition circumstances, can make it difficult for employees to switch leaders or organizations. Thus, employees can get \\\"stuck\\\" in undesirable employer-employee dynamics. These macroeconomic factors and individual-level factors (e.g., work experience and career growth) are contaminating factors that distort findings regarding the consequences of leader and follower characteristics. In online contexts, none of these factors play a role as switching costs for followers are virtually nonexistent. If followers do not agree with one leader or do not derive any benefit from following that person, they can easily \\\"unfollow\\\" their leader. Method Our sample is based on a data set of organizational employees and their company information provided by Crunchbase (crunchbase.com). Crunchbase is a platform that provides business information about private and public companies. Among data sets that include investments and funding, Crunchbase also provides a list of founding members, senior leaders, and organizational employees for each listed company. Many of the provided people profiles also include links to social media accounts, including Twitter. Our original data set, after excluding all non-U.S. companies and dropping profiles for which Twitter links were not available, consisted of 60,872 individuals and their full social media URLs and business information (e.g., company and job title). However, not all of these individuals are suitable for analysis given the parameters of this study. Hence, to avoid selection bias and ensure our examined sample was feasible for analysis, we implemented several selection criteria for both leaders and followers: a. Specific timeframe to measure engagement: To ensure that our analyses would not be affected by censored data, we retained individuals whose tweets were published within an examined time frame of January 1, 2018, to November 15, 2019. Given the pace of activity on Twitter, we reasoned that a sufficient amount of time had passed so that our dependent variable, follower engagement, would not be biased at the end date of our data collection efforts. b. Unique engagement measurement\\/inclusion in original posts: We removed any retweets to prevent double-counting of follower responses. c. Followers engaged in leaders' post identification: We gathered all the tweets from users that engaged with the leader within the aforementioned time period of this study. Follower engagement was defined as the number of unique followers' replies to a leader's tweet. To ensure that our follower engagement dependent variable was not overrepresented by relatively inactive followers, we excluded followers (and their respective leaders) who had engaged with the leader fewer than 4 times. We counted the number of follower replies, rather than follower likes or retweets of leader posts because replying to a tweet requires an expenditure of effort and provides information about the nature of the follower's communication intention. This led to a preliminary data set of 917 leaders and 70,523 followers. Throughout the remainder of this article, we refer to this data set as our base data set. d. Spam removal and behavioral criteria refinements: Using the base data set, we applied various metrics to lower the likelihood of bot-like accounts in our data set. We selected only those leaders who posted a minimum of 100 tweets and who had at least 30 unique followers within our data set. Doing so reduced the leader count to 914 leaders. The minimum criterion of 100 tweets with 30 follower observations allowed us to identify leaders with sufficient data to meaningfully test our hypotheses. As leaders in our data set were derived from a prescreened database, we were confident that all included leaders were real users and not bots. However, to ensure we had not accidentally included bots (or botlike accounts) in our examined follower sample, we also only included followers in our final data set who had posted a minimum of 100 tweets (i.e., 69,490 followers), had at least one friend (i.e., two users follow each other; 69,311 followers), had at least 30 followers themselves (i.e., 65,545 followers), and were included on at least one Twitter list (58,877 followers): Twitter Lists display tweets from a curated list of Twitter accounts, allowing users to customize, organize and prioritize the Tweets they see on their timeline. Twitter Lists can be shared, subscribed to and created based on topics of interests, specific events or groups of people (e.g., inspiring leaders). Based on our definition of leaders as \\\"individuals who are followed by others on Twitter,\\\" we considered leadership to be an influence over others, regardless of the status and authority or power the leader holds over their followers. Having said that, most leaders in our data set (e.g., Model 1) do indeed hold traditional leadership positions in organizations. For example, approx. 41.19% of selected leaders in our data set are executive leaders (CEO, CFO, Founder, etc.), 11.96% are senior leaders (Managing Directors, Executive Vice Presidents, Senior Directors, etc.), and 11.27% hold a managerial position (Senior Manager, Vice Presidents, Marketing Manager, etc.). Therefore, in total, 64.42% of selected leaders in our data set indeed hold leadership positions within their respective organizations. As such, this data set largely (but not solely, see our definition of leaders in this article) constitutes organizational leaders who are using their social media to reach beyond organizational boundaries and influence others. With regard to our dependent variable, while we agree that in principle looking at alternative metrics such as follower engagement is reasonable, we decided not to rely on variables such as retweets or likes, but rather chose to focus on follower engagement (i.e., the number of follower replies to a leader's tweet) for several reasons. First, alternative metrics such as retweets and likes are easily manipulatable by bots. Therefore, such measures oftentimes can provide a false impression of engagement. Hence, using such metrics would not likely provide valid results. Second, assuming such metrics of a leader are solely provided by human followers (not bots), both retweets and likes constitute low-effort variables. Hence, for a follower to read a leader's tweet and hit the like button hardly constitutes a form of engagement given the ease and speed with which likes or retweets can be conducted. And because such metrics do not require reflection on the followers' part, we would hesitate to use such metrics as an equivalent to follower engagement. On the contrary, commenting on a leader's tweet, even when indicating disagreement with the content of the leader's tweet or the leader themselves, constitutes a form of effort and some degree of reflection. Hence, we argue that followers' replies to leaders' tweets are a high effort variable and minimize the noise in our models when compared with other metrics. Third, the growing literature on followership has recognized that not all followers are the same. Indeed, several typologies have been proposed  to account for different types of followers. Of these different typologies, Kelley's (1988, 2008) framework is the most widely cited and used. Kelley identifies five different types of followers that can be differentiated by two underlying dimensions (active\\/passive; independent, critical thought\\/dependent, uncritical). The two types of followers who are more active are Conformist (those who are engaged followers who do what the leader wants) and Exemplary (i.e., followers who are engaged and in-group but will raise issues and argue with the leader if they disagree). Note that Kelley labels such followers \\\"Exemplary,\\\" which describes followers who are not afraid to call out the leader when necessary. Follower engagement reflects all types of followers and hence does not limit the scope of a leader's influence to one specific audience (or type of followers). It should be noted that we considered followers to be any Twitter users who interacted with the selected leaders, regardless of whether these users actually \\\"followed\\\" the leader's Twitter account. For example, a CEO's followers were solely determined based on which users interacted with the CEO on Twitter, regardless of the follower status or whether they belonged to the same organization as the leader. Hence, not all users who engaged with a leader on Twitter were necessarily followers of that leader on Twitter. A Twitter user might, for example, come across a post by a leader because the respective post has \\\"gone viral.\\\" Yet, most of the time, an engaged user (i.e., a user who interacts with a leader on Twitter) would also be a Twitter follower of that leader. Indeed, based on a random subset of our base sample, we found that 78.3% of engaged users in our data set were also Twitter followers of a particular leader account. In other words, most of our users were actively engaged with their leaders' tweets and therefore could accurately be considered followers and not bots or random users. To this data set of leaders and their corresponding followers, we applied pretrained ML models to annotate accounts with narcissism and personality scores. Multiverse Analysis and Data Preprocessing We recognized that the decisions we had to make to clean and preprocess our base data set might affect our results. There was no clear answer with regard to the right cut-offs to use and that introduced the possibility of researcher degrees of freedom as well as a challenge to selecting one path from a series of plausible steps. To provide transparency regarding the consequences of our preprocessing decisions on our results, we conducted and presented a multiverse analysis, as suggested by Steegen et al. (2016). A multiverse analysis refers to creating a new data set for each possible combination of plausible processing steps and rerunning our multilevel model for each sub-dataset. Conducting a multiverse analysis allowed us to determine and document which results are robust across preprocessing options. In our case, the respective preprocessing choices included the (a) number of interactions between leaders and followers (i.e., number of leader-follower engagements per leader), (b) inclusion and exclusion of more popular leaders from our data set, and, finally, (c) ratio between friends count and follower count (friend count\\/follower count2;). We provide more information regarding these options below: a. The number of leader-follower interactions: As specified earlier, the minimum number of follower engagements with their leaders was set at four, to ensure that our follower engagement dependent variable was not overrepresented by relatively inactive followers. In addition, to make broader generalizations and conclusions, we also investigated whether results remained robust in data sets with a higher minimum number of interactions. Previous research has shown that, initially, narcissistic leaders can come across as charismatic, confident, and even likable. However, over time, as followers get to know their narcissistic leaders better, such leaders no longer are seen as positively as they once were. Hence, it is likely that the number of interactions between leaders and followers can influence the robustness of our expected results, as outlined in our hypotheses. Therefore, in our multiverse analysis, we examined results based on a minimum number of four, five, and six leader-follower interactions. The number of chosen leader-follower interactions (i.e., 4, 5, and 6) was primarily due to data limitations. For example, while our data set includes a minimum of four interactions (i.e., baseline), focusing on five interactions results in a drop of 33.61% of available data in our data set, and focusing on six interactions results in a drop of 48.89% of available data. Specifying a higher minimum, for example, 10 would result in a drop of over 75.88% of available data overall. Given the large associated drop in data to facilitate such analyses, we decided against this step. Instead, we show that the outlined number of interactions yields useful and interpretable insights into the role of leader and follower personality traits when the number of leader-follower interactions increases. b. Inclusion and exclusion of more popular leaders: Another factor that might influence our results is the popularity of the leaders included in our database. Recent research examining leader-follower interactions on social media, for example, found that leader popularity changed the expected effects that narcissistic leaders have on highly anxious followers. Put differently, although highly anxious followers tend to interact more often with narcissistic leaders, this effect is mitigated by the degree of leader popularity. Followers might interact with highly popular leaders regardless of whether such leaders are highly narcissistic. It is therefore reasonable that leader popularity might be an important factor to consider in the present study as well. Therefore, we created three types of sub-datasets. For each data set type, we dropped (a) the top 5% of all leader-fol lower interactions resulting in a data set in which only highly popular leaders were removed, (b) top 10% of all leader-follower interactions resulting in a data set in which highly popular and somewhat popular leaders are removed, or (c) top 15% of all leader-follower interactions resulting in a data set that only includes less popular leaders. Doing so allows us to interpret results based on the degree of leader popularity. c. The ratio between followers' friends count and follower count: We also calculated the ratio between friends count and follower count (friend count\\/follower count2;). This ratio is the most important indicator of fake Twitter followers (Table 18;) and has been validated in previous research. A high friend-to-follower count ratio is indicative of bot-like behavior. Unfortunately, the exact cutoff to use for this ratio is unclear. We, therefore, examined our results based on two conditions, namely, (a) exclude followers with a ratio of either higher than 0.5 or (b) exclude followers with a ratio of higher than 1. We also tested the validity of using this ratio to detect bots by annotating a random sample of 1,000 Twitter followers on botometer (https:\\/\\/botometer.osome.iu.edu\\/api). Based on our random sample of 1,000 profiles, zero profiles were categorized as bot-like after our friends to follower count ratio cut-offs were implemented. The results of our multiverse analyses will be interpreted in a fashion consistent with a random-effects model. In a fixed-effects model, all multiverses would be assumed to be drawn by the same population. However, in a random-effects model, the various multiverses are assumed to be drawn from different populations. We choose to interpret the multiverse analyses as a randomeffects model because we do believe that the aforementioned preprocessing decisions can affect the magnitude of the relationships. We can declare support for the hypothesized relationship if the coefficient in question has the same sign across all the multiverses and there is evidence that the majority of the multiverse coefficients are significant. Results For purposes of our study, follower engagement was defined as the number of interactions between leaders and followers, that is, the number of follower replies per leader post. We used the Big Five personality traits and narcissism of both the leader and their followers to predict follower engagement. As followers are nested within leader groups and follower engagement was a count variable, we tested our hypotheses using multilevel mixed-effects Poisson regression. We confirm that for all multiverses, we have reported all measures, conditions, data exclusions, and we have explained how we determined the respective sample sizes. In addition, we follow the guidelines provided by Maas and Hox (2004), who stated that standard errors typically are estimated too small only in the case of fewer than 50 groups, with less than 30 observations per group. Given the parameters of all our data sets, we use normally distributed (standard) errors, specifying a random-effects model for all examined interactions. An overview of the multiverse analysis regarding our main hypotheses is provided in Figure 1, while all two-way interactions across all multiverses are graphed in Figure 2 with model coefficients provided in Table 1. Multiverse analysis results by leader gender are provided in Figure 3, while respective three-way interactions are shown in Figure 4. Leader Narcissism and Follower Personality Traits H1 stated that the leader narcissism-follower engagement relationship will be moderated by follower openness. Specifically, the magnitude of this relationship will be negatively related to followers' openness to experience. As can be seen in Graph A of Figure 1 and consistent with H1, the interaction was negative for all of the 18 multiverses. Except for two models (i.e., Models 9 and 12), the confidence intervals (CIs) for the multiverses did not include zero. Given that the coefficients for all of the multiverses were in the predicted direction and that these coefficients were significantly different from zero for 16 of the 18 (88.9%) multiverses, H1 was supported. We plotted this interaction using the results from Multiverse 16 and this interaction is shown in Graph A of Figure 2. Followers who were open to experience were less engaged as leader narcissism increased. Followers who were less open to experience were more engaged as leader narcissism increased. Interestingly, both multiverses whose CIs included zero included very popular leaders (i.e., the data in both of these multiverses only removed the top 5% of leaders). Overall, H1 was supported but the strength of this interaction was stronger when the most popular leaders were excluded from the analyses. H2 predicted that the leader narcissism-follower engagement relationship will be moderated by follower agreeableness. Specifically, the magnitude of this relationship will be positively related to follower agreeableness. Consistent with this hypothesis, Graph B of Figure 1 shows that the interaction coefficients were positive and significant for all multiverses. We plotted this interaction for Multiverse 16 and this plot is shown in Graph B of Figure 2. Agreeable followers were more engaged as leader narcissism increased, whereas less agreeable followers were less engaged as leader narcissism increased. In summary, H2 was supported across all 18 multiverses. H3 predicted that follower neuroticism will moderate the leader narcissism-follower engagement relationship. Specifically, the magnitude of this relationship will be positively related to follower neuroticism. As seen in Graph C of Figure 1, the interaction coefficients were positive and significant for all multiverses. We plotted this interaction for Multiverse 16 (shown in Graph C of Figure 2). There was a positive relationship between leader narcissism and follower engagement for highly neurotic followers. Specifically, highly neurotic followers were more engaged as leader narcissism increased. Followers who were less neurotic became less engaged as leader narcissism increased. Thus, H3 was supported. H4 predicted that the relationship between leader narcissism and follower engagement would be moderated by follower narcissism. Specifically, the magnitude of this relationship will be negatively related to follower narcissism. As shown in Graph D of Figure 1, the interaction coefficients were negative for all multiverses but significant in 12 of the 18 multiverses (66.7%). Although there was weaker support for this hypothesis compared with the others, we decided to plot it anyway using Multiverse 16. As seen in Graph D of Figure 2, followers lower in narcissism became more engaged as leader narcissism increased. There did not appear to be a relationship between leader narcissism and follower engagement for followers higher in narcissism. Thus, there is weak support for H4. Although we did not state specific hypotheses for the moderating effect of either follower conscientiousness or follower extraversion on the leader narcissism-follower engagement relationship, we nonetheless visualize these results across the 18 multiverses to provide a holistic overview of our results. Concerning the results for follower conscientiousness (Graph E of Figure 1), we found that the coefficients were negative and significant for all 18 multiverses. Graph E of Figure 2 shows the interaction for Multiverse 16. The positive relationship between leader narcissism and follower engagement decreased and became negative as follower conscientiousness increased. Finally, Graph F of Figure 1 shows the results regarding the moderating effect of follower extraversion. Negative interaction coefficients were found for all of the multiverses, but this interaction was nonsignificant in 12 of the 18 multiverses (66.7%). Based on the number of multiverses with nonsignificant interactions, we conclude that we do not find support for a moderating effect of follower extraversion. Given this conclusion, we refrained from graphing this interaction. The Moderating Role of Gender Finally, H5 predicted that there will be a three-way interaction on follower engagement as a function of leader gender, leader narcissism, and follower personality characteristics, given the documented stereotype about females and leadership ability. Specifically, we predicted that the magnitude of this relationship will be larger for males than for females. An overview of the multiverse results of the various three-way interactions is provided in Figure 3 with the coefficients for all the multiverses provided in Table 2. Figure 4 shows the nature of the three-way interactions. Figure 3 shows that, across all the multiverses, the interactions between leader gender, leader narcissism, and follower (a) openness, (b) agreeableness, and (c) narcissism were robust across all multiverses. That is, the three-way interactions were significantly different from zero and the sign of the interaction's coefficients was all in the same direction for all of the 18 multiverses. The three-way interaction between leader gender, leader narcissism, and openness is shown in Graph A of Figure 3. As shown in this graph, the interaction's coefficients were positive for all multiverses. We plotted the three-way interaction for Multiverse 10 and this is shown in Graph A of Figure 4. In contrast to H5, the three-way interaction was evident for female as opposed to male leaders. For female leaders, high openness followers were less likely to engage as leader narcissism increased. However, for followers low in openness, engagement increased as the narcissism of female leaders increased. There was no apparent interaction for male leaders. Consistent with H5, the three-way interaction between leader narcissism, follower agreeableness, and leader gender was significant across all multiverses and was all positive (see Graph B in Figure 3). We plotted this three-way interaction for Multiverse 12. As shown in Graph B in Figure 4, highly agreeable followers were more likely to engage as narcissism increased for male leaders. Followers lower in agreeableness were less likely to engage as narcissism increased for male leaders. There was no apparent interaction for female leaders. Similarly, the results for the three-way interaction leader gender, leader narcissism, and follower neuroticism were consistent with H5. As shown in Graph C of Figure 3, the three-way interactions were positive for all multiverses and were significant for 16 of the 18 (88.9%) multiverses. Graph C in Figure 4 shows this three-way interaction for Multiverse 14. Highly neurotic followers were more likely to engage as narcissism increased for male leaders. Less neurotic followers were less likely to engage as narcissism increased for male leaders. There was no apparent interaction for female leaders. Finally, the three-way interaction between leader gender, leader narcissism, and follower narcissism did not support H5. As shown in Graph D of Figure 3, the interactions were significant and negative for all the multiverses. We plotted this interaction using Multiverse 4 and this plot is shown in Graph D of Figure 4. For female leaders, the more narcissistic followers were engaged as narcissism in the leader increased. No apparent relationship between leader narcissism and follower engagement was found for female leaders and less narcissistic followers. Interestingly, this pattern reversed for male leaders. For male leaders, the less narcissistic followers were more engaged as narcissism in the leader increased. No such pattern existed for narcissistic followers with male leaders. Finally, Graph E of Figure 3 shows the three-way interaction with follower conscientiousness and Graph F of Figure 3 shows the three-way interaction with follower extraversion. As can be seen in these graphs, the three-way interaction coefficients were both positive and negative across the 18 multiverses. In addition, the interaction was significant in only eight multiverses in the case of follower extraversion. Given the lack of consistency for the direction of these threeway interactions, we concluded that leader gender did not interact with these follower personality characteristics. Therefore, we did not plot these three-way interactions. Overall, on one hand, it seems that while leader gender was important to interpret our results, H5 stated that the interaction pattern would be stronger for male leaders as opposed to female leaders. This predicted interaction pattern was found for follower agreeableness and neuroticism. For these follower attributions, larger changes in follower engagement were found for male as opposed to female leaders. On the other hand, in the case of follower openness and follower narcissism, larger changes in follower engagement were found for female as opposed to male leaders. We discuss these results in the following section. Discussion Although research interest in the dark triad traits, and in particular, leader narcissism, has been on the rise over the past few years , prior literature has predominantly discussed leader narcissism from a leader-centric perspective. Although it is important in understanding how leader characteristics shape leader-follower relationships, the follower's perspective of leader narcissism should not be ignored. Understanding which followers are more likely to be drawn to narcissistic leaders is vital in gaining a holistic overview of this topic  and in improving leader-follower relationships, team formation, as well as recruitment and selection of both leaders and followers. Examining the interaction between follower personality traits and leader narcissism using a large sample of observations and interactions on the Twitter platform, on the leader as well as the follower level, is a step in this direction. One major contribution of this study is that we performed multiverse analyses3 to understand the robustness of our results across the various preprocessing decisions that were made to the base data set. Our multiverse analyses examined the extent to which our conclusions were limited by the number of leader-follower interactions as well as various degrees of leader popularity. For example, it is possible that in the short term, narcissistic followers interact more with highly narcissistic leaders compared with less narcissistic leaders. Therefore, we would expect the frequency of interactions to drop after these initial exchanges, as followers realize the degree of narcissism exhibited by the leader. The presented multiverse analysis provided greater clarity in this regard, as we showcased results for multiple leader-follower interaction conditions, namely, four, five, and six interactions. We found that regardless of the minimum number of leader-follower interactions, followers who score high on agreeableness and neuroticism were more likely to engage with narcissistic leaders, whereas followers who score low on openness to experience interacted less often with narcissistic leaders, compared with non-narcissistic leaders. Hence, it seems that, regardless of the number of interactions, followers who are dispositioned to be agreeable or neurotic are likely prone to be more susceptible to narcissistic leaders, likely due to these leaders' charismatic charm and confident behavior. These findings provide support for our hypotheses (H1-H3). In addition, the presented multiverse analysis also allows us to account for varying degrees of leader popularity. Previous studies  found that the relationship between leader narcissism and follower traits, namely, trait anxiety, differs based on observed leader popularity. By restricting our data set in one of three ways (i.e., by excluding the top 5%, 10%, or 15% of all follower observations), we effectively created multiple data sets that include very popular leaders (i.e., leaders with a lot of followers), popular leaders, and less popular leaders. These alternate paths allow us to draw conclusions that would not have been possible beforehand. For example, we found more limited support for the relationship between leader narcissism and follower narcissism (H4). In the case of a low number of leader-follower interactions (Figure 2, Models 1-6), it seems that non-narcissistic followers seem to be more likely to engage with narcissistic leaders. However, as the number of interactions increases (Figure 2, Models 7-8 and 10-11), this effect weakens. Likewise, it seems that leader popularity also plays a role, as the strongest effect was observed in the most inclusive models (i.e., models that only exclude the top 5% of all observations). It might be the case that as leaders gain more popularity, non-narcissistic followers are more likely to follow and engage with them, regardless of the observed degree of leader narcissism. Finally, although not initially hypothesized, we found a significant and negative interaction between leader narcissism and follower conscientiousness (Figure 2). Hence, it seems that less conscientious followers interact more often with narcissistic leaders, compared with highly conscientious followers. And although the observed model coefficients all differed from zero and the effect was unidirectional (and negative), this particular relationship seems to be largely influenced by both the number of leader-follower interactions and leader popularity. Put differently, as the number of leader-follower interactions increases, the less likely highly conscientious followers are to engage with narcissistic leaders. Likewise, the largest effects were observed in models that were based on less popular leaders (i.e., models excluding the top 15% of all observations). Hence, it seems that as leaders gain more popularity, conscious followers are more likely to engage with them regardless of the observed degree of leader narcissism. The Moderating Role of Leader Gender We also found a significant interaction between leader gender, leader narcissism, and follower personality traits, namely, openness, agreeableness and neuroticism, and narcissism. First, concerning followers who scored high on openness to experience, we observed a significant effect in the case of female leaders. It seems that followers who score low on openness to experience tend to interact more often with narcissistic female leaders compared with non-narcissistic female leaders. The same effect does not seem to be the case in male leaders. Second, highly agreeable or neurotic followers seemed to interact more often with narcissistic male leaders compared with non-narcissistic male leaders. Hence, for both of these followers' personality traits, significant effects were mostly only in the case of male leaders. This provides evidence that highly agreeable or neurotic followers are more likely to endorse and interact with male narcissistic leaders, as hypothesized (H4). Finally, narcissistic followers seemed to interact more often with female narcissistic leaders, whereas non-narcissistic followers were more likely to interact with male narcissistic leaders. These results are quite interesting, as they highlight the importance of possible gender moderation effects in leader-follower relationships. However, it is important to keep in mind that we did not account for follower gender differences given that Twitter does not allow the direct collection of user demographic information. It might be that follower gender and leader-follower gender congruence influence the perception of (male and female) narcissistic leaders differently. This would be an important point to examine in future studies. For example, Thoroughgood et al. (2011) found that followers' perceptions of their leaders depend heavily on the corporate climate and financial performance, as well as their leaders' gender. Female leaders who broke the rules were perceived more negatively than male leaders, but only in companies intolerant of aversive leadership and companies that were exhibiting negative organizational performance. Hence, these two factors seem to be not simply distinct moderating effects, but rather a three-way interaction between gender and industry might moderate followers' perceptions of their leader. It might be interesting to examine this threeway interaction further  in subsequent studies. Implications There is growing interest in followership. Unfortunately, most of this literature is primarily focuses on building taxonomies of different styles of followers. The different followership models may provide some suggestive hints about the underlying mechanisms that account for different follower behavior. For example, it has been suggested that the style of followership may be determined by followers' engagement level , followers' courage , or the situational context followers are placed into. The focus of these followership models always is on classifying different follower behavior and not on the underlying mechanisms that might explain these behaviors. In contrast to these followership models, the present study focused on one mechanism (i.e., personality and narcissism levels of leader and follower) and empirically tested whether the interaction of these characteristics can account for follower behavior (i.e., engagement). We found support for the interaction between leader characteristics (i.e., narcissism and gender) and follower characteristics (i.e., Big Five personality and narcissism) in understanding an important behavior in the leader-follower relationship (i.e., follower engagement). Of course, this is only one mechanism and it is unlikely that the range of the identified follower behavioral categories can be adequately explained by this single mechanism. Other mechanisms need to be explored, such as leaders' and followers' needs, communication styles, and attachment orientations  to more completely understand why different followership styles emerge. Another implication is that the present study demonstrated the utility in expanding the context used to study leadership. We identified the overlap in influence strategies used by leaders in brick-and-mortar organizations and virtual contexts. While researchers may have originally questioned whether this study would generalize to more traditional organizations, an equally valid question is whether the prior leadership and followership literature are generalizable to the virtual workplace and interactions. Many organizations survived the COVID-19 pandemic by turning to social media and virtual environments to continue their businesses. The shift in the business model caused by the pandemic not only affected organizations but also affected workers. As a result of the pandemic and the reliance on virtual workplaces, a migration of the population has occurred in the United States due to workers discovering that they could successfully maintain their job even if they move far away from their work. It is still too early to know the long-term implications of this transformation of the workforce, but as of this writing, there is still a shortage of workers due to the resistance of people to go back to less than desirable jobs. Finally, the political upheaval that occurred in the United States in 2020 and 2021 was a result of certain individuals using social media to lead masses of followers to either take actions (e.g., January 6th attack on the U.S. Congress; BlackLives-Matter movement) or not take actions (e.g., refusing COVID-19 vaccine). The power of leadership in a virtual environment has been documented by the actual political and social events that occurred in the recent past. This study has documented that virtual leadership can be meaningfully studied. Conclusion We provide a follower-centric view of leader narcissism based on online interactions between leaders and followers on the Twitter platform using an ML and multiverse analytical approach. We find that followers who score high on certain personality traits, including agreeableness and neuroticism, are most likely to engage with narcissistic leaders. We also examine and present results of the moderation effects of leader gender.\",\"577240265\":\"Today, online services facilitate many of our daily activities, from social interaction (Facebook, Twitter, Snapchat) to information retrieval (Google, Bing, Yahoo) to content consumption (YouTube, Netflix, Spotify). The digital nature of these interactions allows data to be logged precisely and at a scale not possible in previous decades. Companies collect and analyze these data to fuel the services they provide: Google uses user-level search history data to optimize search rankings , Netflix leverages user feedback to personalize content recommendations , and Twitter can use feedback to provide personalized news recommendations. This rich individual-level information provides social scientists with unprecedented opportunities to advance our understanding of human behavior. In this article, we leverage data from Spotify to provide the first ecologically valid behavioral evidence that musical preferences and habitual listening behavior are linked to personality traits. Recent social psychological, personality, and computational research has developed a basis for successfully predicting human characteristics from digital records (i.e., \\\"digital footprints\\\"). For example, personality traits are accurately predicted by Facebook likes, and computer-generated models have more predictive accuracy than humans. Personality is also predicted from language use in Facebook status updates , Twitter posts , and more subtle behaviors such as patterns of keyboard and mouse use. In this article, we use a similar approach using digital records to address theory and research into musical preferences and habitual listening behavior. Contemporary research on musical preferences has applied interactionist theories to music , positing that people select musical environments that reflect their psychological traits and needs. Prior research has found converging support for this theory when applied to musical preferences, finding that across multiple methods, samples, and geographic regions, personality is correlated with preferences for features, genres, and styles. However, a recent meta-analysis on the personality correlates of musical preferences concluded that the effects of personality are small and \\\" . . . barely account for individual differences in musical preferences\\\" . The studies in their meta-analysis though had predominantly used self-report methods and basic statistical analyses which are both profound limitations. To help move beyond these limitations, Nave et al. (2018) examined the link between musical preferences and personality using a stimuli-based measure and an artist-specific measure derived from Facebook likes of musical artists. They then applied out-of-sample techniques and least absolute shrinkage and selection operator (lasso) regression to find that preferences accurately predicted personality. However, this study too had methodological limitations. The stimuli-based approach that measured affective reactions to musical excerpts and measuring Facebook likes are artificial in the sense that they do not measure actual listening behavior. Although they serve as robust behavioral methods, they do not capture the music that a person listens to day-in and day-out. Therefore, the literature is in need of a study that observes musical preferences \\\"in the wild.\\\" Furthermore, stimuli-based methods and digital behaviors do not capture listening behaviors beyond musical preferences, including habitual listening behaviors. For example, the extent to which someone repeatedly plays a song, listens to new music over old music, and temporal aspects of music listening. Indeed, prior research has explored aspects of musical use (e.g., background listening vs. a primary activity) and its links to personality , but these studies too have been limited by self-report methods and small samples. Therefore, needed in the literature are habitual aspects of music listening captured in the real world. Such observations, which require large amounts of data over time with many people, can now be measured using music streaming data. Music streaming data pose at least six advantages over other types of digital records from human behavior. First, listening to music involves a significant amount of individual choice (including not just what type of music but when, where, and how to listen) and therefore has the potential to capture subtle information about personal preferences and attitudes. Second, people listen to music across a wide range of situations--socializing, exercising, sleeping--which captures a more complete picture of a person's daily activities and routines. Third, music induces and communicates emotions, evokes autobiographical memories, affects people's moods, and activates brain regions linked to emotion and creativity , providing a large window into a person's emotional life. Fourth, compared to data sources that involve circumstantial and occasional activity, listening to music spans much longer timescales, sometimes extending over an individual's entire daily activity , potentially capturing more stable behaviors and traits. Fifth, prior big data studies on personality have often relied on data that may suffer from social desirability biases (e.g., liked Facebook pages and Twitter activity). Since a user's full listening history is never shared publicly and Spotify includes an \\\"incognito mode\\\" which prevents others from knowing what a user is currently playing, music streaming data do not suffer from the same biases. Sixth, prior studies used behavioral data generated by participants in an intermittent and inactive way. For example, most Twitter users are active less than once per day  and visit it for the explicit intent of engaging with social media. Whereas music crosses contextual boundaries including passive engagement and spans longer time scales, so while the signal may be less clear, it should represent a broader picture of a listener's personality. For these reasons, music streaming data provide a unique lens in which to observe and understand the complexity of human individuality. Music streaming services can provide moment-to-moment data on the music people listen to, enabling researchers, for the first time, to define musical taste through the accumulation of everyday listening. In this article, we describe the nuances of listening behavior using an extensive set of 211 mood, genre, demographic, and behavioral variables. This more expansive and ecological representation of taste builds upon prior theory and research on music feature preferences  and extends it to habitual listening behaviors. Based on interactionist theories, we hypothesize that real-life musical choices and patterns of interaction with the Spotify music streaming service will be linked to personality traits. Since this study is descriptive, we do not have an explicit blueprint of hypotheses; however, we do have general expectations of the patterns between personality traits and music listening behaviors. For example, consistent with previous research, we hypothesize that those who discover more new music or listen to more varied music will score higher on Openness , those who listen to more aggressive music will score lower on Agreeableness. Method Participants and Procedure All participants were registered Spotify users in the United States. To qualify for the survey, participants had to be 1875 years old and active on the app in the 30 days preceding data collection. Each participant provided informed consent prior to participating in the study, including the option to remove their data from the study prior to publication. Users with no variance in their survey responses or with no streaming in the 30 days prior to data collection were removed, leaving a final sample composed of 5,808 participants (54% male; median age 1\\/4 26, min age 1\\/4 18, max age 1\\/4 75). The selected participants had listened to a combined count of over 17.6 million streams over a 3-month time period, where a stream is defined as a song that was played on Spotify for at least 30 s. This study was given ethical approval by the internal review board (IRB) at Spotify. Features As input to our model, we gathered a variety of data related to users' demography (age and gender), musical taste, and in-app tendencies over a 3-month period from November 2017 to February 2018. Genre and mood vectors. In prior literature examining the connection between music and personality, a person's musical preferences were derived from self-reported information based on a small set of genres, styles, or stimuli. For this analysis, we took users' listening data and mapped it to genre and mood vectors (see Figure 1). Genres are produced internally by Spotify through data curators labeling clusters of artists via a machine-assisted approach that takes into account aggregate listening, acoustic properties, and cultural knowledge. Gracenote, a third-party music metadata service, infers a song's mood by processing the audio signal into features (e.g., rhythm and harmony), then passing these features into a supervised classifier trained to predict the mood from a predefined taxonomy. For each user, their streams were mapped to 66 Spotify genres and 25 Gracenote moods then aggregated and normalized to get the percentage of listening from each genre or mood. Derived metrics. We computed a suite of derived metrics--features constructed using Spotify's data--which quantify aspects of individual behavior on the platform. These included simple aggregations of the user's data, such as the types of platforms used (e.g., Mac, speaker, game console), the total playtime, and the number of playlists created. We also included more complex computations: measures of the diversity of one's musical preferences, their preference for discovering new music, regularity of listening habits, and tendency to listen to music representative of their formative years. Additionally, the audio of the songs listened to over the 3-month period were mapped to a suite of features determined from a supervised learning approach, which served to identify the acoustic profile of each user's listening history. In total, 123 of these features were computed and used in the model. Calculations and definitions of these metrics including discovery, diversity, contextual listening, and audio attributes are detailed in the Supplemental Material (including Table S1). Product flag. Spotify offers two product versions: Free and Premium. The Free product is available without charge but comes with some feature restrictions. Free users are served advertisements between songs and cannot play tracks on demand when using the mobile app. In addition, Free product users have a limited number of skips per hour and are unable to download music for off-line listening. The Premium product requires a subscription and removes these limitations, giving users unrestricted access. These differences between the products have a significant impact on how users engage with the application. For this reason, we added an additional feature to indicate which product the user was on during the 3-month period. In our sample, there were 2,974 users of the Free product and 2,834 users of the Premium product. Measuring Personality Participants completed the Big Five Inventory (BFI: John & Srivastava, 1999), a 44-item questionnaire measuring the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Emotional Stability. Each trait is computed by averaging the responses to 8-10 Likert-style questions on a 5-point scale. The BFI has been well tested, and we return Cronbach's as in excess of.73 for each trait (Openness 1\\/4 .73, Conscientiousness 1\\/4 .80, Extraversion 1\\/4 .85, Agreeableness 1\\/4 .77, Emotional Stability 1\\/4 .84). Model Selection Given that predictors in our model have varying distributions, we transformed the numerical values of each predictor to achieve a more standardized distribution. The standardization technique was chosen for each feature based on its distribution. For example, while a log transformation was used for the number of plays in the last 3 months as it spans orders of magnitude, the percentage of streams of Jazz music was transformed using a logit transformation. Overfitting was a concern when choosing a model, given our sample size and number of predictors (211, including all genre, mood, demographic, and derived metrics). A common approach to avoid overfitting is to use regularized methods such as lasso and ridge regression. Lasso regression is known for its interpretability and bias toward lower dimensional models  and has been used in previous studies on personality and music. Ridge regression  uses an alternative that shows better performance when there are high correlations between predictors  but does not allow predictors to drop out of the model. Neither model outperforms the other in all situations. Instead, we chose elastic net regularization regression, which linearly combines2 both techniques to address concerns of data sets with overly broad feature sets while outperforming either technique individually. To account for possible nonlinearities between predictors--if personality is expressed differently between Free and Premium, for example--we also used random forest regression for completeness. Both models' hyperparameters were decided based on the minimization of mean of the root-mean-square error (RMSE) using grid search. Although the nonlinear models occasionally outperformed the linear models, the overall performance was similar, so we report the best performance among the two models. We constructed all regression models using the same set of metrics (see Figure 2) to predict the numerical values of each of the Big Five personality traits independently. For each model, we performed 10-fold cross-validation to test the out-of-sample accuracy of the model; all variable selection and parameter tuning happens within each training set independently. Any model improvement observed when variables are selected prior to cross-validation would represent an unrealistic estimate of true out-of-sample prediction accuracy. Following similar papers that predicted personality , we measured prediction accuracy by computing the Pearson correlation between the predicted values and the measured personality traits for the test group of each fold. We report the average correlations across the 10 folds. Results Prediction Mean of the RMSE from 10-fold cross-validation showed moderate to high prediction for each of the Big Five personality traits: .811 for Extraversion, .777 for Emotional Stability, .621 for Agreeableness, .618 for Conscientiousness, and.530 for Openness. Independent regressions were then performed for each trait. Table 1 summarizes our prediction results (rs range from.262 to.374). These results are greater in magnitude than those found in previous research by Nave et al. (2018) that use stimuli-based methods and Facebook likes to assess musical preferences. That our results yielded higher correlations is not surprising since we included metrics that assessed not only musical preferences but also habitual listening behaviors. Our regression results do outperform univariate correlates, but not substantially. This is due to the cross-validation technique we use in our study. It is important to stress that these prediction accuracies represent a reliable estimation of our ability to predict the personality traits of subjects outside of the current study. Cross-validation is widely used to make regression results more resilient against out-of-sample effects and possibilities of overfitting. Naturally, these will be smaller than correlation values that could be achievable within the sample. Of the five personality traits, Emotional Stability and Conscientiousness were the two most predictable from our data (rs 1\\/4 .374 and.363, respectively). Since these two traits are significantly related to age and gender, it is not surprising that including these variables with our behavioral metrics yielded significant improvement for our prediction ability. Mood and Genres To gain insight into our models, we have presented the significant correlations for each of the traits in Figure 3. A general observation is that--aside from well-known correlations between demographic characteristics and personality traits like gender and Emotional Stability, and age and Conscientiousness--the largest correlations were for mood and genre information (i.e., features relating to the sonic and emotive aspects of the music; all Pearson correlations between personality and the derived metrics, genres, and moods, are presented in Tables S2-S4 and Figures S1-S3 in the Supplemental Material). The specific moods and genre metrics that correlate with each trait show meaningful patterns. Openness was positively correlated with listening to Atmospheric (r 1\\/4 .139, 95% confidence interval (CI) [.114, .165]), Folk (r 1\\/4 .154, 95% CI [.129, .179]), Reggae (r 1\\/4 .085, 95% CI [.059, .110]), or Afropop (r 1\\/4 .112, 95% CI [.086, .137]). The pattern captures a general preference for less popular genres compared to the U.S. population. For mood metrics, Openness was positively correlated with listening to \\\"Sentimental\\\" (e.g., \\\"Freddie Freeloader\\\" by Miles Davis and \\\"April Come She Will\\\" by Simon & Garfunkel; r 1\\/4 .093, 95% CI [.068, .119]) and \\\"Melancholy\\\" music (e.g., \\\"Dust In The Wind\\\" by Kansas and Frank Ocean's \\\"Moon River\\\"; r 1\\/4 .130, 95% CI [.104, .155]). Emotional Stability correlated positively with Blues (r 1\\/4 .086, 95% CI [.061, 0112]), Old Country (r 1\\/4 .086, 95% CI [.060, .111]), Soul (r 1\\/4 .076, 95% CI [.050, 0101]), and music with \\\"Lively\\\" moods (e.g., \\\"Down On The Corner\\\" by Creedence Clearwater Revival and \\\"Let The Good Times Roll\\\" by Ray Charles; r 1\\/4 .054, 95% CI [.029, .080]), and negatively with \\\"Brooding\\\" (e.g., \\\"Take Care\\\" by Drake and \\\"Karma Police\\\" by Radiohead; r 1\\/4 \\\".088, 95% CI [\\\".114, \\\".063]) or \\\"Defiant\\\" moods (e.g., \\\"Mask Off\\\" by Future and \\\"3005\\\" by Childish Gambino; r 1\\/4 \\\".082, 95% CI [\\\".107,\\\".056]) and music from Indie (r 1\\/4 \\\".099, 95% CI [\\\".125, \\\"0.074]), Emo (r 1\\/4\\\".155, 95% CI [\\\".180,\\\".130]), and Regional Music from Korea (r 1\\/4 \\\".079, 95% CI [\\\".105, \\\".054]). Agreeableness correlated negatively with Punk (r 1\\/4 \\\".103, 95% CI [\\\".129, \\\".078]), Death Metal (r 1\\/4 \\\".093, 95% CI [\\\".119, \\\".068]), or other \\\"aggressive\\\" music (e.g., \\\"Boss\\\" by Lil Pump and \\\"Last Resort\\\" by Papa Roach; r 1\\/4 \\\".122, 95% CI [\\\".147, \\\".097]) and correlated positively with Jazz (r 1\\/4 .124, 95% CI [.099, 0.149]), Soul (r 1\\/4 .124, 95% CI [.098, .149]), and \\\"Sophisticated\\\" music (e.g., \\\"Fly Me To The Moon\\\" by Frank Sinatra; r 1\\/4 .078, 95% CI [.052, .103]). Conscientiousness was negatively correlated with Rock (r 1\\/4 \\\".079, 95% CI [\\\".105, \\\".054]), Comedy (r 1\\/4 \\\".125, 95% CI [\\\".150, \\\".100]), and Alternative (r 1\\/4 \\\".080, 95% CI [\\\".106, \\\".055]) genres, and \\\"Energizing\\\" (e.g., \\\"Happy\\\" by Pharrell Williams; r 1\\/4 \\\".061, 95% CI [\\\".087, \\\".036]) and \\\"Excited\\\" moods (e.g., \\\"I Wanna Dance With Somebody\\\" by Whitney Houston and \\\"California Gurls\\\" by Katy Perry; r 1\\/4 \\\".055, 95% CI [\\\".081, \\\".030]). Conscientiousness was positively correlated with Funk (r 1\\/4 .119, 95% CI [.094, .144]), Easy Listening (r 1\\/4 .055, 95% CI [.029, .081]), or \\\"Romantic\\\" music (e.g., \\\"All of Me\\\" by Billie Holiday and \\\"La vie en rose\\\" by Edith Piaf; r 1\\/4 .087, 95% CI [.061, .112]). Extraversion was negatively correlated with listening to Rock (r 1\\/4 \\\".111, 95% CI [\\\".136, \\\".085]), Metal (r 1\\/4 \\\".102, 95% CI [\\\".134, \\\".083]), and \\\"Urgent\\\" music (e.g., \\\"Locked Out of Heaven\\\" by Bruno Mars, \\\"Cheap Thrills\\\" by Sia; r 1\\/4 \\\".097, 95% CI [\\\".122, \\\".071]), and positively correlated with Funk (r 1\\/4 .118, 95% CI [.093, .144]), Reggaeton (r 1\\/4 .112, 95% CI [.087, .138]), or \\\"Sensual\\\" music (e.g., \\\"LOVE. FEAT. ZACARI.\\\" by Kendrick Lamar and \\\"Same Old Love\\\" by Selena Gomez; r 1\\/4 .084, 95% CI [.058, .110]). Derived Metrics In addition to mood and genre information, we found transparent and theoretically consistent patterns of correlations between our derived metrics and personality traits. Emotional Stability correlated negatively with average skip rate (r 1\\/4 \\\".071, 95% CI [\\\".097, \\\".046]), indicating that participants who were more neurotic tended to be more selective of what they listened to at any given moment. Openness correlated positively with all-time track discovery rate (r 1\\/4 .087, 95% CI [.061, .112]) and genre entropy (r 1\\/4 .152, 95% CI [.127, .177]; see definitions in Supplemental Material), suggesting that users scoring high on Openness are more receptive to exploring different types of music. We also observed significant trends between personality and how and when users play music. People who scored high on Conscientiousness tended to concentrate their listening to a narrow window of time of day across multiple weeks (r 1\\/4 \\\".034, 95% CI [\\\".060, \\\".009]), indicating that they structure their day more rigidly than those who score lower on Conscientiousness. Those who scored higher on Extraversion tended to listen more from others' playlists (r 1\\/4 .066, 95% CI [.040, .091]). This leads to several possible explanations: Extroverts may have (1) a greater reliance on their social network's suggestions, (2) a preference for discovering music based on their peer group, or (3) musical preferences driven by group identity. On the other hand, those who listened more to music suggested by Spotify (rather than others) tended to score higher on Agreeableness (r 1\\/4 .062, 95% CI [.037, .088]) or Conscientiousness (r 1\\/4 .056, 95% CI [.031, .082]), implying a greater likelihood of considering recommendations. Discussion Here we investigated the links between human personality and musical listening behavior on the Spotify streaming service. We used metrics from Spotify that characterize the music people listen to in their daily lives, the context in which they do so, and their habitual listening behaviors. Our results showed several main findings. First, the results showed that the metrics from Spotify behavior were moderately to highly predictive of personality. There is no unified standard for benchmarking predictive performance across big data personality studies, but our regression and correlations perform comparably or outperform results from prior studies of this nature. This suggests that music streaming data are on par or better than services like Facebook and Twitter in its predictive validity of human personality. Second, when compared to prior big data music studies , our regression results suggest not surprisingly that combining both musical preferences and habitual music behavior (e.g., listening contexts) is more predictive of human personality than relying only on musical preferences. Third, and very importantly, our results provide a stark contrast to the recent meta-analysis that concluded that personality plays little role in musical preferences. Our results using big data and advanced machine learning techniques show the opposite: There is a great deal of information about personality that is communicated through musical preferences. This is an important distinction of how different conclusions can be drawn using big data. More specifically, our regression results show that Emotional Stability and Conscientiousness are most predictable from music listening behaviors compared to the other Big Five traits. This result may be driven by the way users engage with music streaming platforms. Regression accuracy may be less for Agreeableness and Extraversion because Spotify's applications are limited in opportunities for social interactions and therefore provide less opportunity for agreeable people and extroverts to engage in a way that aligns with their traits. In contrast, people who score low on Emotional Stability (i.e., high on Neuroticism) may select music to regulate their emotions (e.g., searching for music with matching emotional content), and users who score high on Conscientiousness may choose music based on goal-oriented behavior (e.g., study music, workout music). Our findings are based on cross-sectional and correlational data. We decided to build a model that predicts personality from musical behavior because of the statistical difficulties that would arise from applying data reduction to the more than 200 music usage variables and accounting for variance and individual differences in the millions of streams and hundreds of thousands of listening hours by users, all which were able to be accounted for by making these variables the predictors. Furthermore, our approach is consistent with contemporary research designs in big data music and personality studies, which also used personality traits as outcome variables. However, it is worth mentioning that there are at least two possible underlying mechanisms at work in the interaction between music and personality. On the one hand, people may seek out music that reflects their personalities as interactionist theories would suggest. Alternatively, people's personalities may be shaped by the music they are exposed and listening to. This is not unprecedented considering longitudinal evidence shows musical training impacts brain maturation. Given the cross-sectional nature of our data, we are unable to conclude which mechanism is driving out findings. However, we speculate that a combination of these two mechanisms (and perhaps others) is at play. Future research needs to undertake rigorous longitudinal investigations to develop an understanding of the development of musical preferences and personality throughout the life span and how they impact each other. This is particularly important for understanding childhood and adolescent development when there are increased social pressures and a focus on identity formation. Our study had several limitations. First, the sample was exclusive to U.S. users of Spotify. Therefore, we do not know the extent to which our findings generalize across geographic regions in other Westernized cultures. Furthermore, while recent empirical evidence suggests that music is universal in form and function , since our results are based on streaming services that require users to have internet-enabled devices, we are unable to generalize our findings to non-Westernized cultures. Testing how the links between musical preferences and habitual listening behaviors manifest across cultures (in Westernized and nonWesternized around the world) is a ripe area for future research. Second, our understanding of human personality was reliant on a self-report assessment of the Big Five model of personality. Future research should extend our work by investigating other constructs including psychological values , cognitive profiles , and narrative identities. Third, while the present study takes into account the streaming behavior and self-reported personality of 5,808 listeners, the size of Spotify's audience allows for a much larger study. A larger user sample would allow for more flexibility on model choice (e.g., deep learning, models trained on demographic segments), ultimately yielding more accurate predictions. There is also the potential to assimilate other inputs (e.g., longer listening windows, additional derived metrics) into the models, further improving predictive ability. Fourth, we relied on self-report and behavioral data from Spotify and therefore were unable to draw insights about the role of human biology on musical preferences and habits. Given the vast volume of research on the cognitive neuroscience of music and the emerging literature  on the social neuroscience of music (e.g., the role of oxytocin) , future research could begin to link streaming behavior with brain scanning, genetic, and physiological data. However, such future research and applications must be conducted within the strict boundaries of ethical data usage, collection, and storage policies. A user's digital history is extraordinarily personal and sensitive and should be treated with proper consideration of the conceivable misuses and unintended externalities. We affirm that our methodology, surveying, and data governance were all done under such a framework, including an IRB at Spotify that scrutinized our design and methods before granting formal ethical approval. We disavow any future research and applications which violate ethical standards of data usage and are not transparent about privacy to its users. In conclusion, we used data from a naturalistic environment to show how personality is intertwined with music listening behavior. The observations were drawn from a substantial data set that included 5,808 users, 211 mood, genre, and behavioral variables, 17.6 million streams, and over 662,000 hr of listening collected over a span of 3 months. The observed links between personality and listening behavior are robust and ecologically valid. The present work is a model for how psychological methods can be fused with cutting-edge technology and big data for scientific inquiry. Finally, the results show that personality does in indeed play an important role in musical preferences and warrants continued rigorous investigation.\",\"577240266\":\"Mountains are among the most majestic and defining landmarks of our environment. For thousands of years, they have been subject of legends and religious reverence: Mount Olympus was regarded as home of the 12 gods in Ancient Greece, Mount Sinai is the site at which God gives Moses the Ten Commandments according to the Torah and the Old Testa ment of the Bible, Tibet's Mount Kailash is a sacred place to both, Buddhists and Hinduists, and Maori people worship Mount Taranaki as a life force. Given their outstanding ecological and cultural significance, many people have assumed that mountains can shape human character and have consequently ascribed specific traits to the people living in these environments. The resulting stereotypes about mountain people have varied over time and across places  but also show remarkable consistency in certain elements. Indeed, from the Appalachians  to the Swiss Alps and the Balkan mountains  to the Sierra Madre mountain range in the Philip pines --and from the noble frontiersman  to the southern hillbilly  and the wildlings, or free folk, in HBO's series Game of Thrones, mountain people are typically depicted as independent, resili ent, and--most famously--free. The latter is even reflected in West Virginia's official state motto: \\\"montani semper liberi\\\" (mountaineers are always free). However, despite the widespread proliferation of these perceptions and their continuous popularity in folk psychology, direct empirical evidence on how mountainous areas might affect the identity of their inhabitants is scarce. In a series of small-scale field experiments, Oishi and colleagues showed that introverted  and emotionally stable indi viduals  prefer hilly and mountain-like environments over open and ocean-like environments. Extending the results of Oishi and colleagues, a recent study of 3 million U.S. residents used topographical data to construct an objective measure of the mountainousness of people's living environments. The results indicated that mountainousness was associated with heightened openness and decreased agreeable ness, conscientiousness, extroversion, and neuroticism. In the current research, we expand this work by introducing a new large sample to study the associa tions between mountainousness and personal values. Personal Values Values are broad, trans-situational goals, and guiding principles that reflect what is important to people. Personal values get expressed in individuals' thoughts and actions. They are directly related to attitudes, such as readiness for out-group contact , pro-environmentalism , prejudice toward immigrants , and approval of homosexuality , behaviors such as voting , prosociality , fair trade consumption , military participation and charitable donations , and mental health outcomes, such as life satisfaction and depressive affect. The most widely used value taxonomy is Schwartz's cir cumplex model , which has received support in more than 80 countries. Schwartz's circumplex model arranges 10 basic values (i.e., power, achievement, hedonism, stimulation, self direction, universalism, benevolence, tradition, conformity, and security) in a circular, two-dimensional space that captures their conflicts and compatibilities. Values that are compatible with each other are sorted into the same higher order value category and positioned closely together. In contrast, conflict ing values are positioned further apart, in opposing positions in the circumplex. The four higher order dimensions span two basic conflict dimensions: Self-transcendence (benevolence, universalism) versus self-enhancement (achievement, power) contrasts values that emphasize concern for others with values that emphasize the promotion of personal interests even at the expense of others. Openness to change (self-direction, stimula tion) versus conservation (conformity, security, and tradition) contrasts values that emphasize novelty and autonomy with values that emphasize stability and self-restriction. Self-enhancement and openness to change values have a per sonal focus, whereas self-transcendence and conservation val ues have a social focus. Likewise, self-enhancement and conservation values are considered to drive self-protection, whereas self-transcendence and openness to change values are considered to drive growth. Except for hedonism, all values are exclusively aligned with one higher order dimension. Hedonism shares elements of both self-enhancement and openness to change. The circumplex model is exhibited in Figure 1, and all personal values are summarized in Table 1. While the cross-cultural validity and expression of personal values are increasingly well-understood, comparatively little is known about their origins. Genetic factors and social and cultural influences have been acknowledged, although it is unclear how they interact. Moreover, while it has been suggested that values adapt to environmental demands and affordances , no research has examined the role that the physical envi ronment may play in the formation or change of values. Thus, in examining the association between mountainous ness and personal values, the present study extends previous research on the links between mountainous terrains and human identity and adds to our understanding of the ways in which macro-environmental factors relate to personal values. The Current Research To examine the relationship between mountainousness and per sonal values, we combine a large data set containing personal values of 32,666 U.S. residents with topographical information derived from satellite radar data, which provides a granular, objective assessment of mountainousness. The observed moun tainousness effects are compared against an array of individual demographic (i.e., age, gender, income) and socio-ecological (population density, latitude) predictors to provide a bench mark for evaluating the results. In the absence of prior theoriz ing or empirical findings on the association between mountainousness and personal values, we refrain from making a priori predictions and instead adopt an exploratory, data-driven approach. However, drawing on previous research , we predict systematic relationship pat terns between mountainousness and personal values that reflect the compatibilities and conflicts between individual values. Specifically, because the value circumplex represents a motiva tional continuum, as one moves around the circle and away from the value that is most strongly positively related to the variable of interest--in our case mountainousness--the corre lations should shift from positive to negative. The negative cor relations should peak with the value on the circle that is directly opposite the value with the strongest positive correlation. When graphing the coefficients, with the values arranged on the horizontal axis from left to right in order around the circle, and the correlations between values and mountainousness shown on the vertical axis, the emerging line is expected to form a sinusoid curve (i.e., a sine wave, with one major peak and one major valley). Method Participants and Procedure The current research utilizes the TIME Magazine Basic Human Values Dataset (Gotz et al., in prep). The data collection was conducted in collaboration with TIME Magazine (see Online Supplement for procedural details) and received ethical clearance from the Psychology Research Ethics Committee of the University of Cambridge (application number: PRE..094). In accordance therewith, the collected data may not be made publicly accessible but can be shared with fel low researchers upon request. Between December 2017 and October 2018, 92,886 individ uals took part in the survey and donated their data. In the cur rent research, we included all participants who (1) reported living in the contiguous United States and provided their ZIP code of living1 and (2) were between 18 and 99 years old. This resulted in a final sample of 32,666 individuals (agemean 1\\/4 28.0, ageSD 1\\/4 11.3; see Table 2 for further demo graphic information). Measures Twenty-item values inventory (TwIVI). Values were measured via the TwIVI , a short scale based on the 40-item Portrait Values Questionnaire. Each personal value is assessed through two portrait-type items which describe a fictional person (e.g., \\\"Getting ahead in life is important to this person. This person strives to do better than others.\\\" and \\\"This person really wants to enjoy life. Having a good time is very important to them.\\\") and ask participants to rate the extent to which the described person is like themselves on a 6-point Likert-type scale (1 1\\/4 not at all like me; 6 1\\/4 very much like me). In the current study, nine of 10 Conbach's a ranged from.51 to.78 (see Tables S1 and S2 in the Online Supplement), which is acceptable for two-item scales  and in line with previous findings. The only exception occurred for security (a 1\\/4 .28), which is also consistent with past research (original publication of the TwIVI: Sandy et al., 2017: a 1\\/4 .33). For each value, the two corresponding items were averaged to calculate a value score. Mountainousness. Following the Nordic Centre for Spatial Development , we independently considered two aspects of physical topography: shape (i.e., hilliness\\/moun tainousness) and altitude (i.e., elevation). To assess mountainousness, for each ZIP code in our sample, we calcu lated the standard deviation in elevation above sea level within a 20-mile (default measure) as well as 50-mile radius from that ZIP code's centroid (; for more details, see Online Supplement). A standard deviation of 0 reflects no mountains at all and thus flat land. In contrast, a large standard deviation indicates a hilly, mountainous area (see Figure 2, Figure S1 [Online Supplement], and Gotz, Stie ger, et al., 2020, for further validation of this measure). However, while the mountainousness index is sensitive to changes in elevation and thus shape, it is a relative measure that does not account for absolute elevation (e.g., hilly area at low elevation versus hilly area at high elevation). Therefore, we also calculated mean elevation across people's living environ ments (i.e., within a 20\\/50 mile radius from their ZIP code of living) to capture altitude as the second component of physical topography. Climatic condition. We also considered latitude as an index of cli matic stress, which has been linked to personality in prior research. A lower latitude rep resents southern (warmer) areas, whereas higher latitude (up to 90 degrees, which is on the North Pole) represents northern (colder) areas. Population density per square mile. Population density per square mile for each ZIP code was obtained from American Commu nity Survey. Demographic variables. As part of the online survey, participants reported their age, annual income, ethnicity, and gender. Analysis Strategy Based on past research, we expected an effect size of r * .016 . A power anal ysis suggested that at least 30,658 participants are necessary to reach a power of 80% (two sided; a 1\\/4 5%; r under H0 1\\/4 0). We adopted a two-pronged analysis strategy. In the first step, we conducted correlation analysis to investigate systematic pat terns between mountainousness and personal values (Table S3 in the Online Supplement). Following previous work , we depicted the zero-order relationships visually as a sinusoidal curve; val ues are listed on the horizontal axis in their original order (i.e., moving around the circle), while their correlations with moun tainousness are plotted on the vertical axis (Figure S2, Online Supplement). In the second step, we sought to contextualize the mountainousness findings by controlling for and comparing them to other relevant demographic and socio-ecological pre dictors of personal values. As 40.9% of our participants were the only participants in their respective ZIP codes, our sample did not afford the recommended threshold of five Level 1 units per Level 2 unit (in our case, individuals per ZIP code;). So we decided not to employ multilevel modeling. While a classical multivariate linear regression approach offered one feasible alternative, it was likely to produce biased results due to multicollinearity (i.e., high intercorrelations among predic tors, e.g., mountainousness, mean elevation; see Table S4 in the Online Supplement), which would deprive each variable of its predictive power when entering all predictors at once. To cir cumvent this issue, we first replicated the correlation analyses individually for all predictors to allow for benchmarking and direct comparison of the resulting relationship patterns by cal culating partial correlations (controlling for gender, age, and income). Second, mirroring recent geo-psychological studies , we conducted conditional random forest machine-learning analyses (; for more details, see Online Supplement). All analyses were conducted in R, and the accompanying code is available from our Open Science Framework (OSF) project page (https:\\/\\/osf. io\\/287ad). Results Correlations between all predictors and the 10 personal values are displayed in Table 3 (global and local area-specific vari ables were controlled for age, gender, and income by calculat ing partial correlations). Mountainousness indices were most consistently related to conservation values (see Table 3). Specifically, mountainousness showed significant positive associations with all conservation values (security: r 1\\/4 .022, p < .001; tradition: r 1\\/4 .019, p 1\\/4 .001; and conformity: r 1\\/4 .013, p 1\\/4 .030) and the self-enhancement value of achieve ment (r 1\\/4 .012, p 1\\/4 .039), while mean elevation was most strongly related to heightened tradition (r 1\\/4 .031, p < .001) and also exhibited a negative relationship with hedonism (r 1\\/4 .013, p 1\\/4 .021). Figure 3 visualizes the correlation patterns to facilitate the detection of sinusoidal curves. As shown in Panel C, the partial corre lation curves of mean elevation and--to a lesser extent- mountainousness approximated sinusoidal shapes. Compared to the other two ecological variables under consideration, pop ulation density, and latitude, a similar picture emerged with the correlation curve of population density conforming more to a sinusoidal shape than the curve of latitude (see Figure 3, Panel B). Specifically, participants living in densely populated areas tended to score higher on self-enhancement values and lower on conservation values. In contrast, individuals living in colder climates (i.e., higher latitudes) tended to score lower on self-enhancement and conservation values and higher on self-transcendence. While the amplitude of the curves for mountainousness and mean elevation (|r| < .031) was smaller than those of population density and latitude (|r| < .065), over all, it supported consistent small effects for all ecological vari ables under study. Lastly, as shown in Panel A, the correlation curves of the demographic variables also all tended to form sinusoidal curves, albeit to varying degrees. For men, an upper peak was observed at self-enhancement and conservation val ues and a lower peak at self-transcendence values, whereas this pattern was reversed for women. Older participants put greater emphasis on conservation values and scored lower on self-enhancement values. Individuals who reported earning higher incomes scored higher on self-enhancement and conser vation values and lower on self-transcendence values. Across all demographic variables, the amplitude of the correlation curve (|r| < .20) was decidedly larger than for the socio ecological variables (i.e., mean elevation, mountainousness, latitude, population density). This finding indicates that the proximal personal demographic factors were more strongly associated with personal values than the distal socio ecological factors. The sinusoidal pattern was also quantified with the sinusoidal fit index , which indi cates how well a correlational pattern reflects a sinusoidal curve. As can be seen in Table 3, gender (other), income, and mountainousness (20-mile radius) displayed acceptable fit to a sinusoidal form. All the other sinusoidal patterns were less pronounced. As is shown in Figure 4, both mountainousness indices exceeded the customary random noise threshold. This result identifies the mountainousness indices as meaningful contributors to the prediction of all personal values in the conditional random forest models (see Figure 4, impor tance ranks are reported in the right-most cell of each panel). When considering median importance ranks across all values, age emerged as the most important predictor, followed by gen der (male-female), and income, while mountainousness, mean elevation, latitude, and population density were of equal impor tance ahead of gender (male-other), which, on average, was the least important predictor of personal values (see Table S6 in the Online Supplement). Robustness Checks We conducted various robustness checks to scrutinize the validity of our results. First, we drew a bigger radius when con sidering the mountainousness of participants' surroundings. Our main models were based on a radius of 20 miles from the centroid of participants' ZIP code of residence, reflecting par ticipants' day-to-day environment in keeping with the U.S. national average commuting distance of 18.8 miles. To account for additional effects of the mountainousness of the broader surroundings in which people spend their lives, we replicated all analyses using a 50-mile radius. As shown in Table 3, Figure 3 (Panel C, dotted blue and red lines), and Figure 4 (turquoise and purple bubbles), all find ings remained virtually unchanged. If anything, the 50-mile radius mountainousness measures showed somewhat stronger associations with personal values, replicating prior research. Second, we varied the number of predictors randomly sampled as candidates at each split for the forest trees (i.e., mtry; marked by different colors in Figure 4) as well as the computational starting point for the randomization (i.e., seed; marked by points in Figure 4) of our conditional random forests. Attesting to the robustness of our findings, as shown in Figure 4, these alternative specifications did not change the interpretation of our findings. Third, we replicated our correlational analyses partialing out individuals' mean ratings to account for individual differences in scale use. Relationship patterns with the mountainousness indices remained robust, with minor fluctuations in the 20m-mountainousness index and more pronounced latitude differences (see Table S7 in the Online Supplement). Lastly, we calculated a mean squared suc cessive difference (MSSD) measure  variant of our mountainousness measure, which consid ers variability in elevation (i.e., hilliness) like our standard measure but also accounts for the sequence of elevation points. Mountainousness-MSSD captures both variability and instability, with rising mountainousness-MSSD values indicat ing an increasingly extreme and uneven terrain. Mirroring prior research , rerunning the analyses with mountainousness-MSSD did not yield any noteworthy differences or incremental predictive power compared to our original analyses (see Figure S3, Online Supplement). Discussion The present research employed advanced analysis techniques to investigate whether mountainousness is meaningfully asso ciated with personal values. Correlation curve analysis indi cated that individuals living in hilly and mountainous areas were likely to emphasize conservation values, specifically security and tradition. Individuals living at high altitudes showed a similar pattern but also cared less about hedonism. These results were stable across various robustness checks. Conditional random forest machine-learning algorithms con firmed both mountainousness indices as relevant predictors of personal values when tested against a conservative set of demographic (age, gender, and income) and socio-ecological (population density, latitude) controls. How should we interpret the associations between mountai nousness and personal values? The negative relationship with hedonism appears straightforward. Mountainous areas tend to be secluded and inhospitable, making them ill-suited for the pursuit of worldly pleasures and sensuous gratification. Mean while, the robust association between mountainousness and conservation values may initially seem surprising and even counterintuitive. According to voluntary settlement theory , during the European settlement of the United States, frontier environments like the Rocky Mountains attracted primarily self-reliant, freedom-seeking nonconformists. The accumulation of individuals with such traits laid the foundation for an ethos of independence that con tinues to characterize the inhabitants of these areas today. Indeed, the mountain states still exhibit the strongest individualist tendencies in the United States. Moreover, recent research examining the personality structure of mountain dwellers in the United States found that mountainousness was most strongly related to heightened openness to experience. With openness being negatively related to conservation values , these findings appear to be at odds with the current results. However, from an analytical standpoint, even the strongest correlations between traits and values--which are typically found between agreeableness and benevolence (rsp 1\\/4 .61, Parks-Leduc et al., 2015; r 1\\/4 .45, Roccas et al., 2002; and r 1\\/4 .54, Vecchione et al., 2019) and openness and self-direction (rsp 1\\/4 .52, Parks-Leduc et al., 2015; r 1\\/4 .48, Roccas et al., 2002; and r 1\\/4 .39, Vecchione et al., 2019)--leave sufficient unexplained variance to manifest in differential rela tions with third variables, such as mountainousness. More importantly, from a conceptual standpoint, while personality traits and personal values are similar, they are not the same. Values are evaluative, mutually exclusive (i.e., following a dia metrical organization, wherein endorsement of certain values implies rejection of others), enduring goals that reflect what a person finds important as a member of society. Meanwhile, traits are descriptive, nonmutually exclusive (i.e., following an orthogonal organization, wherein stronger expression of cer tain traits does not affect others), enduring dispositions that reflect what a person is like as an individual. The current findings dovetail well with the dual-pressure model of ecological stress. According to this model, the same ecological stressor, such as the harsh ness of mountain terrains, might simultaneously produce opposing pressures that push people in two different directions. In the current context, mastering the tough ecological condi tions of mountainous areas might require individuals with inde pendent agency and preparedness to confront unknown challenges and thus favor an open personality. Meanwhile, thriving in ecologically challenging environments, such as mountainous terrains, might require social groups that are committed to safety, self-discipline, sta bility, and protection of the status quo--hallmarks of conserva tion philosophy. This conclusion aligns with research showing that experiences of environmental threats and uncertainty (1) prompt individuals to be skeptical of strangers and more ter ritorial about their group domains , (2) lead to increased endorsement of socially and politically conservative positions , and (3) are conducive to the creation of vertical governmental restric tion--laws that impose hierarchies and protect specific groups. Thus, having an open personality (i.e., autonomy and the readiness to confront novel challenges when faced with threats) and conservative values (i.e., support ing a social order governed by norms of security, self-discipline and respect for customs to minimize threats) might be most adaptive for thriving in the mountains. It should, of course, be noted that the observed effects are small. Compared to the average correlation between age and values (M |r| 1\\/4 .098), the average correlation between mountainousness (20 miles) and values was about a 10th (M|r| 1\\/4 .009). However, personal values are determined by many factors , and any single factor is likely to have only a small effect. This argument is especially true in uncontrolled, real-world settings as in the present study, where--compared to classical lab experi ments--effect sizes are typically diminished due to heightened error variance. More over, their small magnitude does not render the observed effects unimportant. Rather, even small effects can make a big difference when considered over time and at scale. The former seems likely as per sonal values influence human attitudes and behaviors daily. The latter is especially probable for socio-ecological influences, such as mountains that--while distal and thus less influential than personal factors (e.g., demo graphics)--simultaneously affect large groups of people who share the same environmental milieu. Taken together, the immediate impact of mountainousness on personal values may be small. But when considered over a lifetime and at population scale, small effects translate into highly consequential outcomes such as election results , cultural capital, and economic growth. Limitations and Future Research The current research has several limitations. First, due to the correlational nature of our data, no causal inferences can be drawn. Longitudinal studies at the individual and community levels are needed to illuminate the psychological underpin nings of the associations between mountainousness and per sonal values (i.e., acculturation effects, selective migration or a combination thereof; Gotz et al., in press;). Second, while our data offered one of and perhaps the largest personal values samples in the United States, it is not nationally representative. Although the ethnic composition and geographic coverage were broadly rep resentative of the general population, which is common in large-scale online samples , the participants in our study were younger, predominantly female, and less affluent than the national average. Third, our assessment of personal values was limited to a 20-item short scale. While the TwIVI displayed respectable psychometric properties in the current study and previous research , its brevity comes at the cost of reduced measurement precision and content breadth. Thus, future research should extend the current work by using longer scales, which might include the extended 19-value version  that could offer even more nuanced insights. Such work may also systematically assess nonlinear trends in mountai nousness-value associations.5 Furthermore, future research might try to dynamically adjust the 20-mile radius as a proxy for the mean commuting distance to the actual commuting distance in each ZIP-code area. Such an adjustment might reduce error variance and isolate the effect of interest more effectively. Lastly, future research should investigate the associations between personal values and other challenging ecologies, including coastlines, swamplands, and deserts. Conclusions We examined the associations between mountainousness and personal values by combining large-scale psychological survey data with objective topographical information and advanced machine-learning techniques. We find evidence for small but robust relationships between mountainousness and heightened conservation values. Our results reaffirm the mountains as a challenging and influential element of our physical environ ment, illuminate its differential associations with values versus personality traits, and advance our understanding of the deter minants of personal values.\",\"577240269\":\"Perceiving one's partner as supportive is considered an essential element in romantic relationships, but we lack knowledge about which factors are central to predicting such perceptions. Several relationship theories (e.g., attachment theory, self-determination theory, and interdependence theory) have underscored the centrality of partner support in promoting well-functioning relationships. Existing research has examined several potential factors that are considered important for perceived partner support, but it has not compared the relative importance of these different factors, in part because traditional statistical analyses are not well-equipped to examine a large number of potential predictors at once. The purpose of this study was to leverage the power of machine learning to compare which theoretically relevant relational and individual variables--from the perspectives of both the support receiver and the support provider--predict the most variance in perceived partner support. Established Relational Predictors of Perceived Partner Support According to attachment and interdependence theories, actors should perceive partners as more supportive when the relationship is characterized by high satisfaction, empathy, commitment, trust, and willingness to sacrifice, and low conflict. This is because partners in these relationships can count on each other to provide support and are thus more open to support when needed or may be more willing to take risks. This in turn leads the recipients to perceive their partners as supportive. Furthermore, the transactive goal dynamics theory suggests that high goal correspondence allows partners to better coordinate their efforts to achieve their goals and thus is likely to be more supportive. Finally, self-expansion theory  suggests that inclusion of other in the self enables greater shared intimacy, in turn leading partners to share resources and to perceive each other as more supportive. Based on these theories, we would expect relationship variables (see Table 1 for the full list of variables) to be important for perceiving partners as supportive, but it is not clear whether there are specific relational variables that contribute to perceptions of support more than others. Established Individual Predictors of Perceived Partner Support Interestingly, few theories on partner support have explicitly discussed which individual differences variables are the most likely to explain why some partners perceive and are perceived as more supportive than others (see attachment theory for an exception;). Attachment theory suggests that avoidantly attached individuals perceive partners as less supportive because they doubt partners' availability , whereas anxiously attached individuals doubt their worthiness of being supported but feel others as capable of providing support, which has resulted in mixed findings for attachment anxiety. According to attachment theory, individuals who trust themselves are also more likely to trust others' capacity to be supportive when needed  and thus are more likely to perceive their partners as supportive. Thus, we expect that individuals higher in promotion orientation (i.e., regulatory focus on dreams and aspirations;), self-control , and self-efficacy  feel a greater sense of autonomy (self-determination theory;) and trust in their ability to achieve their goals (attachment theory;). Furthermore, we expect that individuals high in selfesteem  or self-respect  would perceive their partners as more supportive as they may self-select into healthier relationships or be able to elicit higher quality support from their partners. In addition, while better physical  and emotional well-being  have often been considered as outcomes of perceived partner support, it is also likely that individuals with higher well-being are more easily supported. For example, depression makes people more pessimistic and view everything in a negative light. Thus, we expect that people who have higher well-being are more optimistic in their perceptions of partner behaviors and act in ways that tend to elicit positive behaviors from their partners. Finally, demographic variables, such as relationship length, age, and gender have previously been associated with perceived partner support, with mixed results. Several researchers have hypothesized that support for goals is likely more important in early stages of the relationship, with the importance of support declining over time , whereas other researchers have found that longer relationship length predicted higher perceived partner support. Furthermore, because women are traditionally socialized to be more caring, partners may find women more supportive. Indeed, previous research has found women to be perceived as more supportive; however, both men and women felt equally supported by their spouse. There is no prior literature on education, ethnicity, or children on perceived partner support, but we have provided a rationale for their inclusion in Supplemental Table S1. Machine Learning Previous research has relied exclusively on traditional linear models. Machine learning algorithms have several key advantages over these models: they can learn highly nonlinear relationships between variables, handle a large number of predictors at once, and estimate complex interactions between different variables. As such, they are not susceptible to multicollinearity or functional form (e.g., expecting an association to be linear, whereas the real relationship is cubic) misspecification. Because of this, using machine learning provides a much more flexible and powerful approach to predicting an outcome. Machine learning algorithms are traditionally fed as many predictors as possible to maximize prediction. It then learns which variables are important for predicting the outcome. In this study, we use a random forest algorithm , which is a form of explainable decision tree that can handle highly nonlinear relationships and complex interactions without overfitting to the data. Machine learning models, including random forests, have traditionally been ''black box models'' where the researcher is unable to understand what the algorithm has used for predicting the outcome. However, recent developments in machine learning have provided tools that allow interpretation of the results through explanations of machine learning models. This work is particularly interesting because it enables researchers to combine the use of powerful machine learning algorithms and state-of-the-art tools for model explainability that can provide accurate predictions as well as increase our understanding of which factors are the most important in predicting the model outcome. The latter is of particular importance because one of the principal aims of psychology is to develop a deeper understanding of human behavior. In this study, we take advantage of this new development in machine learning by using Shapley values  to estimate the effect size and direction of the effect of each variable predicting perceived partner support The Current Research Our aim was to examine which relational and individual factors are the most predictive of perceived partner support. We examined two types of perceived partner support : perceived partner responsiveness (i.e., being available and responsive to the partner's needs, and understanding and validating one's overall self;) and perceived affirmation of the ideal self (i.e., perceiving and behaving in a manner consistent with the partner's ideal self;). The former is a broader construct and is considered one possible central organizing theme for the diverse phenomena relationship scientists study , whereas the latter is more specific and focused explicitly on partner's role in helping individuals become closer to their ideal self. As such, although both are frequently used to examine partner support in romantic relationships, they may be predicted by different factors due to affirmation being more specifically about the ideal self. The predictor variable selection for this study was guided by existing theoretical frameworks to test the explanatory power of different relational and individual variables (see Table 1 for the variables, expected direction of the effect, and state of the current evidence). The selection was somewhat limited by the availability of variables across the data sets. Furthermore, because there are (at least) two people in romantic relationships, it is important to understand whether one person's outcome is only determined by their own variables (actor effects) or whether their partner's reports also predict the actor's outcomes (partner effects). Our hope is to add to the current understanding of the factors that are the most and least likely to predict perceived support. We used data from five dyadic data sets that had a large number of common predictor variables and addressed the following research questions: Research Question 1 (RQ1): How much variance in the overall outcomes can we explain? Research Question 2 (RQ2): Are relational or individual variables more important for predicting partner support? Research Question 3 (RQ3): Do partner effects explain additional variance in outcomes above actor effects? Research Question 4 (RQ4): Can we predict support over time? Method Participants and Procedure The preregistration and materials for the project can be found on the Open Science Framework (OSF) https:\\/\\/ osf.io\\/v368c\\/.1 Five dyadic data sets (; 2020b;) were combined in this project to create a large data set of couples. These data sets were chosen because they included a large number of predictor variables that were the same across the samples. We are aware of no other data sets with such high overlap in the variables. All data sets included cross-sectional self-reported data collected from both dyad members in romantic relationships. Two of the data sets included only dating couples (n1 = 74, n4 = 92), one data set included newly committed couples (e.g., engaged, married, or moving in together; n3 = 178), and two data sets included married couples (n2 = 120, n5 = 77). The final sample consisted of 550 couples (1,100 individuals). Data set 3 was also used to predict support 6 months later and included 161 couples. On average, participants were aged 28.32 years (SD = 10.90, range = 18-79) and had been in a relationship for 5.59 years (SD = 8.13, range = 0.08-61.50). Most of the participants were White (n = 876, 80%), with a minority being African American (n = 83, 8%), Hispanic (n = 35, 3%), or Asian (n= 72, 7%). The sample was primarily well educated: 196 (18%) participants had a graduate degree (MS\\/PhD), 466 (42%) a bachelor's degree, 379 (34%) at least some college, and 60 (5%) had no college courses. The couples were married (n = 266, 48%), cohabiting (n = 127, 23%), or dating and not living with each other (n = 220, 40%) and most of the couples did not have any children (n = 462, 84%). All data were collected in the United States. Measures The outcome variable, perceived partner support, was measured using the 18-item responsiveness scale  in four data sets and the partner affirmation scale  in three data sets. The rest of the variables from each data set were included in the final data set as predictors if the variable appeared in at least three of the five data sets. These variables were divided into actor's and partner's individual (n = 17) and relational (n = 11) predictors (summarized in Table 1; see supplemental material for the description of the scales used). Data Analysis Details of the data preparation and analyses can be found in the supplemental material. The results were analyzed using Python 3.7 and the code can be found here: https:\\/\\/ github.com\\/matthewvowels1\\/Aff_Eff_PPR. Each data set was analyzed using a random forest regressor. A random forest is a type of decision tree that trains on bootstrapped subsamples of the data to avoid overfitting. We used the default ''scikit learn'' random forest regressor with tenfold cross-validation. The metrics for test data model performance used were the mean square error (which is the averaged squared difference between the prediction and the observed value), the R2, and the variance explained. The full last model trained was saved and explained using the ''SHapley Additive exPlanations'' package (SHAP;). The results are provided as feature importances, which describe how important the variable is for the model outcome and how much it changes the outcome. The analyses were conducted separately by first including as many participants as possible in each analysis and then by including as many variables as possible. This resulted in a total of four analyses (three for perceived partner responsiveness and one for affirmation) that were conducted twice: once including only actor effects and once including both actor and partner effects. The included variables and the results for all analyses can be found on the OSF project page. Random forests in their current form are unable to explicitly model hierarchies in the data and it is possible that hierarchical data can inflate the predictive performance. However, given we were primarily interested in the relative performance of different predictors, which is not affected by hierarchical data, this is less of an issue in this study. Results Total Variance Explained (Research Questions 1-3) Table 2 presents the overall prediction results for each outcome variable for each model for relational and individual variables as well as for models including actor effects only and for models including both actor and partner effects. In the actor only models, we were able to explain the most variance in perceived responsiveness overall (48.2%55.3%), with relational variables generally predicting the largest percentage of the variance (57.1%-69.2%). Individual variables predicted a total of between 30.8% and 42.9% of the variance. Partner effects did not improve the predictive power of the models; if anything, partner effects contributed noise to the data and made the prediction less accurate. However, in the models with partner effects included, partners' individual variables predicted between 11.6% and 13.4% of the variance. In contrast, partners' relational variables predicted very little variance (3.2%-5.5%). For perceived affirmation, the model with actor effects was able to predict 34.5% of the variance with relational and individual variables predicting similar amounts of variance (48.2% and 51.8%, respectively). In the models with both actor and partner effects, actors' relational variables predicted the most variance (40.8%) followed by actors' individual variables (31.3%). Partners' individual variables contributed 22.3% of the variance, whereas partners' relational variables contributed very little (4.9%). Most Predictive Variables (Research Question 4) In most of the models, the predictive importance of the variables decreased after only a small number of predictors. The rest of the predictors contributed only a small amount of variance into the model individually. We used 5% as a cutoff for percentage change in the model. We present the top 10 variables for each outcome in the figures and all predictors in Table 3 for the percentage model change for the main actor models. In the figures, the left side provides the average effect of each variable on the model outcome. The right side of the figure provides the estimates for each individual participant. Red (dark grey) indicates a higher value of the predictor variable and blue (light grey) indicates a lower value. For example, red is equal to 1 and blue is equal to 0 for binary variables. The Shapley values are additive and can be interpreted similarly to an average effect from a linear model. For example, 1 unit increase in relationship satisfaction predicted a corresponding average increase of 0.33 units in perceived responsiveness. The individual effects show that low relationship satisfaction predicted up to a -3.0-unit change in perceived responsiveness compared with average relationship satisfaction, whereas a high relationship satisfaction score predicted up to 0.5-unit increase in perceived responsiveness compared with average relationship satisfaction. In Table 3, the impact is rescaled to be between 0 and 1 for ease of interpreting and comparing effect sizes. Perceived partner support was measured using two variables: perceived responsiveness and affirmation. There were four relational (relationship satisfaction, empathy toward partner, trust, and commitment) predictors that were consistently predictive of higher levels of perceived responsiveness (see Figure 1) and affirmation (see Figure 2). Experiencing higher conflict in the relationship in general predicted lower perceived responsiveness and affirmation. Willingness to sacrifice and inclusion of other in the self, on the contrary, explained very little variance in the outcomes. Out of individual (attachment, individual differences, individual well-being, and demographics) variables, only higher actor attachment avoidance predicted lower perceived partner responsiveness and affirmation across analyses. Better physical health also predicted higher perceived responsiveness, whereas greater life satisfaction and depression predicted higher perceived affirmation. There were several variables that explained very little variance in the outcome, including all demographic variables and individual differences variables (other than attachment). There were no partner variables that predicted perceived responsiveness and affirmation consistently across analyses. Exploratory Longitudinal Analyses Finally, to examine whether the variables at baseline would be able to predict support 6 months later, we used Sample 3 (n= 322 [161 couples]) to estimate the longitudinal associations between the predictor variables and the outcome. Overall, we were able to predict 27.6% of variance in responsiveness and 18.2% of variance in affirmation with only actor effects. Models with actor and partner effects were somewhat less predictive (26.7% for responsiveness and 16.3% for affirmation). Relational and individual variables were equally predictive of support (see Table 2 for the full model results and Table 3 for normalized impact on the model). The only consistently important predictors across analyses were trust, commitment, attachment avoidance, and life satisfaction. Trust and commitment consistently predicted higher responsiveness and affirmation 6 months later, but relationship satisfaction did not. Both higher attachment anxiety and higher attachment avoidance predicted a decrease in perceived affirmation 6 months later but only attachment avoidance predicted responsiveness. Participants who reported higher life satisfaction at baseline reported higher perceived affirmation and responsiveness 6 months later. Discussion The purpose of this study was to add to the growing body of literature on perceived partner support by using explainable machine learning to understand which variables reliably predict perceived partner support and which variables do not. It was the first study to compare a large number of variables providing novel insights into who perceive their partner as supportive and in which types of relationships. It is important to understand what researchers, practitioners, and policy makers should, and should not, focus on when designing interventions to improve support, whether for quitting smoking, achieving career goals, or beating cancer. Overall, we were able to predict a large amount of variance in both outcomes at baseline and 6 months later but not predict any change over time. Joel et al. (2020) also found that variables included in existing data sets were unable to account for changes in relationship satisfaction and commitment over time. Thus, it appears that, although we can predict outcomes as a field, we are unable to predict changes over time. Because perceived partner support has been robustly associated with better individual and relationship well-being, it is useful to understand variables that predict perception of support. However, we should also be able to predict changes in our outcomes. Predicting actual change will likely become an important challenge for the future of relationship research. Summary of the Most and Least Important Predictors and Implications for Theory There were two types of variables that reliably predicted perceived support both at baseline and 6 months later: general relationship variables and attachment styles. The finding that general relationship variables is important for perceived partner support is unsurprising and in line with major relationship theories suggesting that happier relationships are important for perceived partner support. Specifically, higher trust, commitment, and empathy toward partner, and lower conflict, predicted an increase in perceived partner support. However, there were some relationship variables that varied across analyses and were less robustly associated with perceived partner support. Interestingly, relationship satisfaction was only predictive at baseline but not longitudinally, suggesting that, perhaps when taking away shared method variance, overall relationship satisfaction is not that important for perceived support, at least when compared against other relationship variables. Willingness to sacrifice was the only relationship variable that did not predict perceived partner support. Sacrifice is often seen as a mixed blessing in relationships  and we showed that people who are willing to sacrifice do not perceive their partners as more supportive and are not perceived as more supportive. Of individual variables, actors' attachment avoidance was the only consistent individual predictor of partner support: highly avoidant people perceived their partners as less responsive and affirming. This finding is theoretically consistent, given that individuals high in attachment avoidance are theorized to have a negative model of others and do not trust others' capacity to be there when needed. Previous research has also found avoidance to be associated with perceiving partners as less supportive. Interestingly, attachment avoidance was also more predictive of perceived partner support longitudinally than relationship-related variables, highlighting its centrality for perceived partner support. High attachment anxiety only predicted lower affirmation 6 months later. Results for attachment anxiety are often mixed because while anxious individuals seek reassurance and support excessively, they doubt whether they are worthy of receiving the support. The finding may be explained by attachmentanxious individuals being more focused on relationship maintenance than individual goal pursuit. As such they may perceive their partners also as less supportive. Furthermore, there were five categories of variables that did not reliably predict perceived support: partner similarity, individual differences, individual and relational demographics, individual well-being, and all partner variables. Understanding which variables are not that influential in predicting perceived partner support is important, so that researchers do not spend unnecessary time and resources on examining these variables and can instead focus on variables that are central to perceived partner support. There are several variables (e.g., inclusion of other in the self, gender, goal correspondence, regulatory focus, self-esteem, and self-efficacy) within these broader categories that would be expected to theoretically predict perceived partner support but, when compared against more central predictors, are not that important. Finally, in line with previous research , we found that, whereas partnerreports explained a small amount of variance across outcomes, they did not explain any variance over and above actor-reports, did not predict much variance in the outcome, and even made the prediction worse longitudinally. Together, these findings can help constrain relationship theories to focus more on variables that are central to perceived partner support. Overall, we show that perceived partner support is shaped by the environment in which it occurs (i.e., the relationship itself), suggesting that, to understand perceived partner support, we need to understand the quality of the relationship. We are likely to learn less by examining individual factors, with the exception of attachment styles, which are perhaps the most relational of the individual differences variables in this literature. These findings support attachment theory's notion that, when partners have created a safe relationship environment, each partner feels they can count on the other to provide support when needed and thus perceive each other as supportive. Thus, our findings show that, if we want to better understand relationship processes, we need to look into the relationships which shape these processes and in which these processes are enacted. Strengths, Limitations, and Future Directions This study provides a comprehensive examination of selfreport predictors associated with perceived partner support. The study has several strengths, including the use of explainable machine learning, cross-validation, data across five studies, prediction over time, and examination of both actor and partner effects. Our findings shed light onto which variables future research should focus on and which are perhaps not worthy of further study. However, there are several features that limit the generalizability of the findings. Most couples were relatively satisfied, mixed sex, primarily White, middle class, and from North America. Thus, some demographic variables may have not accounted for much of the variance because of the lack of variability in the samples. Future research is therefore needed in more diverse samples to examine whether the variables found important in the Western, satisfied samples generalize beyond these samples. For example, goal congruence was important in some analyses but not in others. Goal congruence was high in these samples and thus may only become important when goal congruence between partners is very low, leading to more conflict. This is supported by general relationship conflict being central to perceived partner support across the analyses. Furthermore, we were unable to account for all variables that may be associated with support (e.g., personality, the dark triad) due to lack of availability of some measures in the preexisting data sets. Therefore, there may be other variables that are equally important and could help improve the predictive ability of the models. For example, we only included self- and partner-report data predicting perceived support and were thus unable to examine observed support behaviors or enacted support. The variables in this study were shown to be important for overall levels of perceived partner support but there may be other factors in the moment-to-moment support exchanges or enacted support that we were unable to measure in this study. Future research will be important to understand whether the same variables are predictive of momentary support interactions or enacted support, or whether there are other factors that are more central in these situations. In addition, while the machine learning used in the study accounts for functional form misspecification, it does not account for potential structure in the data (e.g., mediators). Thus, our results cannot be used to make causal conclusions. Finally, cross-sectional analyses are susceptible to shared method variance, meaning that the correlations are higher due to the measurement method rather than real correlations. We circumvented this issue by including longitudinal analyses, but baseline analyses should be considered with this in mind. Conclusion In conclusion, this study provided novel insights into which self-report variables are the most (and least) likely to contribute to perception of support by using state-of-the-art explainable machine learning. Our novel results advanced the literature by showing that relationship variables and attachment avoidance hold as central to perceived partner support, whereas partner similarity, other individual differences, individual well-being, and demographics are not important to perceiving partners as supportive. The findings are crucial in constraining and further developing our theories on perceived partner support and suggest that perceived partner support is shaped by the relationship environment.\",\"577240425\":\"Introduction Political and social divisions, amplified by Internet commentary and social media, have saturated popular culture with unflattering depictions of racial and ethnic minorities. Cable and network news have disproportionately portrayed African Americans as perpetrators of crimes and White Americans as law enforcement officers or victims. Meanwhile, Latinos have been overrepresented in critical discussions about undocumented immigrants  and are almost twice as likely as Whites to appear in newspaper articles about social problems. Moreover, these biased patterns of representation are not limited to traditional media. Latinos are often depicted as conforming to negative ethnic stereotypes on YouTube, and content with stereotypical depictions of Racial and ethnic groups receives more views and comments. On Twitter, Latinos are less visible than other groups, which likely hinders their ability to counteract falsehoods spread about them. Such misrepresentations in media are rooted in historically ingrained stereotypes that devalue and derogate racial and ethnic minorities. Understandably, these caricatures make many minority group members feel that they are viewed poorly by society at large. As a result, encountering negative stereotypes can be a source of stigmarelated stress for minority individuals. Substantial work has examined detrimental effects of stigma-related stress on performance in a stigmatized domain. However, little research has examined lingering effects of stereotype exposure on unrelated neural and psychological processes. In the present research, we bring together methods and theorizing from social psychology and affective and clinical neuroscience to examine whether stigmatizing negative stereotypes in the media have 'spill-over' effects on neural systems related to incentive processing and motivated behavior. The deflating effect of negative stereotypes Members of disadvantaged social groups not only contend with stressors that people in non-disadvantaged groups face (e.g. illness, the loss of a loved one, relationship conflicts), but they also confront stressors that are unique to their place in the social hierarchy. Many of these unique stressors emerge from structural inequities in legal, healthcare and financial systems that bias the distribution of socioeconomic resources. However, social devaluation also creates less tangible psychological stressors, such as stigmatizing negative stereotypes, personal experiences with discrimination and expectations of bias, which can strain physiology and harm mental health. Stigmatization via media sources is particularly pernicious because it can spread widely and affect members of targeted groups who have not personally experienced discrimination. Many members of disadvantaged groups are able to maintain positive self-esteem and group image in the face of stigma by attributing away specific instances of discrimination. However, continually managing self-definition and avoiding psychological harm from stigma is cognitively and emotionally taxing. The toll that these racial stressors take on African Americans and Latinos in the United States has been called 'racial battle fatigue'  and compared to feelings of resignation and withdrawal that occur when people experience other persistent stressors that are beyond their direct control. Stress and disrupted neural processing of rewards The 'weathering' effects of stress are well-recognized by researchers who study depression. Experiencing stressful life events is one of the most robust predictors of depression and is a risk factor for poor response to treatment and relapse. Stress is particularly associated with anhedonic depression, which is characterized by a reduced sensitivity to reward. Considerable affective and clinical neuroscience research has examined the neural substrates of reward processing as well as the moderating effects of stress. In humans, neuroscience research on reward processing has focused on the dopamine-rich nucleus accumbens (NAcc) in addition to the ventral medial prefrontal cortex (VMPFC) . Consistent with stress-related reward processing deficits in depression, anhedonic depression in adolescents is associated with reduced volume of the NAcc. Critically, the NAcc and the VMPFC seem to have dissociable roles in reward processing. Berridge (2009) makes a distinction between anticipatory ('wanting') and consummatory ('liking') phases of reward processing. The anticipatory phase, which is thought to engage the NAcc, involves the incentive salience of a cue that signals the possibility of a reward. The consummatory phase, which engages both the NAcc and VMPFC, refers to the experience of receiving a reward. Surprisingly, only a few fMRI studies have directly examined how stress influences the anticipation and consumption of rewards. Dillon et al. (2009) reported that early life adversity was related to reduced basal ganglia activation during reward anticipation, consistent with stress dampening sensitivity to reward incentive cues. However, work with clinically depressed participants and also non-depressed individuals exposed to acute stress has reported blunted NAcc response during the consummatory phase. Thus, it appears that stress spills over to influence the response of the NAcc to rewards, but whether this effect occurs at anticipation or consumption might depend on specific attributes of the stress experience. Current study The present research examined whether stigma-related negative stereotypes act like other stressors to alter how the brain processes subsequent rewards. In part because of stigmatizing rhetoric toward Latinos in the United States, 63% of whom are Mexican American , we focused on stigma experienced by Mexican Americans. We specifically recruited college students in our research because higher education is a vehicle for social mobility, but Latino college students are often subjected to stigmatizing negative stereotypes that can undermine motivation and achievement. In our research, Mexican American participants were randomly assigned to view either stigmatizing or non-stigmatizing video clips in rapid succession. To maximize ecological validity, all the video clips were from actual news stories or documentary-style recordings that were available on the Internet. To measure effects of the video clips on neural processing of the different phases of reward processing, participants completed a monetary incentive delay (MID) task  in an MRI scanner (Figure 1). The MID task has been used widely in affective and clinical neuroscience to assess how the brain anticipates and responds to (i.e. consumes) monetary gains and losses. Based upon contemporary work on the neural impact of stressful experiences on incentive processing, we offer several hypotheses regarding the effects of the present experimental manipulation. We predicted that participants in the stigma condition would show reduced subjective arousal, which would provide evidence for a 'weathering' effect of repeated exposure to negative stereotypes. We also expected stigmatized participants, relative to non-stigmatized participants, to demonstrate lower overall NAcc activity when processing rewards. We further expected that patterns of activity within the NAcc would differ between the stigma and control conditions, which would suggest that representations of rewards can be influenced by recent exposure to negative stereotypes. We did not have a priori hypotheses about whether stigma-related NAcc effects would specifically influence the anticipation or consumption phases of reward processing because past research on stress and reward processing does not provide clear predictions in this regard. We also were not sure whether the stigma manipulation would increase self-reported feelings of negativity and subjugation because past research does not support conventional wisdom that stigma should undermine self-esteem and image about one's group. Methods Participants Forty self-identified Mexican American participants were recruited from the psychology research subject pool at University of California, Santa Barbara (UCSB), and from other campus groups. All participants were right-handed and had normal or corrected-to-normal vision. Exclusion criteria included selfreported psychiatric or neurological disorders, the use of psychoactive medications or drugs and metallic or electronic items embedded in the body that would render MR scanning unsafe. All participants were compensated $40 for their contribution to this research. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Data from four participants were not included in analyses due to excessive motion (greater than 2 mm translation or 2 degrees rotation per run) or partial acquisition failure (final N = 36, 18 per experimental condition). Experimental manipulation After completion of the informed consent process, participants were randomly assigned either to the stigma condition or to the control condition. This assignment determined the nature of the experimental manipulation to which participants were later subjected, but did not otherwise affect experimental procedures. The experimental manipulation in both conditions involved the presentation of eight brief (2-3 min) video clips focused on four topic areas: (i) childhood obesity, (ii) high school dropout rates, (iii) gang-related violence and (iv) teen pregnancy. These topics were selected because they are issues of considerable social importance in contemporary American society and can be described either as predominately affecting Mexican Americans and other Latinos (consistent with common negative stereotypes) or as diffusely affecting American society as a whole. The video clips were introduced as stimulus materials unrelated to the MID task, which the experimenters wished to evaluate for use in future studies. Video clips in the stigma condition focused on the prevalence of each social problem in the Latino community and the effects of these problems on Latinos specifically. Clips in this condition also emphasized negative social comparisons with other portions of American society, suggesting that Latinos were uniquely susceptible and disproportionately affected. Because these issues are associated with various stereotypes of Mexican Americans , presenting the clips in rapid succession was intended to create a stigma-related fatiguing sensation in our Mexican American participants. In contrast, video clips in the control condition focused on the prevalence of a given social problem and its deleterious effects within American society broadly construed, without focusing on any particular racial or ethnic community. Video clips in the stigma and control conditions were otherwise approximately matched for duration, content and tone. Importantly, while the content of the video clips was evocative in both conditions, in neither condition were the selected video clips polemical (i.e. video clips did not assign blame or strongly advocate specific policy solutions). Rather, the clips highlighted the pervasive nature of the social problems depicted and the adverse effects experienced by individuals (either within the Latino community in the United States or within American society in general). Prior to watching the video clips, and again before responding to questions on their personal experience with the depicted social problems, participants were reminded of their Mexican American (or American) identity. In addition to watching the video clips, participants were asked to respond to a series of questions using a scannercompatible button box. After being reminded of their Mexican American (or American) identity, participants indicated, for each topic, whether they had been affected by the associated social problem (e.g. 'Did you suffer from childhood obesity?') because they were Mexican American (stigma condition) or American (control condition). For each item, participants were provided with two response options: 'Yes' and \\\"No\\/I'd rather not say.\\\" Thus, participants were afforded the opportunity to unambiguously affirm that they had been affected by the focal problem, but not afforded the opportunity to unambiguously deny that they had been affected. For participants in the stigma condition, this aspect of the manipulation prevented participants from distancing themselves from the stereotype-consistent content in the videos. Participants were also asked to state their current valence ('How positive\\/negative do you feel?'), arousal ('How calm\\/excited do you feel?') and dominance ('How dominated\\/dominant do you feel?') using on-screen Self-Assessment Manikin scales (scales ranged from 1 to 9, with higher values indicating more positive valence, more excited arousal and perceptions of the self as more dominant than dominated). These questions were intended to indirectly examine participants' selfreported reactions to the experimental manipulation they experienced. In both conditions, clips were viewed at two time points (T1, T2). T1 occurred prior to the first functional run (during the acquisition of structural images) and T2 occurred prior to run four (halfway through the MID task). At each time point, participants viewed four video clips from their assigned condition (one from each topic category), for a total of eight presentations (two clips for each topic). No video clip was presented more than once per participant. Subject to these constraints, video clips were presented in a randomized order. Participants did not know the content areas of the videos beforehand, nor did they know how many video clips would be presented. All video clip stimulus materials are available upon request. Monetary incentive delay (MID) task While undergoing functional magnetic resonance imaging (fMRI), participants completed an adapted version of the monetary incentive delay task introduced by Knutson et al. (2000). In this task, participants have the opportunity to win money or avoid losing money by making a rapid button-press response to a briefly presented visual target. Across 6 separate functional runs, participants completed a total of 180 trials divided into 6 different conditions (30 each). On win trials, a successful response would result in a monetary gain--whereas on lose trials, a successful response would avoid a possible monetary loss. On both types of trials, a button-press response was required, and failure to respond resulted in a worse outcome (failure to gain money for win trials and loss of money for lose trials). In addition, the amount of monetary reward\\/loss at stake varied across trials in three levels of compensation: $0, $1 and $5. Crossing the two levels of win\\/lose with the three levels of compensation resulted in six total conditions (e.g. win $5, lose $1, etc.). Trials from each condition were equally represented across the six functional runs and were presented in a pseudorandomized order designed to maximize contrast efficiency. Analyses focus on the anticipation phase (in which participants prepare to make the required button-press response) and the feedback phase (i.e. consummatory phase in which participants learn whether they have gained or lost money). See the Supplementary Materials for details regarding trial structure. FMRI data acquisition All imaging data were acquired using a 3.0-Tesla Siemens Prisma scanner at the UCSB Brain Imaging Center. Across six functional runs, approximately 3420 T2*-weighted echo-planar images (EPI) were acquired (~570 per run) during completion of the MID task described above, with interleaved acquisitions using a multiband acceleration factor of 8 (slice thickness = 2 mm, gap = 0 mm, 72 slices, TR = 720 ms, TE = 37 ms, flip angle = 52*, matrix = 104 x 104, field of view = 208 mm, phase encoding: anterior-to-posterior). An oblique slice angle was used in order to minimize signal dropout in ventral medial portions of the brain. A gradient echo field mapping image was acquired for each participant (slice thickness = 2 mm, gap = 0 mm, 72 slices, TR = 758 ms, TE = 4.92 ms, flip angle = 60*, matrix = 104 x 104, field of view = 208 mm), with the same slice angle as the EPI images. We also acquired a T1-weighted magnetically prepared rapid acquisition gradient echo anatomical image (slice thickness = 0.94 mm, 208 slices, TR = 2500 ms, TE = 2.22 ms, flip angle = 7*, field of view = 241 mm) during presentation of the first set of video clips and a T2weighted anatomical image (slice thickness = 0.94 mm, 208 slices, TR = 3200 ms, TE = 566 ms, field of view = 241 mm) during presentation of the second set of video clips. Lastly, a diffusion spectrum image (slice thickness = 1.8 mm, 78 slices, TR = 4300 ms, TE = 100.20 ms, flip angle = 90*, field of view = 259 mm) was acquired, but is not analyzed herein. FMRI data preprocessing Structural and functional data were processed using SPM (Wellcome Department of Cognitive Neurology, London, UK) . Within each functional run, image volumes were realigned to correct for head motion (using a rigid body transformation with 6 degrees of freedom) and unwarped (based upon the anatomical gradient field mapping images acquired for each participant). Images were subsequently segmented by tissue type (based upon the T1 and T2 images acquired for each participant) and normalized into standard MNI stereotactic space (resampled at 2x2x2mm). Finally, functional images were smoothed with a 6 mm Gaussian kernel, FWHM. Region of interest (ROI) definition for univariate and multivariate analyses Region of interest (ROI) analyses were conducted to directly assess the recruitment of specific reward-related brain regions during the MID task. A priori ROIs were employed in order to test for group differences in hemodynamic response following the stigma or control video manipulation in regions associated with reward anticipation and consumption in prior studies. In addition, pattern classification techniques were deployed using data from these ROIs in order to determine whether differences in multivariate patterns of hemodynamic response following the stigma manipulation would be sufficient for accurate classification of experimental condition. For these purposes, a priori ROIs were derived from automated meta-analysis through www.neu rosynth.org , using association test masks associated with the term 'reward.' A priori ROIs were derived from association test peak voxels in the right (MNI 12, 10, -8) and left (MNI -12, 10, -8) nucleus accumbens (NAcc), as well as in the ventral medial prefrontal cortex (VMPFC, MNI 2, 58, -8). Eight millimeter radius spheres were defined around each of these peaks and resliced into the 2x2x2 voxel space. An additional functional ROI was defined in early visual cortex based upon whole-brain analysis, as described in the Supplementary Materials. As this early visual cortex ROI was not specified a priori, analysis of data from this region must be considered exploratory. For univariate analyses, parameter estimates from the models described above were extracted from all ROIs using MarsBaR  for statistical comparisons. Parameter estimates from each participant's first-level models were evaluated at the group-level using general mixed-effects models implemented using the lme4 package  in the R statistical software (http:\\/\\/www.r-project.org\\/). The primary factors that were considered in univariate analyses were the compensation level (zero or non-zero), win\\/lose framing of incentives and the experimental condition (stigmatized or non-stigmatized). For trials during the feedback phase, outcome (success or failure) was also considered. Sequential mixed-effects models were employed to avoid over-fitting data with too many explanatory variables, given the limited sample size. Univariate analytic approach General linear models (GLMs) were defined for each participant, in which trials were modeled with separate functions corresponding to (i) the anticipation phase (corresponding to the fixation period prior to target presentation), (ii) the target phase (corresponding to the temporal window in which the target 'star' appeared and successful button presses occurred) and (iii) the feedback phase (corresponding to the final portion of each trial, in which success or failure feedback was presented to participants). The anticipation and target phases were modeled as variable-duration epochs (as their precise timing differed from trial to trial as well as across participants), while the feedback phase was modeled as a fixed duration epoch lasting 2 s. Each task phase was convolved with the canonical (double gamma) hemodynamic response function. All models controlled for six estimated motion parameters (three translations and three rotations), as well as linear trends and differences between runs. The time series was high-pass filtered using a cutoff period of 128 s, and serial autocorrelations were modeled as an autoregressive AR(1) process. Individuallevel statistics were aggregated for group-level comparisons and evaluated with a mixed-effects model. For whole-brain analyses, correction for multiple comparisons was implemented based upon Gaussian random field theory, in order to yield a cluster family-wise error (FWE) of P < 0.05 based upon an initial (voxelwise) cluster-formation threshold of P < 0.001. See Supplementary Materials for further details. Multivariate analytic approach In order to conduct multivariate pattern analysis of brain imaging data from the task, a series of beta images were first derived from univariate general linear models (; see Supplementary Materials). Multivariate classification analyses were conducted using scikit-learn version 0.20.2  pipelines employing three steps: (i) missing voxel imputation, (ii) standard scaling and (iii) classification using support vector machines (SVM) with a radial basis function (RBF) kernel. Importantly, each of the above steps was performed within cross-validation folds for each analysis so that there would be no data leakage from the training dataset to the testing dataset. Missing voxel imputation replaced missing values with the mean value from each voxel. Standard scaling involved mean-centering the data from each voxel and dividing by the voxel's standard deviation. Finally, SVM classification assigned a class label to each case (i.e. trial) within the dataset, using a radial basis function with default cost and gamma hyperparameters (C = 1 and gamma = 1\\/n features). Cross-validation accuracy was assessed using a 'leave-onepair-out' strategy. First, participants were randomly assigned to pairs for each classification, with one member of each pair in the stigma condition and one member in the control condition. As each of the 36 participants completed 180 trials (over the course of all functional runs), this procedure yields 18 crossvalidation folds each of which includes a testing dataset with 360 (180 x 2) cases equally divided between stigma and control participants. Importantly, to successfully classify cases (trials) in the testing dataset on each fold, the classification algorithm must generalize relationships between features (voxels) and stigma\\/control category labels, learned from the training set, to a test set consisting of data from participants that are entirely novel (for that cross-validation fold). A leave-one-subject-out cross-validation strategy could have been used but would have led to a maximally unbalanced distribution of class labels in the test dataset, as all trials for a single subject share a common stigma\\/control class. With the leave-one-pair-out strategy, class labels are balanced for both training and testing, and crucially, no participant is included in both training and testing datasets for any given fold. Separate randomized pairings were performed for each classification analysis, in order to minimize the possibility that specific pairings could help or hinder classification. Besides distinguishing between stigma and control trials, other classification analyses were independently performed to distinguish between levels of compensation, whether trials afforded possibility of gain or of loss and (for feedback only) whether participants succeeded or failed. These classifications also employed the leave-one-pair-out cross-validation strategy for consistency, although in such cases this strategy is not necessary to ensure class balance. For multi-class classifications (i.e. with more than two categories), a one-vs-rest approach was used for the decision function. Permutation testing was employed to determine whether cross-validation classifier accuracy differed significantly from chance. For each permutation, dataset class labels were randomly permuted within cross-validation pairs, and the classification algorithm was repeated. The P-value was then estimated from the proportion of permutations for which classifier accuracy exceeded the accuracy with non-permuted (true) labels. Results Subjective responses to the manipulation The effects of the stigma manipulation were assessed at two time points: (T1) after the presentation of the first set of four video clips, prior to the start of the MID task, and (T2) after the presentation of the second set of four video clips, halfway through completion of the MID task (i.e. after three of six functional runs). On-screen queries assessed subjective experience related to arousal, valence and dominance (see Methods above). Valence and dominance did not show any differences across conditions at either time point (Ps = 0.324). However, at T1, participants in the stigma condition showed significantly reduced arousal relative to non-stigmatized controls (t = -2.668, P = 0.0122). Moreover, for participants in the stigma condition, arousal at T1 was significantly below the midpoint of the scale (M = 3.72 vs midpoint scale value 5; t = -4.10, P = 0.0007). For non-stigmatized controls at T1, arousal was slightly (non-significantly) above the midpoint (M = 5.22 versus 5; t = 0.474, P = 0.641). At T2, arousal for stigmatized participants was also below the scale midpoint, but this effect was only marginally statistically significant (M = 4.11; t = -2.10, P = 0.0513). For non-stigmatized controls, activation was slightly (non-significantly) below the midpoint at T2 (M = 4.44, t = -1.317, P = 0.205). Mean responses for the measures are plotted with standard errors of the mean in Figure 2A. Results of self-reported affective responses suggest that stigmatizing media messages had an effect on the arousal of participants. Specifically, participants subjected to stigmatizing materials seem to have begun the MID task with lower arousal than participants who viewed non-stigmatizing materials, consistent with a fatiguing effect of stigma. Although at T2 there were not statistically significant arousal differences between the stigma and control participants, this effect seems to be driven by a reduction of arousal for the control participants at T2. It is possible that the controls reported less arousal at T2 compared to T1 simply because they had been in the scanner for a prolonged period of time. For the stigmatized participants, the T1 and T2 levels were not significantly different from each other, and both means were below the midpoint of the scale. It is unclear why there were no effects of the stigma condition on valence and dominance, but this result is consistent with work showing that people who experience stigma do not report lower self-esteem or regard for one's social group. Performance during the monetary incentive delay task On average, participants performed slightly below the target two-third success rate, with an overall mean success of 62.1% across all runs of the MID task and all conditions, 95% CI [61.3%, 63.1%] (see Figure 2B). Overall performance did not depend upon the condition of the participants, with stigmatized participants' mean success rate at 62.7% and non-stigmatized participants' mean success rate at 61.7% (t = -1.12, P = 0.27). Aggregating across participants, a mixed-effects model with win\\/lose and compensation as fixed effects (and with participant as a random effect) found no significant effects (win\\/lose: F = 0.942, P = 0.333, compensation: F = 0.492, P = 0.612). The MID task script was coded so as to update (increment\\/decrement) the target response window based upon participant performance independently for each combination of win\\/lose and compensation levels, so no significant differences in performance based upon these factors indicate that the task operated as designed. On average, participants responded during the target phase with a reaction time (RT) of 302 ms across all runs of the MID task and all conditions, 95% CI [0.285, 0.319] (see Figure 2C). Overall RT did not depend upon the condition of the participants, with stigmatized participants' mean RT at 298 ms and nonstigmatized participants' mean RT at 307 ms (t = 0.510, P = 0.614). A mixed-effects model with win\\/lose and compensation as fixed effects (and with participant as a random effect) was employed to assess the impact of these factors on reaction time. Win\\/lose did not have a significant influence on RT (F = 1.583, P = 0.210). However, the level of compensation had a strong effect on RT (F = 28.629, P < 0.001), with greater compensation associated with faster response. Models including interaction terms between win\\/lose and compensation, as well as between both factors and stigma condition, did not find any significant interactions (all P > 0.05). Overall, the correlation between mean RT and mean success rate was not significant (r = -0.188, P = 0.278). Together, the RT data suggest that participants in both conditions approached the task similarly. Evidence for the canonical NAcc\\/VMPFC responses to the MID task Before examining the effects of our manipulation, we began by conducting a traditional univariate analysis in our NAcc and VMPFC ROIs. To our knowledge, this is the first time the MID task has been used with an exclusively Mexican American sample, so it is important to verify that NAcc and VMPFC show MID task responses expected from the literature in this subpopulation. Mixed-effects models (with participants as random effects) were employed to evaluate the effects of factors of interest on the BOLD signal in these regions during the anticipation and feedback phases. All runs were analyzed in general linear models together. All of these univariate results are reported in Table 1. In general, the univariate analyses confirmed that NAcc and the VMPFC were sensitive to the magnitude of the incentives and feedback, results that are consistent with the MID literature and validated our use of this task (see Figure 3 and Supplementary Figures S3 and S4 for ROI effects as well as Supplementary Figures S1 and S2 and Supplementary Tables S1 and S2 for wholebrain effects). These results demonstrate that the canonical effects in the literature do generalize to a Mexican American sample. Moreover, consistent with a blunting effect of negative stereotypes on incentive processing, the mean response in the left NAcc ROI was lower for the stigma vs control condition, although this effect was only marginally significant. Interestingly, this effect was moderated by magnitude of compensation but not win or loss frame and only during the anticipation phase. There were no effects of condition on the VMPFC response at either phase. Although the univariate analyses suggest that stigma may have influenced the NAcc response during reward anticipation, we are cautious about interpreting the condition effects because relevant P-values were marginally significant and only one exceeded the 0.05 significance level. Multivariate patterns differentiate stigmatized and non-stigmatized participants Contemporary research has increasingly employed multivariate methods in the decoding and analysis of incentive-related brain processes. These methods are sensitive to differences in hemodynamic response patterns that vary within and between regions on a variety of different spatial scales. Importantly, research on reward processing using MVPA decoding suggests that multivariate patterns reflect more than the mere value of a stimulus: they can also encode information about the identity of the expected outcome, the actions necessary to achieve that outcome and contextual factors related to the appraisal of incentives. For this reason, we conducted a multivariate pattern classification analysis of incentive processing during anticipation and feedback, with the goal of determining whether information in patterns of activity in NAcc would differ based upon stereotype exposure. According to this analysis, patterns of activity within the NAcc significantly differentiated stigmatized and control participants during the critical anticipation period. This pattern classification result replicated in both the right and left NAcc, although accuracy was greater in the right NAcc (left NAcc accuracy = 55.5%, P < 0.0099, right NAcc accuracy = 59.8%, P < 0.0099, Figure 3). Whether the incentives were framed as gains or losses did not moderate these effects. To examine whether the pattern classification results at anticipation could be attributed specifically to incentive processing, we examined a region in auditory cortex as a control, given that the task was visual and not auditory in nature. The auditory cortex could not classify stigmatized from nonstigmatized participants during anticipation (accuracy = 50.2%, P < 0.406). The results also appear to be temporally specific, emerging during anticipation of incentives but not during subsequent feedback. Pattern classification did not distinguish participants by condition during feedback (left NAcc accuracy = 48.6%, P < 0.956, right NAcc accuracy = 50.5%, P < 0.317). The pattern classification analysis of VMPFC voxels showed no significant effect of condition, either at anticipation or at feedback. Although we focused our analyses on the NAcc and VMPFC because of empirical work connecting these regions to incentive processing, an exploratory univariate whole-brain analysis indicated that the early visual cortex also differed between the stigma and control conditions (Supplementary Figure S5). During the feedback phase, signal in the early visual cortex was higher in control versus stigmatized participants (right lingual gyrus, peak t = 5.088, cluster FWE P < 0.05, k=260; see Supplementary Table S3 and Supplementary Figure S5). Pattern classification analyses also indicated that this region of the early visual cortex could classify stigmatized from non-stigmatized participants at feedback (accuracy = 53.2%, P < 0.0099). This early visual cortex effect was not expected, so we are hesitant to interpret this result. It is possible, however, that stigmatized participants were not encoding feedback to the same extent as non-stigmatized participants because of the potential threat-confirming nature of adverse feedback. Discussion Our results provide evidence that exposure to negative stereotypes about one's group can spill over and influence neural processing of incentives and suggest that such effects may be both regionally and temporally specific. We found that exposure to negative stereotypes bilaterally influenced patterns of responding within the NAcc during the anticipation of incentives. Moreover, this finding was not moderated by gain vs loss frames. There was no evidence that the VMPFC response at anticipation or feedback was influenced by stereotype exposure. Given that clinical disorders, such as major depressive disorder, are characterized by dysfunctional incentive processing, our work suggests that stereotype exposure effects on the NAcc when anticipating incentives could be important for understanding mental health disparities experienced by negatively stereotyped groups. It is particularly notable that the NAcc effects of stigma during the anticipation phase pertained to monetary incentives generally and were not specific to gains. Our research was motivated by existing work showing that stress blunts reward processing, so we did not predict loss trials to show similar effects as gain trials. In fact, loss trials were included in our design because they are a standard part of the MID task that we used. However, in hindsight, our results are consistent with a recent meta-analysis that found that the NAcc is responsive to both positively and negatively valenced incentives. Moreover, the finding that negative stereotype exposure influences NAcc activity to the anticipation of both gains and losses is in line with theorizing that depression is characterized by a general disengagement from the environment, resulting in blunted sensitivity to both positive and negative stimuli. Contrary to our hypothesis, exposure to negative stereotypes did not robustly blunt the overall NAcc reactivity to incentivizing cues, although the magnitude of the response in this region was marginally lower in the stigma condition. While we hesitate to interpret non-significant results, it is possible that stronger evidence for the predicted affective blunting might be obtained using a considerably larger sample size and\\/or a more potent exposure to aversive stereotypes. Conclusions regarding the generality of our effects should be tempered both with respect to the populations considered and the nature of stigmatizing stimuli. Our Mexican American student participants represent only one group of Latino demographic, albeit a demographic that is critical to closing socioeconomic gaps in America. Additionally, the stereotype exposure in this research represents only one type of ethnicityrelated stressor that disadvantaged minorities experience. It is possible that stressors not investigated in this research, such as hate speech, elicit different affective responses from those reported here, and thus, the downstream effects on incentive processing might not be the same. It is also unknown how our participants' past experience with stigma and discrimination influenced their response to the stigma manipulation. Future large-scale studies powered for investigating individual differences would provide insight into how effects of negative stereotype exposure on neural processing of incentives are modulated by a variety of life experiences and coping strategies. Additional research should also explore whether non-Latino individuals demonstrate a neural response similar to that of our Mexican American participants when exposed to negative stereotypes about Latinos. It is possible that non-Latinos with high egalitarian values would demonstrate similar responses as our participants; however, it is also possible that the responses would be different because of ethnicity-related differences in lived experience. Given these limitations, our work should be viewed simply as a proof of principle that negative stereotype exposure can influence representations of rewards and costs in the NAcc. Although it is well established that stress influences incentive processing, the ways that marginalized groups cope with stigmatizing experiences and the impact of these experiences on incentive processing remain understudied. Incentive processing is crucial to many life outcomes, including financial decisionmaking, academic achievement, job performance, depressive disorders, drug addiction and health behaviors. Thus, delineating the association between stigmatization and incentive processing may help advance understanding of psychological mechanisms that perpetuate societal disadvantage.\",\"577240432\":\"Introduction In popular media and academic circles, video games in general and those with violent content in particular have raised considerable concern regarding their influence on behavior. Scholars, leaders, policy makers, and others have sought to ban, boycott, or otherwise remove violent video games from store shelves due to their perceived relationship to aggression and violence. For example, in 2014, Australian retailers banned the sale of a particularly violent video game under the auspice that it may increase aggression and violence as players mimicked the actions shown in the game. Essentially, the retailers argued that the games created a script for people to follow. All of these responses to video games have arisen out of the argument that there is a significant link between video game violence, individual aggression, and ultimately societal violence due to video game play. More recently, there has been considerable attention regarding this claim from the American Psychological Association in the form of special issues and research requests. In addition to the attention from academic circles, multiple western governments have examined a large cross section of empirical studies to attempt to generate legislation to resolve the question of the relationship between video games and violence. However, upon closer examination of the extant research, policy makers have decided and via the courts ruled that the research into the link between video games and aggression\\/violence is inconsistent and methodologically flawed. Quoting the United States Supreme Court, \\\"Studies purporting to show a connection between exposure to violent video games content and harmful effects on children do no prove that such exposure causes minors to act aggressively.\\\" In essence, the studies linking video game play and violence have been rejected by every court to consider them. This indictment of psychological, educational, and sociological research sets the conditions for new research methods for measuring and assessing the impact of violent media such as video games on children and adolescents, specifically, from a cognitive perspective. Unfortunately, as it stands there are few tools available to researchers other than self-reporting and secondary observation instruments to measure the relationship between video games and aggression. The authors of this study propose the use of a new class of research tools making use of computational models to allow examination of this question. Purpose, research question, and hypothesis The primary purpose of this study is to computationally model and compare the General Aggression Model (GAM) to the Diathesis-Stress Model (DSM) of Aggression as they relate to the play of violent content in video games. A secondary purpose is to provide a novel method of aggregating studies and data, measuring relationships in the data, and examining and testing models of individual aggression such as the GAM and DSM as they are influenced due to video game play. Research Question 1: Is it possible to computationally model the General Aggression Model (GAM) and the Diathesis-Stress Model (DSM) to capture the relationship between aggression and video game play? Research Question 2: Which of the proposed models, GAM or DSM, provide sufficient predictive value in assessing aggressive outcomes related to video game play? Research Question 3: Does the presence of violent video game content in each of the computational models of aggression produce increased aggressive tendencies in adolescents and children? Consideration of these research questions results in the following hypotheses: Hypothesis 1: Examination of outcomes related to each model will provide evidence of the incompleteness of each model GAM and DSM individually. Hypothesis 2: Comparison of the GAM and the DSM computational models will provide evidence of the role of aggression and suggest the presence of a combined model. Hypothesis 3: The presence of video game play will not result in statistically significant differences in aggression probabilities. Substantiation of the hypotheses will provide evidence, which characterizes the relationship between video game play and aggression. Results will also clarify and provide evidence for each model, GAM and DSM, helping to move the models from a descriptive model to a mechanism of action capable of being tested in future studies. The computational model will also provide a means for the examination of the role of video games in aggression and provide an approach to test and add to the theoretical foundations related to each of the models. Literature review While there are multiple aggression constructs available for examination within the realm of this study, the authors of this study limit their study of aggression to three forms. The forms of aggression covered in this study are proactive aggression, reactive aggression, and general aggression. Proactive aggression Proactive aggression is aggression that results from one's own preemptive acts of aggression. This form of aggression is motivated by personal gains, such as popularity or power. One may choose to engage in proactive aggression with the goal to steal, tease, scare, or otherwise coerce another for his or her own gain. However, proactive aggression is not usually followed by anger or loss of control like reactive aggression may be. A number of studies have been conducted in order to investigate the probable causes of proactive aggression in children. According to Merk, Orobio de Castro, Koops, and Matthys (2005), in the beginning of the behavior development in children, the children tend to exhibit reactive aggression during which the children discover that aggressive behavior can create positive results for them. As a result, the child links aggression with outcomes and this can lead to proactive aggression as a means to acquire a desired outcome. Researchers found that proactive aggression leading to violence is mainly correlated with a high need to control others. Tucker et al. (2014) conducted a study that indicated that proactive aggression could lead to increased risk for problem substance use and delinquent behavior. In a study conducted by Murray-Close and colleagues (2010), it was found that males displayed a higher likelihood to take part in peer-directed proactive aggression. Rathert, Pederson, Fite, Stoppelbein, and Greening (2015) conducted a study that indicated that parenting styles, age, and gender could serve as possible moderators for proactive aggression, as well as the symptoms of oppositional defiant disorder (ODD) and conduct disorder. Parenting styles had the greatest influence on the link between proactive aggression and symptoms of ODD for females and older children. Similarly, in a study conducted with 1164 preschool children in China, Jia, Wang, and Shi (2014) found that there was a significant correlation between hostile\\/ coercive parenting styles and low educational attainment for the father with aggression in children. This supplies evidence for the need to address male scholastic outcomes as educational institutions feminize often ignoring the educational growth and needs of boys (Roberts). However, none of these studies indicated a linkage between media and aggression through this was also explored. Additionally, studies have shown that genetics can play a role in one's predisposition for proactive aggression. According to Beitchman, Mik, Ehtesham, Douglas, and Kennedy (2004), the monoamine oxidase A highactivity variant (H) gene (MAOA-H), has been linked to aggression in boys. Manuck, Flory, Ferrell, Mann, and Muldoon (2000) also found that MAOA-H has been associated with aggression in adult men and may be part of the genetic basis for aggressive tendencies. According to Silva, Larm, Vitaro, Tremblay, and Hodgins (2012), an addition factor in the development of proactive aggression is the insensitivity to others emotional or physical well-being. In conclusion, it can be seen that a variety of factors, such as gender, emotional stability, parental influence, and genetics, can influence one's tendency for experiencing proactive aggression. Reactive aggression The second form of aggression examined in this study is that of reactive aggression. In order to understand risk factors and develop interventions that curb aggression and violence, researchers have attempted to develop models that identify and predict reactive aggression. The models developed by researchers have attempted to understand aggression in terms of the motivations that drive the behavior. The frustration-aggression model  hypothesized that frustration leads to anger. In this model, a perceived threatening stimulus gives way to some form of visible anger as a reactive outcome. Berowitz (1962) even goes on to say that aggression and hostility are synonymous. These two theoretical descriptions emphasize the instigators of the aggression such as interfering with or even blocking an expected goal  and are the foundation for information processing theories such as the GAM. Social learning theory  postulates that reactive aggression is an acquired behavior often controlled by observation and the environment with little input from genetic determinants, thus essentially ignoring biological factors. Bushman and Anderson (2001) have criticized these classifications of reactive aggression and suggest that aggressive acts are misrepresented in studies of aggression. Nevertheless, there continues to be an abundant amount of data supporting two major forms of aggression in children: reactive and proactive. According to Bushman and Anderson's description, most aggressive acts are not pure and are the result of mixed motives complicated by other competing interests and emotions. They believe that proactive aggression and reactive aggression are distinguished by multiple social variables  such as anger, motivation, and the role of premeditation, planning, and impulsivity. The models suggested by Bushman and Anderson (2001) do not address a genetic determinate, thus ignoring model such as the DSM and fields such as neuroscience which examine the biological correlates of aggression. Reactive aggression by definition represents aggressive behavior that results from an act that is perceived to be hostile or intentional, such as a child hitting another after being pushed. Theoretical explanations of reactive aggression are consistent with the frustration-aggression model. Reidy, Shelley-Tremblay, and Lilienfeld (2011) define reactive aggression as aggression that rises from provocation inducing anger and triggering an aggressive event. Thus, for a person engaging in reactive aggression, the main overall goal is for the individual to perform an aggressive act. Reactive aggression shows greater hostility than proactive aggression and children who exhibit reactive aggressive characteristics display more hostile attribution bias than other children. Reactive aggression has also been associated with higher levels, or feelings of anger, anxiety, depression , and more sadness than proactive aggression in social situations. Mechanisms that find relationships between negative emotions and reactive aggression have not been found as of yet. Both types of aggression are considered to have heterogeneous constructs meaning that the development of one type of aggression may not be in agreement with the other. However, there is evidence that there may be some overlap between the two constructs making differentiation difficult. Based on the information available from current research, Bushmann and Anderson theorized that aggressive acts in general may be caused by both reactive and proactive motivations acting in combination. Overall, the attempt to understand the motivation of aggressive behavior, and understand risk factors amongst our youth, can only aid interventions that curb aggression and violence. In addition, since we live in an era of constant media presence, where individuals are always \\\"plugged in,\\\" it may be beneficial to develop models that can assist in the determining if media and video games, which are so common in our society, have an effect on aggressive behavior. General Aggression Model The GAM is an attempt to explain aggression as a function of social-cognitive interactions. There is an underlying notion that exposure to aggression in real life and virtual environments will lead people to imitate the aggressive behaviors and become aggressive themselves. This notion, though not conclusively supported with empirical evidence, is discussed throughout the historical and current literature in psychology, education, and sociology as fact. Modern conceptions and models of the imitative nature of aggression arose from the Albert Bandura's Bobo doll experiment. It is the imitative nature of aggression, which underlies the GAM. In these experiments, children after observing adult aggression toward the doll modeled similar behaviors when presented with the doll. The results of these experiments are often used as the foundation for the development of ideas around social learning of aggression. These experiments ignore the presence of an adult prompting the children to engage with the doll. The prompting activity may have resulted in the children attempting to please the adult through mimicry. Further development of the Social Learning Theory of Aggression occurred through its combination with Social Cognitive Theory. The combination of these models incorporate elements of priming, in which the stimulus activates cognitive and\\/or affective networks resulting in arousal of specific behaviors. The arousal of specific behaviors can result in the imitation of aggressive behaviors in the form of passive, rigid, and mechanistic processes even though social imitation may not be the intent. Imitation behavior is innate and particularly prevalent in young humans. Within Social Cognitive Theory, long-term aggression influences cognitive attributes during development and activation of cognitive schemata. The interrelationship between the cognitive schemata and the behavioral outcome generates social scripts that individuals invariably follow as an automatic mechanism; that once activated continue to completion. In the GAM, much of the writing of the social script occurs subconsciously through social interaction and in the case of video game play is practiced without conscious thought. In this way, the GAM integrates multiple social theories of aggression, such as script and desensitization, social learning, and social cognition theory but does not incorporate biological actions and predispositions. The overview of the GAM is shown in Figure 1. Within GAM inputs and outcomes arise from structure and scripts stored in long-term memory. In this light, there is little accounting for individual traits associated with personality, genetics, and prior behaviors. To be explicit, GAM suggests that knowledge structures are implicitly social with little contributed from genetics. This means that each violent incident is cumulative and acts in the solidification of knowledge around aggression and violence. In this way, short-term video game violence acts as a primer, script, and trainer for real-world acts of violence. More importantly and concerning is that when biology and genetics are mentioned as a potential casual variable, they are routinely dismissed as some variation in social cognition and not pursued any further. This is particularly true in fields such as education. The authors of this computational study explicitly state and admit in their positionality, along with others, that they believe there are biological factors that operate on learning , decision-making, arousal, and affective processing. The authors also suggest this position is supported by modern neuropsychology and neurosciences, which are often ignored in favor of social construction in fields such as education and sociology. Recent examinations of GAM suggest the presence of biological factors without admitting that these biological factors play a role. This is concerning as education and in some cases psychology have moved so far into the social constructivist components of learning and aggression that there is at best, summary dismissal of anything examined with a biological basis, and at worst outright suppression of the discussion of the possibility of biological action on behavior. A second area of weakness in GAM, is that under GAM theory, all aggression is maladaptive, this is simply not the case. Aggression in the defense of one's person or family would hardly be maladaptive. Lastly, GAM theory lacks explanatory power to address biological maturation and its relationship to aggression. This ultimately places support of GAM as a complete model into a tenuous position of offering a moral guide without proper empirical evidence to support these claims. Effect size outcomes resulting from GAM studies are small to nonexistent creating further evidence of its lack of power to operate as a meaningful predicative theory of aggression. Within GAM, acute violent exposure through video game play primes aggressive social scripts generating hostile attributions that move from the virtual world to the real world and is largely automatic. The underlying assumption is that the human brain does not distinguish between virtual worlds and real worlds. This assumption is easily refuted in that children between the ages of 3 and 5 are able to distinguish between fantasy and reality environments with ease. Even brain imaging studies at best are inconclusive regarding the topic. The entire premise of acute exposure to violent video games again seems to rest on moral grounds and not empirically derived grounds. Figure 1 provides an overview of the GAM in relation to video game play. Diathesis-Stress Model The DSM attempts to explain aggressive behavior outcomes, as genetic or biological predisposed toward aggression, resulting in vulnerability, and stress inducement from environmental moderators interacting with one another. Aggressive behaviors occur among multiple age groups and are present in almost every species on Earth. This is not to suggest that aggression does not sometime indicate more serious socialization problems, but to provide evidence that the role of aggression is far more faceted than is recognized under GAM. Unfortunately, the etiology of these behaviors is not well understood from a cognitive or genetic perspective and is an ongoing area of research. Despite the lack of understanding related to aggression, cognitive theory has provided an alternative framework to study aggression and video games. Studies making use of this framework (DSM) often isolate and focus on either cognitive process or cognitive content without incorporation of other aspects and examination of the system of interactions. Evidence supports the prevalence of dysfunctional aggressive cognitive styles in aggressive children and adolescents resulting from a combination of genetic and developmental variations in combination with environmental triggers. In this way, researchers have attempted to link individual vulnerabilities with antecedent influences from video game play (i.e., environmental triggers). Young, Klosko, and Weishaar (2003) referencing schema theory suggest that stress experiences throughout one's life form the basis of noxious experiences that will trigger later in life via proximal stresses such as video game play. In this context, aggression schemas are latent and biological in basis, but once activated, can guide affect and behavior biasing thought processes resulting in aggressive outcomes in children. In this way, maladaptation of schema due to genetic predisposition is the underlying core cognitive and genetic vulnerability and not video games. In this model (DSM), video games act as a moderator of aggression not at the causal variable. Figure 2 provides an overview of the mechanisms of the DSM. Combined model The combined model is a theoretical model developed through the examination of overlap between the DSM and the GAM by the authors. This is a completely theoretical model and has not been tested or substantiated prior to this study. Within this study, the authors offer the combined model only as an alternative for comparison to the DSM and the GAM. Figure 3 provides an overview of the Combined GAM+DSM. Cognition and violence Cognition refers to the processing, knowing, and understanding the world around us. Huesmann (1998) refers to cognitive processes as a set of mechanisms that play a role in learning and make the process of knowing possible. Behavior is defined as an individual's response to an action, the environment, a person, or stimulus. In Huitt's definition of cognition, a person's behavior is influenced by themselves through memory, another person's actions, or the surrounding environment. Essentially, an aggressive act or behavior is achieved through a five-step process. The steps of this process are as follows: (1) Encoding cues to begin the cognitive processing and interpretation of the encounter based on the cues; (2) A search in retrospective memory for previous experiences with the cues; (3) An examination of prospective memory for a behavioral response to see which responses are available; (4) Evaluation of the response to see if the individual is capable of performing the response; (5) Finally enactment of the response. This entire process relies heavily on memory and development of schemas as a means to develop aggressive behaviors. Although violence and aggression have a common cognitive link, this does not necessarily mean they arise and externalize in the same manner. One can be aggressive without being violent, but all violence exhibits aggression. Violence as defined in this study results in physical harm, whereas aggression behavior is directed toward another with the intent to disrupt, even if the disruption does not materialize. For example, a football player may be aggressive in his intent to disrupt the opposing player, but does not necessarily seek to cause harm. Many studies link violence and aggression together and treat them as synonymous; even using the terms interchangeably due to their highly correlated relationship. Violence in video games and violence Over the years, a number of researchers have attempted to demonstrate that video games have negative and harmful effect leading to aggression and ultimately to violence in those persons who played them. However, more recently, researchers have developed an opposing view regarding this issue of video games, aggression, and violence. Researchers holding opposing views suggest that the links between video games, aggression, and violence in real life are insubstantial in comparison to other influences that one may face during the developmental periods of childhood and adolescence. As stated by Anderson et al. (2003, p. 81), \\\"the scientific debate over whether media violence increases aggression and violence is essentially over.\\\" According to Ferguson, Olson, Kutner, and Warner (2014), there is a decrease in the youth violent crime rates over time, despite an experiential increase in the availability of violent video games. When Grand Theft Auto IV was released in 2008, it gave rise to a lot of negative feedback from politicians, journalists, and scholars since they were concerned that the video game could possibly lead to negative influences on youth. As a result, these concerns lead to 10 states in the United States passing laws with the goal of reducing youth access to violent video games. However, all of these laws were blocked or overturned by courts in the United States due to lack of empirical evidence directly linking games to aggression and violence. For example, the U.S. Supreme Court did not reach the conclusion that video games lead to violence, the court was unconvinced due to the lack of credible scientific evidence concerning this topic. A second example of this is seen in the decision that resulted from the court case of Brown v. EMA (2011). In this decision, the court arrived at the conclusion that the evidence provided by the state of California to prohibit the sale of violent video games to minors was not persuasive enough to warrant enactment. The court expressed that the state evidence was not successful in providing the strong evidence needed to support the claim that violent video games are a cause of real-life aggression. According to Hall et al. (2011), peer review studies in the fields of education, psychology, and sociology have been inadequate and not made use of sufficient research rigor to answer the question. Methodologically, the lack of rigor arises from the sampling techniques as a majority of psychological research studying the effects of violent video games is comprised of laboratory experiments with mainly college students as the study participants. These participants typically major in either psychology or communication and are not necessarily reflective of the wider population of video game players. The laboratory study participants in this research play either a nonviolent game or a violent video game, which is frequently a first-person shooter video game. Measurements of physiological arousal are then collected either prior to, directly following game play, or simultaneously during game play. Following data collection, the study participants are asked to retrospectively complete a test or a questionnaire in order to examine their aggressive behaviors, cognitions, and emotions. Results of the retrospective examination of aggression are then compared between the control and experimental groups. Ignoring the inherent problems with self-reporting questionnaires, retrospective analysis is often problematic and inaccurate. According to Elson and Ferguson (2014), other aspects related to aggression and violence in addition to displayed violence should be taken into account, when quantifying one's predispositions for aggressive thoughts. For instance, in a study conducted by Schmierbach (2010), they found that collaborative game play leads to lower tendencies for aggressive thoughts than when one plays video games alone or competitively. Additionally, Jerabeck and Ferguson (2013) conducted a similar study in which they investigated how violent video game play, when played alone versus cooperatively, could also discourage aggressive behavior and promote prosocial behavior. Additional studies found that when video games were played cooperatively, that cooperation resulted in less aggressive behavior regardless of whether or not the video games were violent. A secondary consideration is that there has been a deficiency of observable data in the natural setting (i.e., outside of the laboratory) to support the influence of playing violent video games on crime rates. In a study conducted by Baldaro et al. (2004), a significant effect could not be found on physical aggression, indirect hostility, irritability, negative thoughts, resentment, guilt, or suspicious activity when playing violent video games. Wallenius and Punamaki (2008) conducted a longitudinal study to assess the relationship between video game violence and direct aggression on adolescent participants, while controlling for the variables of age, sex, and parent-child communication. As a result, violent game play was not a significant predictor of direct aggression up to 2-years later. In a 3-year longitudinal study conducted by Ferguson, San Miguel, Garza, and Jerabeck (2012) with a sample of 165 Hispanic youth, no effects were found from violent video game play on aggressiveness, delinquency, or dating violence. However, results of the study indicated that the best predictors of aggression were a blend of biological and social factors. The authors of the study found that antisocial personality traits, depression, peer influence, and exposure to family violence were most predictive. Ferguson, Garza, Jerabeck, Ramos, and Galindo (2013) also conducted a study in which they did not find evidence to support the effect of violent video game play on youth violence and bullying 1-year later. Rather, it was found that bullying and delinquency were affected by the stress level and trait aggression of the child, which is more in line with the DSM. Ferguson and Olson (2014) conducted a second study with a sample of 377 children, mean age of 12.93 years, from mixed ethnicity backgrounds in order to learn if children who had preexisting mental health problems could be negatively influenced by violent video game exposure. The children who participated in this study had clinically elevated attention deficit or depressive symptoms on the Pediatric Symptom Checklist. Results from the study did not find evidence to support that violent video game play leads to increased bullying or delinquent behavior in youth who displayed clinically elevated mental health symptoms. As can be seen based on the studies presented in this article, violent video games are not necessarily a predictor of youth violence, aggression, or bullying. Methods This study occurred in three phases. Phase 1 was the development and quantification of the profile combination of traits via latent class profile analysis. The resultant profile probabilities were used to train an artificial neural network. Phase 2 was the training of the artificial neural network and substantiation of computational model fit and robustness. Phase 3 was the comparison of each computational model with and without the presence of video game violence. The researchers made use of a computational model to study acts of violence, which could not be studied ethically in the laboratory or easily captured in natural settings in which children find themselves. In using a computational model, the authors of this study were able to test the causal effects of video game violence without ethical concerns. Study design The study design was an observational computational comparison intact group design. The advantage of this design was that it allowed for the complete control of tested variables and was minimally invasive. The use of a computational component for comparative purposes allowed the authors to examine the effects of changes on the individual variables comprising the model and the system of variables as a whole. In addition, the use of the computational model reduced ethical concerns associated with the examination of violence and aggression. Table 1 provides an overview of the study design. Participants Initial data for profile modeling purposes were taken from past projects, which took place in the Southeastern and Mid-Atlantic portions of the United States. Total participants examined for this study are N = 1065. Participants grade levels ranged from 9th grade to 12th grade and ages ranged from 14 to 18 (M = 15.89, SD = 1.78) years. Of the total participants, 52.4% were female. Table 2 provides an overview of the participant sample characteristics. As needed the authors weighted sample imbalances via auxiliary variable weightings. Data collection for demographic variables occurred during the review of student classroom files. Proficiency in mathematics and reading percentages was gathered from student course testing and attitudes were gathered via an interest inventory and self-efficacy measure. Data used in this study were collected at the onset of the school year prior to any interventions. Demographic information was collected via student responses on demographic questionnaire and from school files. Attitudes were loosely defined in this study as motivation consisting of self-efficacy and interest measured using the Science Interest Survey (SIS) and Self-Efficacy in Technology and Science-Short Form (SETS-SF). These affective measures (SIS and SETS-SF) consisted of 18 and 21 questions respectively on a Likert-type scale. Cronbach's alpha internal consistency and reliability is.79 for the SIS and.93 for SETS-SF. Psychometric properties of these instruments are within acceptable limits based upon previous papers. Reading and Mathematics proficiency was assessed using End of Course testing results for each of the students. Internal consistency and reliably along with other psychometric properties for these measures were unavailable as the district and the test manufacturer does not release this information. Estimates and assumptions Estimation of the aggression and violence component within the population was estimated using the average rate of violent crimes in the United States reported via the Uniform Crime Reports from 1993 to 2012 . The authors note that the Uniform Crime Reports do not differentiate between proactive and reactive aggression. The number of persons between the ages of 14 and 18 playing video games was estimated from a 2008 Pew Internet & American Life Project. A third estimation accounting for genetic factors related to aggressive behavior occurred via examination of work relate to the presence monoamine oxidase A and cadherin 13 . The limitations of this approach are more thoroughly outlined in the \\\"Limitations\\\" section of this study. Analysis The study authors used multiple forms of analysis to examine affective and cognitive components associated with each of the models (GAM, DSM, and GAM+DSM). Upon completion of data screening and cleaning, the authors made use of Latent Class Profile Analysis to create profile combinations of traits for introduction in to the computational model. Latent class analysis (LCA) is the use of a mathematical model to assist in the characterization of the latent variables as a function of class assignments. Latent class profile analysis The study authors used several variables to examine and build the profile combinations of traits related to affective and cognitive characteristics of the sample. Positive and negative and high and low designations or classifications are based upon class intervals using the equation i = L-S\\/R. Each of the characteristics in the profiles is thought to contribute to aggression related to video game play. The factors included within the derived variables are SES and attitudes. Examination of each of these two variables making use of the interval equation suggests that there are two class intervals. This is not to suggest that the attitudes or SES themselves are positive or negative but that the attitudinal and SES score clusters are lower and higher, respectively, for the calculation of the outcome probabilities in the LCA. The probabilities associated with the profile combination of traits are loaded into a computational model of the GAM, the combined model, and the DSM. LCA allowed the researchers to use observable variables and estimated variables to cluster profile combinations of traits into probabilistic classes of membership, thereby examining possible outcomes in relation to these variables. These probabilities are particularly useful for the development of computational model via machine learning algorithms. For the purposes of this study, profile traits were identified as present (1) or absent (0). Computational model: artificial neural network The authors of this computational study use a generalized Bayesian Network in the form of an artificial neural network (ANN) as the foundation of the computational models making up the GAM, DSM, and the combined model (GAM+DSM). Mathematical parameterization of the variables within the profiles creates a secondary data structure in the ANN consisting of a set of conditional probabilities for the interaction of each component variable across the network system. By combining the network system interactions as a function of the LCA model probabilities, the authors combine the LCA and ANN model creating the computational models used within this study. A computational model is defined as a mathematical model, which is used to simulate complex systems when those systems are unable to be accessed due to their complexity. A complete description of the methods the authors used to generate the computational model is found in Lamb (2013, 2016); Lamb, Annetta et al. (2014); Lamb, Firestone, & Ardasheva (2016); Lamb, Vallett et al. (2014). The ANN acts as the underlying learning component and the LCA acts as the content to be learned. Outputs from the ANN-based computational model represent the propensity toward aggression. The GAM, DSM, and GAM+DSM ANN allowed researchers to compare each model directly and to adjust parameter variables to examine the role each variable plays in outcomes related to aggression. Comparison of mean aggression was accomplished using one-way ANOVA using a random subsample of 500 outcomes from the computational model. A post hoc Tukey-HSD analysis was used to identify differences betweenmodel outcomes. A subsample of 500 outcomes allows a.90 probability of detecting a d = .5 (moderate) effect for the one-way ANOVA. Summary of analysis and model design Figure 4 provides an overview and summary of the phases of the study and model design. Each phase of the study informed subsequent phases of the study ultimately resulting in the development of a computational model through which to study the effects of video games on aggression. Results Table 3 provides 17 example profiles and outcome probabilities for aggression (Pi). Entropy or the stability of the LCA classification model is rated at.97, which is sufficient for a profile analysis of this type. Probabilities consist of composite probabilities calculated from individual profile characteristic probabilities. For example, Profile 7 shown in Table 3 Environmental Moderators illustrates the individual category probability is.79 and is part of the composite probability and classification found in the Aggression(Pi) column. Comparison of the fit statistics AIC, BIC, and Adjusted BIC suggests sufficient model fit to the data. Table 4 summarizes the latent class model fit by the number of classes proposed for each rival model. Examination of Table 4 illustrates that Model 2 with three classes provides the best model fit. This is evidenced by the large reduction in the fit statistics frommodel 1 tomodel 2, in addition to the increase in entropy. Examination of Model 3 in Table 4 suggests that there is not significant change in the fit statistics from Model 2 to Model 3 to warrant the addition of an addition class. This result (the change in model fit statistics) in conjunction with the reduction in entropy suggests that Model 3 is of poorer fit. Overall consideration of each of themodels, fit statistics, and entropy suggests that Model 2 is the more parsimonious and proper fitting model for development of the latent classes and resultant probabilities. ANN results The three artificial neural networks in Figure 5 computationally, mathematically, and graphically represent the DSM, the GAM, and a theoretical combined model (GSM+DSM). In this capacity, the ANN acts as a computational model designed to mathematically represent a complex nonlinear dynamic system for study. Within these models, the input layer provides no computational function other than to distribute the stimulus (the LCA probabilities) into the network. For the purposes of these models, video game play and the latent class profile probabilities act as the input. The multiple hidden layers represent the factors (environmental and genetic) that interact with the video game play. The output layer is the average level of aggression and is examined over 100,000 iterations. The ANN in this portion of the study was designed using the R neural network package neuralnet. Training of the neural network used a k-fold hold back of one-half with cross validation to reduce overfit error (N1\\/2 = 23,328). The training data set teaches the ANN how to process the data for later computational experimentations. Once the ANN was trained and optimized, the authors of the study presented the second half of the profile probability data to the ANNs for analysis. This data output is then used to compare each of the three models and assist in the determination of which model best describes the role of video games as related to aggression. Table 5 illustrates the results of the comparison of aggression across the main effect of model type. In addition, the same table illustrates relative model fit, variance explained, and the average change from the training model to the test model. Results suggest that there is a statistically significant difference in the levels of aggression based upon the model type. Comparison of the aggression levels related to the play of video games is statistically significant t(498) = 20.82, p < .001, d = 1.86 large. This result suggests that based upon the currently accepted models of aggression that video games do play a role in development of aggression through they do not necessarily cause the aggression. R-squared values suggest that the combined model best accounts for the variance within the profile combination of traits, that DSM accounts for more variance than the GAM and that the GAM accounts for the least variance suggestion there are several unaccounted for variables in the DSM and GAM. Table 6 illustrates the planned comparisons between each of the models on the variable of aggression. Discussion Aggressive elements within video games have been steadily increasing as this genus of media has matured. The frequency of real-world violence does not seem to be a byproduct of social factors and video game play alone. Computational modeling of some of these social pressures with the addition of genetic predispositions provides clearer evidence of a much larger and more complex system of interactions related to aggression and video game play. The interactions of the social environment, cognitive attributes, and genetic predispositions set the conditions for increased aggressions with violent video games as a potential triggering event. While the authors do not suggest this is by any means the final word and that, the computational models provides an exact answer. However, the computational model does provide a new direction and is an attempt to merge the two competing models of how aggression arises related to video games. Results from the two studies, the computational study and the latent class study, suggest that socialization models alone are insufficient to understand the interaction between the media associated with video games and the player's behavior. In turn, models solely making use of biological predispositions are also inadequate to explain behaviors related to real-world aggression. The authors suggest there is a complex system of interaction, which arises out of an affective and cognitive drive with individual differences due to biological variation creating a much larger profile combination of interacting traits. Meaning individual profiles can vary with an almost countless number of combinations, which accounts for the inconsistent findings when examining singular variables around video game play and aggression. Using computational models, researchers can vary the node estimates representing specific values related to input variables such as personality factors limited only by the computational capacity of the computer. Node propagation weighting provide additional information to researchers examining the systems of interactions. This information allows researchers to identify the relationships between the system of variables as they interact with one another and the overall system. Isolating the role of video games in the wider system of the model interactions can be accomplished using model comparison and provides evidence that the role of video games in the creation of aggression is minimal. However, this linkage is only present with a specific profile combination of traits and increases only when including genetic factors. The genetic contribution when isolated shows a moderate effect on the outcomes suggesting this may be a distal cause to increased aggression. However, the distance between genotypic expression and phenotypic expression is quite large. The final addition moving the profile to high probability of aggression is the addition of personality aspects related to motivation. The highest probability of aggression is when there is an interaction of environmental moderators such as motivation, video games, and genetics. This outcome provides evidence for the development of a combined model of aggression related to video game play. The computational experiments in this study examined the profile combinations of traits that seem to result in acts of aggression. The authors found evidence to support hypothesis one (H1), that existing models of aggression such as GAM and DSM attempting to explain and predict aggression are inadequate and require further development and testing. The results also evidence the need to develop a combined model of aggression related to video games with violent content specifically when attempting to ascertain proximal and distal causes. Combining variables from both models provides greater explanatory power to the model and supplies researchers with greater clarity related to the system of interactions as opposed to attempting to isolate variables and examine their individual impact. This is not to suggest that there is not value examining individual variables but that a concerted effort is needed to understand the system on variables. This computational study also provides moderate evidence linking aggression to video game play. While this outcome is certainly going to be debated and rightly, so, the author would like to suggest that this study provides a potentially new powerful tool to target, explore, and develop research around the questions of video games and aggression. Limitations The authors of the study acknowledge that there are several limitations related to computationally modeling of complex dynamic systems. While the use of sophisticated artificial intelligence and their ability to account for nonlinear system dynamics, the model is relatively unsophisticated in its representations and will require more data and specifications as time goes on. This is often the case with other types of computational models such as weather models and water flow models. Specifically, this is an acknowledgment that modeling nonlinear dynamic systems such as aggression related to video games is in its nascent form and requires more research to bring its full potential to fruition. In addition, the use of secondary data makes the assumption that the data adequately reflects outcomes seen in the real world and includes all of the assumptions associated with the use of secondary data. As with any computational model continuous examination of the model to real outcomes should be made and the model adjusted to reflect outcomes which may have been anomalous to actual outcomes. The authors would like to explicitly note that they understand that genetics do not directly link to behavior but predispose people to aggressive behaviors. Each of these statistics is estimated at the population level and it is assumed there is random equal distribution of the genetic factors within the sample. Random distribution within the sample was achieved via a random number generator assignment of characteristics to each member in the sample, which may or may not reflect the actual distribution of the genetic factors across the sample. Conclusion The successful use of computational modeling to examine complex systems and interactions of variables in this study provides a means to experiment with difficult-to-examine constructs and relationships such as the relationship between video games and violence. This computational model provides the opportunity to not only isolate variables and examine them via traditional means but also as a system of interacting variables. In addition, the findings of this study generate questions that can be applied and direct future studies allowing for the testing of competing models.\",\"577240475\":\"Introduction Implicit motives are non-conscious, affectively-based dispositional preferences that direct and energize behavior and serve as key drivers of individual differences, guiding shortterm as well as long-term preferences and behavioral and emotional outcomes. They have been related to differences between individuals in such diverse life domains as organizational behavior and worker management , sports , inter- and intra-group dynamics , and intimate relationships. Implicit motives have been contrasted with explicit motives that are traditionally assessed with declarative measures such as the Personality Research Form  and the Unified Motive Scales. Implicit and explicit motives are statistically as well as conceptually distinct, such that scores on implicit motive measures correlate weakly or not at all with scores on explicit motive measures , and have been argued to predict different classes of behavior. Since they are non-conscious and cannot be assessed directly via self-report, implicit motives have traditionally been assessed using non-declarative measures that rely on the analysis of thematic content in freely generated texts, such as elicited by the Picture Story Exercise (PSE;), in which participants write imaginative stories in response to a battery of pictures that depict characters in everyday situations. Human coders then read these stories and make determinations regarding the presence of particular motive imagery in the stories, based on coding categories in established empirically-derived coding systems. Each motive coding system contains a set of guidelines for coding motive related imagery. As an example, in Winter's (1994) coding system for scoring the implicit motives for achievement (nAch), affiliation (nAff) and power (nPow) in running text, a sentence is coded for nAch if it indicates a concern for competition against a standard of excellence, it is coded for nAff if it indicates a concern for the establishment or maintenance of friendly relations with others, and it is coded for nPow if there is a concern for having or maintaining social influence. Furthermore, the Winter (1994) coding system contains five sub-categories for nAch, six sub-categories for nPow, and four sub-categories for nAff. Besides learning and internalizing these categories and sub-categories, human coders are required to achieve at least 85% agreement with training materials contained in standardized coding systems such as Winter's (1994). It takes at least 20 h of coding practice to achieve this standard , and even after the human coder has fulfilled the minimum 85% agreement criterion of correspondence on training materials, all stories must be coded by at least two independent coders who are required to establish inter-rater reliability (again, the 85% standard applies) on a subset of the stories (typically 10%) before they proceed to code remaining text. Finally, researchers also need to factor into their research process the time required for a human coder to read and evaluate each story for motive imagery. This could be anywhere from 3 to 10 min per picture stimulus, depending on the length and density of the text produced for each picture by a particular respondent, as well as the coder's experience and ability to quickly apply the coding rules. When evaluating 100 participants on 5 pictures, with an average of 5 min to code a single picture response, this easily reaches upwards of 41 h of coding, per coder. Consequently, the assessment of implicit motives is timeconsuming and effortful, which can serve as a significant deterrent to researchers. Nonetheless, given the wide range of outcomes and research areas in which implicit motives have contributed theoretically-interesting findings--from parent-child relationships  and income growth trajectories  to racial and ethnic relations  and users' profile content on online social networks --it would be a significant boon to researchers in multiple fields if the time burden for implicit motive assessment could be reduced. To this end, our research aims to evaluate the feasibility and the validity of a machine-learning derived automated coding process for implicit human motives. Previous automation research: the marker word approach Previous published research on automating implicit motive coding includes Smith (1968), Pennebaker and King (1999), Hogenraad (2005), and Schultheiss (2013), among others. These works are all based on the marker word hypothesis, which asserts that there are specific words related to a single motive that can be identified beforehand and counted in order to generate an approximation of the human-coded motive score. Schultheiss (2013) undertook the most recent and systematic published endeavor based on the marker word approach and used the popular and readily available Linguistic Inquiry and Word Count (or LIWC) software and dictionary (; for the German version) for his analysis. He showed that, although certain LIWC categories were associated with the human-coded implicit motive scores, only 89 out of 684 of the computed zeroorder correlations passed the.05 significance threshold, and the absolute values of these statistically significant zeroorder correlations ranged between.19 and.40, with most rs hovering around.25. He further developed regression models, which accounted for between 35% (nPow) to 54% (nAff) of the variance of the hand-coded motive scores, and when he computed sample-specific linear regression equations based on the aforementioned LIWC categories, variance accounted-for increased to as high as 63% (nAff for the US sample). Schultheiss (2013) thus demonstrated that computer estimations of implicit motive scores using a marker word approach can achieve modest to moderately-high correspondence with human coders (with a custom-built linear regression estimation approach), and that computer-based implicit motive score estimations possess some degree of causal validity (study 2). However, lexicon-based approaches suffer from the limitations of oversimplification, reduced sensitivity to more subtle or ambiguous instances of motive expression, and a lack of verisimilitude or generalizability. Specifically, with respect to oversimplification, as Schultheiss (2013) suggests, a human coder is attuned to the unique combination of words in a sentence, rather than just the individual words of the sentence. While individual words such as \\\"reporter\\\", \\\"skeleton\\\", \\\"closet\\\", and \\\"embarrass\\\" would not be coded for any motive, the combination of these words in a sentence such as \\\"The reporter sought to find a skeleton in the officer's closet that would embarrass him\\\" and within the context of a passage of text about retaliative one-upmanship, would cause the human coder to make the judgment that the sentence should be coded for nPow (a similar sentence appears on page 19 of the training materials for Winter's 1994 manual). This information is lost in a marker word approach. Marker words are also likely to only be associated with human-coded motive scores when there is strong or unambiguous motive intent expressed in a piece of text. Schultheiss (2013) notes that \\\"accomplished\\\" is a strong marker word for nAch that would be scored under Winter's (1994) coding sub-category of \\\"unique accomplishment,\\\" but it is unlikely that there would be many marker words that are just as unambiguous. This problem is compounded by the fact that many words possess multiple meanings. Such cases of lexical ambiguity are often used in humor, such as in an advertisement now banned by the Australian government, in which a picture-framing shop claims \\\"We can shoot your wife and frame your mother-in-law...we can hang them too\\\" . Human coders are usually able to cope with cases of lexical ambiguity by referring to the context of the text in which a sentence occurs, but this is difficult for automation research that uses a straightforward lexicon-based approach. Additionally, the inferences that a human coder makes are partially dependent on the body of experience that he or she has cultivated from coding and from discussing texts with other experienced coders. This reference to prior experience and extra-linguistic real-world knowledge is related to the phenomenon of 'semantic prosody', which refers to the conceptual co-occurence of words in natural language that imbues otherwise neutral words with attitudinal overtones. Classic examples are happen and cause, which have been shown to be collocated with negative words, and bring about, which is associated with positive words. A human coder would be familiar with the typical contexts in which a word is used, as well as whether these contexts have positive or negative connotations, which would color the implicit evaluations that the human coder makes about any new sentences in which they encounter the word. Marker word approaches thus lack verisimilitude because they miss out on such common concordances between words and phrases that human coders are able to detect because of their wealth of experience with combinations of words used in many different contexts. Furthermore, the marker word approach lacks generalizability as it relies solely on the dictionary and the lexical equivalences built into the dictionary by the researchers in question. As such, the marker word approach is highly dependent on the theoretical biases of said researchers when generating motive score predictions, as well as the particular set of stimuli that were used to generate dictionaries. The marker word approach thus suffers from a lack of generalizability because it is impossible for a single researcher or research group to account for the multitude of word combinations that might be produced in various natural language contexts (e.g., across different PSE picture sets or in non-PSE textual data). Machine learning and natural language processing In order to avoid some of the limitations inherent in the marker word approach, we attempt to use machine learning to approximate the holistic judgment of human coders, using the sentence as the unit of measurement, whereas most previous research on automating motive scoring used the word as the unit of measurement. Moreover, since an aim of machine learning is to approximate the process of humanbased classification of objects, one underlying objective of our research is to derive a process that would even account for cases in which a motive score is assigned where there are no obviously strong marker words associated with the motive. Thus we focus on building a neural network model from the perspective of natural language processing (NLP;). We take this approach since we share an underlying assumption of marker-word approaches, namely that it is likely that the individual semantics of words and their combination in clauses (as opposed to extra-linguistic world knowledge of human coders, which cannot be easily accounted for in any computational approach) contribute to some of the nuances of meaning that are classified by human coders as motive imagery. The field of NLP has moved from using dictionaries to more complex semantic representations (; for a review), following insights from the subfield of Distributional Semantics, whose central idea is that a difference of meaning between words correlates with a difference of distribution in word occurrence in natural speech or text. This idea was popularized by Firth's (1957, p. 11) statement \\\"You shall know a word by the company it keeps,\\\" and is the foundation of many computational approaches to understanding natural language. For common NLP tasks such as machine translation and sentiment analysis, a normal workflow includes the use of such distributional models or 'word embeddings'  to transform the text into computational features. These models allow words in a corpus of natural(istic) texts to be represented in continuous vector space, so that words with more similar meanings\\/ distributions are numerically 'closer together' than words with less similar meanings\\/distributions. A commonly cited example from Mikolov et al. (2013, p. 2) shows that analogous relations, i.e. \\\"king is to queen as man is to woman\\\" are encoded in word embedding matrices, such that the vectors for the respective words have the approximate relation of equations, i.e. king--man = queen--woman. As input for a machine-learning model, word embeddings allow for a relatively nuanced semantic representation of a word (at least theoretically), based on the training data that underpins the word embeddings. That is, a word embedding matrix of 300 dimensions trained on a corpus of billions of words in naturally-occurring sentences will represent each individual word as 300 numbers (a vector) that mathematically encode its occurrence in a sentence and its relationship with other words. These vector representations are a better measure of meaning than a simple dictionary which does not clearly capture relations between words. Since they are based on observations of actual sentences, word embeddings also help to mitigate researcher bias and account for more of the diversity in language than in a dictionary approach. A large number of word embeddings have been made available by various research groups, the most well-known of which are Google's Word2Vec, Stanford's GLoVe, and Facebook's FastText (see references above regarding 'word embeddings'). The use of such pre-trained word embeddings allows NLP researchers to benefit from distributional semantic models without access to the computational resources or the large amounts of data required for training such models. In our research, we use word embeddings (or 'word vectors') to encode sentences as input to several neural network models, with the goal of classifying unseen data according to implicit motives. There are many different kinds of neural network models available, with differing performance on various natural language processing tasks. The most commonly used architectures for text classification (; for a review) are the Long Short-Term Memory (LSTM) network and its Bidirectional variant (Bi-LSTM), both types of Recurrent Neural Network (RNN), as well as the Convolutional Neural Network (CNN) and the more recent Temporal Convolutional Network (TCN). LSTMs are very good at capturing dependencies such as might be found between words in a sentence but can be extremely time-consuming to train. CNNs and TCNs, with some modification, can also capture such dependencies and have shown excellent results on text classification tasks (; on CNNs, and Kalchbrenner et al. 2016; on TCNs). These neural network architectures have the added advantage of being much faster to train than RNNs. Newer architectures include Attention networks and the Transformer , but these have a higher bar for implementation in terms of computational resources, and our initial experimentation with these architectures yielded no significant benefits. Thus, in this paper we only report results from a one-dimensional CNN (which captures local dependencies between words in sentences), a two-dimensional CNN (which can capture non-local dependencies in word vectors), and a TCN model (which can capture non-local and longerrange historical dependencies than CNNs and LSTMs). The current research The aim of the current research program is to ascertain whether it would be possible to move the field of implicit motive research forward by automating the coding of implicit motive imagery in running text. Specifically, we evaluate whether a machine-learning approach aided by natural language processing, using three different neural network architectures, could generate motive score predictions that correspond to those generated by human coders, or at least show higher correspondence than previous approaches. The development of an automated motive coding process would allow researchers who are interested in implicit motives to assess them without the significant costs to their research timeline that this would normally entail. It would also help researchers in cases where it is physically impossible to administer the PSE, or other implicit motive measures that require the generation of imaginative material (e.g., the Operant Motive Test;). Specifically, with an automated process, archived materials could be 'fed' into the machine, thus opening up an avenue for the assessment of implicit motives of individuals who are deceased, incarcerated, or otherwise physically unavailable for assessment. The availability of an automated coding process could also allow for the processing of larger amounts of textual data than otherwise possible with human coding. This would open up the possibility of assessing motives in bigger and more diverse datasets. Finally, an automated coding process would hypothetically enable the 'real-time' assessment of implicit motives, which would make the research process more dynamic. We evaluate for the first time whether motive score predictions from a machine-learning derived automated coding process using neural network model architectures and aided by natural language processing achieves correspondence with human-coded scores from several 'unseen' datasets that were not used to train the machinelearning models. We also examine whether automated motive score predictions possess convergent validity, in terms of showing moderately high and significant correlations with human-coded scores for conceptually similar implicit motives, as well as divergent validity in terms of showing low or no correlations with human-coded scores for conceptually dissimilar implicit motives. Finally, in order to examine whether the motive score predictions from the automated coding process possess criterion and causal validity, we use data from two of the unseen datasets to ask whether theoretically-consistent group differences observed with human-coded motive scores can also be observed with the automated motive score predictions. While previous work on automating implicit motive scoring described some limited success, these researchers relied on the marker word approach, which, as argued, has some significant shortcomings. Moreover, they did not consistently demonstrate that their methods achieved the various forms of validity that Schultheiss (2013) gave as necessary for an automated coding process to be adopted by researchers. Thus, our main goal is to develop a process for automating motive scoring for all three motives of achievement, affiliation, and power, while a secondary goal is to apply our trained neural network models to unseen datasets in order to identify whether such models can achieve the level of validity required for adoption by implicit motive researchers. To be clear, in this paper, we are not proposing a tool that can replace human coders for implicit motive research. Instead, we wish to provide a benchmark for training and validating machine-learning models in the years-long task of automating implicit motive coding, with the ultimate goal of developing such a tool. We use a \\\"top down\\\" approach  in training machine-learning models, relying on humancoded scored sentences as input. We gathered Englishlanguage PSE datasets from various sources that had previously been coded by trained human coders for nAch, nAff, and nPow using the Winter (1994) scoring system. We re-parsed all datasets and had independent coders (who were blind to study hypotheses) re-score the datasets for motives at a more granular, sentence level. We then used these human-coded data for training and validation of several machine-learning models. Assessing validity of the neural network models In order to assess the validity of this approach for use by motive researchers, we examined the correspondence between the human-coded motive scores and the machine predictions of the motive scores for three different unseen datasets (i.e. those that the machine-learning model had not been trained on). The first unseen dataset (UD1) contains 1357 sentences from practice sets in Winter's (1994) coding manual. The second unseen dataset (UD2) contains 28,686 sentences from the Enron Email Corpus  from 18 individuals in upper management. The third unseen dataset (UD3) contains 475 sentences from an unpublished study on goal visualization. If the correspondence between the human-coded and the machine-predicted motive scores is high for all three unseen datasets, this would indicate that the machine-generated classification algorithm built from the training dataset is generalizable to a wide range of implicit motive data. The use of these datasets allowed us to conduct assessments of convergent, divergent, causal, and criterion validity for our machinepredicted motive scores. Convergent and divergent validity For convergent validity, we used human-coded scores for nAch, nAff, and nPow in UD1 and UD2. We expected machine predictions of these motive scores to correlate positively with their corresponding human-coded scores. We used human-coded scores for implicit hope of success and fear of failure motivation in UD3 to examine convergent as well as divergent validity. Specifically, we computed correlations of these human-coded scores with the machine predictions for nAch, nAff, and nPow. Insofar as the motives for hope of success and fear of failure are approach and avoidant orientations of the generalized implicit achievement motive , machine predictions for nAch should correlate positively with human-coded scores for hope of success and negatively with human-coded scores for fear of failure, thus demonstrating convergent validity. Conversely, there should be minimal correlations for hope of success and fear of failure scores with the machine predictions for nPow and nAff, thus demonstrating divergent validity. Causal validity In addition, the participants in UD3 had been randomly assigned to visualize an achievement goal that was either positively framed (approach goal) or negatively framed (avoidant goal). We analyzed differences in the mean machine-coded achievement motive scores for each experimental group. Insofar as scores on a measure of an attribute should show mean differences in scores for groups which are known to differ in that attribute or which have been experimentally induced to be at different levels of that attribute , we expected that the machine-predicted nAch would be higher for participants in the approach goal visualization condition than for those in the avoidant goal visualization condition, thus demonstrating causal validity. Criterion validity Finally, we used UD2 to demonstrate criterion validity. Insofar as the machine-coded scores are a valid measure of implicit motives, they should predict outcome variables that have theoretically been associated with these motives. McClelland (1975) has theorized that achievement motivated individuals are spurred by the need to succeed so that they could be driven to obtain this success by any means possible--including unethical ones. While previous research has not investigated the direct relationship between implicit motives and unethical workplace behaviors, some have looked at the effects of the achievement motive on unethical behaviors in other settings. Johnson (1981) showed that individuals with a high achievement motive were more likely to cheat on college examinations than those with low achievement motive. Whitley's (1998) meta-analysis found that achievement motivation had a small positive effect (d = .250) on cheating in examinations. However, Johnson's (1981) and Whitley's (1998) research relied on explicit measures of the achievement motive. Smith et al. (1972) showed that implicit nAch is related to self-reported dishonesty, however one could argue that a self-reported measure of dishonesty is only marginally related to unethical behavior. Winter (2010) also showed in several case studies of US presidents, that being highly achievement motivated could lead a president to take shortcuts within the political process--sometimes illegally (e.g., Nixon in the case of Watergate). As Winter (2010) has argued, the reason that highly achievement motivated individuals tend to succeed in business but not in politics is because they are drawn to situations in which they are able to execute personal control and they become frustrated by the lack of control inherent in politics. Thus, the question of whether highly achievement-motivated individuals are more likely to engage in unethical behavior in the workplace--even though they are more likely to be able to maintain personal control of their actions--is an empirical one. Although prior evidence for McClelland's (1975) theory is scant, we expected that machine predictions of nAch should be related to unethical behavior in the workplace. Specifically, UD2--from the Enron Email Corpus --contains a subset of emails from 18 of roughly 150 employees of the Enron Corporation, some of whom were investigated for fraud resulting from insider trading. We examined whether machine-coded nAch and\\/or nPow scores differentiated between individuals in upper management who had been clearly implicated in the scandal (henceforth denoted as \\\"misbehavers\\\", n = 9) versus those who had not (\\\"non-misbehavers\\\", n = 9). Following McClelland (1975), we expected that machine-coded nAch would differentiate between misbehaver and non-misbehaver status. However, given that the members of the misbehaver group were extremely influential employees in Enron, another possible hypothesis is that the misbehavers were driven by nPow to conduct unethical behaviors in order to maintain their positions of influence. If this were the case, we might expect to see that machine-coded nPow differentiates between misbehaver and non-misbehaver status. Method Method for building the machine-learning model Training datasets and data processing To develop a set of data for training the neural networks we contacted several implicit motive researchers to request use of their anonymous, scored PSE data from studies conducted in English. A total of 11 datasets were collected from six researchers in the USA and Singapore. All datasets were checked for grammatical, spelling and punctuation errors, and acronyms (e.g., \\\"POW\\\" = Prisoner of War) and abbreviations (e.g., \\\"Prof.\\\" = \\\"Professor\\\") were expanded. Some contractions (e.g., \\\"gimme\\\" = \\\"give me\\\") that were judged to be colloquialisms were expanded, but proper contractions (e.g., \\\"don't) went uncorrected. All data were split into single sentences which were re-scored by six different independent coders who were blind to study hypotheses, using Winter's (1994) scoring system. Additionally, the Winter (1994) manual recommends applying a \\\"second-sentencerule\\\" for coding, such that, when the same motive imagery appears in two consecutive sentences, only one of the sentences is counted towards the raw motive score. However, our independent coders did not apply this second-sentencerule as it would likely falsely increase the frequency of the null category and hence distort the diagnostic fidelity of the training dataset. In other words, each sentence was evaluated for motive imagery independently from its preceding sentence. Each coder was responsible for coding a subset of the total PSE dataset. All six coders obtained at least 85% reliability with the expert coder in the training materials provided by Winter (1994). Since 85.4% of the PSE data were coded by three coders, these three coders coded a further 30 stories so that we were able to calculate the intraclass correlation coefficient (ICC;) using a two-way, mixed-effects model (average rating, consistency measure). The ICC for the nPow scores was.83, which could indicate good reliability. However, the 95% CI had a large range from.69 to.91, indicating that the level of inter-rater reliability for nPow scores range from moderate to excellent. The ICC for the nAch scores was.90, and the 95% CI ranged from.82 to.95, indicating that the reliability for nAch scores should be regarded as ranging from good to excellent. Finally, the ICC for the nAff scores was.96, and the 95% CI ranged from.93 to.98, indicating that the reliability for nAff scores should be regarded as excellent. The average ICC for all three motives was.87, which is indicative of good reliability.4 There were altogether 65,681 sentences across 11 datasets containing PSE data--key characteristics of each of the datasets are given in Table 1. Additionally, since the linguistic style and content of emails are likely to differ substantially from PSE data, a portion of sentences from the Enron emails not included in UD2 were also scored and included in the training dataset, bringing the total number of sentences to 73,907. From this set we removed single-word sentences, giving a final training dataset of 73,003 sentences. Building, training, and validation of the models The final set of 73,003 sentences was used as the input for training using fivefold cross-validation, meaning that in a given training session 80% of the sentences were used for training, while the remaining 20% were used to evaluate the model. We used a balanced split to ensure that the absence\\/presence of motive codes were proportional across the training and validation datasets. Additionally, given the imbalance of codes (positively-coded sentences were far outweighed by sentences without any code), before training we computed weight values for each motive based on the proportion of sentences in the dataset that were positively-coded for that motive. These values were used during training to penalize\\/reward the models for incorrect\\/correct predictions on the training data. In pilot experiments we evaluated a variety of pre-trained word embeddings with a baseline CNN , finding that 300-dimensional embeddings performed better than lower dimensional embeddings, as did word embeddings trained on larger datasets. Based on these experiments we decided to use Facebook's FastText subword embeddings of 300 dimensions trained on Common Crawl (600 billion tokens). This is the set of pre-trained vectors that we used to derive word features from sentences for all the experiments that we report below. We trained three neural networks with different architectures: A one-dimensional convolutional network (CNN), a two-dimensional convolutional network (CNN2D), and a temporal convolutional neural network (TCN). Input to each of the neural networks was vectorized via the pretrained FastText word embeddings, and then fed into the convolutional layers. We used grid search with tenfold cross-validation (CV) to optimize the number of neurons and kernels for each architecture. The hyperparameters that gave the best average performance (high accuracy, low loss) during model-internal validation (on the held-out kth fold) were used to train each of the final models. Final CV accuracy\\/loss scores for the hyperparameter selection phase, by model, were 0.929\\/0.206 (CNN); 0.928\\/0.202 (CNN2D); 0.920\\/0.215 (TCN). The final CNN and CNN2D models had four convolutional layers with kernel widths of 1, 2, 3, and 5 instantiated with 64 neurons. These layers were concatenated and passed through a max-pooling layer before being fed into the final dense layer. The TCN was implemented with 32 neurons, a kernel size of 4, 5 stacks, and dilations ofand a final dense layer. For all three networks, the final layer consisted of 3 neurons (one for each motive) which each returned a probability between 0 and 1 (rounded using a threshold of 0.5) to provide the machine predictions regarding the presence of motive imagery in a given sentence. For each network, batch normalization was used after each convolutional layer to prevent overfitting , and kernels were initialized using He normal weights. All final training sessions were run in Python\\/Keras with Tensorflow 1.15 on a workstation equipped with an AMD 1700x CPU and NVIDIA RTX 2060 GPU running Ubuntu 18.04 LTS. Additionally we used the TensorFlow Determinism library  to ensure reproducible results. The CNN trained for 100 epochs with a mini-batch size of 64, the CNN2D trained for 40 epochs with a mini-batch size of 32, and the TCN trained for 20 epochs with a mini-batch size of 64. Each neural network model achieved the accuracy and loss figures as given in Table 2. It is important to keep in mind here that, in machine learning terms, the high accuracy and low loss values for the validation set in Table 2 only describe how well the neural network has learned from the dataset it is trained on, which in theory should also reflect how well it generalizes to unseen data. However, there is always the danger of overfitting, i.e. that the neural network will essentially memorize the data it is trained on, meaning that it will not be robust enough to generalize and make valid predictions on data that it did not 'see' in training. In much machine learning research there is a focus on developing models that achieve the best accuracy and loss scores on a given dataset, to such an extent that reporting a 0.0001 increase in validation accuracy on a given dataset can be seen as justifying a particular model architecture. While it is useful to understand how model architecture affects learning performance, our focus here is to assess more generally whether the machine learning process might allow for automation of motive coding. Since models that achieve high accuracy and low loss do not necessarily generalize well to unseen data, we focus less on these measures and more on the assessment and evaluation of predictions generated by the trained models on the unseen datasets (UD1, UD2, UD3). Preparation and pre-processing of unseen datasets 1, 2, 3 Each of the unseen datasets were processed in the same manner as the training dataset (e.g., split into sentences, spellchecked). Additionally, UD1 and UD2 were also coded by human coders using the Winter (1994) system in order to check the correlations between the human-coded and machine-coded scores. Any disagreements between human coders were resolved by discussion. In the case of UD1, the 1357 sentences were transcribed from practice sets A to F and the calibration set A of the Winter (1994) manual's training material. Next, UD1 was \\\"coded\\\" by a different set of coders than the datasets from which the model was built. Since the human coders' task was simply to transcribe and check the expert coding from the Winter training materials for absence or presence of motive coding on a sentence-level basis, the two coders reached 100% inter-rater agreement in this task. For UD2, we first identified individuals in upper management who were visible in the media during the Enron scandal and subsequent trials (see Online Appendix for details on the sources we consulted). This group of 18 individuals were then divided into \\\"misbehavers\\\" (convicted or involved in a plea bargain) and \\\"non-misbehavers\\\" (found not to be at fault). All the emails were manually coded according to Winter (1994) motives by two coders, each coder being responsible for approximately 50% of the emails. To establish inter-rater reliability for motive coding, the coders independently scored a common set of 393 PSE stories written by 65 research participants. The two-way mixed ICC (average rating, consistency measure) between the two coders was.84 for the power motive (.74 < 95% CI < .90), .85 for the achievement motive (.76 < 95% CI < .91), and.78 for the affiliation motive (.64 < 95% CI < .87), so that on average, the ICC was.82 for all three motives, indicating that the reliability is good. The personally written (sent) email content for each of the 18 Enron employees for the period of fraud investigation (2000-2002) was extracted from the email corpus, resulting in 28,686 sentences for coding. For UD sentences were extracted from 38 participants (25 women, 12 men, mean age = 22 years, SD = 1.2) who took part in an unpublished study on achievement goal visualization. Participants in this study were asked to spend two minutes visualizing a previously attempted achievement goal and then to spend 4 or 5 min typing out the scenario and background during which the goal took place, as well as their thoughts and feelings that they encountered during the goal progress. Participants were randomly assigned to one of two conditions: In one condition, participants were asked to recount a previously attempted approach achievement goal (i.e., one which was framed in a positively worded direction and focused on obtaining a desired outcome), while in the other condition participants were asked to recount a previously attempted avoidant achievement goal (i.e., one which was framed in a negatively worded direction and focused on preventing an undesired outcome). Since UD3 was intended to be used to assess convergent and divergent validity with respect to correlations of machine scores with human-coded scores of hope of success and fear of failure motives (;, as translated by Schultheiss 2001), these sentences were not re-coded for the motive scores in Winter's (1994) system. Results All subsequent analyses reported here with UD1, UD2, and UD3 were conducted with wordcount corrected scores for both human-coded as well as machine-predicted motive scores, using the procedure recommended by Winter (1994; i.e., [(motive score\\/wordcount) x 1000 words]). For ease of readability, we have grouped our findings according to the criterion of validity that is being addressed (i.e., convergent, divergent, etc.). Finally, we also applied a Benjamini-Hochberg procedure  using a false discovery rate of.05 in order to minimize the risk of false positive findings. Convergent and divergent validity In order to examine convergent validity of our machinepredicted motive scores, we calculated bi-variate correlations between human-coded and machine-predicted motive scores for nPow, nAff, and nAch for UD1 and UD2, and between machine-predicted nAch scores and human-coded hope of success and fear of failure motive scores for UD3. In order to examine divergent validity, we calculated bivariate correlations between machine-predicted nPow and nAff scores and human-coded hope of success and fear of failure motive scores for UD3. For UD1, each neural network model predicted a raw nPow, nAch, and nAff score for each of the 1357 sentences. Table 3 presents the individual rs, ps, and corresponding 95% confidence intervals between human-coded and computer predicted scores for each motive and each model (all correlations were significant, all ps < 0.001). The total averaged correlations between human-coded motive scores and their corresponding machine-predicted motive scores for the CNN, CNN2D and TCN models were 0.423, 0.457, and 0.370 respectively, indicating that on average, the computer predictions of motive scores were significantly and moderately positively correlated with the human-coded motive scores. The correlations in Table 3 that originally achieved the p < .05 threshold remained significant even after the Benjamini-Hochberg correction. For UD2, each neural network model also predicted a raw motive score for each sentence. Motive codes were then further aggregated by email sender. Table 4 presents the individual rs, ps, and corresponding 95% confidence intervals between human-coded and computer predicted scores for each motive and each model. The total averaged correlations between human-coded motive scores and their corresponding machine-predicted motive scores for the CNN, CNN2D and TCN models were.694, .683, and.540 respectively. The correlations in Table 4 that originally achieved the p < .05 threshold remained significant even after the Benjamini-Hochberg correction. For UD3 we again predicted per-sentence scores with each neural network model and aggregated the scores by participant. Table 5 depicts the correlations between computer-predicted scores for nAch, nAff, and nPow, and the human-coded scores for HS and FF motives. As expected, the computer-predicted scores for nAch from the CNN and the TCN models (but not for CNN2D) were moderately positively and statistically significantly correlated with the human-coded score for HS, even after we applied the Benjamini-Hochberg correction. Moreover, the correlations between the computer predictions for scores of nPow or nAff and the human-coded scores for HS and FF were low to modest (rs ranged from - .06 to.25), with absolute values of r across all model predictions with the human-coded motive scores averaging.15 and.18 for nAff and nPow respectively. The relative size of these correlations compared to those for machine-predicted nAch are suggestive that the humancoded HS and FF motive scores are more closely associated with the machine-coded motive score for nAch than with the machine-coded motive scores for either nPow or nAff. Overall, the pattern of findings across UD1, UD2, and UD3 suggests that the computer predictions from the machine-learning algorithm demonstrate some degree of convergent and divergent validity. Causal validity In order to examine causal validity of the machine-predicted motive scores, we conducted Mann-Whitney U tests to compare the mean achievement motive scores by experimental condition in UD3. As shown in Fig. 1a, participants who were asked to visualize and to recall an episode where they pursued an approach achievement goal demonstrated higher scores for human-coded hope of success (HS) motivation (M = 19.75, SD = 16.41) compared to participants who were asked to visualize and to recall an episode where they pursued an avoidant achievement goal (M = 2.26, SD = 5.12; Mann Whitney U = 29.50, p = .001, e2 = .56). This result remained significant even after we applied the Benjamini-Hochberg correction. Conversely, participants who were asked to visualize and to recall an episode where they pursued an avoidant achievement goal demonstrated higher scores (M = 34.43, SD = 19.65) for human-coded fear of failure (FF) motivation compared to participants who were asked to visualize and to recall an episode where they pursued an approach achievement goal (M = 5.08, SD = 7.89; Mann-Whitney U = 16.00, p = .001, e2= .64). This result remained significant even after we applied the Benjamini-Hochberg correction. This finding is consistent with prevailing theory about the distinction between HS- and FF-motivated individuals, in terms of their differential sensitivity toward positive versus negative achievement goals. Additionally, as shown in Fig. 1b-d, machine predictions of the nAch score from the CNN, CNN2D, and TCN models were also higher for the participants in the approach achievement goal visualization condition (MCNN = 6.27, SDCNN = 8.48; MCNN2D = 5.42, SDCNN2D = 7.68; MTCN = 7.18, SDTCN = 9.67) than for the avoidant achievement goal visualization condition (MCNN = 1.67, SDCNN = 4.13; MCNN2D = 2.55, SDCNN2D = 4.64; MTCN = 1.80, SDTCN = 3.91), mirroring the results for human-coded HS scores. Moreover, the difference in mean computer-predicted nAch scores between conditions was statistically significant for the CNN model (Mann-Whitney U = 122.50, p = .040, e2= .11) and the TCN model (Mann-Whitney U = 122.00, p = .043, e2= .11). These results remained significant even after we applied the Benjamini-Hochberg correction. These findings are consistent with the prevailing theory about the measures for generalized achievement motivation being more aligned with approach motivation rather than avoidance motivation. Furthermore, if one examines the sub-categories in Winter's (1994) coding manual for nAch, it would become apparent that these focus on positively framed achievement goals and their accompanying goal-directed behaviors and affect. Taken together, the findings from UD3 provide partial support of the causal validity of the machine-predicted nAch scores. Criterion validity In order to examine the criterion validity of the machinepredicted motive scores, we conducted Mann-Whitney U tests to compare the mean nAch and nPow scores between misbehaver and non-misbehaver groups in UD2. There were no statistically significant differences between misbehavers and non-misbehavers in terms of human-coded or machinepredicted scores for nAch or for nPow. Discussion The aim of this research was to examine the possibility of automating implicit motive coding using a machine-learning approach. Across three unseen datasets containing diverse textual content (PSE-type stories in UD1, corporate emails in UD2, and goal visualizations in UD3), machine predictions of motive scores for nAch, nAff, and nPow demonstrated moderate to moderately high correspondence with their related human-coded motive counterparts (rs ranged from.30 for nPow to.86 for nAch). Convergent and divergent validity was partially demonstrated in UD3 when machine predictions of nAch for two out of three models were significantly and moderately correlated with humancoded motive scores for hope of success motivation, while machine predictions of nAff and nPow were weakly related to the human-coded hope of success scores. Also in UD3, causal validity was demonstrated when, as expected, the mean scores of machine-predicted nAch were higher for the experimental group who was asked to visualize an approach achievement goal, when compared to the mean scores for the group who was asked to visualize an avoidant achievement goal. These mean differences reached levels of statistical significance in two out of three models. Moreover, as was previously mentioned, machine predictions of nAch were positively correlated with human-coded scores for hope of success motivation, the mean scores of which were also significantly higher in the approach visualization condition compared to the avoidant visualization condition. Taken together, the findings from UD1, UD2 and UD3 indicate that a machine-learning approach to automating implicit motive coding is able to produce motive score predictions that are not only moderately correlated with their related human-coded motives, but also with motive-relevant outcomes in theoretically consistent ways. Moreover, the computer predictions seem to demonstrate convergent, divergent, and causal validity across a range of different types of textual data that are notably different from the primary training data that were used to construct the machine-learning models. Specifically, UD2 and UD3 contained emails from a multinational corporation and recall data from goal visualizations of Singaporean undergraduates respectively, while the UD1 stories were from Winter (1994) training materials which were collected from Thematic Apperception Tests (TATs;) that were administered to undergraduate students from elite universities in the USA. Thus, UD1 contained written text generated in response to different picture stimuli, administration conditions, time period, and cultural contexts than the PSE stories that contributed to our training and validation dataset. This divergence of textual data in the unseen datasets versus that in the training datasets suggests that a machine-learning approach could generate motive score predictions that are externally valid and would generalize to data gathered from other diverse research and cultural contexts. Our findings bode well for future work in automating motive coding and could eventually lead to a decrease in the resource burden that implicit motive researchers currently face during the measurement process. Our research also improves on previous automation attempts as it moves beyond a straightforward marker word approach, which as explained, suffers from the shortcomings of over-simplification, lack of generalizability and verisimilitude, as well as a tendency to overlook more subtle or ambiguous motive imagery expressions in text. To be clear, however, our claim is not that neural network models can actually capture lexical ambiguities of the sort we discuss (yet), but that the use of word embeddings has more promise toward this goal than marker word approaches. Given sufficiently large amounts of training data from diverse sources, deep learning NLP approaches can better account for the substantial complexity involved in natural language expression, and may lead to predictive models that are stable, reliable, and generalizable to unseen data. These advantages of machine learning seem to bear fruit, as correspondences between our machine-predicted motive scores and human-coded scores are markedly improved from those in Schultheiss (2013), where correlations between LIWC word categories and motive scores ranged mostly between 0.2 and 0.3. Thus, while previous attempts to automate PSE coding using marker-words (e.g., LIWC) typically reinforce the continued need to rely on human coders , our findings suggest that machine-learning techniques have the potential to streamline the measurement process. Although Schultheiss (2013) did not use a machine-learning approach to automation, we make explicit comparison of the performance from our approach with the performance of Schultheiss (2013) because his was the most recent automation endeavor to date that also includes data on various forms of validity (convergent, causal, criterion). There are some interesting theoretical implications which we would like to note. First, machine predictions of nPow consistently demonstrated lower correspondence with human-coded scores than the machine predictions of the other two motives. This could be related to the fact that the majority of the datasets we used for training were intended to study nAch, but another possibility has to do with the nature of power motive expression being more diverse than the other two motives in its representation, acceptance, and legitimation in societies, which in turn leads to a greater diversity of its expression in language. Specifically, power can be pursued either in socialized or in personalized ways, and societies differ in their provision of structures and cultural norms for socially acceptable ways to exert influence on others. In situations where a direct expression of dominance is either physically uneasy or socially unacceptable, individuals who have strong personalized power motivation would likely experience power stress , or would need to internalize their need for power, resulting in physiological agitation and hostility on the one hand  or repression, anxiety, and emotional displacement on the other. Due to the fact that direct expressions of power are generally incompatible with notions of social agreeableness, we believe that power imagery is likely to elicit more figurative and metaphorical language than for the other two motives. NLP techniques are easily misled by irony and sarcasm, whereby the words used in an utterance might convey a literal meaning that is in fact the opposite of what the speaker intended. Additionally, the pre-trained word embeddings we used assume that the meaning of individual words is relatively stable across sentences, such that the polysemy inherent in word forms is not taken into account. Advances are being made in the field of computational linguistics to deal with classification problems in texts that use figurative language and problems of polysemy (;, for a review), and these developments should improve future attempts at automating implicit motive coding. Second, contrary to predictions, we found that neither machine-predicted nAch nor nPow scores were significantly related to misbehaver status in UD2. Since the human-coded motives were also not found to be related to misbehaver status, we are at present unable to determine whether our machine-predicted motive scores for nAch and nPow possess criterion validity. The null findings could indicate a lack of support for the criterion validity of machine-predicted motive scores, or they could be a reflection of the true result of a lack of relationship between the implicit motives and unethical corporate behavior. Classical theory about nPow states that it should be associated with those who occupy positions of power and leadership. It is thus possible that nPow might not have been discriminative between the two groups because all 18 individuals belonged to Enron's upper management and thus probably had high nPow to begin with. As for nAch, a possible reason that nAch scores in the Enron emails were unrelated to misbehaver status could be due to the particular unethical behavior which these employees were accused of. Specifically, they were accused of insider trading, which is the use of confidential and privileged information to one's financial advantage. Perhaps the financial incentive was far too attractive and thus the major impetus for the unethical behavior, overriding the influence of any personality factor. Nonetheless, to our knowledge, our study is the first empirical test of McClelland's (1975) theory regarding achievement motivation and cheating behavior in a corporate setting. Considering this and given the idiosyncratic and selective nature of our sample, more research is required before ruling out McClelland's theory. There are some limitations of our research that need to be addressed. A possible shortcoming of our approach is that we sacrifice some degree of theoretical grounding. Machine learning is essentially a data-driven approach, whereby a statistical model is given a large number of examples containing data correctly sorted into different groups\\/classes\\/ labels and tasked to derive its own set of rules for classifying the examples. This process of model building is in contrast to a theoretically-driven approach such as was developed by Stone et al. (1966) using the General Inquirer system, which is an algorithm that included marker words in a series of rules for each motive which were theoretically consistent with the motivational behavioral sequence for that motive. For instance, according to Smith (1968), in the General Inquirer, if a sentence contained specific types of words in a particular sequence such as Authority role + Subservient role + Influence verb, then the sentence would be scored for power motive imagery. Thus, sentences were coded for motive imagery only when a certain combination of features occurred together. Schultheiss (2013) followed a similar procedure by combining specific marker words in a regression approach, e.g., groups of words tagged as positive feelings, friends, and sexuality belonged to the regression equation that he computed for nAff for his USA sample. Much has been written about the \\\"black box\\\" metaphor  of machine-learning approaches whereby the algorithm achieves high accuracy in predictions, but the individual components that the machine has used to build its model are not apparent to the user. This makes the task of explaining why the algorithm works a difficult one, and may even introduce other kinds of unpredictable bias or ethical concerns. Another limitation is the handling of parts-of-speech and contextual information in the language data that was used to build the machine-learning models. In principle, the pre-trained word embeddings that we used should take part-of-speech information as well as the multiple contexts in which words derive their meanings (the problem of polysemy) into account. NLP deep learning experiments with word embeddings show that they do capture some syntactic information similar to parts of speech. However, as we previously noted, the current state of affairs is such that polysemy still presents a significant problem for most publicly available pre-trained word embeddings. While human coders are naturally able to process extra-linguistic information and rely on their wealth of experience to deal with polysemy, much research is needed before NLP machine-learning approaches can approximate the human in this respect. Nonetheless, given the intensity of research interest on word embeddings in deep learning, it is likely that new techniques will soon become available to better account for polysemy and word order. An additional limitation of our research stems from a challenge inherent in building representative training datasets for data such as ours--PSE stories--which notoriously have an imbalance in positively coded sentences relative to sentences where motive code is absent (see online supplementary material which presents a breakdown of motive codes by sentence in our training data). Since the vast majority of sentences in our datasets had no motive imagery present, we controlled for this imbalance by ensuring that training and validation sets had roughly the same number of motive codes for each motive, but this meant that we could not control for persons by ensuring that their data occurs completely in the training or completely in the validation set. It is thus possible that some of what the machine learns from the training data is the linguistic style of persons whose sentences occur in both the training and test datasets. This potential confound should be mitigated in future research with the use of larger training datasets that would allow blocking of persons so that their data appears entirely either in the training or in the validation set. Finally, the fact that the three models we used showed such great variation in learning mappings between words within sentences and appropriate motive codes highlights the difficulty of such research. For instance, in UD2, while correlations between CNN and CNN2D model predictions for nAch and the human-coded scores were high (rs above.80) and statistically-significant, the correlation for the TCN model prediction was substantially lower at r = .42, and non-significantly so. This suggests that the TCN model was unable to represent nAch scoring very well, compared to the other two models. As illustrated by the 'black box' metaphor, since we don't have a clear idea of what kind of mappings the computer is learning in order to predict unseen data, it is difficult to identify exactly what to change or address in the underlying dataset in order to improve how and what the computer learns. This is one of the biggest issues in machine-learning research and is inherent in using stochastic gradient descent algorithms, which include some degree of random activation. We have tried to partially mitigate this by using machine-learning libraries that allow for deterministic output, such that training a model with exactly the same data and settings on the same hardware leads to the same results. This may allow us to 'reverse engineer' the neural mappings in the future, but as a small change in one of the parameters can easily change how and what the model learns, and given limited resources, it is as much an art as a science to make educated guesses that improve the results. This is particularly the case given the size of our training dataset (less than 100,000 training samples), which in machine learning terms is tiny. Another possible reason for the uneven performance of our models on different motives is the imbalance in source data for the PSEs in the training dataset: There were more contributing studies that focused on nAch as the motive of interest, compared to the other two motives. We requested for PSE training data from close collaborators in our immediate social network, and since one of the authors (JSP) of the current research focuses on achievement motivation, this may have resulted in more contributions from other researchers who also examine nAch more closely. While we were initially unaware of the need to include a balanced representation of the three motive categories in building the training data, we know now that there is most likely an advantage for future automation researchers to be more intentional about including datasets from a more diverse range of studies that examine multiple motives. Our research presents a first look at whether it is possible to build a machine-learning based algorithm to approximate human-coding of implicit motives. Over three unseen datasets, it seems that the computer is able to predict motive scores at a moderately high level of concordance with human-coded motive scores. Moreover, there is evidence of convergent, divergent, and causal validity for machinepredicted scores. Given that coding of implicit motives is a difficult task for humans, the results we have achieved with this research suggests that there is merit in pursuing machine-learning-based automation of implicit motive coding. Several directions already come to mind for future researchers to undertake, such as: Developing larger training datasets with more diverse sources of data, including training data from studies which investigated all three motives in a balanced manner, using deeper machine learning model architectures, and experimenting with more complex contextual embedding models of words and sentences. Should the automation enterprise be successful in the near future, it would benefit motive researchers in terms of reducing the research burden for manpower and time, as well as appeal to interested researchers who are otherwise unfamiliar with implicit motive measurement.\",\"577240491\":\"Text data are everywhere. Researchers may obtain text data from sources such as the internet, literary collections, archival entries, and experimental psychology's open-ended responses. Compared to traditional response scales in psychological research, embracing text data allows more unobtrusive and unconstrained approaches to measurement. For example, social media data provide information about participants' cognitions, free from demand characteristics associated with some laboratory studies. Using open-ended (vs. forced-choice) responses in controlled settings also enables more ecologically valid and data-driven study of psychological processes and content. These benefits appear in studying emotion  and racial categorization , challenging previously held findings by employing free-response measures that circumvent researcher constraints on participants' responses. For example, despite several studies showing that Americans categorize Black-White mixed-race faces as Black when only allowed to make Black versus White categorizations , in a free response task participants most frequently indicated perceiving these targets to be Hispanic or Middle-Eastern. These kinds of online and open-ended text data, however, often need some form of dimensionality reduction and numerical representation for interpretation (e.g., due to the large number of words that may refer to the same overarching construct of interest), making text analysis methods necessary. Language and text analysis have a long history in psychology (; for a review) and affiliated fields ranging from Sociology and Political Science  to Computer Science. In social and personality psychology in particular, numerous studies have made use of text analysis to obtain novel insights into human traits and behaviors. For example, studies into status differences in language use have shown that higher (vs. lower) status individuals tend to use we more often than I as pronouns. Other studies have applied text analysis to interpersonal relations, finding for example that members of longer-lasting relationships tend to match their linguistic style and use more positive emotional words. Tapping into the vast amounts of online data, text analysis in the field even shows promise of predicting health-related behaviors  and bringing in more explanation into descriptive frameworks of personality. Despite the advantages, creating and validating text analysis instruments such as dictionaries differs considerably from developing traditional scales, and currently not many appropriately reviewed guidelines exist. As a result, many areas have yet to fully incorporate text analysis methods into their repertoire. An example is stereotyping, which despite being one of the largest research areas within social psychology, suffers from a dearth of specialized text analysis methods and literature that may support new avenues of research (reviewed below). 1 | CURRENT APPROACHES TO TE X T ANALYSIS IN PSYCHOLOGY Recently, advances in natural language processing in machine learning allow easier extraction of information about psychological processes and content. The most common method to analyze text data in psychology has traditionally been human coding. In this approach, each text is evaluated by a group of human judges in terms of how much it reflects a construct of interest. Measures of agreement between human judges often document reliability. Evidently, however, this approach is time-consuming and resource-demanding, and these limitations rapidly worsen the more data that need to be coded. Furthermore, this approach for text analysis lacks standardization--that is, judges coding may vary across studies or laboratories. An increasingly popular alternative to per-study human coding of text is offered by dictionaries. Dictionaries list words that are indicators of the construct of interest. Once created, dictionaries are a standardized approach for coding text data, across studies, without additional human judge intervention. For this reason, they are also less resource-intensive and time-consuming for users. Dictionaries are also easy to use in analysis (vs. some more advanced natural language processing methods). The analysis process most often consists of counting the number of words in a text that are included in the dictionary. The larger the number of words from the dictionary that are present in the text, the higher the score for the construct of interest measured by the instrument. To illustrate, if evaluating the positivity of a particular text (e.g., a self-description, or a diary entry), a researcher would count the number of words that fall into a positive valence dictionary (e.g., \\\"good,\\\" \\\"nice,\\\" \\\"amazing\\\") as a measure of the constructs. The most widely used set of dictionaries in psychology and akin areas is the Linguistic Inquiry and Word Count (LIWC;). LIWC has been the benchmark for studying text related to content as varied as emotion, social relationships, thinking styles, among others. The original creation and wide usage of the LIWC dictionaries highlights both that available dictionaries are cheaper and easy to implement, and that the cost of creating new dictionaries in the first place may be prohibitively expensive and time-consuming for many researchers. For example, many of the LIWC dictionaries used up to 8 judges across several stages in order to manually expand the word lists. Given that the constructs measured by existing dictionaries are inevitably limited compared to the diversity of constructs studied by psychologists, more accessible methods to facilitate dictionary creation are useful. For example, even topic areas as central to social psychology as stereotype content  lack comprehensive specialized instruments for their measurement in text, an issue we sought to address in the current paper. 2 | STEREOT YPE CONTENT Stereotypes are beliefs about social groups and are encoded and shaped through language. In fact, a myriad of linguistic factors matter in the perception of social groups: from the natural language itself (e.g., the presence of grammatical gender in a language affecting gender representations;), to the concreteness of the language used (e.g., ingroups being described more abstractly than outgroups when performing positive actions;), to the part of speech used (e.g., group cues presented through nouns rather than adjectives lead to stronger stereotype-congruent inferences;). The study of stereotypes through explicit person descriptions, in particular, has one of the longest traditions within social psychology (;, Study 4;). However, to date, no comprehensive instruments for the analysis of text data have been developed in the area. This article provides such an instrument for measuring several relevant dimensions of content. The stereotype content model (SCM;), a wellknown current framework, proposes that people primarily use two dimensions to think about individuals and groups: warmth (i.e., is this target a friend or a foe?) and competence (i.e., can this target act on their intentions?). A large body of research has corroborated that evaluations along these dimensions occur cross-culturally. The combination of the two core dimensions also predicts intergroup emotions and behavioral tendencies. More recent models of stereotype content have either defined different facets of warmth and competence, or proposed novel, distinct dimensions of stereotype content. For example, Abele and colleagues (2016) suggest subdividing Warmth (also called Communion) into friendliness\\/sociability and morality facets and Competence (also called agency) into ability and assertiveness. The recent Agency-BeliefsCommunion model (ABC;) introduces beliefs (i.e., religious-secular beliefs and political orientation) and status. Thus, stereotype content dimensions are still contested, and open-ended data could shed some light on this issue. 3 | TE X T ANALYSIS IN STEREOT YPE CONTENT The stereotype content literature has so far largely relied on traditional metrics of measurement, in particular Likert-type scales measuring how much a social group allegedly possesses a particular dimension of content. A couple of studies  have used some LIWC dictionaries to measure warmth (e.g., the family and friend dictionaries) and competence (e.g., the work and achievement dictionaries). However, because these dictionaries were not designed to measure those constructs, they may cover both a small subset of appropriate words and correlated constructs rather than the target concepts. For example, the LIWC affiliation dictionary includes words such as friend or friendly, but not low-directional antonyms (e.g., enemy or unfriendly). In the absence of a specialized indicator or separate dictionary for the antonyms of these dimensional constructs, text data that include responses along the whole dimension (such as stereotypes) will suffer from lack of coverage or loss of information, depending on the application. Finally, a recent study  developed dictionaries of communion (similar to warmth) and agency (similar to competence) using the LIWC development approach, but these included only a subset of possible words (e.g., only high directional), did not provide explicit indicators for the different facets of these dimensions, and did not cover other stereotype dimensions. For an area such as stereotype content, where responses go beyond a small set of categories, to a large number of possible nouns and adjectives, developing a more comprehensive instrument becomes even more vital to faithfully characterizing text content. Potentially, this instrument could expand current theoretically derived models of social cognition by exploring open-ended responses in controlled experiments, in addition to examining stereotype content in multiple untapped sources of text data online. For example, Fiske et al. (in press) argue that stereotypes obtained through open-ended and text measures may differ from traditional scale-based stereotypes, providing information into which stereotypes are more central to social groups' representation and improving predictive models of discrimination arising from stereotyping. For example, while traditional scale ratings of Warmth and Competence would place Doctors and Nurses as similarly high on both dimensions, spontaneous text responses (e.g., coded through dictionaries) suggest that Warmth is more representative of the stereotype content of Nurses while Competence is more representative of the stereotypes of Doctors. Furthermore, this type of information derived from text responses significantly improves predictions of attitudes toward social groups. Others argue that stereotypical explicit person descriptions extracted from large online corpora (e.g., social media) may sometimes function more like implicit than explicit stereotypes measured in traditional laboratory scales. These kinds of theoretical advances are greatly facilitated, and sometimes only possible, through the use of automated methods dependent on the existence of valid stereotype content dictionaries. In fact, the simple exploration of the structure of dictionaries in this article may provide some insights into the structure of stereotypes in natural language, as we briefly explore. In this article, we introduce novel stereotype content dictionaries that fill a void in the study of stereotyping in text. We develop these dictionaries using an approach (incorporating some natural language processing methods in novel ways) that is described in the text and made available through an R package for other researchers to use when developing dictionaries for other constructs of their interest. Finally, we use traditional and emergent techniques to evaluate the coverage, reliability, and validity of the dictionaries. This new approach provides a complementary way to automatize many processes, in order to facilitate new dictionaries that are also less coder-reliant, may handle more words, and address distinctive topics, among other benefits. We make available helper functions used to create dictionaries using this approach in the R package SemiAutomated Dictionary Creation for Analyzing Text (SADCAT), available at https:\\/\\/github.com\\/ganda lfnic olas\\/SADCAT. The package also contains functions to code text into the stereotype content dictionaries developed here. All data and code for the analyses presented here are also available at https:\\/\\/osf.io\\/yx45f\\/ ?. 4 | COVER AGE , RELIABILIT Y, AND VALIDIT Y Dictionary creation aimed to achieve three indicators of quality: coverage, internal reliability, and convergent validity. 4.1 | Coverage A traditional psychological scale can measure a construct with a few items sampled from a larger pool of intercorrelated items without wasting any data. However, with text measures, where participants choose the items (i.e., words) they wish to convey about the construct, a larger pool of items is needed to code the participants' responses. Coverage refers to the proportion of possible participant responses that is covered by the dictionary (i.e., the pool of items). Coverage will be domain-dependent. Thus, our dictionaries aim to explain a majority of participants' responses when prompted to provide stereotype content of social groups (i.e., stereotypes). Here, we use WordNet , a lexical database with semantic relations between words, in order to automatically expand an initial set of words into their synonyms, antonyms, etc., to increase the coverage of open-ended stereotypes provided by a sample of American respondents. 4.2 | Internal reliability We refer to internal reliability as the consistency and intercorrelations of the pool of items that make up the dictionaries. In other words, reliability measures whether words within a dictionary bear higher semantic similarity than words in different dictionaries. To measure semantic similarity, we adapt recent methods in natural language processing  to generate numeric vector representations of text data. Obtaining pairwise similarities from these vectors enables calculations of traditional metrics of internal reliability, such as the average inter-item \\\"correlation\\\" (in this case average cosine similarity) or Cronbach's alpha. 4.3 | Validity Validity is relatively straightforward as it is most similar to scales. Using the dictionaries allows us to code the construct of interest from participants' responses, which may then correlate with other constructs expected to be theoretically related (i.e., convergent validity) or unrelated (i.e., divergent validity). Here we test validity against multiple data sources and compare our comprehensive dictionaries with existing dictionaries used to measure stereotype content in text. We note that the current dictionaries are validated for the domain of explicit person descriptions. This is one of the most relevant and widely studied topics in social psychology, ranging from studies on social group stereotypes  to face impressions. Explicit and blatant stereotyping is very much alive and widespread (; Roberts & Rizzo, in press), and these instruments aim to measure their use in experimental and online settings. However, based on factors such as social desirability, text data may instead portray stereotypes implicitly or indirectly, or may provide information on the author rather than targets described in text. Our dictionaries may likely be useful for these applications as well , but future validation of these applications is necessary. 5 | DIC TIONARY CRE ATION OVERVIE W Dictionaries evolved through an iterative process that subsequent sections will explain, and that is summarized in a flowchart in Figure 1. To anticipate: In Study 1 we identified from the literature words covering relevant stereotype and person perception dimensions, forming an initial set of seed words dictionaries. In Study 2 we collected stereotype content text data to test how much the initial seed words accounted for participants' responses (i.e., coverage). In Study 3, we used WordNet to expand the seed words to a larger dictionary. We iterated the process of testing coverage and adding words until we reached a good proportion of dictionary coverage. After completing the dictionaries, in Study 4 we tested dictionary reliability using the similarity metrics discussed above. Finally, we tested the validity of the dictionaries in four ways. First, we explored the convergent and discriminant validity of our dictionaries in comparison to existing dictionaries that measure related constructs (Study 5). Second, we tested validity in relation to scale ratings, that is, whether experimentally requested responses coded with our dictionaries correlated with stereotypes measured by scales (Study 6). Then, we tested validity in relation to human ratings, that is, whether human coders identified the semantic meaning of each dictionary from a small subset of its items (Study 7). Finally, we used the dictionaries to code for real-world data and correlate these with human coding along the dimensions (Study 8). In these studies, we report all measures, manipulations and exclusions. Depending on the within-subject variance, power analyses for all studies reveal over 80% power to detect small effects of r or f between 0.1 and 0.2 in our main tests. Sample size was determined before any data analysis. All validity studies with human subjects were approved by the University ethics committee, and adhered to the ethical guidelines specified in the APA Code of Conduct and the US Federal Policy for the Protection of Human Subjects (including informed consent, right to withdraw, and debriefing). 6 | STUDY 1: CRE ATING SEED DIC TIONARIES In Study 1, we identify stereotype content dimensions that have been previously formalized in stereotype content models and create initial, theory-driven seed dictionaries. 6.1 | Methods We reviewed the literature  for lists of words used to measure friendliness\\/sociability, morality\\/trustworthiness, ability, assertiveness\\/ dominance, status, political beliefs, and religious beliefs in relation to social groups. For every word, if not already included, we also obtained its antonym. 6.2 | Results The final seed dictionaries consisted of 341 distinct words, with their corresponding theoretical direction (i.e., high or low on their corresponding dimension, based on how they were labeled in the reviewed literature). See Table 1 for example words; for a full list see online repository. Because words can have multiple senses (e.g., warm can refer to psychological or physical warmth) the researchers independently went through the list of seed words and decided on the most appropriate sense(s), based on their part of speech, definition, and example sentences, which resulted in a list of 455 senses. The final senses were those which two of the researchers agreed on, 90% of the total senses selected by either of the two researchers. Dictionaries were mostly balanced in terms of high and low senses, but there were some slight imbalances such as more low (vs. high) Morality words and more high (vs. low) Ability words. Note that by high and low we do not mean valence: It is simply an indicator of which end of the antonymy dimension the word refers to; whether one or the other antonym is coded as high versus low is arbitrary. For example, we coded beliefs as ranging from progressive to traditional, and thus high direction in this dictionary means that the word is more about traditional beliefs than progressive beliefs. For more information about the seed dictionaries please refer to the Supplement. 6.3 | Summary In an initial theory-driven and human-dependent step, we collected from the literature small dictionaries containing seed words for the constructs of stereotype content. These seed word dictionaries would be expanded in subsequent steps to obtain the final instruments. 7 | STUDY 2: SEED DIC TIONARIES COVER AGE In Study 2 we perform an initial test of coverage on development data. That is, we explore how many of participants' open-ended stereotypes about salient U.S. social groups are accounted for by our seed dictionaries. 7.1 | Methods Development data allowed for initial tests of coverage and validity of the dictionaries. The development data consisted of a survey (N = 201, Mage = 37.8, 55% female; 85% White, 6% Black, 3% Hispanic, 3% Asian) asking for participants' spontaneous thoughts about characteristics that different social groups would have. We used a total of 20 social groups (e.g., \\\"Asian\\\", \\\"Elderly\\\", \\\"Wealthy\\\"), sampled from the literature, and showed five to each participant, in random order. Participants provided 10 open-ended single-word responses for each target. Next, participants saw the same social groups again and rated them on warmth (items: friendly, sincere) and competence (items: efficient, competent) using a scale ranging from 1 (not at all) to 5 (extremely), as well as a measure of familiarity with the social group. Finally, participants completed some demographic questions. The open-ended responses were preprocessed (e.g., lower cased, deleted grammatical signs; see Supplement). 7.2 | Results As expected, the words used in the existing literature to describe content dimensions were not a good measure of the diversity of open-ended responses, accounting for only 20.2% of our development data (6.2% of distinct responses). Mapping the content of spontaneous stereotypes requires accounting for most of the responses. However, open-ended responses allow for any number of synonymous terms that have not been exhaustively listed in previous studies. For instance, even though we were able to find in the literature words such as thief referring to morality, other synonyms such as robber were absent. For this reason, in the next study we expand the dictionaries using WordNet to improve coverage. 7.3 | Summary In this study we tested how many spontaneous stereotypes provided by a sample of American participants in response to a salient sample of social groups were covered by our seed dictionaries. This coverage was very low, meaning that deploying the seed dictionaries to analyze laboratory or online text data would result in large amounts of missing data and undercounting of construct-relevant words. In the next study we address this limitation. 8 | STUDY 3: E XPANSION AND FINAL DIC TIONARIES Given the low coverage of the seed dictionaries, in Study 3, we use WordNet  to automatically expand the seed dictionaries and improve coverage. 8.1 | Methods Although one could manually gather many words using suggestions from field experts, that labor- and time-consuming method would be limiting. WordNet offers one automated way to obtain a large pool of items by adding words that are semantically associated with a smaller pool of seed words obtained from the literature. WordNet  is a large lexical database for the English language. The database contains metadata about English words, including part-ofspeech (i.e., noun, adjective, verb, and adjective), glosses (i.e., short definitions), and usage examples in sentences. Most importantly, WordNet distinguishes words' different senses (e.g., warmth may refer to both psychological warmth and temperature), and these senses then associate with other words\\/senses through several relations such as synonyms and antonyms. Previous research in other fields has used WordNet to expand dictionaries. Here, we apply the procedure to the creation of Stereotype Content Dictionaries, as well as formalizing the procedures in an R package (https:\\/\\/github.com\\/ganda lfnic olas\\/SADCAT) made available for researchers to create other dictionaries of psychological constructs using this semi-automated approach. An in-depth explanation of the expansion procedures is offered in the Supplement. After the first round of dictionary words expansion, we explored unaccounted-for words to identify some potential additional topics and used WordNet to expand on these topics. A few specific unaccounted-for responses were added manually to a catch-all dictionary with words that denoted lack of knowledge (e.g., \\\"I don't know\\\" or \\\"?\\\"). We note that some items in the dictionaries contain multiple words, as WordNet includes some multiple-word entries (e.g., for disease names). Unlike traditional stereotype content models , which focus on two or three dimensions believed to be primary, the goal here was to create dictionaries with high coverage for stereotype content, including dimensions that may be used less often than the theoretical dimensions, resulting in this data-driven step. For a follow-up on this topic, see Nicolas et al. (under review). Additional versions of the dictionaries (e.g., shorter versions) are discussed in the Supplement. 8.2 | Results The final dictionaries contained 14,449 words across 28 dictionaries. Final dictionaries varied in length from seven (lack of knowledge) to 2,402 (morality) preprocessed words (see Table 2 for descriptives, and online repository for full dictionaries). Differences in length may to a small degree reflect biases in WordNet or seed list, but most likely reflect differences in the semantic generality of the dimensions. For example, morality encompasses a wider set of related constructs in the WordNet network than concepts related to uncertainty. Some of these differences in semantic generality may be further explored for the generation of hypotheses on the linguistic nature of different stereotype contents. For each word, we also obtained its SentiWordNet  valence. This metric indicates how positive or negative each word is, based on human coding. This differs from direction, which is specific to each dictionary (e.g., aggressive is low valence, i.e., it is evaluated as a negative trait, but high direction for the assertiveness dictionary, i.e., it indicates an assertive trait). For words with multiple senses, the direction and valence were averaged across senses. The final dictionaries accounted for 77% of the development data responses, indicating a significant improvement in coverage from the WordNet expansion. Given the potential for overfitting to these specific data, in a confirmatory study (described later in the first validation study), the dictionaries accounted for 84% of the responses. Thus, the dictionaries can account for the vast majority of stereotype-relevant responses in traditional groups explored in stereotyping research. 8.3 | Summary In this study we created the final dictionaries by expanding them into semantically related words in an automated fashion. The final dictionaries covered multiple dimensions of stereotyping, from Morality and Ability, to physical features. These expanded dictionaries accounted for over three-quarters of the stereotypes of salient American social groups, suggesting they would be useful in minimizing missing data in the analysis of stereotypes from text. 9 | STUDY 4: INTERCORREL ATIONS AND INTERNAL RELIABILIT Y In this study we explore how some of the dictionaries intercorrelate and how internally reliable is each dictionary. Internal reliability in the context of dictionaries refers to the semantic internal consistency of the dictionaries. That is, are words within a dictionary semantically similar to each other? 9.1 | Methods To numerically represent text and calculate semantic similarities we used word embeddings, which are numerical vector representations of words derived from models trained on large corpora of natural language text (; see Figure 2). The specific word embeddings used here are Word2Vec's model pretrained on Google News  and Glove's model pretrained on the Common Crawl (; presented in Supplement). These vector representations encode each word's position in a multidimensional semantic space, derived from word co-occurrences in large corpora of text. The background model computations are beyond our scope but a brief explanation of the intuition behind training these models is provided in the Supplement. If we then take two of these vectors representing two words in our dictionaries, we can measure their semantic similarity by calculating their cosine similarity. Cosine similarity can be interpreted similarly to a Pearson correlation, with larger numbers indicating more similarity. In theory, cosine similarity can range from -1 to 1, but for word embeddings the values tend to be high for words in a common domain, such as words used to describe people, as in our dictionaries. Using this metric, we may obtain high similarity for words such as river and ocean, as they are semantically related, and lower scores for words such as river and stove. Others have recently noted the novel technique described here for internal reliability as a measure of semantic coherence , but not directly for the purpose of evaluating dictionary quality. In order to obtain correlation measures for the different dictionaries, we computed their numeric representation using the previously described word embeddings. Specifically, we obtained the vectors for each dictionary's word and averaged them, which resulted in a vector representation for the dictionary, allowing us to calculate cosine similarities between them. Alternatively, one can use traditional Cronbach's Alphas to assess reliability. We used the previously described measures of similarities as the inter-item correlations and applied the formula for alpha. 9.2 | Results We present a subset of dictionary intercorrelations (using cosine similarities derived from Word2vec's pretrained model) in Table 3. Additional similarities are included in the Supplement. These results show general patterns such as higher similarity between Beliefs and Warmth (vs. Competence) that replicate previous laboratory findings (e.g., association between Progressive-Conservative beliefs and Warmth;) in a natural language corpus. These results also hint at other theoretical insights derived from the semantic relatedness of these stereotype contents in natural language. For example, Deviance-related words were more closely associated with Competence than Warmth, suggesting that, at least in the natural language corpora used here, descriptions about targets' distinctiveness tend to be more semantically related to the Abilities and Agency domains than the Morality and Sociability domains. We present this as a hypothesis-generating secondary finding derived from the development of instruments through our automated procedure. A straightforward inferential test for the internal reliability of dictionaries is to check whether the average pairwise similarity between words from the same dictionary is larger than the average pairwise similarity between words from different dictionaries. Indeed, using the pretrained Word2vec's model similarities such a test reveals that words within a dictionary are more co-similar (M = 0.177) than words between dictionaries (M = 0.097), t(29.14) = -8.69, p < .001, d = 1.67. This large effect size denotes the semantic consistency of the WordNet network, and therefore our dictionaries. The results for Cronbach's Alphas also indicated high internal reliabilities (> .9) for most dictionaries, with the exceptions of the \\\"lacks knowledge\\\" dictionary (.37) and \\\"fortune\\\" dictionary (.85), which had very few items found in the word embeddings model. We do note that alpha has some limitations as a measure of internal consistencies of dictionaries which are discussed in the Supplement. 9.3 | Summary Our dictionaries showed remarkable internal consistency. Words within a dictionary were much more similar in meaning than words from different dictionaries. This pattern was also reflected in very high Cronbach's Alpha scores. This suggests that indeed the dictionary words cluster together as necessary for a construct's indicators. Appropriate internal reliability allows for further explorations of validity in the following studies. 10 | STUDY 5: VALIDIT Y A S REL ATED TO E XISTING INSTRUMENTS In order to obtain estimates of convergent and discriminant validity in the context of existing dictionaries, we compare our dictionaries to the recently developed Communion (akin to Warmth) and Agency (akin to Competence) dictionaries , as well as some of the LIWC dictionaries. We expected to find that our Warmth and Competence dictionaries correlate with the Communion and Agency dictionaries, while some of our additional dictionaries correlate with relevant LIWC dictionaries (e.g., our Religion dictionary with LIWC's Religion dictionary). We also expected lower correlations with theoretically unrelated dictionaries, such as LIWC's Numbers dictionary (including words such as dozen or nine). 10.1 | Method We adapted the procedure employed by Pietraszkiewicz and colleagues for this study, as well as using their Communion\\/Agency dictionaries. The Communion\\/Agency and LIWC dictionaries contain word stems that need to be expanded into sense-appropriate words (e.g., commun* into communion). In order to do this, we adapted Pietraszkiewicz et al.'s (2018) procedure of using the 2,500 most frequent words in a large English corpus (Google Web Trillion Word Corpus;) as possible expansions. In order to obtain correlation measures for the different dictionaries, we computed their numeric representation using the previously described word embeddings. Specifically, we obtained the vectors for each dictionary's word and averaged them, resulting in a vector representation for the dictionary. With this information, we could obtain the cosine similarity between different dictionaries. Most of the dictionaries we developed had no appropriate existing comparison, but those that did include: Warmth and Communion (Pietraszkiewicz et al.); Competence and Agency (Pietraszkiewicz et al.); Status and Power (LIWC); Religion and Religion (LIWC); Health and Health (LIWC); Appearance and Body (LIWC); Work and Work (LIWC); and we included LIWC's \\\"numbers\\\" dictionary for an irrelevant comparison to all dictionaries. 10.2 | Results As expected, corresponding dictionaries had higher similarities than non-corresponding dictionaries. Results based on word2vec embeddings are shown in Table 4. As shown, our dictionaries showed the highest similarity to their corresponding theoretical constructs measured by similar instruments. For example, Competence is more similar to Agency (cosine similarity = .61) than other dimensions and Warmth is most similar to Communion (.57). As a comparison, our dictionaries are dissimilar to theoretically irrelevant dictionaries, such as LIWC's numbers (on average around.1). Additional co-similarities between other dictionaries which also support our conclusions are presented in the Supplement. 10.3 | Summary To summarize this first validation study, we found evidence that our dictionaries correlate with existing, theoretically relevant, dictionaries, providing evidence for convergent validity and situating our dictionaries in relation to current measures. Similarly, our dictionaries showed much lower similarity to theoretically irrelevant constructs, such as number words. 11 | STUDY 6: VALIDIT Y A S REL ATED TO R ATING SC ALES Given that scales are the traditional and most commonly used way of gathering information in psychology, we next tested how our dictionaries related to scale stereotype ratings of social groups. For each social group, in addition to seven open-ended responses, we collected participants' Likert-type ratings on stereotype content dimensions of warmth, competence, and beliefs. We planned to test how well the scale ratings were predicted by our sociability, morality, ability, assertiveness, beliefs, and status dictionaries, all of which have been linked to these dimensions in the literature. 11.1 | Method Participants (N = 251) were recruited through Amazon Mechanical Turk (Mage = 33.3, 52% female; 76% White, 10% Black, 5% Hispanic, 4% Asian). In an initial block, participants saw a sample of four social groups, from the same social groups as the development data. The instructions read: \\\"Please indicate how the following people are viewed by society. Please note that we are not interested in your personal beliefs, but in how you think these people are viewed by others.\\\" They were also told to use one word per box, two maximum, and then saw the prompt \\\"As viewed by society, what are the characteristics of a person who is...\\\" followed by the social group and seven boxes for responses. These responses were preprocessed in the same way as those from the development data. In a second block, they saw the same groups, but rated them on scales. The prompt read \\\"Please indicate how the following people would be viewed by society. Please note that we are not interested in your personal beliefs, but in how you think these people are viewed by others.\\\" This was followed by \\\"To what extent would most individuals in our society view a person who is (social group) as...\\\" and a 1 (not at all) to 5 (extremely) scale for the items \\\"Friendly\\/Sociable\\\", \\\"Trustworthy\\/ Moral\\\", \\\"Self-confident\\/Assertive\\\", \\\"Competent\\/Skilled\\\", \\\"Wealthy\\/ High-status\\\", \\\"Politically conservative\\\". \\\"Religious\\\". These items corresponded to the facets of sociability, morality, assertiveness, ability, status, politics, and religion. To form indexes, \\\"Friendly\\/Sociable\\\" and \\\"Trustworthy\\/Moral\\\" were combined for warmth (alpha = .76), \\\"Selfconfident\\/Assertive\\\" and \\\"Competent\\/Skilled\\\" were combined for competence (alpha = .86), and \\\"Politically conservative\\\" and \\\"Religious\\\" were combined for beliefs (alpha = .7). After these blocks, participants completed demographic questions. Analyses were mixed-effects models with participants and social groups as random factors, and observations were each participant's responses to a group (i.e., averaging across the seven responses for the text data). In terms of relevant variables, note that while scales measure only direction (e.g., low to high competence in a 5-point scale), our theory-driven dictionaries measure as separate variables both prevalence and direction. Prevalence refers to the number of words related to the dimension (e.g., out of a participant's seven responses, more competence-related words indicate higher prevalence of competence). Direction refers to the antonymy dimensional end of the word. Words high on a dimension (e.g., friendly for Warmth) were coded as 1 for that dimension's direction, and words low on the dimension (e.g., unfriendly for Warmth) were coded as -1. If direction was unknown it was coded as 0, and if the response was not in the dictionary, it was coded as missing. Thus, dictionary direction variables ranged from -1 to 1. If the dictionary is valid, dictionary direction should predict scale ratings: the higher the direction score, the higher the scale score. In the main text we present only the direction results, and in the Supplement, we include results including prevalence, as well as a comparison with the previously introduced Communion and Agency dictionaries. Both these results support the incremental validity of these dictionaries. 11.2 | Results We found the expected patterns of results, with all dictionary direction indicators predicting scale ratings. For example, responses coded as high warmth significantly predicted higher scale ratings on warmth dimension (r = .36, p < .001). Responses coded as high competence significantly predicted higher scale ratings on competence dimension (r = .30, p < .001). See Table 5 for all direction results. We note however that the religion direction indicator was not significant for predicting the religion item. This was probably due to the low rate of religion-related open-ended responses, which greatly lowered the useable data for models with this variable. Also as expected, informal observations of crossdictionary models (e.g., competence direction predicting Warmth scales) showed smaller and\\/or non-significant results compared to models for congruent dimension dictionaries (see Supplement). Additional secondary and potentially hypothesis-generating observations include the finding that scaled Warmth was better predicted by the Morality (vs. Sociability) spontaneous stereotypes (in line with models arguing for the priority of the Morality facet;). 11.3 | Summary Results from this study support the validity of our main Warmth and Competence dictionaries. We found that these dictionaries applied to open-ended stereotypes of social groups predicted how these social groups were rated using traditional numerical scales. This finding suggests that the dictionaries indeed capture judgments of warmth and competence in the context of social groups. 12 | STUDY 7: VALIDIT Y A S REL ATED TO HUMAN JUDGMENT A third test of validity used human ratings of thematic identification to study the extent to which the dictionary words reflect human semantic judgments. Specifically, we expected to show that human coders appropriately identify words from a dictionary as belonging to it. 12.1 | Method Participants (N = 245) were recruited through Amazon Mechanical Turk (Mage = 33.3; 61% male; 78% White, 9% Black, 6% Asian, 2% Hispanic). Participants saw 13 blocks, each of which presented a random sample of six words of a dictionary. Instructions asked participants to identify the common theme of the six words and to rate on a scale from 1 (Not at all) to 6 (Extremely) how well they fit into a condensed list of our dictionaries, in lay terms: sociability\\/friendliness, morality\\/trustworthiness, confidence\\/autonomy, ability\\/skill, socioeconomic status, political or religious beliefs, health, work\\/ professions, body properties\\/parts\\/appearance, familiarity\\/family, feelings\\/emotions, and geography. Participants were told to consider words from both directions (e.g., both morality and immorality) to refer to the same theme and were asked to base their responses on the objective meaning of the words rather than personal opinion. Validity in this case is indexed by whether human coding of the content of a dictionary's words matches the construct they are intended to measure, and to a higher extent than constructs they are not intended to measure. Analyses consisted of a series of mixed models (participants as random intercepts), one for each dictionary. Thus, for example, the morality model was based on data from the block which showed items from the morality dictionary, and so on for all other dictionaries. Because each block had 12 questions, one for each dictionary label (e.g., morality\\/trustworthiness, confidence\\/ autonomy), each of these questions (or labels) became a level in a contrast-coded predictor. The response to each question was the outcome variable, allowing us to statistically compare the mean response for each question, for each dictionary. Thus, for example for the morality model, using words from the Morality dictionary, we could statistically compare if coders had higher ratings for the morality\\/trustworthiness item than for the other 11 items. 12.2 | Results Analyses largely supported the expected patterns. Given the large number of tests, results are summarized in Table 6. In general, the congruent score for each dimension was significantly higher than the score for all other incongruent dimensions. For instance, human coders rated words randomly sampled from the Ability dictionary to refer more to the concept of Ability (M = 3.77) than the concepts of Morality (M = 2.73) or geography (M = 2.47), ps < 0.001. Exceptions were only for the Sociability and Assertiveness dictionaries, where scores on the emotion response option were not significantly lower than scores on the congruent dimensions. Possibly, sociability and assertiveness are simply more highly correlated to emotional words; for example, the Stereotype Content Model posits that stereotypes trigger emotions , such as others' positive or negative emotions telling us about their friendliness, or because approach and avoidance emotions  relate to assertiveness. However, future studies could further explore this issue, and understand the overlap between these dictionaries when making inferences. In other words, at least on this metric of coder identifiability, the dictionaries for sociability and assertiveness should be expected to also reflect emotional content. 12.3 | Summary To reiterate, Study 7 provided evidence that human coders were able to identify the dimension that the dictionaries meant to measure from small samples of words from the dictionary. This provides evidence for the validity of the dictionaries as they are correctly identified by human coders. 13 | STUDY 8: VALIDIT Y IN NATUR AL L ANGUAGE In previous tests of validity, we have isolated the content words to test their semantic validity in relation to the construct of interest. This isolated use of the words is also useful in an experimental setting where researchers may ask for single-word responses. However, much of the use given to dictionaries is on natural language data in longer formats (e.g., social media posts;), where the words are not isolated but rather parts of sentences. Thus, it is important to test the validity of the dictionaries in the context of longer texts, as this is one of the most relevant uses of the instrument. In order to do this, we collected obituaries from the web and analyzed them using our dictionaries. Subsequently, we predicted human coders' ratings of the obituaries along multiple dimensions, in order to evaluate the dictionaries' validity. 13.1 | Method We collected 500 obituaries from various newspaper websites indexed by obituaries.com. Obituaries made an appropriate sample for several reasons. First, they are largely person descriptions, making them the most likely type of text for the dictionaries' use. Second, they were likely to include multiples of the dimensions measured by the dictionaries (e.g., sociability, beliefs, status, health), allowing for validation of most dictionaries using the same sample. Finally, their format is highly standardized, allowing for removal of irrelevant information for more efficient coding and analysis. To code the obituaries with the dictionaries, we used a similar strategy to the one used in Study 6. Specifically, we obtained a direction score (ranging from -1 to 1) by matching all the words in each obituary to each dictionary and averaging them. For the human coding, we recruited two coders and asked them to code the dimensions for which we had a directional variable using the following scale, illustrated with the sociability dimension: Based on the text, how unsociable-sociable do you believe the person described is? (1--not at all to 5--a lot; or NA if the text does not provide enough information to rate the target's sociability). This scale was used for sociability, morality (immoral-moral), ability (low-high ability), assertiveness (unassertive-assertive), status (low-high status), and beliefs (progressive\\/non-religious-conservative\\/religious). Both coders rated all 500 obituaries on all dimensions, allowing us to calculate their interrater reliability. The Intraclass Correlation Coefficients (for average fixed raters) are also reported in Table 7. The scores of the coders were averaged. To test for validity, we ran a set of linear models predicting the human numerical direction code from the dictionary direction. Additional results including prevalence scores and comparison with the benchmark Communion and Agency dictionaries are presented in the Supplement and further establish the incremental validity of these and additional secondary dictionaries. 13.2 | Results All the results are presented in Table 7. We found that, regardless of the model used, the dictionaries' validity was supported. Specifically, we found that human perceptions of whether a dimension was high or low in a text correlated with the dictionaries' coding of this direction. For instance, an obituary including descriptions of an individual such as \\\"member of the national honor society\\\" and \\\"vice president of the... club\\\" received very high scores on Socioeconomic Status from both the dictionary and human raters, while a text describing an individual as working in the food services industry received low scores on this metric from both the dictionary and the human coder. On average, this translated, for example, to a correlation of.24 between the Status dictionary and human coding, p < .001. The exception for the expected pattern was the Beliefs' direction indicator not reaching statistical significance (although other indicators were significant, see Supplement), potentially as a result of a lower rate of Beliefs-related words resulting in more missing data in the model. We note that a limitation of obituary data is that the person descriptions tend to include mostly positive words, which could potentially impact the estimates presented here. Nonetheless, as indicated by the significant direction results, our dictionaries were still able to capture the obituaries' subtle valence variations (valence correlates with direction, see Supplement). 13.3 | Summary In naturalistic data obtained from the internet, a potentially vast application for these dictionaries, we find that indeed the dictionaries showed validity in predicting human coding of obituaries. Our dictionaries were able to capture subtle variations in person evaluations in text along multiple dimensions of stereotype content. 14 | DISCUSSION In this article we created novel stereotype content dictionaries that have excellent coverage, reliability, and validity. To do this, we used a novel approach that is more automated than existing human-coded approaches and based on standardized sources such as WordNet and word embeddings models. Furthermore, we provided guidelines for the evaluation of text analysis instruments, including coverage, reliability, and validity. 14.1 | Summary of current studies The field of stereotype content is ever-growing but suffers from a deficit of studies exploring open-ended stereotype responses, and a lack of access to online text data due to the limitations of current instruments. The current studies used the following steps to develop dictionaries for the measurement of stereotype content: 1. Creating Seed Dictionaries (Study 1): We identified 341 words for the literature-relevant constructs of sociability, morality\\/ trustworthiness, ability, assertiveness\\/dominance, status, and political and religious beliefs, as well as indicators of their direction (i.e., high or low on the dimensions). In our case, this was a fully theory-driven step, but it was complemented by data-driven dictionaries in the following steps. 2. Seed Dictionaries Coverage (Study 2): We collected development data in which participants provided open-ended stereotypes about social groups. We tested how many of their responses were covered by our seed dictionaries. Seed words accounted for only about 20% of participants' stereotypes, an unacceptable level that prompted us to expand the dictionaries. 3. Expansion and Final Dictionaries Coverage (Study 3): We used WordNet to expand the seed words into fuller dictionaries. We also identified additional seed words from unaccounted-for responses and expanded those as well. The final version of the instrument has 28 dictionaries and 14,449 words. We also obtained the valence for all these words, in addition to direction for most of the dictionaries. The expansion of words resulted in over 80% coverage. We considered this coverage to be acceptable, and it was a considerable improvement on the existing items in the literature. 4. Internal Reliability Testing (Study 4): We obtained the pairwise similarities between all words in our dictionaries using word embeddings. These metrics indicated that words within dictionaries were more semantically similar than words between dictionaries, suggesting the expected internal consistency. 5. Validity testing (Studies 5-8): We found evidence for the validity of our dictionaries across four separate metrics. First, we established that an important subset of our dictionaries showed convergent and divergent validity based on pre-existing dictionaries. Second, for the theory-driven dictionaries, we used openended data to predict scaled warmth, competence, and beliefs ratings. These results showed that our dictionaries predicted social groups' predicted warmth, competence, and beliefs. In a third study, we presented human coders with subsets of words from each dictionary and asked them to rate how much the subsets referred to different contents. Participants were able to place the words in the expected dictionary with great accuracy. In a final study, we validated the dictionaries using natural language text that is likely to be the target of the instrument in non-experimental settings. Thus, we successfully created and validated high-coverage dictionaries in an area that lacked such instruments. Text data is vital for new and renewing fields of psychology. Developments in machine learning fields such as natural language processing have opened the door for psychologists to tap into these so-far underused sources of information. Being able to identify constructs of interest in text and create instruments for their measurement will generate opportunities to expand the science by allowing us to ask new questions or extend previous findings to a wider variety of contexts of potential higher ecological validity. In fact, preliminary versions of these dictionaries have already been used in research including the study of spontaneous stereotyping (Nicolas et al., under review), how social opinions may leak through non-verbal gestures (Lakshmi, Fiske, & Goldin-Meadow, in prep.), and how stereotypical biases in investment decisions from social interaction verbal data. Boghrati and Berger (under review) used the preliminary dictionaries to study a quarter of a million songs over 50 years and found that women were less likely to be associated with competence traits in song lyrics, with relative improvements over decades, and with variations across genres. In the future, we expect these dictionaries to be useful in the study of first impressions from faces, schematic processing (e.g., does stereotyping a target along one dimension activate another?), and dual process theories (e.g., recent research suggests that stereotypes extracted from online data resemble more implicit vs. explicit attitude processes;). In practical terms, the dictionaries may also be useful, for example, in recognizing hate and discriminatory text online or biases in machine learning models. Finally, we believe that the dictionary creation method used here and made accessible through the SADCAT package (https:\\/\\/github.com\\/ganda lfnic olas\\/SADCAT) will be useful in the creation of other needed dictionaries for psychological constructs in social and personality psychology (e.g., group entitativity, psychological essentialism beliefs). This is particularly the case given the increasing understanding that the digital behavioral footprint, including text data, must play a central (yet well-considered) role if we are to continue advancing our science. In this article we have created novel dictionaries for the measurement of stereotype content, and described their properties, coverage, reliability, and validity. We have also provided a tutorial on how to create semi-automated dictionaries for the measurement of other psychological constructs in text data. Previous approaches in psychology heavily rely on multiple human judges to create the totality of the dictionaries over multiple iterations of group discussion and subjective decisions, which is resource-intensive and time-consuming, and risks introducing selection biases based on a specific group of judges who might differ from other judges. On the other hand, the approach provided here largely automates the process, greatly reducing the time and resources necessary for the creation of the dictionaries. (We provide all the R code used here for readers to be able to implement the procedures for creating dictionaries of their own.) The use of WordNet in the development of dictionaries has multiple other advantages. Given WordNet's multi-sense network it is possible to obtain valence scores for specific senses using SentiWordNet. While most sentiment analyses rely on the words, SentiWordNet provides different valence scores for each sense. This is important for many psychologically relevant words that share meaning with less psychologically relevant words, such as warmth, a central concept in the field of stereotype content illustrated here, which has multiple other meanings, including of course physical warmth. This advantage reduces noise in sentiment analyses when the context of interest is known. In addition, knowing the sense of a word allows for superior translation to other languages using tools such as Babelnet. Babelnet allows translation from WordNet senses into their corresponding sense in other-language WordNets. Translation can depend on context, and this is facilitated by WordNet's structure, otherwise requiring manual translation by fluent speakers of the target language. Given the neglect of cross-cultural research, using WordNet and Babelnet to study text in multiple languages can provide a fruitful avenue for the generalizability of psychological findings. In fact, Spanish translations of the dictionaries using Babelnet on the corresponding senses are included in the R package as a preliminary instrument (pending further validation). Others have used automated methods to translate, for example, the LIWC dictionaries , as well as multi-lingual research using methods such as topic modeling and the Meaning Extraction Method to study issues ranging from self-schemas to sexual assault experiences. 15 | LIMITATIONS AND FUTURE DIREC TIONS Dictionaries, like any instrument, have limitations, particularly when used in long-format text data routinely found in most online sources such as social media. When dealing with sentences and paragraphs, a simple word-counting approach misses some of the sentence-level structure, potentially resulting in more noisy estimates. For example, negations, modifiers, and sarcasm are missed by dictionaries in a word-counting approach (however, our last validation study demonstrated that our dictionaries are valid measures of stereotype in long text, outperforming existing instruments). Additionally, context and domain-dependence may be issues when applying these dictionaries in text from different domains from those validated. Thus, validation context must be kept in mind when deploying dictionaries. Some words included in our dictionaries may not show face validity in certain contexts, and sense disambiguation may help. For example, our Morality dictionary includes the word setup (\\\"an act that incriminates someone on a false charge\\\", see WordNet Online: https:\\/\\/wordn et.princ eton.edu\\/). However, in a context where other senses of setup (e.g., \\\"equipment designed to serve a specific function\\\") are more common\\/appropriate, this may be problematic, particularly for frequent words. Combining dictionaries with modern word embeddings that incorporate some degree of sense disambiguation (; these embeddings are included in the R package), in a way similar to that described in the Supplement, may be helpful for these purposes. Finally, we validated our dictionaries in four different ways, but the focus for both development and validation was on their use as an instrument in explicit target impression descriptions (and not, for example, implied content, measurement of speaker traits or writing style, etc.). Among other limitations, dictionaries provide categorical measures of a word's semantic association with an overarching topic. However, words differ in their prototypicality for the specified construct (e.g., in a dictionary for sociability, the word sociable may be more prototypical than the word extroverted) and may belong to multiple constructs simultaneously to different extents (e.g., the word extroverted may be classified as related both to sociability and to assertiveness to different extents). To address some of the limitations of dictionaries, they can combine with additional natural language processing methods (e.g., see Supplement;). Finally, a multi-method approach is recommended to study text data, when possible, to balance out potential biases in the dictionary creation process (e.g., biases in WordNet or other training data used for the creation of the models;), to incorporate domain expertise from human coders, among other benefits of robustness checks. 16 | CONCLUSION In this article, we created and validated novel stereotype content dictionaries that accounted for over 80% of the stereotypes of a representative sample of social groups. We also provide guidance and examples on how to import natural language processing methods, specifically WordNet and word embeddings, into the automation, creation, and evaluation of psychological text instruments. Text data open the possibilities to ask novel questions about behavior in the laboratory and beyond and may provide a way to improve both psychological theory and practice. We hope that the procedures outlined here greatly facilitate the use of text data to complement traditional approaches in social psychology and the study of stereotypes.\",\"577240497\":\"On February 23, 2020, a 25-year-old unarmed Black man, Ahmaud Arbery, was shot and killed by two people claiming to make a \\\"citizen's arrest\\\" . Within the next 3 months, the murders of Breonna Taylor and George Floyd at the hands of police officers sent shockwaves through the USA. These incidents exacerbated deeply entrenched racial tensions throughout the country. Nationwide protests and social media campaigns demanded accountability of those responsible and galvanized people to acknowledge the existence of systemic racism against Black Americans and its devastating consequences. Indeed, the senseless deaths of Black Americans at the hands of law enforcement have reignited calls and movements for social justice across the globe. During this time, scholars across disciplines have sought to make sense of these sweeping events and their potential consequences. To better conceptualize such phenomena, Leigh and Melwani (2019) extended the construct of social mega-events  and proposed the concept of a megathreat: a negative, large-scale, diversity-related episode that receives significant media attention, which occurs when an individual or group is targeted, attacked, or harmed because of their social identity group, and that event is then highly publicized. Although there exist many forms of mega-threats in recent history, the current research specifically focuses on the mega-threat of racism as it is a large-scale phenomenon involving intergroup and inter-racial behaviors that have direct important theoretical and practical implications for organizational research. Within this nascent area of the research of mega-threats of racism, scholars have predominantly focused on the intra-psychic and group effects of mega-threats. For example, group members, who identify with the targeted social group, may experience cognitions and emotions that change the relationship between their identities and behaviors , and Black Americans may suffer vicarious trauma from exposure to police violence. As such, these lines of research suggest that mega-threats have lasting effects on individuals and groups. Despite these important findings and theoretical advancements, however, little research has systematically explored how organizations respond to the mega-threat of racism or examined the corresponding implications for organizational outcomes. Indeed, following the mega-threat of racism in 2020, many organizations released diversity statements (aka \\\"DEI statements\\\"), designed to denounce racism and affirm their stance on values of diversity, equity, and inclusion (DEI). Scholars have previously examined how these public-facing messages are presented, as well as how they impact their public image and particular groups. However, to our knowledge, none of these lines of inquiry have examined diversity statements as a response to acute societal events, such as the mega-threat of racism. This gap in the literature deserves closer inspection for several reasons. Under normal circumstances, organizational diversity statements may fly under the radar, alongside other public-facing communications. However, mega-threats represent potential sea changes within society, which consequently amplify the importance of these diversity statements. Indeed, the mega-threat of racism can influence how organizations perceive and brand themselves, communicate with their stakeholders, and ultimately conduct human resource management and business. Such a situation thus compels organizations to develop their diversity statements carefully, given higher stakes than ever before. Therefore, we are interested in how these highly visible corporate messages are characterized and perceived during tumultuous times. In addition, as these corporate missives provide a fundamentally different type of organizational data with which to study long-standing issues of racism, thus, this wave of releasing corporate diversity statements has inherently created a novel opportunity for organizational research with societal impact. However, little is known about the specific content addressed by various diversity statements--particularly, how companies respond to a mega-threat of racism and consequently communicate their views on diversity vis-a-vis statements. More importantly, current literature has yet to explore the potential effects of both releasing (vs. not releasing) a diversity statement and emphasizing certain particular topics within the statement. Indeed, when the mega-threat of racism looms large in the public consciousness, what happens when organizations publicly respond to, or fail to acknowledge, larger societal issues? As such, scientifically analyzing the quantity (release) and quality (content) of diversity statements can help advance both organizational research and practices. Therefore, the goal of the current research is to systematically examine corporate diversity statements as the most common form of organizational reactions to the mega-threat of racism, as well as the effects related to organizational outcomes, such as employees' perceptions on the organizations. Specifically, we conduct two studies in the current research, taking an inductive and a deductive approach, respectively. In Study 1, we collected and analyzed a massive body of corporate diversity statements publicly released by Fortune 1000 companies in response to the George Floyd protests in late May and early June 2020. We were thus able to investigate: What are the major topics\\/themes conveyed by these major corporations in their diversity statements responding to the mega-threat of racism? Then, in a follow-up study (Study 2), we drew from a well-established identity-consciousness theoretical framework and leveraged millions of data points of employees' online ratings on diversity and inclusion. We hypothesized and tested whether or not companies that released (vs. did not release) a diversity statement tended to be more favorably rated by their employees on organizational diversity and inclusion and how emphasizing different latent topics in a statement may differentially impact important organizational outcomes. Across these studies, we make several key contributions to the diversity literature and theoretical advancement. First, we scientifically and systematically assess and identify the latent semantic topics underlying diversity statements, which enables us to better taxonomize how different organizations respond to the mega-threat of racism. This type of characterization provides an empirically based foundation for future research and practice. Second, and relatedly, by novelly applying the identity-conscious (acknowledging group identities) vs. identity-blind (minimization of intergroup differences;) framework to the topics that emerged, we advance our current understanding of how diversity statement composition may provoke differing reactions within stakeholders. Although the identity-conscious vs. identity-blind theoretical framework has been widely used within the DEI literature to describe the nuances of and responses to diversity messaging, it has yet to be applied to systematically characterize different themes within organizations' diversity statements. Thus, the current research expands the theoretical understanding of the dichotomy framework. Third, we examine how diversity statements may impact organizational stakeholders, specifically employees, in the immediate aftermath of a mega-threat. During a mega-threat of racism, these communications (or lack thereof) can shape employee perceptions, which may then influence their decisions to select into and remain engaged with their organization. To this point, Glassdoor.com's recent addition of \\\"diversity and inclusion\\\" as a new metric of organizational satisfaction reflects its growing importance in the workforce. This study, therefore, examines how organizational treatment of diversity may result in measurable impact through large-scale assessment of unsolicited employee reactions. Thus, we provide scientific evidence in the diversity literature regarding important organizational effects of diversity management and advance the research in this area. Finally, by investigating how employee ratings may reflect if and how organizations address diversity in their publicfacing messages, our research clarifies which diversity messaging topics are positively associated with stakeholder perspectives (e.g., employees' ratings) and, thereby, offers practical recommendations for organizational scientists and practitioners on designing effective DEI communications. Study 1 In our first study, we utilized unsupervised machine-learning models to text mine and analyze how organizations communicate about DEI topics in response to the mega-threat of racism. Given that this work is motivated by serious social and societal issues, we begin our literature review by outlining the real-life and evidence-based foundations for this research by highlighting the organizational motivations for releasing diversity statements in responding to the megathreats of racism; then, we discuss the importance of understanding the text topics underlying the diversity statements is critical for organizational research. Releasing Diversity Statements as an Organizational Response The mega-threat of racism in current-day society has become unignorable, with many organizations experiencing mounting pressure to respond appropriately  and confirm that they share their stakeholders' values. As a result, organizations have become increasingly motivated to respond through DEI initiatives, particularly by publicly releasing diversity statements--a type of official corporate document that emphasizes diversity-related practices, such as equal opportunity employment and\\/or values. Such statements often go beyond mentioning affirmative action policies and speak more about how diversity is valued and managed in the organization, tending to result in more positive attitudes among women and\\/or racial\\/ethnic minorities. We believe that organizations' motivations to release diversity statements may be well understood through the lenses of impression management theory  and signaling theory. Organizations have a vested interest in cultivating a positive social image. Reputation has been thought to enhance many favorable organizational outcomes, including performance. To this end, impression management theory posits that organizations may engage in strategies in order to earn approval and respect. These tactics, which include advertising, public relations, and social responsiveness , may be used to demonstrate that organizations value more than financial profit. One approach to earning general approval and respect is to release public statements condemning and\\/or supporting events in the public consciousness. According to signaling theory , a company must send out a signal to resolve any information asymmetry between itself and its stakeholders. That is, the company (signaler) can provide necessary perspectives in their communications to the public (receivers). Once the receivers process and respond to the signal, they send potentially affectively charged feedback back to the signaler. In the case of the mega-threat of racism, companies can release a statement to make the stakeholders aware of the information they are unlikely to access on their own --that is, the company's stance on DEI issues. Corporate signals, such as those conveyed by releasing diversity statements, may be met with positive benefits, including increased customer loyalty and employee commitment. Moreover, acknowledgment of the mega-threat of racism may demonstrate a company's investment in important societal questions and may therefore position organizations as responsible and responsive to mega-threats. Identifying the Underlying Text Topics in Corporate Diversity Statements Racism is a sensitive and complex issue that can mean many things to different parties; perspectives can be colored by unique experiences, relationships, salient group identity, political ideologies, the ability to empathize, and a multitude of other factors. Given the sensitivity and misunderstanding around racial issues, as well as DEI broadly, conceptualizations of and statements about DEI may differ substantially from one organization to another. Therefore, it is helpful to understand how organizations are conceptualizing DEI in their publicly released statements in response to the mega-threat of racism. Although there are several aforementioned theories that explain why organizations might release diversity statements, questions remain regarding the language and framing companies use to make these DEI communications. That is, it is essential to investigate not only if companies release statements, but also how they address such a multifaceted and complicated topic in their statements. Indeed, the nuances of diversity statements play a critical role in the overall reception of the message and the reputation of the organization. Generally speaking, critics of social justice movements have responded to diversity initiatives with skepticism and even backlash. For example, color-blind approaches, which ignore differences between groups, often draw negative reactions. Relatedly, diversity communications may treat racism in a vague manner in order to make unpleasant realities more palatable. During the most acute days of the mega-threat, diversity statements would vary in how they acknowledged racism, ranging from blatantly and explicitly to, at least, inadvertently and vaguely. In our research, we expand on this thread by examining to what extent diversity messages specifically address the groups targeted by the mega-threat of racism. Indeed, the murders of 2020 made clear that Black Americans are especially marginalized and vulnerable to acts of violence and aggression--but how explicitly did organizations call attention to this? Did firms discuss these racism-related issues in broad strokes (e.g., as generic terms) or was leadership more explicit about their role in the maintenance of these systems? Were organizational statements identity-blind or identity-conscious (i.e., explicitly naming and discussing the discrimination faced largely by Black Americans)? Essentially, how, qualitatively, did organizations navigate and embody these socially sensitive topics in the public arena? These questions, and others, point to the ambiguity surrounding DEI framing, highlighting the lack of scholarship and empirical understanding in this sensitive time period. Therefore, as an initial step in this arena, we aim to text-mining analyze corporate operationalizations of DEI by exploring: Research Question 1: What are the major latent topics\\/ themes underlying the corporate diversity statements publicly released in response to the mega-threat of racism (e.g., George Floyd protests)? Study 1 Method Collecting Corporate Diversity Statements Our research team manually searched and collected diversity statements from the Fortune 1000 companies. Every year, in approximately July or August, Fortune Magazine ranks and publishes the largest US companies by revenue, known as the Fortune 1000. We focused on the Fortune 1000 metric as it included all major companies across all industries. For the current study, we first obtained a complete list of 2020 Fortune 1000 companies from https:\\/\\/ fortu ne. com. The list ranked the companies from 1st to 1000th and provided important company information. Using the Fortune 1000 list, we undertook extensive searches for diversity statements or open letters released by each company that addressed racial injustice. The goal of our search was to be as comprehensive as possible, with search strategies emphasizing the companies' statements that were motivated by the killing of George Floyd. Thus, all the collected statements were released by organizations after May 2020, in the context of the mega-threat of racism. To complete the search, two graduate research assistants were trained to separately search for statements by each company, one by one, on the list. They first searched by using the following terms, and then all possible combinations thereof: \\\"racial equity,\\\" \\\"diversity,\\\" \\\"inclusion,\\\" \\\"George Floyd,\\\" \\\"open letter,\\\" \\\"newsroom,\\\" \\\"news press,\\\" and \\\"press release.\\\" These were crossed with each company name, CEO name, and public relations department. The graduate research assistants performed searches using tools and websites that are publicly accessible, including Google, Bing, Yahoo, Twitter, and the company's own websites, particularly on the company's press release webpage. If a company released follow-up statements after their first one, we only included the first statement in order to preserve consistency. See an example of the statement released by Walmart's CEO Doug McMillon in Appendix 1. After each graduate assistant completed the search, they compared the lists and resolved any potentially different results. This process resulted in 511 statements from Fortune 1000 companies (see Table 1 for a summary). Analytical Strategy: Topic Modeling Topic modeling--a type of statistical model aiming to understand the hidden topics underlying a collection of documents--is a relatively new method developed in the machine learning (ML) and natural language processing (NLP) areas. Although it appears complex, the statistical logic behind topic modeling is rather straightforward. It calculates the probability of how different words occur together in a document, and, based on the probability of word cooccurrence, classifies words into different groups, which are then labeled as topics. For example, in a collection of documents, one may find that \\\"doctor\\\" and \\\"nurse\\\" more frequently appear in a medical document, and \\\"dog\\\" and \\\"cat\\\" appear more often in a document about animals. Thus, in the collection of these documents, we may find two topics emerge: the medical topic and the animal topic. In practice, topic modeling is guided by two general principles : (1) every topic is a mixture of words--e.g., the medical topic includes the words doctor and nurse; and (2) every document is a mixture of topics-e.g., a document is 85% medical topic and 15% animal topic, while another document is 30% medical topic and 70% animal topic. As such, topic modeling categorizes words into different groups to form topics and calculates the probabilities of each topic in a document. Specifically, in the current study, topic modeling was used to statistically identify word groups underlying all the diversity documents released by the Fortune 1000 companies and also calculate how much each company's diversity statement emphasized each topic. Although various methods have been developed for topic modeling, in the current study, we used Structural Topic Models (STM;), an advanced textmining technique that has been widely used in social sciences such as management and political science. Similar to the classic topic modeling methods such as Latent Dirichlet Allocation (LDA;) and Correlated Topic Model (CTM;), the STM method takes an unsupervised machine-learning approach to identify and organize latent topics based on the semantic structure in a textual corpus. In other words, the STM model statistically classifies similar words together to form a latent semantic topic--similar to the factor analysis (FA) method that classifies similar items together to form a latent factor and also estimates the probability that a document is associated with a certain topic (a.k.a., topic prevalence). However, comparing LDA and CTM, the major advantage and innovation of the STM method is its ability to further model the relationships between the topic prevalence and document-level variables (a.k.a., metadata; e.g., company size and CEO race). Please see such analyses in the Supplemental Materials. The STM modeling involved four computational steps, and all the analyses were performed in R Statistical Programming version 4.1.0 . The first two steps were inputting the text data and document metadata, preparing and pre-processing the data, removing stop words (e.g., \\\"a,\\\" \\\"the,\\\" etc.) and punctuations, stemming words (e.g., converting words \\\"diverse,\\\" \\\"diversely,\\\" \\\"diversity,\\\" \\\"diversified,\\\" \\\"diversification\\\" to the stem \\\"divers\\\"; converting words \\\"inclusion,\\\" \\\"inclusive,\\\" \\\"inclusiveness\\\" to the stem \\\"inclus\\\"; etc.). Some words and documents were also removed in this pre-processing step because of extremely low frequency. For example, infrequent words that only appeared in one document were dropped for the subsequent analyses. A document with less than ten words was removed as well. This pre-processing step resulted in 469 documents\\/ companies for the final text modeling analyses. After preparing and pre-processing the text data, the third step was estimating STM models. In this step, we included metadata in the model, which included the company level variables such as industry sector, Fortune 1000 rank, the number of employees (company size), revenue growth from the previous fiscal year, CEO race and gender, and corporate political orientation. To normalize highly skewed variables and improve model convergence, the number of employees was logarithm-transformed and normal standardized, the rank variable was Z-scored, and the revenue growth was cube root transformed. The fourth and last step was evaluating and selecting models, in which we first ran models with a various number of topics ranging from 2 to 15. Then, following the guide by Roberts et al. (2014), we selected a model with the best model fit based on the criteria of semantic coherence and exclusivity. Semantic coherence is concerned with the maximum probability of a set of words in a given topic co-occurring together. Exclusivity balances word frequency across topics based on the FREX metric--the weighted harmonic mean of the word's rank in terms of exclusivity and frequency, as shown in Eq. 1 below : where ECDF is the empirical cumulative distribution function (CDF), o is a prior-to-optimize exclusivity, k K is the kth topic of all the K topics, and b is the topic-word (1)FREXk,v = ECDF k,v K j=1 j,v + 1 - ECDF k,v -1 distribution for the kth topic. After considering those criteria, we selected a final model with K = 6 latent topics. After modeling the topics, we also computed the proportion of each topic and its overall prevalence across all the documents in our corpus. After all the computational steps, we labeled each topic by following the two-step procedure demonstrated by Stamolampros et al. (2019). First, the five authors reviewed the representative words in each group and created labels after reviewing the top keywords generated by the topic solution (Table 2) and thorough discussion. These labels were developed based on one important criterion: each could summarize the highly frequent and representative words in a group. The diversity statements that involved each topic at the highest probability were also referenced in this process. Second, we conducted a concordance study to make sure that people who were blind to our research questions were able to match the topic labels and representative words. To do so, we recruited a group of 10 graduate students who, after blindly reviewing the representative words and the scrambled labels, all showed a perfect match. We further conducted exploratory analyses to examine how each of the topics was associated with company characteristics (i.e., metadata), including industry sectors, the number of employees, Fortune 1000 rank, revenue growth, CEO race and gender, and the corporation's political orientation. These additional results are presented in the Supplementary Materials. Study 1 Results and Discussion Topic Solutions The 6-topic solution, along with the corresponding representative word stems and topic labels, is presented in Table 2 and Fig. 1. Not surprisingly, the topic of general DEI terms (topic 1) was the most popular, occupying 36.32% of the diversity statements and entailing words with a positive and general connotation (such as \\\"diversity\\\" and \\\"inclusion\\\"). This was followed by three topics with terms that explicitly named the targeted group during the mega-threat of racism (\\\"Black\\\"). Topic 2, supporting Black community (24.63%), touched on terms like \\\"NAACP\\\" and \\\"Juneteenth,\\\" demonstrating themes of community-building and social justice organizing. Topic 3, acknowledging Black community (20.73%), referred to concepts such as \\\"communities\\\" and \\\"neighbors,\\\" as well as \\\"racism\\\" and \\\"killing,\\\" describing the lived experiences of Black individuals. Topic 4, committing to diversifying workforce (10.97%), incorporated themes of organizational operations with regards to DEI, through its use of phrases including \\\"companies,\\\" \\\"talent,\\\" and \\\"hire.\\\" The least popular topics involved miscellaneous words (topic 5; 5.46%) and titles and companies (topic 6; 1.89%). These, respectively, involved commonly-used words across statements (e.g., \\\"know\\\" and \\\"people\\\") and terms describing functions within a company (such as \\\"CEO\\\" and \\\"chairman\\\"). We also calculated the correlation among the 6 topics, and presented the descriptive results in the upper panel of Table 3. The correlation results revealed that topic 1 (general DEI terms) was negatively related to topic 2 (supporting Black community; r = - 0.52, p < 0.01), topic 3 (acknowledging Black community; r = - 0.24, p < 0.01), topic 4 (committing to diversifying workforce; r = - 0.33, p < 0.01), and topic 5 (miscellaneous words; r = - 0.17, p < 0.01). Similarly, topic 2 (supporting Black community) was also negatively related to topic 3 (acknowledging Black community; r = - 0.39, p < 0.01), and topic 5 (miscellaneous words; r = - 0.17, p < 0.01); topic 3 (acknowledging Black community) was negatively related to Topic 4 (committing to diversifying workforce; r = - 0.25, p < 0.01). Discussion These analyses revealed six topics underlying the statements: general DEI terms (topic 1), supporting Black community (topic 2), acknowledging Black community (topic 3), committing to diversifying workforce (topic 4), miscellaneous words (topic 5), and titles and companies (topic 6). The identification of these topics allows us to understand how organizations respond to the mega-threat of anti-racism and operationalize DEI. For example, topics 2 and 3 both explicitly identify Black communities and acknowledge their lived experiences. This is an important first glimpse into the nature of diversity statement content but does not speak to the potential implications of this content. We will return to this finding and discuss the consequences of this explicit group identification for perceptions of diversity and inclusion in the following study. Overall, Study 1 systematically uncovered how organizations communicate the sensitive DEI topics in their publicly released diversity statements in response to racism-related mega-threats. These findings laid a foundation for further investigation in Study 2. Study 2 Leveraging millions of data points on employee ratings and the latent semantic topics, Study 2 aims to further extend our understanding of the effects of releasing diversity statements and the emphasis of different types of text topics in a statement. Specifically, Study 2 investigates if the companies that released diversity statements are rated more favorably on organizational diversity and inclusion by their employees than by the companies that did not release such statements. In addition, for organizations that did release a diversity statement, we further examine if the companies emphasizing certain topics (e.g., identity-conscious topics such as Black community) were rated more favorably by their employees than those highlighting other topics (e.g., identity-blind topics such as general DEI terms). Releasing vs. Not Releasing a Diversity Statement Not only are the qualitative framings of diversity statements understudied, but their effects are also poorly understood. Scant research has investigated the specific outcomes of the corporate release of anti-racist or diversity-minded messages in response to mega-threats. It bears repeating that reactions to diversity statements may even be negative, given the aforementioned research surrounding DEI-related movements and initiatives. The mega-threat of racism has often been politicized; for example, the anti-racist Black Lives Matter campaign has been met with reactionary Blue Lives Matter and All Lives Matter movements. Organizations, therefore, may not want to comment on such social and societal issues, given beliefs that work and politics should not mix. Indeed, stakeholders who do not agree with a company's decision to release diversity statements may react with an overall backlash. Existing research and journalism have documented the push against diversity statements and DEI-related efforts. For example, substantial swaths of the American population perceive anti-racism as counterproductive, threatening, and even dangerous. To these individuals, activism against the mega-threat of racism invokes negative concepts like \\\"cancel culture\\\" and \\\"the liberal or woke agenda\\\" . In this view, corporations that release diversity statements may be legitimizing and advancing racial division. When organizations choose to take a stance against the megathreat of racism, they thus touch upon a hot-button issue that can result in polarizing emotions among stakeholders. Nonetheless, although DEI messaging may not align universally with specific consumer bases, a large body of research does suggest that diversity statements may have net positive societal and organizational benefits. As previously mentioned, signaling theory suggests that organizations can resolve informational asymmetry by publicly communicating their stance on important issues, such as anti-racism. When organizations release diversity statements, they may be communicating information with positive connotative associations, such as care for organizational culture and community well-being. The subtext in these signals may lead to more favorable perceptions among stakeholders, especially during a mega-threat. Altogether, these findings point to the fact that diversity statements may enhance organizational perceptions among minoritized employees and their allies and colleagues. Given these theoretical rationales, we begin our investigation by comparing perceptions of organizations that do not speak out against racism with those that do release diversity statements and establish the baseline effect of releasing a diversity statement: Hypothesis 1: Companies that released a diversity statement tended to be rated more favorably on diversity and inclusion by their employees than companies that did not release a statement. Identity-Conscious vs. Identity-Blind Topics Next, we return to the specific latent semantic topics identified in Study 1, conducting deeper analyses of their effectiveness on organizational outcomes of employee ratings. Not all diversity statements may be perceived equally. According to social information processing theory (SIP;), individuals construct meaning based on contextual cues, including relevant information from work and social environments. When a diversity statement is released by their organization, employees may take the contents--more specifically, the underlying topics--conveyed in the statement as relevant social cues. They then make inferences regarding the corporation (e.g., values and culture) based on the valence and strength of those cues. As such, diversity statements with certain types of topics may provide more positive and stronger social cues regarding anti-racism than other topics, thus forming more powerful impressions on employees. In the rest of this section, we elaborate on how different topics in a diversity statement may contain cues of varying strengths that, when processed by employees, form the basis for organizational evaluation. There is reason to believe that the nature of diversity statements could have a differential impact. For example, organizations that explicitly and publicly address racism (e.g., using phrases like \\\"anti-racist\\\") may be seen as more committed to DEI compared to those who release diversity statements with more generic emphases (e.g., \\\"equal opportunity employment\\\"). Indeed, these organizations may be perceived as taking a stronger stance against racism, given the cues of clear and definitive language. One way to increase the assertiveness of diversity statements is to \\\"name names\\\" or specify the identities of stakeholders. How clearly do diversity statements identify the at-risk populations? Indeed, scholars have studied how diversity management practices differentially acknowledge identity, distinguishing between identity-blind and identityconscious framings. Identity-blind policies focus on minimizing differences, with the goal of promoting equality and fairness between groups. Proponents argue that a \\\"color-blind\\\" approach avoids making stereotypes salient and highlighting unequal power dynamics between majority and minority groups. Conversely, identityconscious approaches acknowledge and value differences in order to promote diversity. Those in favor of identity consciousness argue that it is critical to acknowledge the systematic disadvantages faced by minorities (;, for a review). With regard to diversity statements, one particularly strong identity-conscious cue is the \\\"calling out\\\" of groups and their differences. However, a large body of research and policy lends support to identity consciousness as a more effective approach. A recent meta-analysis of the relationship between diversity ideologies and intergroup\\/policy outcomes found that identity-consciousness (i.e., multiculturalism) was associated with high-quality intergroup relations and support for diversity policies, regardless of group membership. In the same meta-analysis, identity-blind framings led to notably mixed outcomes. Furthermore, in practice, the American Psychological Association has stated that we \\\"cannot be nor should we be color-blind\\\" since 1997 --a sentiment that has been echoed by other diversity scientists and practitioners. Overall, these pieces of evidence demonstrate that DEI initiatives may be more readily received and enacted when there is transparency regarding intergroup differences, i.e., identity-consciousness. Taken together, these results suggest that DEI management practices that are in response to mega-threats, including diversity statements, must be approached with appropriate sensitivity and specificity. Per SIP theory, identity-conscious framings may provide stronger cues that organizations value diversity and may more directly address these issues, especially if they are explicitly relevant to the group experiencing the mega-threat. We would therefore expect the presence of identity-conscious topics in the statements to have a stronger impact on employees' perceptions of organizational diversity and inclusion, compared to identityblind topics. We also anticipate this result regardless of the racial composition of the organization given that (1) the statements are in response to a group-specific mega-threat (i.e., racism towards Black people in society) and (2) we are examining organizational diversity and inclusion perceptions, not overall perceptions of the company. Although scholarship is growing in the area of identityblindness and identity-consciousness, this duality has typically been applied to diversity research broadly (both as individual and organizational ideologies) but has yet to be applied to corporate statements in particular. We thus put forth: Hypothesis 2: Identity-conscious text topics (e.g., supporting Black community, acknowledging Black community, committing to diversifying workforce) covered in a corporation diversity statement tended to be more strongly associated with employees' organizational diversity and inclusion ratings than identity-blind topics (i.e., general DEI terms). Study 2 Method Data Collection and Preparation Topic Probability Scores Based on the STM text-mining analysis, we computed topic probability scores for each company's statement on the six topics. These scores represented the odds that a company's statements fell into a given topic category. For example, one company's statement may have 16% odds of including topic 1, 25% odds of including topic 2, 38% odds of including topic 3, 12% odds of including topic 4, 8% odds of including topic 5, and 1% of including topic 6. In contrast, another company's statement may have odds of 9%, 45%, 14%, 13%, 11%, and 8% on the six topics, respectively. Mathematically, the sum of a company's probability scores across all the 6 topics has to be 1. Employee Ratings To retrieve employee ratings for each of the Fortune 1000 companies, we turned to Glassdoor.com, one of the most popular job-listing websites and \\\"the most dominant company review website by far\\\" . Glassdoor is free to use, allowing current and former employees to anonymously review their companies. As the massive number of reviews provides valuable insights for potential job seekers, the website attracts about 60 million users per month. At the time of writing, Glassdoor allows current and former employees to anonymously rate companies on ten dimensions, including seven dimensions rated on a 1-5 star rating scale (e.g., diversity and inclusion, overall rating, culture and values, work\\/life balance, senior management, compensation and benefits, career opportunities). Most importantly, the diversity and inclusion rating dimension was not implemented on Glassdoor until September 2020, which meant that diversity and inclusion were rated after the companies released their diversity statements. This created a temporal separation that enabled us to test whether the text topics conveyed in diversity statements might potentially impact employees' anonymous ratings on organizational diversity and inclusion. In contrast, all other dimensions of organizational rating were implemented far before the release of the diversity statement. Given this and its relevance to the mega-threat of racism, we chose to focus on the diversity and inclusion rating as the relevant criterion in the current study. As such, we manually retrieved each company's scores on the diversity and inclusion ratings that were aggregated across all the full- and part-time employees' ratings for the time period of September 2020 to June 2021. For example, as of June 28, 2021, Apple's average diversity and inclusion rating was 4.4 out of 5.0. Categorizing Identity-Conscious vs. Identity-Blind Topics As a team of subject matter experts, we independently reviewed the topics identified in Study 1 and achieved a consensus when categorizing them under either identity-blind or identity-conscious ideologies. Topic 1, general DEI terms, was categorized as identity-blind, given that its common and representative terms did not explicitly address anti-racism. Conversely, topics 2 and 3, supporting Black community and acknowledging Black community, specifically named the minorities vulnerable in the face of the racism mega-threat; these topics are thus inherently identity-conscious. Topic 4, committing to diversifying workforce, does not have a titular emphasis on race but qualifies as identity-conscious for two reasons. First, analyses of the topic's representative and common terms include identity-conscious words, as shown in Fig. 1. Second, a commitment to diversifying the workforce inherently involves acknowledging differences within a labor pool; one must attend to intergroup differences in order to make sure that employee composition is representative demographically. Finally, topics 5 and 6, miscellaneous words and titles and companies, do not discuss identityrelated differences meaningfully. Therefore, although they appear in our analyses, they are less frequently occurring in diversity statements and cannot be theoretically sorted into the identity-conscious vs. identity-blind dichotomy. These categorizations are presented in Table 2. Control Variables In order to test the robustness of the text topic effect on employees' ratings, we also text analyzed the affectivity reflected by the word choice in the diversity statement. Research suggests that positive (and negative) emotion words impact recipients' evaluations and attitudes. Accordingly, it is important to clarify that employees' favorable ratings on diversity and inclusion were solely associated with the text topics covered in the diversity statement rather than the positive words written in the diversity statement. To analyze the text affectivity, we utilized the Language Inquiry and Word Count (LIWC;) technique, which has been widely adopted for psychological and organizational research. The LIWC method calculates the percentage of positive and negative words in a document based on its built-in dictionaries. The positive emotions dictionary included 620 words (e.g., \\\"love,\\\" \\\"nice,\\\" \\\"sweet,\\\" etc.), and the negative emotions dictionary included 744 words (such as \\\"hurt,\\\" \\\"ugly,\\\" \\\"nasty,\\\" etc.). Our data found that, on average, the corporate diversity statements used more positive emotion words (4.51%) than negative emotion words (2.02%). Analytical Strategy To examine the first hypothesis, we ran independent t-tests to compare diversity and inclusion ratings between companies that released vs. did not release a diversity statement. We also visualized the differences with boxplots, which not only showed the medians and first and third quartiles (i.e., the box), but also the distributions of the individual observations (the dots). To test the second hypothesis, we focused on the companies that released diversity documents. We first calculated the descriptive statistics and correlations, and then we performed two regression models to examine the effects of the text topics. The first model only included topics 1 to to 53 as the predictors; in the second model, we controlled for the positive and negative emotions in the diversity statements to examine the robustness of the topic effects. In addition, we also analyzed the relative importance weights of each predictor in the two models by using the method by Tonidandel and Lebreton (2015), with a recommended 10,000 iterations for the bootstrapping procedures. Study 2 Results and Discussion Effects of Releasing vs. Not Releasing Diversity Statements The results of t-tests that compared employees' ratings on diversity and inclusion between companies that released vs. did not release a diversity statement were statistically significant (t(944) = 8.27, p < 0.0001; d = 0.53, medium size). Specifically, the analyses revealed that companies that released a diversity statement tended to be rated more favorably on diversity and inclusion by their employees (M = 3.90, SD = 0.43) than companies that did not release a diversity statement (M = 3.64, SD = 0.54), supporting Hypothesis 1. We visualized the rating distributions and differences among the two groups of companies in Fig. 2. Effects of Emphasizing Identity-Conscious vs. Identity-Blind Topics We presented the correlates in Table 3 and the results of regression models and relative importance in Table 4. To confirm our findings regardless of the effect of affective content, we present our findings with and without controls. Without controlling for positive and negative emotions (model 1, Table 4), topic 3 (acknowledging Black community; b = 0.65, p < 0.01; relative importance = 48.09) and topic 2 (supporting Black community; b = 0.59, p < 0.01; relative importance = 34.80) were mostly predictive of the diversity and inclusion ratings, followed by topic 4 (committing to diversifying workforce; b = 0.54, p < 0.01; relative importance = 8.09) and topic 1 (general DEI words; b = 0.44, p < 0.05; relative importance = 8.04). Topic 5 (miscellaneous words; b = 0.45, n.s.; relative importance = 0.98) was the least important in the prediction. The effect of text topics on diversity and inclusion ratings showed a similar pattern after the positive and negative emotions were controlled (model 2, Table 4). Specifically, the effects of topic 3 (acknowledging Black community; b = 0.66, p < 0.001; relative importance = 34.07) and topic 2 (supporting Black community; b = 0.51, p < 0.01; relative importance = 29.80) were still the two strongest predictors for the diversity and inclusion ratings, followed by topic 4 (committing to diversifying workforce; b = 0.46, p < 0.05; relative importance = 7.02) and topic 1 (general DEI words; b = 0.32, n.s.; relative importance = 6.52). Again, topic 5 (miscellaneous words; b = 0.43, p < 0.05; relative importance = 0.82) was the least important predictor. More importantly, the effects of positive and negative emotions were not statistically significant. These findings not only support Hypothesis 2, but also indicate the robustness of the effect of text topics in predicting employees' diversity and inclusion ratings. Discussion Extending the findings of six latent semantic topics from Study 1, Study 2 made use of both theory and empiricism: applying the identity-blindness and consciousness theoretical framework and analyzing big data on employees' ratings on the company's diversity and inclusion. We hypothesized and tested if companies that released a diversity statement tended to be more favorably rated by their employees than companies that did not release a diversity statement, and if companies whose diversity statement emphasized identity-conscious topics (vs. identity-blind topics) were more favorably rated on diversity and inclusion. Our results consistently showed that the release of a diversity statement was associated with enhanced employees' ratings on diversity and inclusion. Moreover, for those companies that did release a statement, we found that diversity statements that focused on identity-conscious topics (e.g., supporting Black community, acknowledging Black community, committing to diversifying workforce) were more strongly associated with employees' favorable ratings compared to identity-blind topics (e.g., general DEI terms). We found that our prediction still held even after controlling for the affective content of the statements. We believe this finding provides valuable evidence in favor of explicit anti-racist communication. Although these findings offer important practical implications, we need to be cautious in drawing causal effects, as the effect could be confounded with third variables. For example, it is possible that companies who publicized their stance on DEI also possessed other characteristics that helped them earn higher diversity and inclusion ratings from their employees. In addition, identity-conscious diversity statements might be a signal that organizations had authentic, inclusive, and healthy organizational climates and were thus rated more favorably by their employees. Organizations might therefore consider calling attention to Black communities in their diversity statements as a step towards creating these positive working cultures. Further studies are needed to parse apart these potentially confounding effects. General Discussion The current research utilizes STM for the first time in organizational research on DEI to comprehensively analyze the public diversity statements released by Fortune 1000 companies and important organizational outcomes. Our research revealed that companies operationalized and embodied DEI from six perspectives. More importantly, we found that companies that released a diversity statement were evaluated more favorably by their employees than their peers who did not, and that identity-conscious topics included in the company diversity statements were more strongly associated with employee diversity and inclusion perceptions than identity-blind topics included in the statements. We believe these findings have important implications for both theoretical advancements and practical recommendations. Theoretical Contributions Altogether, this research makes several theoretical contributions to the literature. First, our research work calls upon multiple theoretical perspectives, including mega-threat , signaling , corporate image , and social information processing  theories, and identity-consciousness vs. identity-blind frameworks , to understand the phenomenon of organizations releasing diversity statements. Notably, our research questions and hypotheses regarding diversity statements cannot be sufficiently described by a single theory. Indeed, an inductive approach was necessary given the dearth of relevant science around how corporate diversity statement topics are used in response to mega-threats. Accordingly, we invoked many literatures in order to explain the interplaying drivers of diversity statement publication and design. Not only do these theoretical components describe why organizations release statements, but they can also explain how organizations publicize their stances on DEI. By calling upon these bodies of research, we are able to answer key questions, including those related to the overall and topically differentiated effectiveness of diversity statements used in response to mega-threats. Notably, we also found that an identity-blind topic in the diversity statements, i.e., employing general DEI terms, was less strongly associated with diversity and inclusion perceptions than topics that more explicitly referenced the plight of Black people. It is possible that companies with higher status and growth may be stable (e.g., resistant to backlash) and thereby more motivated to take a stronger stance against racism through identity-conscious diversity statements. Another explanation could be that these companies may be better positioned in the DEI space, such that their human resources, public relations, and other relevant departments are more aware of identityconscious ideologies. We pose this as one of many potentially interesting relationships that emerge as a result of our paired studies and that may be explored in future studies. Finally, as previously mentioned in the literature review, this is the first study that applied the latest advanced text mining technique, STM , in this organizational diversity research content area, to our knowledge. Our research has demonstrated the utility and capability of powerful text mining analytical tools in diversity research. As text data have become increasingly popular in organizational research, particularly in the diversity management arena, we expect more applications of the STM technique will emerge in this area to help develop and advance theory. Practical Implications and Recommendations In the wake of the Black Lives Matter movement, organizations have had to grapple with sensitive DEI concepts as never before. However, different organizations do not necessarily understand the mega-threat of racism in the same way, nor is there consensus regarding their roles in improving the position of Black Americans. This research draws directly on phenomena in the real world, thus holding importance for organizations in practice. First, this research creates a taxonomy of DEI communication in response to mega-threats, based on evidence from the field, that can help begin to organize public dialogue around these sensitive issues. Given that the DEI aspect of company visions is relatively novel (particularly when under the strain of mega-threats), leadership and public relations firms may be unaware of the nuances and possibilities of anti-racist messaging. This work uses existing data to provide a tested palette of topics and themes that can be used to develop intentional and accurate communication. For example, organizations can consider their DEI goals and the extent to which they need to improve perceptions of diversity and inclusion among their employees. They can then choose to bolster their messages by incorporating specific language. Leadership should engage in reflective exercises and, given the array of DEI topics, understand why they are (or are not), including specific emphases in their company vision and public communications. Importantly, we find that companies who released diversity statements were more favorably rated by their employees compared to those who did not. Although these analyses were correlational and not causal, it bears repeating that employees' ratings of an organization's diversity and inclusion, specifically, were collected after the release of their diversity statements. This suggests that the presence of diversity statements may have had an impact on internal assessments of an organization's DEI climate. As previously mentioned, organizations may shy away from publicizing their stances on societal issues, especially given the potential for backlash and the politicization of social justice movements. However, this research demonstrates that releasing diversity statements as a whole does not hamper employee perceptions (and likely other stakeholders' perceptions) of organizational quality. In fact, being explicitly identity-conscious, i.e., calling attention to Black communities and communicating anti-racist policies, is most strongly associated with positive diversity and inclusion ratings. Indeed, this research draws clear distinctions between statement types by applying the identity-blind and identity-conscious dichotomization. Perhaps one of the most compelling takeaways from this research is the stronger link between identity-conscious diversity statements and higher employee ratings of DEI, relative to identity-blind diversity statements. This underscores trends in both literature and practice: more explicit, identity-conscious approaches to diversity are linked to stronger outcomes. This should encourage organizations to take not just public, but also assertive, stances on important societal issues. During a time when Black Americans are fighting for real, actionable change , our findings suggest that organizations can and should rise to the challenge and be vocal advocates for anti-racism. Importantly, this research is not meant to position diversity statements in response to mega-threats as a standalone diversity management practice. Organizations should also consider how their statements are only one part of a much larger company image and vision. Research shows that diversity management programs fail when processes are not set up that will allow for effective follow-through on policies. In light of this, companies should avoid appearing to be \\\"all talk and no action\\\" by continuing to follow through on their voiced commitments. Sustaining these statements can take a variety of forms, including continuing to be vocal about anti-racism, developing plans at multiple levels and durations, and executing interactions. It is especially impactful when these initiatives are driven by non-minorities, given that White Americans have historically possessed the power and propensity to implement lasting policies and impress cultural values and standards on others (which has also resulted in their group retaining higher status in US society compared to racial and ethnic minorities;). In particular, creating long-term initiatives can convey to stakeholders that organizations are not only speaking up about DEI, but are genuinely invested in effecting change and potentially disrupting long-standing hierarchies. Overall, companies play a large role in shaping societal dynamics, given their importance and status. US companies are largely operated by and employ White Americans, who have tended to not always acknowledge racism , especially institutional- and cultural-level racism. This pattern of overlooking persists, even after prior events that reflect blatant societal racism. Therefore, the discussion of events like the George Floyd protests by major US companies is notable. As power holders in society, if firms show support for issues that directly impact the lives and wellbeing of Black Americans, they would be bringing attention to issues that may not get consideration otherwise. Further, if they propose and implement policies and procedures that increase career opportunities provided to Black Americans, there could be positive impacts on Black communities that would begin to combat some of the ills that years of multi-level racism have produced. We therefore strongly encourage organizations to use their platform to take a strong stand against racism, not only through diversity statements in response to racism-related mega-threats but through multi-pronged and sustained efforts. Limitations and Future Directions This research canvasses the Fortune 1000 landscape, characterizing the ways in which organizations have responded to demands for DEI. Although we made every effort in searching for such statements, our text mining analyses were only limited to the companies that publicly released such statements, leaving unknown the companies that did not choose to make public (e.g., only internal) statements. In addition, the ratings on Glassdoor might not be a representative sample of a company, as the reviews were likely filled out by highly happy or angry employees. Also, as Glassdoor.com did not disclose the specific number of ratings on diversity and inclusion for each company, as this rating was implemented after September 2020. Although we reasonably believed that the number of diversity and inclusion ratings were sizable, nevertheless, if they were small sample sizes, they could have easily been biased. Moreover, just as anti-racism is an ongoing process, so is a program of investigation such as this. Future research may need to explore how these statements translate into actual actions in terms of managerial practices and organizational interventions. How is this lip service used to effectively transform organizations and society (if at all)? This is a critical question that may be focused on in future research. For this research agenda, organizational researchers will likely need access to comprehensive, longitudinal, and internal data about the firms. We, therefore, encourage transparency from organizations regarding their DEI efforts; saw unprecedented unrest, such that it may have become politically unwise to stay silent. However, race-motivated violence is entrenched in the history of the USA. Indeed, the deaths of George Floyd, Breonna Taylor, and Ahmaud Arbery were not the first to attain recent renown, but they did strike a chord of unmatched pitch. Given this, it is interesting to consider if corporations would frame themselves as being continually invested in questions of race and racism or if they would acknowledge their failure to discuss DEI in the past. Conclusion In summary, by leveraging multiple theories and applying novel text-mining analytical techniques, our study has advanced the current understanding of how companies are addressing DEI in response to the mega-threat of racism. Our research identified six latent semantic topics underlying a sizeable body of diversity statements publicly released by Fortune 1000 companies. Perhaps more importantly, by taking advantage of millions of employee rating data points, we tested and confirmed that companies that released (vs. did not release) a diversity statement was more favorably rated on diversity and inclusion by their employees, and that companies whose diversity statement emphasized identity-conscious (vs. identity-blind) topics were more positively rated by their employees. As a whole, our work suggests that it is beneficial for corporations to respond to anti-racism by releasing diversity statements--and emphasizing identity-conscious topics in the diversity statement can help maximize that positive impact. We hope our research sheds meaningful light on the current diversity research and offers practical recommendations for organizations to develop effective public stances and policies.\",\"577240509\":\"Introduction Millions of people have their identities stolen every year. There is no fool-proof way to pinpoint fakers, especially when faked identities are used to register online. Traditional methods of lie detection include face-to-face interviews and polygraphs that measure heart rate and skin conductance. Leaving aside the debated accuracy of the polygraph, these techniques cannot be used remotely or with large numbers of people. Recently, researchers have developed latency-based measures to determine whether the respondent is the real owner of a certain identity. Latency-based lie detection techniques find their roots in the cognitive load theory, according to which lying requires a greater cognitive effort than truth-telling; this higher workload is reflected by a number of indices, including, for example, reaction times (RT) . Indeed, people show an increase in RT and error rate when they lie in response to questions. So far, RT-based techniques have been almost exclusively tested in the laboratory setting, without taking into account some variables that could limit the application in a real scenario (e.g., the presence of interfering stimuli that can distract the participant and affect the response time). Although these techniques are still far from finding an application in ecological contexts, different proofs of concept demonstrated the feasibility of this approach for false identity detection. Verschuere and Kleinberg used the CIT-RT technique to evaluate whether respondents were lying about their identity. The CIT-RT consists in presenting critical information (the concealed information which is known only by the guilty subject) within a series of similar but noncritical information (stimuli which are irrelevant both for guilty and innocent subjects). The aim of this technique is to evaluate if the examinee recognises specific information through indirect measures. When applied to verify the autobiographical information that the examinee claims to correspond to the true identity, CIT efficiently succeeds in distinguishing the identities of liars and truth-tellers. Monaro et al. compared performance of mouse-guided responses to true and false questions about identity (;, c;). They asked liars to study a new identity (name, surname, date of birth, place of birth, place of residence) and to respond pretending that these faked identities were their true ones. Findings revealed that lying triggered a more erratic mouse trajectory and longer RTs, especially when questions about identity were unexpected. Indeed, whereas unexpected questions about identity, such as the Zodiac, can be easily addressed by a truth-teller, they typically require a deceptive subject to engage in mental computations to come up with the correct information (e.g., the correct zodiac corresponding to the false date of birth). The authors capitalized on unexpected questions that do not permit liars to prepare themselves and to anticipate a response to a predictable question. Indeed, planning makes lying easier and planned lies typically contain fewer cues to deceit than do spontaneous lies. In line with this, the analyses based on expected questions correctly discriminated liars from truth-tellers with accuracies ranging from 65 to 67%, whereas the classification based on unexpected questions reached a 95% accuracy. The efficiency of unexpected questions in detecting faked identities was proved in two additional studies in which the authors applied this technique to analyse the keystroke dynamics while participants were engaged in typing their personal information on the computer keyboard. Like for the mouse dynamics, liars took more time to type their responses, especially to unexpected questions. Thus, based on the fact that changes in mouse trajectory and keyboard dynamics as well as changes in response time were relevant in distinguishing liars from truth-tellers , here we investigated whether combining a choice reaction time paradigm with the technique of unexpected questions could efficiently detect individuals lying about their identity. Materials and methods Participants Fifty native Italian-speaking individuals (23 males and 27 females) took part in the experiment. Power calculations indicated that a sample size = 50 in a between-subject design (n = 25 for each group) would have been sufficiently large to achieve at least a statistical power (1 - b) = 0.90, given a significance level (a) = 0.05 and an effect size (Cohen's d) = 2.33 (note that the effect size is referred to the variable IES unexpected) . Most participants were students and were recruited at the University of Padua (Italy). They were all volunteers over 18 years of age. All of them provided a written informed consent before the experiment and did not receive any monetary compensation for the participation. Inclusion criteria were age equal or greater than 18 years and being native Italian speakers, to exclude any influence in response times due to reading or comprehension difficulties, as the experiment was run in the Italian language. Data collected from 40 participants were used as a training set to build machine learning (ML) models and data from the remaining 10 subjects were used as a test set, to evaluate the model generalization capabilities. The demographic data of training and test samples are reported in Table 1. Participants were randomly assigned either to the liar or to the truth-teller group. Therefore, half of the sample performed the task as liars and the other half performed the task as truth-tellers. Experimental procedure The Ethics Committee for Psychological Research at the University of Padua approved the experimental procedure. The experimental procedure was similar to that previously reported. Participants assigned to the experimental condition (liar group), were asked to learn a fake identity profile that included faked first name, family name, date and place of birth, residence address, profession and civil state. Participants were required to rehearse the fake information until they were able to recall them by heart with no mistakes. Between the rehearsals, they were required to solve mathematical and logic tasks, to increase the cognitive load and to distract them from the learned information. This procedure was adopted to make sure that the fake identity profile was not stored merely in the working memory, but also in the long-term memory so that subjects could recall their fake identity for the whole duration of the experiment. Two distinct experimenters conducted the two phases of the experimental procedure. The first one assisted the subjects in memorizing their fake identity profile, as described above, while the second experimenter was in a separate room and gave instructions on how to perform the computerized task. The peculiarity of this procedure is that a \\\"fake-blind\\\" condition was created. The first experimenter (the one who trained the subjects) told the participants that the other researcher was not aware of the condition of each subject (liar or truth-teller). In this way, participants of the experimental condition were invited to do their best to cheat the second experimenter. Participants assigned to the truth-teller group were asked to fill in their own data in a facsimile of an Italian identity card (ID). They were required to solve the same mathematical and logic tasks as the liar group, to balance the cognitive load before undertaking the computerized task. They repeated their personal data (name, surname, date and place of birth, residence address, profession and civil state) only once after the distracting mathematical riddles. After the learning phase, both liars and truth-tellers entered the room where the second experimenter was allocated. They were required to show their fac-simile IDs and to wait a few minutes while the second experimenter entered their personal data (real or fake) into the system. Finally, they were asked to complete the computerized task that required them to respond to questions about their identity. The experiment was programmed in E-Prime(r) 2.0 . The experiment was run on a single laptop ASUS K56C with a 15.6'' diagonal screen LCD. Stimuli Each participant was required to respond to 78 questions in total, including 18 control questions, 20 expected questions and 40 unexpected questions. All the questions were in the form of affirmative sentences that required a \\\"yes\\\" or \\\"no\\\" response, with a perfect balance, so that half of them required a \\\"yes\\\" response and the other half a \\\"no\\\" response, for both liars and truth-tellers. The expected questions consisted of personal information provided by each subject in their ID, such as first name, family name, date and place of birth, residence address, profession and civil state. Liars expected to be tested on these pieces of information, as the first experimenter had made sure that each liar could perfectly recall the new identity and recommended each of them to cheat the second experimenter. The unexpected questions concerned information that, though not explicitly presented in the IDs, could be extracted from the basic information contained in the IDs. Truth tellers did not need to think about the right answers to the unexpected questions, as extracting derived-from-the-ID-data information was an automatic and easy process for them. For instance, if you were born on April 20th, you should also know that your zodiac is Aries; if you lived in Padua, you should know the area zip code. By contrast, as the liars had learned their fake ID data, they needed a much greater cognitive effort and a longer time to answer unexpected questions. As a result, the liars showed increased response times and higher errors rates. Finally, the control questions included some objective features of the participants that were directly verifiable by the experimenter, such as their gender, hair and eye colour or what they were wearing during the experiment. In summary, each subject responded to 9 control questions requiring a \\\"no\\\" response, 9 control questions requiring a \\\"yes\\\" response, 10 expected questions requiring a \\\"no\\\" response, 10 expected questions requiring a \\\"yes\\\" response, 20 unexpected questions requiring a \\\"no\\\" response and 20 unexpected questions requiring a \\\"yes\\\" response. Examples of questions are reported in Table 2. Stimuli appeared in random order in the center of the computer screen and two response labels were placed, respectively, in the right and in the left upper corners of the screen. To give their response, subjects were instructed to press either the key \\\"A\\\" or the \\\"L\\\" on the computer keyboard that corresponded respectively to the left and the right response label. Moreover, they were instructed to press the response key on the computer keyboard as fast as they could and, at the same time, they had to try to be as much accurate as they could. Each stimulus appeared automatically after the response to the previous one, so no action was required to the subjects to bring up each new question. No temporal response limit was fixed and no feedback was provided for responses. Latency-based measures During the task, we recorded RTs and numbers of errors. For errors, we mean the wrong responses provided by subjects according to the information that they reported, independently from the fact that they were liars or truth-tellers. Then, for each participant we computed the average RTs and the average number of errors, separately for control, expected and unexpected questions. Moreover, RTs were calculated separately for wrong and right responses. Then, we calculated the Inverse Efficiency Score (IES), an index that combines speed and accuracy. As a matter of fact, subjects can increase the response speed during the task, but this usually leads to a higher proportion of error (PE). The IES considers the number of errors and increases proportionally the average RT of the subject according to the following formula: Equation 1: Calculation of the Inverse Efficiency Score. The IES was calculated separately for control, expected and unexpected questions. The final list of predictors is the following: RT control, RT expected, RT unexpected, RT control right responses, RT expected right responses, RT unexpected right responses, RT control wrong responses, RT expected wrong responses, RT unexpected wrong responses, errors control, errors expected, errors unexpected, IES control, IES expected, IES unexpected. Analyses and results Feature selection Feature selection consists in the process of automatically selecting the best subset of predictors, to maximize the model accuracy, that is, in our specific case, the accuracy in discriminating between liars and truth-tellers. Feature selection is a widely used procedure in machine learning (ML) , as it allows to remove redundant and irrelevant features and to increase the model generalization by reducing over-fitting and noise in the data. Here, feature selection was performed using a correlation-based feature selector (CFS) algorithm, as implemented in WEKA 3.9  and was applied to the original set of predictors (RT control, RT expected, RT unexpected, RT control right responses, RT expected wrong responses, RT unexpected right responses, RT control wrong responses, RT expected wrong responses, RT unexpected wrong responses, errors control, errors expected, error IES = RT (1 - PE) unexpected, IES control, IES expected, IES unexpected) using a tenfold cross-validation procedure. The CFS algorithm identifies the best subset of features by considering the individual predictive ability of each predictor, along with the degree of redundancy with the other predictors. The subsets of features that are highly correlated with the class (the dependent variable; in our case truth-tellers vs liars) and, at the same time, lesser inter-correlated with each other, are preferred. To search the subset of predictors through the spaces of features, the Greedy Stepwise search method was chosen (with forward search). We finally retained the four features most frequently selected in the tenfold: RT wrong expected (rpb = 0.51, selected in four out of tenfold of the cross-validation), RT wrong unexpected (rpb = 0.19, selected in ten out of tenfold of the cross-validation), IES expected (rpb = 0.54, selected in nine out of tenfold of the cross-validation), IES unexpected (rpb = 0.77, selected in ten out of tenfold of the cross-validation). Note that responses to control questions were discarded by the feature selection algorithm, as they did not carry any useful information to distinguish the two groups (truth-tellers vs liars). Table 3 reports the correlation matrix of the four selected features and their correlation with the dependent variable (liar vs truth-teller). Descriptive statistics and analysis of variance Table 4 reports the descriptive statistics for the four selected features in the original sample of 40 participants. It is worth to notice that liars provided on average 0.95 (SD = 0.89) wrong responses to expected questions and 12.95 (SD = 3.94) wrong responses to unexpected questions, while truth-tellers gave on average just 0.15 (SD = 0.37) wrong responses to expected questions and 4.45 (SD = 2.82) wrong responses to unexpected questions. An ANOVA was run to investigate the difference between the two experimental groups (liars vs. truth-tellers), both for RT in wrong responses and IES. RTs to wrong responses of liars were longer than those of truth tellers [F(1,38) = 7.80, p < 0.01, e2 = 0.06]. In addition, both liars and truth-tellers had longer RT in responding to unexpected questions compared to expected questions [F(1,38) = 77.31, p < 0.01, e2 = 0.44]. No statistically significant results emerged from the interaction condition (liars vs. truth-tellers) X type of question (expected vs. unexpected). As far as IES is concerned, ANOVA indicated that liars had a greater IES than truth-tellers [F(1,38) = 51.06, p < 0.01, e2 = 0.25]. Moreover, both liars and truth-tellers had greater IES in responses to unexpected questions compared to expected questions [F(1,38) = 151.60, p < 0.01, e2 = 0.36]. Finally, the interaction condition (liars vs. truth-tellers) X type of question (expected vs. unexpected) was statistically significant [F(1,38) = 47.04, p < 0.01, e2 = 0.11]. Indeed, data showed a larger difference between liars and truth-tellers based on unexpected compared to expected questions (see Cohen's d values in Table 4). Analyses were run using \\\"ez\\\" package in R software (2016). Machine learning models In the last years, researchers from different scientific fields have emphasized the utility of focus on prediction rather than explanation when data are analysed. Attention to predictive models has increased mainly thanks to the significant spread of machine learning (ML) techniques, which allow to train algorithms on samples of data (training set) to make predictions on completely new data (test set) without being explicitly programmed. As far as psychology is concerned, ML techniques are particularly useful to predict human behaviour, including deception. Indeed, ML makes it possible to draw inferences at the individual level, while traditional statistical methods focus on a group level. Thus, by applying ML models, one can assess individual subject behaviour. The four selected features (RT wrong expected, RT wrong unexpected, IES expected, IES unexpected) were entered in five different ML algorithms: Logistic , SVM , Naive Bayes , Random Forest , LMT. A useful strategy to avoid cherry peaking the best performing model is to verify that classification accuracy does not change significantly among different classes of classifiers. If similar results are obtained by ML models relying on radically different assumptions, one may be relatively confident that results are not dependent on specific assumptions. For this reason, we developed the five models mentioned above. (Fig. 1) All models were validated following a tenfold crossvalidation procedure. Cross-validation is a resampling procedure used to reduce variance in the model performance estimation. The procedure uses parameter k, where k is a positive integer and splits the data set into k groups. One group is used as a hold out of the validation set, while the rest is used to train the model. Next, we trained our model on the training set and evaluated the performance of the validation set. We kept the score of each validation, reshuffled the data set randomly and repeated the procedure for k times, hence the name k-fold cross validation. Finally, the five models, which were validated through the tenfold cross-validation procedure, were tested on a new sample of 10 participants. Indeed, as ML models are built to fit the data, it is important to test how an existing model fits new unseen data. For this reason, part of the data (training set, n = 40) was used to train and validate the model, while another part (test set, n = 10) was set aside to test the model accuracy on new examples that had never been seen by the ML classifier. This procedure guarantees the generalization of the model and increases the replicability of results , a crucial issue in behavioral experiments. Results obtained by the tenfold cross-validation procedure are reported in Table 5. Finally, we tested the generalization of the model performance on the new set of ten participants who had not been included in the development of ML models. The results confirmed that all the models reached an accuracy of about 90% in classifying subjects as liars or as truth-tellers, both in training and test (see Table 5). The comparable results between the tenfold cross-validation and the test set indicated that cross-validation is a valid conservative estimate of the replicability power of the model. Moreover, the relatively constant performance on the out-of sample 10 participant test set indicated that the estimate accuracy does not depend on the specific assumptions of the models. About the rate of false positive and false negative, the confusion matrix showed that the number of misclassified liars and truth-tellers was not equal for all the algorithms. Logistic regression produced a balanced number of false positives and false negatives, failing in detecting two liars and two truth-tellers in the training set and one liar and one truth-teller in the test set. The SVM was completely unbalanced towards the false negatives, misclassifying four liars in the training set and one liar in the test set. Naive Bayes had an opposite performance, failing the classification of four truth-tellers in the cross-validation and one truth-teller in the test set. Random forest misclassified only one liar in the training set and one truth-teller in the test set. Finally, logistic model tree (LMT) failed in recognizing two liars in the cross-validation procedure and one truth-teller in the test set. Discussion Detecting liars of personal identities is becoming an increasing important goal, as faked identities plague more and more the web and social networks. This is due to the fact that personal identities information can be easily learned and rehearsed to a point that lies are expressed as naturally and automatically as truths. Different researches capitalised on the use of unexpected questions and the use of mouse or keystroke dynamics to overcome automaticity in rehearsed lies. The present experiment expanded previous research using choice reaction times in reply to statement like questions requiring \\\"yes\\\" and \\\"no\\\" responses. We developed ML classifiers to evaluate the out-ofsample accuracy of the models in distinguishing liars from truth-tellers. Machine learning was used to complement the standard statistical analyses for the following reasons : * ML models focus on the predictive power of the models and most lie detection research is about accuracy in spotting liars. * ML models allow to estimate out-of-sample accuracy. The results reported here confirm that the most informative features in distinguishing between liars and truth-tellers are the IES and the RT to wrong responses, both to expected and unexpected questions. More precisely, our analysis indicated that: * RT-based test of liars about identity had a similar accuracy as mouse-based or keystroke dynamics-based detection (all these techniques reached at least 90% of accuracy in the test set). * The most relevant predictor that contributed to detecting liars was IES (a measure that is intended to handle speed-accuracy trade-off). Moreover, as concerns IES, the differentiation between liars and truth-tellers is much stronger with unexpected compared to expected ques tions, confirming that using unexpecting questions is a promising approach. * The time taken to wrongly respond to expected and unexpected questions also contributed to the classification model performance. It should be noticed that truth-tellers had very short RTs when they gave wrong responses to expected questions. This indicates that when a truth-teller fails in responding to expected questions, this is probably due to the speeded impulsivity in the response. On the other hand, the errors of the liars likely were due to incapacity to retrieve the correct information. * The results are not model-dependent, as a variety of ML models that rely on very different assumptions performed at similar levels of accuracy. To conclude, it is possible to spot liars declaring faked identities by asking unexpected questions and measuring RT and errors with an accuracy equivalent to that of mouse and keystroke dynamics recording. While the overall accuracy achieved with choice reaction times and mouse dynamics is comparable, mouse dynamics seems more resistant to countermeasures, as many parameters must be kept under control at the same time to fake the results. Countermeasures are strategies implemented by the liars to avoid detection. As detection of liars using mouse dynamics is based on a multitude of parameters that encode timing and erraticism of the mouse movement, they are more likely to be resistant to explicit strategies to doctor the results. On the other hand, the advantages of using choice reaction time, as reported here, are that the experiment is simpler to build and to analyse. In addition, in this case countermeasures are not easy to develop without an explicit coaching aimed at selectively teach the cheater to modify the latencies in responding to erroneous responses. As it is the case for a mental chronometric approach to lie detection in general, the current main limitation is represented by the difficulty to apply this technique in ecological contexts, in which the subject behaviour is not under the researcher control. What is different from the laboratory to the daily reality is the number of external stimuli, which may interfere with the task and which can lead to wrong conclusions. Indeed, any other activity that may be usually carried out by individuals during an ID recording, as well as problems with bandwidth, could result in longer response times and, therefore, may produce false-positive liars. Therefore, to further evaluate this approach, future experiments should be conducted by recruiting participants via the Web in a more ecological setting. A first attempt in this direction has already been done by Monaro et al. who measured temporal keystroke features when participants were asked to fill an online form with their real or faked identity information. The classification model built on a first sample of participants who were recruited in the laboratory showed a high generalization to a second sample of participants who were recruited via the Web, reaching high accuracy also in the ecological setting (89-94%).\",\"577240548\":\"Background Only in a few month, the COVID-19 epidemic developed into a serious pandemic affecting all countries around the globe. Physical and social distancing and global lockdown of public, social, and work life was and still is a necessity in many countries to fight the pandemic without vaccine. Scientific progress in understanding the behaviour of the virus has grown rapidly since the outbreak of the pandemic, while scientific understanding of the psychological consequences of the pandemic is still at a developing stage. Empirical studies investigating mental health, well-being, subjective experience and behaviour during the COVID-19 pandemic are currently underway and several survey studies from several countries have meanwhile been published. First published surveys investigated the mental health of Covid-19 survivors or of health care professionals enrolled in the treatment of COVID-19 patients. Moreover, first observations from surveys investigating psychological reactions of the general population in the hot spot countries immediately after the outbreak of the COVID-19 pandemic in 2020 have meanwhile been published e.g., [3-5]. The results suggest a significant increase in mental ill health among populations during the first few months of the COVID pandemic, supporting earlier observations from previous epi- and pandemics. The World Health Organization (WHO) expects mental health burdens in the general population to be particularly pronounced in people who have already been at risk of or suffering from affective disorders before the pandemic (see for an overview). Similarly, patients in general as well as patients with a chronic mental disorder in particular, are expected to suffer from impairments in mental health and well-being due to their medical and psychotherapeutic treatment being reduced or cancelled as a consequence of the pandemic lockdown. In addition, health care professionals involved in the treatment of COVID-19 patients as well as workers with system-relevant jobs are supposed to be at special risk of developing stress-related symptoms and diseases such as post-traumatic stress disorder, chronic fatigue, anxiety, and depressive disorder. However, the current COVID-19 pandemic is not just threatening specific parts of the population. On the contrary. The spread of the virus around the world, its exponential increase in infection probability, and its high lethality bear constant threats for whole societies and for each individual as the pandemic is still evident now, one year after the pandemic outbreak. Therefore, according to the WHO, primary mental health prevention targeting either the general public or specific population groups should be an indispensable goal of crisis management of the current COVID-19 pandemiccomprising all age-groups from youth, adolescence to adulthood. Notably, fighting the COVID-19 pandemic currently still requires behaviour change in everybody including daily behaviour (work, business, family, and leisure) as well as changes in health behaviour and social behaviour. In each country so far, the COVID-19 pandemic lockdowns affected daily behaviour routines including work, business, family, and leisure time activities. The COVID pandemic lockdowns started in China in January 2020 and only a few months later, lockdowns followed in many countries around the globe including Germany and Egypt in March 2020. Crucially, in all countries, the first lockdowns came by far and large unexpected to the population. The restrictions in daily life and behaviour may therefore not be tolerated equally well by everybody. Accordingly, health care professionals and the WHO have suggested that counseling programs supporting and assisting people in behaviour change need to become part of the COVID-19 pandemic prevention initiativesto avoid unnecessary mental health burdens in the general public. However, in order to successfully support mental health, well-being, and behaviour in those social domains of life most seriously affected by the current COVID-19 pandemic, a better scientific understanding is required of how individual people experience and psychologically react to the current COVID-19 pandemic, how they think, feel, suffer and cope with the situation, and how they are handling threat perception, how they perceive and regulate emotions and behaviour. Academia and education are two social and public domains that have been seriously affected by the pandemic lockdown in every country. Concerning Germany, in March 2020 the different states of Germany decided to postpone all academic teaching at higher education institutions to an indefinite period. The universities' infrastructure including libraries were closed and students were not allowed to come to the university. Similarly, concerning Egypt, public and private universities responded in a similar manner as mandated by the government by closing the campus for students and switching all teaching activities to e-learning. Teaching courses including classes, laboratory courses, seminars, preparatory and induction courses were suspended for the summer term 2020. Teaching during the summer term was announced to be offered as online e-learning format. The lockdown situation in the two countries was thus almost identical for university students concerning the aspects of their social and academic life. Working at home without any possibility of coming to the university campus and not being able of attending to lectures and courses face-to-face together with peers, tutors, and teachers require from students to learn and adapt to new behaviour rules. Psychologically, pandemics increase uncertainty. Uncertainty causes stress and increases the risk for mental ill health if it conflicts with behaviour routines and habits. Despite most of the students being digital natives, the abrupt switch from face-to-face communication to digital, computer-assisted forms of teaching and sole reliance on digital interaction as the only means of social interaction might not be tolerated mentally and physically equally well by all students. Whether the current pandemic situation and its consequences are experienced as a threat may depend on the students' individual character, i.e., the student's personality and self-concept as well as his\\/her current cognitive, affective, and motivational state. Recent observations from published survey studies among Chinese students after the lockdown reported an increase in general anxiety within about 25% of the student participants. Anxiety symptoms ranged from mild to moderate to severe anxiety. Moreover, pandemic self-isolation was found to be associated with complex patterns of psychopathology amongst students including an increase in symptoms of obsessive-compulsive disorder, hypochondria, depression, and neurasthenia. Meanwhile published survey studies from several countries in Europe and across the world support negative changes in mental health among university students immediately after the first lockdowns in 2020, specifically in relation with quarantine and self-isolation. Nationwide surveys conducted before the COVID-19 pandemic already reported elevated mental health problems and stress-related symptoms including anxiety and depression among university students, and this, although university students across countries might belong to the young educated low-risk population. In a recent online study including N = 185 university students studying in Germany, 36.6% of the students (women and men) reported to experience depressive symptoms, 41.83% (women and men) reported high levels of state anxiety, and mental stress due to excessive demands and uncertainty in finances, job, or social relationships. This prevalence of academic stress and mental health burdens have been found among university students all over the globe, including Egypt. Thus, as a population group, university students may be particularly vulnerable to stress-related lifestyle changes affecting mental health that are associated with the current COVID-19 pandemic. Individual differences in mental health may also exist and influence how the students perceive and how well they adapt and cope with the current COVID-19 pandemic situation and to what degree they are motivated to change their behaviour in response to the pandemic consequences in social and academic life and teaching. Psychological theories and models of behaviour change, e.g., Health Belief Model, Transtheoretical Model, or Social Cognitive Theory, all agree in that individual factors, specifically those related to emotion- and self-regulation can explain how people perceive themselves, whether and why they change their behaviour and why others do not. Threat perception has been suggested to play an outstanding role, because pandemics threaten the whole person, i.e. our self and the self-concept. Personality traits although considered stable may play a critical role in threat perception, in mental health and behaviour because they influence and modulate the person's feelings, beliefs, and the person's trust in one's own self-regulatory abilities required to change one's own behaviour. Moreover, stable personality traits and a positive self-concept are considered general important stress buffers and protectors of mental health, whereas neuroticisms, trait anxiety, difficulties in describing and identifying feelings as well as an overall negative self-concept are considered significant risk factors of mental ill-health, specifically of anxiety disorder and depressive disorder. These examples underscore the complexity and dynamics of how individual traits and state-like individual psychological factors as well as characteristics of the situation interact and influence subjective experience and behaviour. Methodologically, this raises questions of how interactions between situation, person and behaviour can best be assessed, investigated, modeled and predicted in relation to the COVID-19 pandemic in which little empirical evidence is available so far and different aggregated data measures of qualitative and quantitative origin might be used to best capture the internal personal variables of interest (e.g., feelings, worries, self-concept, or personality traits) that provide insight into the subjective experience and the perceived changes in health and behaviour of individual persons behaving in the context of the COVID-19 pandemic. Computational modeling and machine learning have been already successfully applied in the field of pandemic research to predict transmission rates of the virus based on global behavioural changes of the general population. These approaches require huge data sets (big data). In health behaviour research, first attempts have been made to apply computational models to data sets comprising smaller sample sizes to model behaviour of individuals, for instance, in response to behavioural interventions supporting health prevention. These computational models build on psychological theories of human behaviour. Character Computing is one of these psychologically-driven approaches, whose computational models include stable character traits (e.g., personality, self-concept) and cognitive, affective, and motivational state variables and behavioural indicators as input to take into consideration the dynamic interactions between situation (S), person (P) and behaviour (B) (for an overview, seeand Fig. 1). The computational models are not fixed but can be improved and extended, e.g., by ontologiesor automated data processing, the more empirical evidence and data is available. Methods Aim of this online survey study Based on the challenges of the COVID-19 pandemic outlined above, this online survey study is aimed at contributing to the scientific understanding of the psychological consequences of the pandemic by investigating mental health, subjective experience, and behaviour among university students studying in Egypt or Germany after the first pandemic lockdown in May 2020. As outlined above, university students may be particularly sensitive to lifestyle changes related to the COVID-19 pandemic, negatively affecting the students' mental health, their subjective experience and behaviour. Moreover, as also explained above, the students' personality traits and self-concept might constitute important stable psychological variables that could influence mental health as well as subjective experience and behaviour related to the COVID-19 pandemic. Therefore, to fully capture these psychological aspects, psychological assessment included a number of psychological variables ranging from stable personality traits and self-concept to state-like psychological variables sensitive to situational change and related to (a) mental health (current depressive symptoms and state anxiety), (b) pandemic threat and emotion perception including current feelings, (c) worries about health including perceived changes in paying attention to bodily symptoms, and (d) self-reported perceived changes in health behaviour (weight, eating, sleeping, physical activity), social and learning behaviour (difficulties in self-regulated learning). To capture all aspects summarized under (a)-(d), the assessment methods comprised a mix of self-report tools (survey items, standardized psychometric scales, psychological questionnaires, and linguistic self-report measures). Data analysis included (a) descriptive analysis for prevalence estimation of mental health variables, (b) linguistic analysis of self-concept, personality and feelings during the pandemic and (c) correlational analysis and machine learning tools. Machine learning tools were used for exploratory purpose only to further explore the idea of whether machine learning algorithms could despite small sample sizes be trained to predict stable personality traits from the self-report data of the students. Knowing whether stable personality traits (that due to their stability cannot easily be changed by health care interventions) can be predicted from the students' self-report data could help develop individualized health care interventions that take the students' personality development into account. The online survey was distributed among university students studying at universities in Egypt and also in Germany. Both countries were equally affected by the lockdowns in May 2020. With respect to the already published survey studies (see above), all attesting an increase in mental ill health among university students during the COVID-19 pandemic the following main research questions were addressed: * RQ1 Mental health: Can the present online survey study confirm high state anxiety and depressive symptoms reported in previous studies in the current sample of university students during the time period of the first COVID-19 pandemic lockdown in May 2020? Crucially, are the self-reported symptoms of anxiety and depression when assessed on standardized psychological screening and assessments tools beyond the cut off scores of clinical samples, and comparable or even higher than the prevalence rates reported in pre-pandemic surveys? * RQ2 Threat perception and worries about health: Do university students report to experience threat, negative feelings and worries about health during the COVID-19 pandemic? * RQ3 Emotion perception: Do university students report to perceive difficulties in emotion perception in the time period of the first pandemic lockdown relative to before the pandemic? * RQ4 Health behaviour, social behaviour and learning: Do university students report to perceive changes in health behaviour (e.g., weight, eating, sleeping, physical activity, paying attention to bodily symptoms), and do they report to experience difficulties in selfregulation during learning (teaching), and in social behaviour in the time period of the first pandemic lockdown? * RQ5 Self-concept and personality: Do university students report a positive or a negative self-concept? Are mental health variables correlated with the students' personality? * RQ6 Exploratory analysis: Can machine learning despite small data sample sizes predict stable personality traits from the self-report data of the students? Participants The survey study was designed and conducted by the Department of Applied Emotion and Motivation Psychology of Ulm University and administered via Ulm University and LimeSurvey software (https:\\/\\/ www. limes urvey. org\\/ de\\/). The survey was advertised among others via the university's international office to reach specifically students studying in Egypt. The survey was provided in English language (i.e., the academic language), and proficiency in English language was a prerequisite for taking part in the study. Participants were fully debriefed about the purpose of the survey, participation was voluntary and anonymous (see ethics statement). After registration, participants answered questions about their language proficiency, age, gender, their university, study year, and their living situation (alone, with friends or family). Only university students who were aged 18 years and older, and who provided informed consent were able to participate in the study. The survey items were structured in blocks of items and questionnaires: sociodemographic (1), personality (Big-Five) and anxiety (state and trait) (2), survey items about teaching, survey items about health including the linguistic task (self-concept) (3-4), and finally, emotion perception and depression screening (5). The blocking of the serial order of these topics lead to partial drop-outs across the survey, particularly across blocks (see below). An overview of the complete study-design is provided in the flow-diagram in Fig. 2. An overview of the online survey items and questionnaires can be found in the Additional file 1. Study sample, survey drop-out and missing data In total, N = 453 university students registered for the study and answered the inclusion and exclusion criteria. Of these, n = 3 were pilots and n = 11 participants did not give informed consent or did not explicitly state that they want to get their data published in scientific research, and were therefore excluded from the study sample. N = 439 volunteers (n = 215 men, n = 219 women, n = 5 did prefer not to name their gender; mean age: 20.69 years, SD = 2.87 years) completed the sociodemographic questions. Of these, n = 19 (4.3%) did not report to study in Germany or Egypt and were excluded. Of the 420 university students who reported to study in Egypt or Germany, n = 325 participants (n = 167 men, n = 156 women, n = 2 did prefer not to name their gender; mean age: 20.38 years, SD = 1.76 years, range: 18-33 years) filled in the personality and anxiety questionnaires only, while n = 220 participants (n = 112 men, n = 107 women, n = 1 did prefer not to name the gender; mean age: 20.45 years, SD = 1.88 years, range: 18-33 years) completed the entire survey. This corresponds to a survey completion rate of 0.49 (division of the number of participants who complete the entire survey (n = 220) by the total number of participants who register for the survey (n = 453)). This rate falls within the rate expected for online surveys (20-50%). Analysis of the drop-outs (including e.g., univariate measures of variance (ANOVA)), showed no difference in age between the groups (i.e., the sample who filled in the sociodemographic items only (n = 95) versus the sample who filled in the personality and anxiety questionnaires only (n = 105) versus the final sample (n = 220), F(417,2) = 1.72, p = .18. In addition, the student samples did not differ with respect to gender, i.e., the % of the number of women and men. Analysis of anxiety and personality scores likewise suggests that the final sample and the sample who dropped-out after filling in the personality or anxiety questionnaires (n = 220 versus n = 105) did not differ in state anxiety or in the scores on any of the Big-Five personality dimension. (state anxiety: F(323,1) = 1.77, p > .18; Openness: F(323,1) = 0.16, p > .69; Conscientiousness: F(323,1) = 2.82, p > .13; Extraversion: F(332,1) = 0.94, p > .33; Agreeableness: F(323,1) = .062, p > .43; Neuroticism: F(323,1) = 1.22, p > .27). Mean scores of trait anxiety differed between the final sample and the sample who dropped out (n = 220: mean: 46.02, SD = 11.2, range: 26-79 vs. n = 105: mean: 49.02, SD = 10.98, range: 26-77, F(323,1) = 5.78, p = .017). However, using median tests (which are less susceptible to outliers) showed no significant difference in the distribution of trait anxiety scores between the samples (mediantest = 1.59, p = .21), see Fig. 3 for an overview. The survey was programmed such that it produced as little missing data as possible. Therefore, missing data of single items in a questionnaire or in a block of open items could be excluded and missing scores were therefore not imputed. Regarding the self-generated prompts, participants were free to answer the prompts (self-concept and feeling descriptions). Inspection of the data shows that in the full sample, 5 participants did not fill in all of selfdescriptive prompts, leaving open 1, 2 or 3 of the descriptions, respectively. Measures: survey items and questionnaires The online survey included several self-report measures comprising a mix of single items with open and closed questions, standardized psychometric scales, and standardized psychological questionnaires. The section below and Table 1 provide an overview of the survey items, questionnaire measures and hypotheses grouped according to the psychological domains and research questions of interest (for an overview, see also RQ1-RQ6 in the section \\\"Aim of this online survey study\\\"). Mental health: anxiety (trait\\/state), current depressive symptoms (last 2 weeks) As illustrated in Table 1, the participants anxiety proneness including trait and state anxiety as well as their current self-reported depressive symptoms (last 2 weeks) were assessed with psychological questionnaires including the Spielberger Trait and State Inventory (STAI, [37]), and the Patient Health Questionnaire (PHQ-2, [38]). The STAI is available in many different languages and has shown similar values of internal consistencies among university students from European and Arabic countries. Whereas the trait scale of the STAI asks for how one generally feels, the instruction of the state scale of the STAI asks for how one feels right now. The PHQ-2 has proven to be a robust screening for depressive symptoms across different cultures including European and Arabic countries. It asks for the presence of depressive symptoms over a time period of the last two weeks. Threat perception, feelings, and perceived difficulties in emotion perception during the COVID-19 pandemic Threat perception as well as discrete emotions and feelings in response to the COVID-19 pandemic situation were assessed by single survey items. Specifically, these items asked the participants about how the current COVID-19 pandemic situation makes them feel in terms of valence (positive\\/pleasant-negative\\/unpleasant), arousal (low\\/ calm-high\\/aroused), and dominance (feeling in or out of control of the situation). The 9-point Self-Assessment Manikin scales (SAM, [41]) were used for valence, arousal and dominance assessment. The SAM scales are one of the most robust and frequently used scales for the unbiased, non-verbal assessment of emotions and feelings on the three dimensions of emotions including valence, arousal and dominance. In accordance with the literature, the SAM scales ranged from 1 (negative\\/unpleasant, low arousal\\/calm, out of control) to 9 (positive\\/pleasant, high arousal\\/aroused, in control). In addition, we asked the participants to indicate which kind of discrete emotions they experienced in response to the COVID-19 pandemic. Participants could choose among six discrete emotions (sad, anxious, angry, disgusted, happy, surprised, or neutral). In addition, participants were given five prompts to describe their current feelings in response to the COVID-19 pandemic situation (\\\"I feel....\\\"). In order to assess potential difficulties in emotion perception, participants filled in the Toronto Alexithymia Scale (TAS-20; [42]), which comprises the three subscales \\\"Difficulty Describing Feelings\\\", \\\"Difficulty Identifying Feelings\\\", and \\\"Externally-Oriented Thinking\\\". Since we were interested in perceived changes since the pandemic outbreak, participants were instructed to answer each item of the TAS-20 questionnaire relative to before the pandemic. Worries about health and perceived changes in behaviour during the COVID-19 pandemic Worries about health, perceived changes in paying attention to bodily symptoms (e.g., taste, smell, cardiovascular, respiration\\/breathing, appetite\\/eating\\/drinking), as well as perceived changes in health behaviour (weight, eating behaviour, sleep and physical activity behaviour) as well as perceived difficulties in social behaviour (social distancing) and self-regulatory learning (i.e., difficulties in paying attention to the content provided by e-learning, difficulties in studying with the same effort as before the pandemic situation) were assessed via single survey items. The single item questions that asked for worries and perceived changes in behaviour could be answered with \\\"yes\\\" or \\\"no\\\"; \\\"yes\\\" meaning an increase and \\\"no\\\" meaning no change in relation to before the pandemic. The items on health behaviour included items asking in both directions, e.g., whether one eats more or less, sleeps more or less, exercises more or less than before the pandemic. The single item questions of paying attention to bodily symptoms could be answered on 10-point Likert scales such that change scores could be calculated based on the participants' answers allowing evaluation of the degree of change as increase, decrease or no change during the pandemic situation in relation to before the pandemic (see Table 1 for an overview). Personality and self-concept As illustrated in Table 1, the participants' personality traits were assessed with the Big Five Personality Inventory (BFI, [43]). The BFI-40 is a standardized self-report measure that has been validated in different cultural populations and age groups. The self-concept was assessed using a modified short version of the twenty statements tests (TST, [45]). The TST is a cross-cultural tool for the assessment of different facets of the self-concept including actual, ideal, and ought selves. In the present study, participants had to generate self-descriptions for the actual self only. In line with the instruction of the TST, participants were asked to provide five words to the prompts \\\"I am....\\\" in order to describe themselves. Hypotheses Mental health: anxiety (trait and state) and current depressive symptoms In line with previous pre-pandemic surveys among university students (see Background for an overview), we expected a high prevalence of anxiety and depressive symptoms in the present sample of university students irrespective of their culture or country in which they study. Prevalence rates for self-reported current depressive symptoms assessed with the screening tool of the PHQ-2 asking for depressive symptoms in the last 2 weeks (PHQ-2 items: item1: \\\"little interest or pleasure in doing things\\\"; item 2: \\\"feeling down, depressed or hopeless\\\") and state anxiety (asking for how one feels right now) might be expected to be even higher than prevalence rates reported in previous surveys before the pandemic situation. Threat perception, feelings, and difficulties in emotion perception We expected threat perception to the COVID-19 pandemic to be associated with self-reported unpleasantness, feelings of moderate to high levels of arousal, self-reported perceived lack of dominance (feeling less in control of the situation) on the Self-Assessment Manikin (SAM) scales. In addition, we expected self-reports of feelings of anger, sadness, and anxiety towards the pandemic as assessed by the survey items assessing discrete emotions. We also explored whether students report to perceive changes in emotion perception since the pandemic outbreak relative to before the pandemic outbreak. Specifically, we explored whether participants report difficulties in describing and identifying feelings and report externally oriented thinking on the TAS-20 as potential maladaptive adaptions in coping with the pandemic lockdown. As mentioned above, the instruction of the TAS items asked the participants to answer the items in relation to before the pandemic. Worries about health, perceived changes in behaviour during the COVID-19 pandemic We expected that the majority of students will report to be more worried about their mental and physical health than before the pandemic. Moreover, we expected a higher awareness of bodily symptoms (i.e., paying more attention to perceived changes in smell, taste, cardiovascular functions, breathing\\/respiration, and appetite\\/eating\\/drinking) relative to before the pandemic. Given that the lockdown in every country had effects on the students' work and leisure time activities, we also expected that participants will report changes in health behaviour including a decrease in regular physical activity compared to before the pandemic lockdown including self-reported changes in eating- and sleeping behaviour and weight. We also expected difficulties in learning and social behaviour (see Table 1). Personality and self-concept Moreover, we examined how university students see themselves (self-concept). In particular, we explored whether the students would report a positive or negative self-concept and compared their linguistic descriptions of the self to their descriptions of their current feelings pandemic-related feelings (\\\"I feel...) and their personality. Regarding personality, we explored whether stable psychological personality traits (Big Five and trait anxiety) would be correlated with state anxiety and depressive symptoms and the students' perceived changes in emotion perception. Finally, we examined for exploratory purpose, whether machine learning could predict the students' personality traits from their reports (for details see \\\"Data Analysis\\\" section). Descriptive analyses and statistics To answer the hypotheses outlined above, the participants' answers (questionnaires, single items) were analysed descriptively to provide insight into how many students on average reported anxiety and depressive symptoms as well as how many students reported to perceive changes in subjective experience (threat perception, difficulties in emotion perception, worries about health, bodily symptoms) and behaviour (health, social, learning). Analysis of the questionnaires (PHQ-2, STAI, TAS-20, BFI-40) followed the guidelines and manuals and were calculated as sum scores or mean scores (nonnormalized). For the PHQ-2, STAI and TAS-20, cut off scores are available from the literature (see \\\"Results\\\" section). These cut off scores were also used in the present study to discriminate between high versus low trait anxiety, high versus low state anxiety, depressive symptoms, and difficulties in emotion perception. Means and standard deviations were calculated for all questionnaire data and for the closed survey items using Likert scales or the SAM scales. The questionnaire data and answers to the survey items were tested statistically for significance by means of non-parametric or parametric statistical tests as appropriate. The respective test statistics are presented in brackets in the \\\"Results\\\" sections. Given the dropout across blocks of the survey (see section about Sample size, survey drop-out and missing data), the results for each scale, item or questionnaire were calculated for the available sample who filled in the questions and the final sample (n = 220) who filled in the complete survey and who reported to study in Egypt or Germany. P values are reported uncorrected and two tailed if not otherwise specified. The SPSS software (IBM SPSS Statistics Software, Version 27) was used for all statistical testing including correlation analysis (see below). Correlational analysis Correlation analyses (Pearson) were used to assess the relationships between the Big Five personality traits (BFI-40), mental health variables (STAI: trait and state anxiety, PHQ2: screening for depressive symptoms), and difficulties in emotion perception (TAS-20). P values are reported uncorrected and two tailed if not otherwise specified. Linguistic analysis of self-concept and feelings The open-ended linguistic answers assessing the selfconcept (\\\"I am...\\\") and feelings in response to the pandemic (\\\"I feel...\\\") were analysed with computer-assisted text analysis tools including Linguistic Inquiry of Word Count (LIWC; [46]). The dictionary of the LIWC software contains words and word stems, grouped into semantic categories related to psychological constructs. The categories provided by the LIWC allow the assessment of the polarity of words (positive or negative). The LIWC analysis produces reliably results with about 500 words and more. Therefore, in the present study, words generated by each participant were accumulated across participants and entered as a whole text corpus for words generated for the prompts \\\"I am...\\\" (self-concept) or for the prompt \\\"I feel...\\\" (feelings in response to the pandemic), respectively. This allows the evaluation of the self-concept and current pandemic feelings of the university sample as a whole. For the linguistic analysis no statistic testing was performed. Machine learning (exploratory analysis) Machine learning (ML) was used for exploratory purpose only and the ML algorithms were chosen to combine the different psychological variables that were descriptively analysed in order to explore whether individual personality traits including the Big Five and trait anxiety can be predicted and classified by automated machine learning tools. To this end, the questionnaire scores and answers to the different survey items were preprocessed according to the following procedure: the participants' Big Five personality traits from the BFI-40, the state and trait anxiety scores (from the STAI including for each individual, a difference score for self-reported trait and state anxiety), depression (PHQ-2), perceived changes regarding difficulties in emotion perception (TAS-20) as well as the participants' answers on the SAM scales for threat perception (e.g., valence, arousal, dominance) were normalized (z-scores). The participants' answers to the discrete emotions elicited during the pandemic, difference scores assessing increase in current anxiety (difference score comparing STAI state vs. STAI trait) as well as the participants' answers to the survey items asking for worries and perceived changes in health and behaviour were labeled as positive or negative or set to zero if the students reported no change. The answers to the survey items asking for perceived changes in paying attention to bodily sensations\\/symptoms were combined to a total score denoting the total perceived changes in attention towards bodily sensations\\/symptoms and the total change was labeled as positive or negative depending on whether attention increased or decreased relative to before the pandemic or set to zero if there was no change. Sociodemographic variables such as country or university were no contribution factors in prediction and classification. After data preprocessing and data labeling, the dataset for machine learning comprised continuous features and discrete categorical features. The whole dataset was denoted \\\"X\\\" and the continuous or discrete features were denoted \\\"y\\\" in the feature matrix. The machine learning libraries of the Python software package (https:\\/\\/ www. python. org\\/) were used for automated data analysis. Data analysis was based on regression models. Gradient Boosting Regression (GBR) and Support Vector Regression (SVR) were chosen for the regression models. The principle of Gradient Boosting Regression is to build multiple regression models based on decision trees. Decision tree models are supervised machine learning algorithms that have tree structures that recursively break down the dataset into smaller datasets through branching operations while comparing the final node results with the target values. Decision tree models provide the best fit for small sample sizes to avoid overfitting the data. The same holds true for support vector machine algorithms. Support Vector Regressions (SVR) aim at finding the best fitting line in continuous data within a predefined threshold error. The evaluation of the accuracy of the prediction is evaluated based on the root mean squared error (RMSE). Depending on the type of data to be predicted, RMSE within 10-20% of the range is considered a good result. Especially with human self-report, data accuracies are usually much lower than in other more deterministic domains of machine learning e.g., natural language processing or bioinformatics. One reason for the lower accuracies in human behaviour data is the higher variance in the data itself. To account for this, we accepted a RMSE of up to 16.6% as sufficient for the decision that the data can be predicted by the model accurately. We used the classical train\\/test split approach with a ratio of 8:2. Train\\/test split is a common validation approach frequently used in ML studies including those with smaller sample sizes [for a critical review see). No k-fold cross validation (CV) approach was chosen as it has been shown that k-fold CV can lead to overestimation especially with small sample sizes, whereas train\\/ test split and nested CV approaches have been shown to be equally reliable even with small sample sizes. We also performed hyperparameter tuning, an algorithm frequently used and recommended in machine learning to choose and select during training the best model while avoiding biasing the data, and the number of features and the feature-to-sample ratio) was kept in an optimal range (less features than samples) for avoiding overfitting. Results Descriptive data analytics Mental health: anxiety (trait and state) and depressive symptoms The mean state and trait anxiety scores of the university students who completed the entire survey and who studied in Egypt or in Germany (n = 220) were above the cut off scores that according to the literature distinguishes between high versus low anxious subjects. The mean state anxiety score as measured with the STAI inventory was significantly above the cut of score of 40 (n = 220, mean: 50.04, SD = 3.77; T = 39.47, df = 219, cut off: 40, p < 0.001). A cut off score below or above a score of 44 in the trait STAI scale differentiates between low trait anxious and high anxiety prone individuals. The mean score for trait anxiety was significantly higher than this cut off score (n = 220, mean: 46.02, SD = 11.56; T = 2.60, df = 219, cut off: 44, p < 0.01). Given the drop-out of n = 105 students, the analysis of the mean state and trait anxiety scores were recalculated for the final sample including those students who dropped out. The analysis showed that also in this larger sample of n = 325 students the cut off scores were significantly above the cut off scores (state anxiety: n = 325; mean: 50.23, SD = 3.75; T = 49.13, df = 324, cut off: 40, p < 0.001; trait anxiety: n = 325; mean: 47.08, SD = 11.52; T = 4.72, df = 324, cut off: 44, p < 0.001) and in addition, trait anxiety scores (trait) did not differ significantly between women and men in this sample (trait anxiety: n = 325; mean-woman: 47.94, SD = 11.82; men: 45.96, SD = 10.91; F(321,1) = 2.45, p > 0.12). However, women reported higher state anxiety scores than men. This difference in state anxiety scores between women and men was significant (state anxiety: n = 325; mean-woman: 50.81, SD = 3.62; men: 49.63, SD = 3.79; F(321,1) = 8.08, p < 0.005) and was also significant in the n = 220 sample. There was no significant difference in state anxiety scores between students studying in Egypt or Germany, neither in the n = 220 sample nor in the sample comprising n = 325 students (n = 220, state anxiety: Egypt-mean: 50.16, SD = 3.75, Germany-mean: 49.08, SD = 3.86, Mann-Whitney-U = -1.39, p = 0.16; n = 325, state anxiety: Egypt-mean = 50.32, SD = 3.70, Germany-mean: 49.45, SD = 4.22, Mann-WhitneyU = -1.24, p = 0.22). However, students studying in Egypt reported higher trait anxiety compared to the students studying in Germany (n = 325, trait anxiety: Egypt-mean: 47.62, SD = 11.60, Germany-mean: 42.24, SD = 9.75, n = 220, trait anxiety: Egypt-mean: 46.49, SD = 11.57, Germany-mean: 42.40, SD = 10.93), but this difference was not significant in the final sample (n = 220, MannWhitney-U = - 1.39, p = 0.16). The results are illustrated and summarized in Fig. 3. For the PHQ-2 screening for depressive symptoms a sum score greater than 3 on both items is associated with depression proneness. In the sample of university students who completed the entire survey and therefore had filled in the PHQ-2 depression screening, the mean sum score was mean: 3.48, SD = 1.58, and significantly above the cut off score (T = 4.51, df = 219, cut off = 3, p < 0.0001). 51.82% (n = 114) of the students had sum scores greater than the cut off (> 3), and 19.09% (n = 42) had a sum score of 3 (cut off). Only 26.82% (n = 59) of the sample scored below the PHQ-2 cut off score (< 3), and only 2.27% (n = 5) did report to not suffer from loss of interest or pleasure in doing things (PHQ-2 item 1) or from feeling down, depressed or hopeless during the last two weeks (PHQ-2 item 1) (see Fig. 3 for an overview on state anxiety and depressive symptoms). The PHQ-2 scores did not differ between students studying in Egypt or Germany (n = 220, Egypt-mean: 3.51, SD = 1.56, Germany-mean: 3.24, SD = 1.79, Mann-WhitneyU = - 0.643, p = 0.52) nor did they differ between women and men (n = 220, woman-mean: 3.48, SD = 1.54, menmean: 3.47, SD = 1.63, F(217,1) = 0.00, p = 0.98). Threat perception, feelings, and difficulties in emotion perception Descriptive analysis of the items assessing threat perception (SAM; Self-Assessment Manikin scales ranging from 1 (unpleasant, not aroused, or no control) to 9 (pleasant, very highly aroused, in control)) showed that, the students (n = 220) felt slightly unpleasant (mean: 4.19, SD = 1.97). In addition, 55% (n = 120) of the final study sample (n = 220) reported a score from 1 to 4, i.e., from high unpleasantness to moderate unpleasantness on the 9-point SAM valence scale. On average, the students did not feel much in or out of control of the situation (mean: 5.07, SD = 2.41) on the 9-point SAM scale for dominance. Nevertheless, 37.55% of the study sample reported a score from 1 (no control) to 4 (loss of control) on the SAM scale for dominance. Mean physiological arousal was rated as moderate (mean: 5.40, SD = 2.22). However, 50% of the university students (n = 110) reported an arousal score of 6 (aroused) to 9 (very high arousal) on the SAM arousal scale. Given the drop-out of students, comparisons of the ratings (valence, arousal, or control) were performed between samples (n = 220 and n = 59 who completed the ratings but did not fill in the entire survey). This showed that the ratings did not differ between the samples (MannWhitney-U-tests, all p > 0.70). From the set of discrete emotions (including sadness, anger, fear, disgust, happiness, surprise, or neutral emotions), 66.8% reported to feel not neutral, 93.2% reported to feel not happy, 56.4% reported to feel sad, 75.9% reported to feel angry, 92.3% reported to feel surprised, 87.7% reported to feel disgusted, and 52.7% reported to feel afraid by the current pandemic situation. The distribution of \\\"yes\\\" versus \\\"no\\\" answers differed significantly for the categories feel neutral, happy, surprised, disgusted, or angry, respectively, (non-parametric test for binomial distribution: all p < 0.001). From all students who completed these items (n = 277) the same significant results were obtained for the answers concerning discrete emotions. 16.88% of the students of the final sample (n = 220) had a total TAS-20 score greater than the critical TAS cut off score (TAS-20 cut off > 60, [30]). From the three subscales of the TAS-20 questionnaire, changes in self-reported difficulties in emotion perception in relation to the pandemic as compared to before the pandemic were reported by 62.27% (n = 137) for items belonging to the subscale \\\"Difficulty describing feelings\\\", and by 71.82% (n = 158) for the items belonging to the subscale \\\"Difficulty identifying feelings\\\" and by 50.91% (n = 112) for the items belonging to the subscale \\\"Externally Orienting Thinking\\\". The distributions of the TAS-20 scores of the three subscales did not differ between students studying in Egypt or Germany (Mann-Whitney-U, all p > 0.50). However, woman (n = 107) reported higher scores on the subscales \\\"Difficulties identifying feeling\\\" compared to men (n = 112), F(217,1) = 217.1, p = 0.035. Worries about health In the final sample who completed the survey (n = 220), 65.5% (n = 144 students) of the study sample reported to worry about their mental health more due to the COVID-19 pandemic than before the pandemic, whereas 34.5% (n = 76) answered to worry not more than before the pandemic. 71.4% (n = 157) of the students reported to worry more about their physical health than before the pandemic, whereas 28.6% (n = 63) answered to worry not more about their physical health than before the pandemic. The distributions of \\\"yes\\\" versus \\\"no\\\" differed significantly for both, worries about mental and physical health, respectively (non-parametric test for binomial distribution: all p < 0.001) and this also held true when considering all students who filled in these items (n = 227). Self-reported worries about mental health and physical health were significantly related (kh2 = 100.43, df = 2, p < 0.001). 65% (n = 143 of n = 220) reported to worry in both domains (mental health and physical health) more than before the pandemic and this also held true when considering all students who filled in these items (n = 227), see Fig. 4a. Behaviour: health Across health behaviour domains (weight, eating, sleep, physical activity), 52.3%, 58.2%, 31.8%, and 76.4% of the study sample (n = 220) reported to have gained weight, to eat more than before the pandemic and to not sleep more or exercise more than before the pandemic situation. The distributions of \\\"yes\\\" versus \\\"no\\\" answers were significantly different for the domains of eating, sleep and exercise\\/physical activity (non-parametric test for binomial distribution: eat, sleep, exercise\\/physical activity all p < 0.001) and this again held true when considering all students who filled in the items (n = 227). Paying attention to bodily sensations and symptoms (i.e., changes in taste, smell, appetite\\/eating\\/drinking, cardiovascular functions, breathing\\/respiration) did however not change significantly relative to before the pandemic outbreak. On average, on Likert scales ranging from 1 (\\\"decrease\\\") to 5 (\\\"no change\\\") to 10 (\\\"increase\\\"), participants reported not to pay more attention to or to be more aware of bodily sensations and symptoms than before the pandemic (smell: mean: 5.18, SD = 1.21, taste: mean: 5.15, SD = 1.27, bodily symptoms: mean: 5.84, SD = 1.74, cardiac symptoms: mean: 5.78, SD = 1.66, breathing: mean: 5.77, SD = 1.64, eating and drinking\\/appetite: mean: 5.52, SD = 2.09). The answers on these rating scales did not differ between students studying in Egypt or Germany (all p > 0.16), but comparisons between women and men showed that women scored significantly higher on the scale asking for attention to bodily symptoms than men (woman-mean: 6.18, SD = 1.90, men-mean: 5.50, SD = 1.53, F(217,1) = 8.50, p > 0.002). This again held true when considering all students who filled in the items (n = 227). Behaviour: social distancing and learning Being asked about their social situation of self-isolation, teaching and learning behaviour, 54% of the student sample (n = 220) replied to have difficulties in not going out during the pandemic. 76.4% replied to have difficulties in self-regulated learning, being unable of focusing their attention on the teaching content. Of these students, 60.9% replied to have difficulties in studying with the same self-regulatory effort because of being anxiously preoccupied with the current pandemic situation (see Fig. 4b). The distributions of \\\"yes\\\" versus \\\"no\\\" answers were significantly different for the domains of learning (non-parametric test for binomial distribution: eat, sleep, exercise\\/physical activity all p < 0.002) and this again held true when considering all students who filled in these items (n = 305, all p < 0.001). Linguistic self-concept and self-descriptions of current feelings Linguistic self-descriptions (\\\"I am...\\\") showed a positivity bias. Overall, more positive words than negative words were used by the students to describe themselves (see Fig. 5). As mentioned above, linguistic analysis of the university students' self-descriptions about how the current COVID-19 pandemic situation makes them feel (\\\"I feel...\\\") showed the reverse pattern with more negative words than positive words being used by the study sample to complete the prompt \\\"I feel....\\\" (see Fig. 5). In addition, Fig. 6 shows the most prominent examples, i.e., the words most often used by the students to describe their feelings during the pandemic.in the prompt \\\"I feel...\\\". Personality: Big Five The final student sample (n = 220) scored low on the BFI subscales for extraversion (mean: 24.5, SD = 5.65), neuroticism (mean: 25.37, SD = 6.51), and reported moderate scores on the conscientiousness scale (mean: 30.69, SD = 6.07), the openness scale (mean: 36.85, SD = 5.07), and the agreeableness scale (mean: 33.42, SD = 4.50) and as described earlier (see section \\\"Study sample, survey drop-out and missing data\\\"), the BFI-40 scores of the samples (n = 220 vs. n = 105 who dropped-out) did not differ in the five personality dimensions. The Big Five personality traits were significantly correlated with selfreported depressive and anxiety symptoms as well as with the self-reported difficulties in emotion perception. Table 2 shows a summary of the correlations between measures of personality traits (BFI-40), trait anxiety (STAI-trait scale), state anxiety (STAI-state scale), selfreported depressive symptoms (PHQ-2), and perceived difficulties in emotion perception (TAS-20) as obtained from the final sample (n = 220). Automated data analytics, machine learning (exploratory) The university students' personality traits (Big Five) and trait anxiety could be predicted from the psychological variables (trait and state) summarized in Table 3 through feature importance extraction by Support Vector Regression. The table and the numbers in percent show the major contributing factors to the prediction of the respective trait listed in the left column (under \\\"Measure\\\"). Table 4 shows the prediction accuracy suggesting that prediction of all trait attributes have similar error rates. Discussion The COVID-19 pandemic is taking its toll. Concerns have been raised by the WHO (2020) [8], that the COVID-19 pandemic will cause \\\"a considerable degree of fear, worry and concern in the population\\\" (; [8]) and that stress and anxiety as well as depression will increase considerably during the COVID-19 pandemic, rendering affective disorders a public mental health concern of the COVID-19 pandemic. In the present survey, mental health (depressive symptoms, state and trait anxiety), subjective experience (threat perception, current feelings, perceived difficulties in emotion perception, worries about health during the pandemic) as well as perceived changes in behaviour (related to health, social behaviour and learning\\/teaching) was assessed among university students studying in Egypt or Germany, respectively. The survey was administered in May 2020, shortly after the lockdown in these countries. Going beyond previous surveys, the students' self-concept and the Big Five of human personality were additionally assessed to explore psychological patterns between personality traits, mental health, and perceived changes in subjective experience by means of correlation analysis and machine learning. Mental health among university students Regarding pandemic risk groups, previous cross-cultural pre-pandemic surveys have shown high prevalence rates of anxiety and depression among university students across countries. Therefore, the WHO's concerns about the psychological consequences of the COVID-19 pandemic on mental health and well-being might affect university students as a population group as well. The results obtained from this sample of university students who study in Egypt or Germany during the first lockdown period confirm these concerns. In particular, the results confirm previous pre-pandemic results about mental health of university students and they seem to confirm the concerns of the WHO regarding mental health and threat perception during the current pandemic. The mean state anxiety score (assessed with standardized questionnaires including the Spielberger Trait-State Anxiety Inventory, STAI) was significantly above the cut off score that, according to the literature, discriminate high from low anxious subjects. In addition, state anxiety scores were significantly higher in woman than man. Moreover, 51.82% (n = 114) of the students had sum scores greater than the cut off (> 3), and 19.09% (n = 42) had a sum score of 3 (cut off). Only 26.82% (n = 59) of the sample scored below the PHQ-2 cut off score (< 3), and only 2.27% (n = 5) did report to not suffer from loss of interest or pleasure in doing things (PHQ-2 item 1) or from feeling down, depressed or hopeless during the last two weeks (PHQ-2 item 1), and self-reported depressive symptom did not differ among students studying in Egypt or Germany or in woman or men (see Fig. 3 for an overview on state anxiety and depressive symptoms). Thus, in total, 51.82% and 19.09% of the final student sample (n = 220) reported depressive symptoms at and above the cut off score for depressive symptoms, thus feeling depressed or hopeless and reporting a loss of interest and pleasure in the items of the PHQ-2 questionnaire during most of the days of the last 2 weeks of the COVID-19 pandemic. Prevalence rates from previous surveys among university students reported a prevalence of anxiety symptoms or depressive symptoms above 35% among university students before the pandemic (e.g., for depression or anxiety). A recent online study, including N = 185 university students studying in Germany found that 36.6% of the university students (women and men) report experiencing depressive symptoms, 41.83% (women and men) reported experiencing high levels of state anxiety, and all students reported experiencing stress due to excessive demands and uncertainty in finances, job, or social relationships. These prevalence rates have actually been found in cohort studies including university students all over the globe, irrespective of culture before the outbreak of the pandemic. In relation to these pre-pandemic prevalence rates, the prevalence of state anxiety and of depressive symptoms in the current sample seem to have more than doubled during the pandemic time period. The scores for state anxiety need to be seen in relation to the results obtained for trait anxiety. As mentioned above, trait anxiety scores were even higher in those students who dropped-out, however state anxiety scores did not differ across students who completed the survey and those who did not. Students with high state anxiety during the pandemic may be at special risk of suffering from anxiety proneness in the long run. Therefore, surveys among university students should be continued to further explore the development of anxiety and particularly also of depressive symptoms during the current pandemic as well as the comorbidity of anxiety with depressive symptoms as a consequence of the COVID-19 pandemic. Very recent surveys among university students from Greece (Europe) and the United States conducted in a similar time period (during the first lockdowns in these countries) report similar high percentage numbers of anxiety, depression and mental health burdens). Given that the STAI asks for feelings of stress, worry, discomfort, experienced on a day to day basis one could expect changes in other psychological domains as well (see below). Threat perception and perceived difficulties in emotion perception Being asked about their feelings during the pandemic, 55% of the students reported unpleasantness and 37.55% of the students rated to be in loss of control of the situation, and about 50% reported moderate to high physiological arousal. Moreover, university students reported a mix of discrete emotions in response to the pandemic. In particular, there was a significant loss of happiness, and a change in feelings of surprise, disgust and anger. In line with this, as illustrated in Fig. 5, linguistic analysis of the participants' answers to the questions \\\"I feel...\\\" also suggest a negativity bias in the linguistic descriptions of the students' feelings: In summary, there was more intense use of negative than positive words to describe one's feelings in response to the pandemic. Thus, feelings of threat and negative emotions were also reflected in the self-generated linguistic answers of the students, supporting a general increase in anxiety during the first period of the COVID-19 pandemic among university students. Similarly, and in line with the scores obtained from the depression screening instrument (PHQ-2), linguistic analysis of the questions \\\"I feel...\\\" revealed a high percentage of words such as feeling depressed, down or hopeless (see Fig. 6). Thus, anxiety and depression related words were amongst the most frequently used words when participants were asked to describe in their own words, how the current COVID-19 pandemic situation makes them feel. The study sample also reported to have perceived difficulties in emotion perception during the pandemic. Using the three subscales of the Toronto Alexithymia Scale (TAS-20), the participants were instructed to rate whether they experience difficulties in emotion perception relative to before the pandemic situation. Especially difficulties in identifying and describing feelings were reported. Moreover, the sum scores of the TAS-20 were significantly correlated with the students' anxiety scores and the intensity of self-reported depressive symptoms (see Table 2). Taken together, these results are of particular interest in light of discussions which mental health interventions might help university students to cope with the threat provoked by the pandemic situation. Given that previous research has shown that high scores on the TAS-20 promote psychopathology, the reports of the students about them perceiving difficulties in identifying one's feelings in response to the pandemic situation relative to before the pandemic outbreak should be taken seriously and investigated in further studies in larger student cohorts. Worries about health and health behaviour during the COVID-19 pandemic Moreover, the university students' worries about health should be taken seriously. Chronic worrying is a sign of chronic distress and constitutes a risk factor of later development of general anxiety disorder. In the current study, 65.5% of the final student sample (n = 220) reported being worried about their mental health and 71.4% reported to worry about their physical health more often than before the pandemic. The majority of the student sample did, however, not report to pay more attention to bodily sensations or symptoms (taste, smell, cardiovascular, respiration\\/ breathing) than before the pandemic. However, worries about mental and physical health were accompanied by perceived changes in health behaviour. The percentage of \\\"yes\\\" and \\\"no\\\"-answers differed significantly for changes in health behaviour related to eating and physical activity behaviour since the outbreak of the pandemic. We did not ask the students for their eating behaviour or their physical activity level before the pandemic. Thus, the questions asking for perceived changes during relative to before the pandemic might have the potential of a memory bias. Nevertheless, pre-pandemic surveys report that up to 30% of university students do not exercise at a regular basis and do not meet the WHO's weekly or daily physical activity recommendations (for an overview see). The present results suggest a reduction in physical activity during the pandemic and physical inactivity and sedentarism are among the major risk factors promoting negative lifestyle-related diseases in the long run. Learning behaviour during the COVID-19 pandemic The pandemic might have negative effects on student's teaching and learning behaviour. In the present sample of university students, difficulties in teaching and learning were reported by the majority of students. One interpretation of these results is, that pandemic situations such as the current COVID-19 pandemic are characterized by uncertainty, fear, and threat, i.e., factors that are known to impact self-regulation. Previous research has shown that self-regulation is negatively related with threat perceptionbecause responding to fear, anxiety and to threatening events depletes top-down control and self-regulatory resourcesthat are also required for academic performance. In line with this, students reported having difficulties in focusing and concentrating on the teaching content during the current COVID-19 pandemic situation (see Fig. 4b). Self-learning formats such as e-learning may accentuate these effects. Self-concept and personality of university students, and machine learning When asked to describe themselves with a modified version of the TST asking for descriptions of the students' \\\"actual self\\\", positive word use outweighed negative word use. When the student sample was considered as a whole, linguistic analysis of word use (see Fig. 5) supported a clear bias towards positivity that also accords with previous results that seeing yourself in a positive light correlates with positive self-descriptions and preferential processing of positive words. Although this result must be seen in relation to a general positivity bias in written and spoken language (most languages having more positive than negative words, the analysis of word use suggests that the pandemic situation at the time of the survey did not provoke a threat to the self-concept of this university student sample and this, although linguistic analysis of the answers to the prompt that asked for feelings during the pandemic (see also Fig. 5) revealed a negativity bias as immediate negative responses to the pandemic situation in line with the results observed for the survey items asking for threat perception. Symptoms of state anxiety and current depressive symptoms may therefore reflect temporary changes of the university students to the pandemic situation that however occur immediately in response to the pandemic lockdown. Psychological theories agree that individual factors such as one's personality are correlated with subjective experience, well-being, mental health, and behaviour, e.g., [63, 64]. In line with this, analyses showed correlations between the Big Five (BFI-40) personality traits and the university students' self-reported symptoms of anxiety, depression and their perceived difficulties in emotion perception. Statistically, correlation analysis, linear regression analysis, multivariate structural equation models, mediator analysis, or moderator analysis may all be feasible statistical methods to describe the relationship between psychological variables. However, in the present study we attempted to apply supervised machine learning algorithms that are built on regression models to further explore whether personality traits were not only correlated with mental health variables but could be predicted from the self-reported subjective experience of the participants obtained from this survey's multimethod assessment. The observed results are promising despite the relatively small datasets used for training and prediction. The algorithms provided relatively accurate models for the prediction of personality traits from self-report data. As illustrated in Table 3, neuroticism as one of the big five personality traits (shown to be related to mental ill health) and in the present study sample significantly correlated with both, self-reported anxiety and depressive symptoms (see Table 2) could best be predicted by changes in current anxiety (threat perception, difference scores state vs trait anxiety), by the students' self-reported trait and state anxiety, by their self-reported perceived difficulties in emotion perception (describing one's feelings reported on the TAS-20), by self-reported changes in physical health behaviour (eating) and by self-reported difficulties in social distancing. Very recent results from surveys investigating the role of personality factors during the current COVID-19 pandemic also found that people's self-reported psychological perceptions of and reactions towards the pandemic also depend on stable personality traits including the Big Five (for an overview). Interestingly, there is also evidence that expression on personality traits such as the Big Five can change in conjunction with mental ill health. Our results and these recent results suggest that future studies exploring the psychological consequences of the COVID pandemic should include the assessment of personality traits in their anamnestic exploration of mental health and self-reported experience. Limitations The present study adds to the evidence reported in the literature about the negative consequences of the current COVID-19 pandemic on mental health and well-being of university students. By using a mix of self-report measures it allows detailed insight into the subjective experiences associated with the pandemic in this population group in the psychological domains of mental health, health behaviour change and learning. However, some limitations already discussed in the sections above should be stressed. First, there was a high drop-out whose percentage was within the upper range of the expected drop-out rates for online surveys (20-50%). Although drop-outs were statistically assessed and compared to the final sample as far as appropriate, suggesting no bias by age or gender or the student's personality, the drop-out reduced the final sample size reducing the power of the study. Thus, further data is required to demonstrate the generalizability of the present observations and to further explore possible cultural differences. In the present study sample, the reported significant differences between gender and students studying in Egypt or Germany might be tentative due to the small study samples. Power calculations suggest an ideal sample size of about N = 271 (90% confidence) or N = 385 (95% confidence) participants (margin of error of 5%). Although this sample size was reached in the beginning, it was reduced by the successive dropout across the blocks of survey items. Second, statistics revealed significant results for the quantitative measures, however, the results of the linguistic tasks (self-concept and feeling prompts) could be reported only descriptively. The LIWC software was used for linguistic analysis. This allowed word categorization with high accuracy and validityproviding interesting insight that otherwise might have gone unnoticed and confirmed the results obtained from quantitative measures. Third, due to the small sample size the machine learning approach is exploratory and challenged by limitations. While machine learning tools have already been applied in many domains of psychology (e.g., in the domain of Affective Computing and Health Psychology), their use is still relatively under investigated in studies using psychology data obtained from multimethod approaches as the current one. Existing studies using machine learning for analyzing personality- and behaviour-related data, mainly target personality prediction from larger datasets (e.g., [68]). In the present study, we followed guidelines and recommendations from existing machine learning studies discussing possible solutions for application of machine learning tools with small sample sizes (see for an overview), using sample size of about 200 and support vector machines (SVM similar to SVR used in our study) for estimation of depressive symptoms, for personality trait and perceived stress prediction based on sample sizes ranging from 150 to 250 participants, as in the present study. In line with these previous studies applying machine learning tools to smaller sample sizes, we applied machine learning to a mix of measures that captured subjective experience in relation to the current COVID-19 pandemic situation in line with the recommendations from psychologicallydriven computational approaches that suggest to include trait and state measures for prediction. Nevertheless, the present approach is exploratory and application of machine learning to small sample sizes need to be critically discussed, e.g., for a detailed discussion see, as it can lead to overfitting or overestimation. One recommendation to avoid such problems with small sample sizes is to use nested cross validation and control feature-to-sample ratio. It will be interesting to follow-up the present ML results in future COVID-19 survey studies and use additional data collected during the course of the pandemic for validation and training in order to confirm the results from ML in hopefully larger samples, supporting the combination of machine learning and classical data analytics in the domain of psychology. Conclusion This survey investigated the subjective experience of university students studying in Egypt or Germany during the COVID-19 pandemic in May 2020, i.e., in the time period after the first pandemic lockdown in the countries. Perceived changes in all psychological domains including state anxiety, depressive symptoms, threat perception, emotion perception, worries about health and behaviour (health, social distancing, and learning) were reported in the majority of students taking part in the survey. Recent COVID-10 surveys report similar high prevalence rates among university students across the globe. Although the results of this survey are tentative, the multimethod approach of this survey, using multiple scales, descriptive, correlational, and linguistic analysis, provides a valuable contribution to previously published COVID studies. Moreover, the approach of combining descriptive analysis with machine learning should and could be followed-up in larger samples during the second period of the current pandemic. Crucially, despite the small sample size, the present results of self-reported anxiety and depressive symptoms among university students, that also seem to be supported by recent surveys including university students from other countriesshould be taken serious as they suggest that there is an urgent need to develop interventions that help prevent mental health among university students in order to avoid negative consequences in health and learning behaviour in response to the pandemic and provide health care to those students who might be at special risk of mental ill health.\",\"577240552\":\"Sexual satisfaction (i.e., \\\"an affective response arising from one's subjective evaluation of the positive and negative dimensions associated with one's sexual relationship\\\";, p. 268) is considered for relationships and is closely associated with relationship satisfaction  and stability  as well as individual wellbeing. While many variables have been investigated as potential predictors of sexual satisfaction , previous research has not compared the different predictors to understand which variables are the most, or least, likely to contribute to sexual satisfaction. Comparing the relative importance of different factors is important to understand which predictors are the most likely to change the outcome and thus the most useful for potential interventions. Therefore, the aim of the present study is to add to the literature by comparing a large number of potential factors on how well they each predict sexual satisfaction. The biopsychosocial model suggests that there are biological, psychological, and sociocultural factors that affect individual's functioning. Determinants of sexual satisfaction can also be examined from this perspective. However, because sometimes it is difficult to clearly categorize variables into these three categories (e.g., to what extent is one's gender a biological factor and to what extent is it a social one), we have focused on describing the previous literature in terms of individual and relational factors. This division is in line with other previous research examining relationship processes using machine learning. Individual factors Results for demographic variables such as gender, age, race, socioeconomic status, and education generally show mixed findings with some studies finding one group has a higher satisfaction than another group with other studies either finding no significant difference or a difference in the opposite direction. For example, several studies have found no significant difference between men and women in their level of sexual satisfaction  whereas others found that men were consistently higher in sexual satisfaction compared to women. Many other individual factors such as mental and physical health , contraception , attachment , self-esteem , body image , trait mindfulness , and sexual trauma  have also been associated with sexual satisfaction. Relational factors Relational variables also play a role in sexual satisfaction. People in romantic relationships are generally more satisfied sexually than single people ; in long-term relationships, partners are more likely to know each other's likes and dislikes making it easier to satisfy one's partner, and especially women are less likely to orgasm in casual encounters compared to established relationships. Furthermore, relationship and sexual satisfaction are closely linked with the association generally thought to be bidirectional in nature. Love, commitment, intimacy, and relationship stability have all also been associated with greater sexual satisfaction. Therefore, we would expect relational variables such as relationship satisfaction, dyadic sexual desire, and love to be important for sexual satisfaction. In romantic relationships, individuals' sexual satisfaction can be associated with both their own as well as their partners' variables. For example, if one partner has issues with sexual functioning, this is likely to be associated with both the individual's own sexual satisfaction as well as that of their partner's. In fact, a recent study showed that women's low desire and difficulty having an orgasm and men's erectile dysfunction predicted lower sexual satisfaction for both partners. Therefore, in addition to examining how self-reported individual and relational variables predict one's own sexual satisfaction (actor effects), we also examine whether these variables predict one's partner's sexual satisfaction (partner effects). Previous research has shown that partner variables tend to explain less variance and be less important for relationship satisfaction and commitment  and sexual desire  but these have not been systematically examined for sexual satisfaction. Sexual behaviors in relationships are also likely to predict both partners' sexual satisfaction. For example, one member of the dyad may really enjoy oral sex but if their partner does not wish to engage in the sexual act this can predict both partners' sexual satisfaction regardless of whether oral sex occurs or not. Therefore, in addition to examining individuals' own variables, we also included data from a sample of couples where we examined both actor and partner effects. Using machine learning to predict sexual satisfaction Traditional linear regression models are ill-equipped to examine many predictors simultaneously as they suffer from issues of multicollinearity, cancellation, and suppression effects. They are highly sensitive to the choice of control variables and thus often do not provide meaningful estimates of the effect of predictor variables on the outcome essentially leaving the models uninterpretable. Furthermore, linear models rely on parametric assumptions and often assume that there is a linear (or prespecified non-linear) relationship between the predictors and the outcome. This is problematic because complex real-world phenomena may vary according to both unknown and arbitrary functional relationships. Because of the problems with traditional methods, a move toward more predictive modeling with machine learning has been advocated by Yarkoni and Westfall (2017). A few studies in relationship science to date have used machine learning to predict relationship outcomes from self-report variables (; 2020;). Most notably, Joel et al. (2020) used a random forest algorithm (i.e., a form of decision tree that can estimate a large number of predictors simultaneously and can handle highly non-linear relationships and complex interactions without overfitting to the data) to analyze data across 43 dyadic samples of over 11,000 couples to examine individual and relational predictors of relationship satisfaction and commitment. However, the study was only able to establish which factors contributed \\\"meaningfully\\\" to the outcome and in howmany samples. This does not necessarily mean that all the variables that contributed meaningfully to the outcome are equally important. For example, if two variables were both classified as meaningful in 90% of the samples but one variable explained 30% of the variance and another only 5%, the study would have classified them as equally robust, but this does not help us understand the relative predictive power of each, which is the focus of the present study. There has been a great deal of development recently on making machine learning algorithms explainable. This work is particularly interesting because it allows social scientists to combine the use of powerful machine learning algorithms and state-of-the-art model explainability1 tools that can provide accurate predictions, an understanding of which factors the model uses to predict the outcome, and the size and direction of the effect. Given social scientists are usually interested in explaining and understanding phenomena, the latter is of particular importance. In the present study, we took advantage of this new development in machine learning by using Shapley values  to interrogate the results from the random forest algorithm. The Shapley value approach involves systematically evaluating changes in model performance in response to including or excluding the influence from different combinations of predictors. It provides an estimate of the effect size, direction of the effect, and evaluates any interactions in the predictor variables. The current research Except for Laumann et al. (2006), previous research has focused only on examining a small number of predictors rather than comparing different factors. Laumann et al. (2006) examined several correlates (demographic variables, physical and psychological health, sexual functioning, relationship characteristics, sexual behaviors, and sexual attitudes) of sexual satisfaction and provides a comprehensive overview of sexual well-being across many countries and participants. However, because of the issues with traditional linear models discussed above, the results of the study are difficult to interpret, a comparison of the relative importance between variables is not possible, and the model is limited to estimating only linear effects. However, it is possible that the association between some (or all) of the variables and sexual satisfaction is non-linear and thus might appear nonsignificant despite a high non-linear correlation. Therefore, to address these gaps in the literature, the primary aims of the present study were to determine (a) how much variance in sexual satisfaction can we explain, (b) which variables contribute the most, and least, amount of variance in sexual satisfaction, (c) whether these variables differ for men and for women, and (d) whether partner variables explain additional variance beyond actor effects. We used a random forest algorithm with Shapley values to evaluate individual and relational predictors of sexual satisfaction across two samples (one individual and one dyadic sample). Method Sample 1 Participants and procedure. The data were collected as part of a larger cross-sectional study. Participants were recruited through mTurk and were asked to complete an online survey and paid 30 cents for the task. Recruitment was also conducted through social networking sites (e.g., Facebook, Twitter), email list servs, and targeted recruitment for sexual minority participants on online forums. Participants recruited from these mediums were entered into a draw to win one of four US$40 Amazon gift cards. Participants were eligible for the study if they were over 18 years of age and had experience with at least one romantic relationship. Ethical approval was obtained from the University of Kentucky institutional review board and all participants received a written informed consent at the start of the baseline survey. A total of 1097 participants consented to participate (See supplemental Table 1 for full demographics). Participants who had not completed the study (n = 198) or were missing the outcome variable (n = 8) were removed from the analyses and any remaining missing data were imputed2. The final sample consisted of 891 participants: 557 (62.5%) cisgender women, 279 (31.3%) cis-gender men, and 25 (2.8%) genderqueer. Around half of the participants were straight (n = 483; 53.9%), 189 (21.2%) identified as bisexual, 101 (11.3%) gay, and 60 (6.7%) lesbian. Majority of the participants were white (88.4%), married or cohabiting (62.7%), had no children (75.5%), had at least some level of college (95.8%), and did not identify with any religion (54.5%). The average age of the participants was 32.70 years (SD = 9.63) and the average relationship length for those who were in a relationship was 6.21 (SD = 7.12). Measures Because the variables included in the study were selected for their relevance to sexual satisfaction, we included all measures as predictor variables that were collected in the study, a total of 93 variables included recoded categorical variables into dummy variables. The full list of the variables can be found in the codebook on the OSF project page. These included demographic questions on age, race\\/ethnicity, gender, partner's gender, sexual orientation, relationship status, children, country, religion, and education. Participants also completed questions around their contraceptive use (their or their partner's), sexual behaviors (e.g., masturbation, oral sex, intercourse participants had engaged in either in the past week or ever in the current or most recent relationship), desire discrepancy, sex regularly (at least once a week), communicate regularly about sex (at least once a week), whether they wanted sex or communication more or less than they were currently engaging in, and mental and physical health (\\\"Would you say in general your mental\\/ physical health is,\\\" scored from 1 = excellent to 5 = poor). The following constructs were assessed using previously validated questionnaires: Sexual satisfaction was assessed using the General Measure of Sexual Satisfaction Scale (GMSEX; a = .95;). The GMSEX is a 5-item measure used to assess satisfaction with the sexual relationship. Relationship satisfaction was assessed using the General Measure of Relationship Satisfaction (GMREL; a = .97;). Both GMREL and GMSEX are scored on a 7-point semantic differential scale and higher scores are indicative of greater sexual satisfaction. Sexual desire was assessed using the Sexual Desire Inventory (SDI;). The scale was used as both a single scale (13 items) as well as divided into dyadic (nine items; a = .77) and solitary desire (four items; a = .91) and assesses an individual's interest sexual activity over the past month with higher scores being indicative of higher sexual desire. Sexual desire was also assessed using the Halbert Index for Sexual Desire (HISD; a = .95;) which measures sexual desire using 25 items with higher scores being indicative of higher sexual desire. Dispositional mindfulness was measured using the Five Facet Mindfulness Questionnaire--Short form (FFMQ-SF;). The scale comprises of a total of 24 items that are divided into five subscales: being non-reactive (a = .80), observant (a = .74), acting with awareness (a = .85), describing feelings (a = .86), and non-judgmental attitude (a = .83). The items are scored on a 5-point Likert scale with higher scores indicating participants' agreement with the statement. Attitudes Toward Sexuality Scale (ATSS; a = .84;) was used to assess participants' attitudes toward sexuality. The scale comprises of 13 items that are measured on a 5-point Likert scale with higher scores indicating the participant is more liberal, lower more conservative. The Perception of Love and Sex Scale (PLSS;) measures one's perception of love and sex comprising of four subscales: love is most important (six items; a = .76), sex demonstrates love (four items; a = .79), love comes before sex (four items; a = .81), and sex is declining (three items; a = .67). The items are measured on a 5-point Likert scale with higher scores indicating lower agreement. Attachment style was assessed using the Experience in Close Relationships Scale-Short form (ECR-S;). The ECR-S consists of two 6item Likert scales: one for anxiety (a = .75) and one for avoidance (a = .80). Higher scores indicate higher levels of insecure attachment. Sample 2 Participants and procedure. The second sample used a combined dataset across two studies on mixed-sex couples. The couples for both studies were recruited through various list servs, websites, and social media (e.g., Facebook, Twitter). Participants who were 18 years of age or older, in a mixed-sex relationship for a minimum of 3 years, currently living with that partner, with no children under the age of one, and not pregnant at the time, met the inclusion criteria and were directed to provide their partner's email address. For the second dataset, one member of the couple had to also be bisexual to be eligible due to a broader aim of that study to examine the dynamics of bierasure in mixed-sex relationships. The respondent first completed the online survey in which they provided an email address for their partner who was then contacted to complete the survey. Ethical approval was obtained from the University of Kentucky institutional review board and all participants received a written informed consent at the start of the baseline survey. Participants who had not completed the study (n = 14)3 or were missing the outcome variable (n = 6) were removed from the analyses. The final sample consisted of 955 participants (377 intact mixed-sex couples and 201 individuals4); 538 (56.3%) cis-gender women, 405 (42.4%) cis-gender men, and 12 (1.3%) genderqueer (See in supplemental Table 1 for full demographics). The participants were either straight (n = 534; 55.9%) or bisexual (n = 397; 41.3%). Majority of the participants were white (87.4%), married (60.4%), had at least some level of college (90.8%), and did not identify with any religion (51.9%). The average age of the participants was 30.50 years (SD = 8.01) and the average relationship length was 7.41 years (SD = 6.22). Measures Sample 2 had a total of 69 variables. The full list of the variables including the dummy coding of the categorical variables can be found in the codebook on the OSF project page. Most of the variables were the same in Sample 2 as in Sample 1. The following questionnaires were not available in the sample: attachment styles (ECR-S), attitudes toward sexuality (ATSS), trait mindfulness (FFQM-SF), and perception of love and sex (PLSS). The study had an additional scale measuring romantic love, the Romantic Love Scale (a = .89;). The scale consists of 13 items that are meant to measure affiliative and dependent need, a predisposition to help, and orientation of exclusiveness and absorption. The scale is scored on a 9-point scale with higher scores indicating higher romantic love. For dyadic analyses, both dyad members' scores were included as predictors. Data analysis Data preparation. All categorical variables were dummy coded (0 and 1) with each option included in the models (e.g., ethnicity was coded into \\\"Asian,\\\" \\\"black,\\\" \\\"white,\\\" and \\\"multiracial\\\"). Less than 0.1% of the data were missing, and any missing data points were imputed using the scikit-learn package Iterative Imputer  with a Bayesian ridge estimator. Analyses. The data from individuals and dyads (in which both members of the couple had responded to the questionnaire) were analyzed for all participants, just men, and just women5. We ran the dyadic models with actor effects only, partner effects only, and both actor and partner effects to determine where the source of the variance was coming from. The code for the analysis was written in Python 3.7 and can be found on the OSF project page: https:\\/\\/osf.io\\/ehzkm\\/. We analyzed each dataset using a random forest regressor. A random forest is a type of decision tree that trains on bootstrapped sub-samples of the data to avoid overfitting. Because it uses bootstrapping to estimate a subset of predictors at once, it does not require large sample sizes , and is relatively robust to multicollinearity in the predictor variables. Each model is tested on a randomly selected out of bag sample (i.e., a sample that the model has not seen before). The use of this out of bag sample is what helps to mitigate overfitting during the training process, and this is what makes it a convenient choice, particularly when the number of variables is high in relation to the sample size. By taking a subset and out of bag sample testing thousands of times (i.e., by bootstrapping), the random forest can derive the best \\\"average\\\" decision tree for the training data. The tree can model highly non-linear relationships, and therefore represents a significantly more flexible model than a linear regressor. While there is some concern that in cases where two predictors are similar and equally predictive, random forests can arbitrarily latch onto one predictor variable over the other , we found our results to be robust to the removal of highly correlated variables and random initializations. There are many machine learning algorithms to choose from, but we chose to use random forests because they have been shown to perform well with default settings, without the need for extensive hyperparameter tuning. Hyperparameters are settings which determine the algorithm's behavior, and their selection generally requires the use of a separate dataset, thus reducing the quantity of data available for testing. We used the default \\\"scikit learn\\\" random forest regressor with k-fold crossvalidation. A ten-fold cross-validation schemewas used to train and test the model. This means the total dataset is randomly split into 10 equally sized folds. The model is trained on nine out of 10-folds, tested on the 10th, and the test fold performance is recorded. This is repeated until all 10-folds have been used as a test set. The average performance, as well as the standard error across the 10-folds, provides an estimate of model performance on unseen data. The metrics for test data model performance are the mean squared error (which is the averaged squared difference between the prediction and the observed value), the R2, and the variance explained. The last model to be trained was then saved and interpreted using the \\\"SHapley Additive exPlanations\\\" package (SHAP) . The SHAP package is based on game theory  and can be used for model explanation; the framework conceives of predictors as collaborating agents seeking to maximize a common goal (i.e., the regressor performance). The approach involves systematically evaluating changes in model performance in response to including or restricting the influence from different combinations of predictors. Traditional approaches (e.g., using the coefficients from a linear model, or importances from a random forest) are unreliable and \\\"inconsistent,\\\" and the Shapley approach has been shown to provide interpretations which are coherent with human intuition. The SHAP TreeExplainer function provides estimations of the per-participant, per-predictor impact on model output, as well as the average predictor impacts. It produces estimates that show how much impact and in which direction each variable, and each interaction, has on the model outcome, for each individual (i.e., it provides per-individual, perpredictor estimations of impact). For the analysis, the default settings of the SHAP package TreeExplainer were used on the entire dataset. Results All scale items were summed together prior to data acquisition in Sample 1 whereas they were averaged in Sample 2. The mean for sexual satisfaction in Sample 1 was 36.11 (SD = 8.84, range 5-45; women:M = 36.51, SD = 8.67; men:M = 35.71, SD = 8.74). In Sample 2, the mean was 7.30 (SD = 1.65, range 1-9; women:M = 7.23, SD = 1.65; men:M = 7.44, SD = 1.69). In Sample 1, we used a total of 94 variables and in Sample 2, we used 71 variables (142 when partner effects were considered) to predict sexual satisfaction. In Sample 2, we performed the analyses first at the individual level (N = 955) and then at the dyadic level (N = 377). A full list of variables included in each model with descriptions of the variables as well as all results can be found on the OSF project page: https:\\/\\/osf.io\\/ ehzkm\\/. Total variance explained6 The results for prediction accuracy can be found in Table 1 including the percentage of variance explained, the mean squared error (MSE), and R2 for each sample. Overall, the model predicted 62.4% of variance in Sample 1 and 55.7% of variance in Sample 2 for all participants. The model was somewhat better at predicting men's sexual satisfaction (62.2% in Sample 1 and 55.2% in Sample 2) compared to women's sexual satisfaction (56.0% in Sample 1, 51.4% in Sample 2). Partner effects explained little additional variance in the total models (6.4%) but did not explain additional variance in sexual satisfaction for men or women. To determine whether we could predict actor's sexual satisfaction from variables that they did not report on themselves to avoid shared method variance, we also ran a model in which only partner effects were included as predictors. Partner effects alone could predict 27.6% of the variance in actor's sexual satisfaction with partner's sexual and relationship satisfaction being the highest predictors. Furthermore, because sexual and relationship satisfaction have been closely linked in a large body of literature, we used related measures for the variables (GMSEX and GMREL), and relationship satisfaction was by far the most important predictor, we also reran the models for all participants without relationship satisfaction. When relationship satisfaction was removed from the models, the other predictors explained 51.4% of the variance in Sample 1 and 28.7% and 38.2% of the variance in Sample 2 with actor effects only and with actor and partner effects, respectively. Most predictive variables7 In most of the models, the predictive importance of the variables decreased rapidly beyond a small set of primary predictors. The rest of the predictors contributed only a small amount of variance into the model individually. Therefore, we only present the top variables for each model in the figures (see Figures 1-3 for the results). In the figures, the left side provides the mean effect of each variable on the model outcome. The right side of the figure provides the estimates for each individual participant, which allows for estimation of the effects for each individual. Red indicates a higher value of the predictor variable and blue indicates a lower value. For example, red is equal to 1 and blue is equal to 0 for binary variables. It is important to note that Shapley values project the results from the random forest into a pseudo-linear space to aid interpretation of the results. This does not, however, mean that the relationships are linear. It is possible to interrogate each variable individually against the outcome to identify any non-linearity that the model used to predict the outcome. For the sake of interpretability, we have discussed the results in accordance with a typical linear interpretation. The two samples differed somewhat in the predictor variables that were available and therefore the results for the most important predictors vary somewhat across the two samples. Consistently across the two samples (See Figure 1 for results for Sample 1 and Figure 2 for individual results for Sample 2), relationship satisfaction was the strongest predictor of sexual satisfaction. In Sample 1, for example, relationship satisfaction contributed to a three-point increase in sexual satisfaction on average. Participants who scored high in relationship satisfaction scored up to five-points higher in sexual satisfaction compared to those with average relationship satisfaction. In contrast, individuals who scored low in relationship satisfaction, scored up to 15-points lower in sexual satisfaction compared to those with average level of relationship satisfaction. Both dyadic and solitary desire also contributed to sexual satisfaction in both samples. In Sample 1, all subscales from the perception of love and sex scale were in the top-10 predictors of sexual satisfaction. The results showed that when participants were still having sex regularly (or it was not declining) they reported higher sexual satisfaction. Participants who believed that sex demonstrates love, highly valued sex (love was not most important) and did not believe that love came before sex also reported higher sexual satisfaction. Interestingly communication was only among the top-10 predictors for men: men who communicated regularly (at least weekly) and felt they communicated as much as they wanted to reported higher levels of sexual satisfaction. Neither communication nor perception of love and sex were measured in Sample 2. In Sample 2, romantic love scale contributed to sexual satisfaction; participants who reported greater romantic love toward their partner reported higher sexual satisfaction. Several sexual behaviors including receiving or giving oral sex or masturbating with partner predicted higher levels of sexual satisfaction in Sample 2. Relationship length was in the top-10 variables in Sample 2 (12th in Sample 1) with participants who had been in a relationship for longer reporting lower levels of sexual satisfaction. Men's reports of poorer physical health predicted lower levels of sexual satisfaction. In the dyadic analyses (Figure 3), both actor and partner variables were in top-10 with partner's sexual satisfaction, romantic love, dyadic desire, and relationship satisfaction all contributing to the actor's sexual satisfaction. For women, partner's sexual satisfaction was almost as predictive of women's sexual satisfaction than their own relationship satisfaction. For men, this association was much smaller. Discussion Our results showed that we could predict between 48 and 62% of the variance in sexual satisfaction using a random forest algorithm, up to two to three times more than previous studies even after deleting relationship satisfaction from the model. The algorithm is also explainable because it does not suffer from suppression and cancellation effects or multicollinearity. The results show that using machine learning can help move psychological research into a new era of highly predictive and accurate models that generalize better to the population and have a higher utility in practice. The strongest predictors Because of the importance of sexual satisfaction on relationship quality  and overall well-being , understanding factors that are the most, and the least, strongly associated with sexual satisfaction is important. This can enable researchers and practitioners to target individuals who may be at a particular risk of poor sexual satisfaction and helps to address factors that are the most likely to induce changes in sexual satisfaction while ignoring those that are the least likely to produce change. Thus, we added to the literature by examining which factors were the most, and least, predictive of sexual satisfaction in two samples. Several variables that have previously been identified as important predictors of sexual satisfaction were included in the top-10 predictors: relationship satisfaction , dyadic desire , romantic love , sexual communication , and perception of love and sex. Importantly, when relationship satisfaction was low, it had up to three times higher impact on the model outcome compared to when relationship satisfaction was high. Furthermore, participants in Sample 1 who viewed sex as an important part of their relationship and those who had sex regularly also had higher sexual satisfaction compared to participants who placed less importance on sex and more on love and had sex less frequently. Similarly, participants who reported a higher frequency of more varied sexual behaviors such as giving and receiving oral sex and mutual masturbation in Sample 2 reported higher levels of sexual satisfaction. These results suggest that frequency and value of sex as well as a more varied sexual repertoire in relationships are important predictors of sexual satisfaction. More varied sexual repertoire is also likely to lead to more satisfying sexual experiences, especially for women given that women have a higher likelihood of orgasm from clitoral stimulation than from intercourse. These results confirm earlier findings using traditional statistical models. Gender was not an important predictor of sexual satisfaction suggesting that men and women overall had similar levels of sexual satisfaction in both samples which is consistent with some studies  and inconsistent with others. Men's sexual satisfaction was overall more predictable than women's. This may be because women's sexuality is thought to be more complex than men's. There were also some notable differences in the top-10 predictors for men and women. Attachment avoidance was only in the top-10 predictors for women's sexual satisfaction (18th for men changing the outcome very little). Women who were higher in attachment avoidance reported lower sexual satisfaction compared to women lower in attachment avoidance. Attachment avoidance is associated with fear of closeness and intimacy, which tend to be more strongly tied to sexuality for women than men , which may explain why attachment avoidance was particularly important for women. Consistent with previous studies using both traditional analyses  and machine learning , including partner effects added little additional variance. However, both actor and partner variables were among the top-10 most important predictors. Partner effects alone could also explain around half as much variance as only actor effects. Important partner variables included partner's sexual satisfaction, romantic love, relationship satisfaction, and dyadic desire. Interestingly, for women, their male partner's sexual satisfaction was just as important a predictor for their own sexual satisfaction than their relationship satisfaction. This is consistent with several studies finding that women partnered with men tend to answer questions of sexual satisfaction relative to their partner's satisfaction as much as their own  and may be due to there being a societal expectation on women to prioritize men's pleasure. For men, their female partner's sexual satisfaction only accounted for about third as much change in sexual satisfaction compared to their own relationship satisfaction. These findings suggest that while we may be able to predict actor's sexual satisfaction relatively well using only their own variables, accounting for both partners' variables can provide important additional insights. The present study also provided an important addition to the literature by evaluating which factors were unimportant for sexual satisfaction. Many of the variables that have previously been associated with sexual satisfaction in traditional analyses were less important compared to other predictors. These included variables such as gender, sexual orientation, children, religiosity, attitudes toward sexuality, and mental health. This suggests that even though differences in demographic variables may be statistically significant in some studies especially when sample sizes are large , this does not mean that the differences are meaningful. In fact, the present study suggests the opposite; couple's overall relationship and sexual behaviors are more proximal to sexual satisfaction and appear more important than who the person is. Understanding which variables are less related to the outcome is important, so that researchers and practitioners do not waste their time and resources on factors that are less likely to change the outcome. Implications for research, theory, and practice The study has several strengths as well as important implications for research, theory, and practice. We used explainable machine learning and cross-validation in which the model performance is tested on unseen data to avoid overfitting and thus improve the generalizability of the results. The code used in the study is readily available and provides a pipeline to relationship researchers to conduct more robust and predictable science. The results showed that dyadic level variables are the most likely to contribute to sexual satisfaction while individual predictors are less important. Furthermore, examining individuals' perceptions of love and sex , keeping sex as a central element of relationships, and broadening couple's sexual repertoire may enhance their sexual satisfaction. Finally, we expect many of these variables to have a bidirectional association with sexual satisfaction meaning that improving one (e.g., introducing more varied sexual behaviors) may produce a positive change in the other (e.g., enhanced sexual satisfaction) which will in turn improve the first variable (e.g., increased desire to try new things). Limitations and future directions The study also has several limitations that should be considered when interpreting the results. While the study included many predictors that have been associated with sexual satisfaction in previous research, there are other variables that we did not account for, that predict sexual satisfaction (e.g., responsiveness, self-esteem, personality, sociocultural variables). We also only had access to self- and partner-report measures. Thus, the algorithm could only make the predictions based on the variables that were available in the dataset. Therefore, future research should consider a greater number of individual, relational, and societal factors and include behavioral measures to predict sexual satisfaction. We also used data from two relatively large samples including a large subset of couples, the data were convenience samples and limited in their generalizability; most of the participants were white and well-educated and all participants in Sample 2 were in mixed-sex relationships, albeit nearly half the participants were bisexual. We also did not ask participants about any disabilities which may have contributed to their sexual satisfaction. Therefore, future research is needed to examine predictors of sexual satisfaction in a more representative sample. Random forests are a powerful tool that will take advantage of any correlations and interactions in the data, no matter how non-linear, it cannot be used to estimate causality. However, in the absence of a means to reliably estimate causality when examining factors relating to sexual satisfaction, we believe that using a predictive model is perhaps the best option. There are limitations to the Shapley method which have been discussed elsewhere , and the notion that the human-interpretable Shapley model sufficiently explains our model suggests that a simpler model may be adequate to begin with, even if the simpler model is harder to identify. Furthermore, the data were cross-sectional and therefore we could not examine which predictors may account for the most change in sexual satisfaction over time, or indeed whether sexual satisfaction is predictable over time. Joel et al. (2020) found that they could predict little relationship satisfaction longitudinally. Cross-sectional self-report measures are also prone to shared method variance which results in higher correlation among variables collected at the same point in time. We attempted to overcome some of these issues by testing the models without relationship satisfaction given its high correlation with sexual satisfaction and only using partner effects to predict actor's sexual satisfaction. The models with relationship satisfaction excluded were still predictive but predicted less variance. The models with partner effects alone could predict nearly 30% of the variance in actor's sexual satisfaction which is higher than most other previous studies using actor or actor and partner effects. Future longitudinal and behavioral research is needed to understand whether the self-report variables measured in this study are predictive over time or whether behavioral measures could also be predictive. Finally, we examined whether men and women differed in the predictors that were important for their sexual satisfaction and future research could also examine whether the predictors of sexual satisfaction differ by sexual orientation. Conclusion In conclusion, the present study showed that sexual satisfaction is highly predictable with relationship variables (relationship satisfaction, dyadic desire, romantic love, perception of love and desire) explaining the most variance in sexual satisfaction. We used explainable machine learning allowing us to not only estimate variables that may contribute meaningfully, by some undeterminable amount, to the outcome but to also estimate the direction and size of the effect of each predictor variable. As such, the study enables researchers, policymakers, and practitioners to target variables that may be the most likely to improve sexual satisfaction.\",\"577240569\":\"1 | INTRODUCTION Studies have shown that social media (e.g., Facebook and Twitter) provides substantial quantities of autobiographical language and linguistic behavior that are related to users' psychological characteristics. Twitter averages about 330 million monthly active users , with about 500 million daily tweets. People use social media to discuss thoughts, opinions, feelings, and the activities and relationships that constitute their everyday lives. For these reasons, social media platforms are rapidly gaining recognition as research tools for the social sciences. Among studies which show that social media can be used to generate insights and predictions concerning psychological constructs, personality traits have received considerable attention in recent years. Research using social media to study psychological traits has so far focused primarily on the Five-Factor Model of personality (FFM or the Big 5, see Azucar, Marengo, & Settanni, 2018, for an overview). For example, results reveal that extraverts are more likely to mention social words , are more prone to use social media , connect with more friends on social media , and tend to have more Twitter followers  than introverts. Individuals who are high in Openness are more likely to use words related to creativity and imagination , tend to have larger networks , express more \\\"likes,\\\" have more status updates, and engage in more group activities on social media  than individuals low in the trait. Individuals with high Neuroticism use more negative words in their posts , are more prone to use social media as a safe place for self-presentation , have fewer Twitter followers , and are more likely to be addicted to the Internet  than low Neuroticism individuals. Agreeable individuals are relatively likely to express positive emotions in their posts  and to display positive emotions in their profile pictures. Individuals with high Conscientiousness appear to be cautious in their online self-presentation; they tend to post fewer pictures , join fewer groups, and use \\\"likes\\\" less frequently on social media  than low Conscientiousness individuals. Their tweets tend to be more clicked, replied, and retweeted. In addition to looking at links between social media behavior and the Big 5, recent studies also have explored how social media can be used as a tool to predict the Big 5. For example, Park et al. (2015) provide evidence that language-based assessments (Facebook language) agree with self-reports and informant reports of personality. Surprisingly, Facebook Likes are more accurate than peer-ratings of personality, that is, those made by participants' Facebook friends. Another study has shown that Twitter profiles (e.g., followers, following, and listed counts) can accurately predict users' Big 5 traits with a root-mean-squared error below 0.88 on a 1-5 scale. Despite growing interest in research on social media and the Big 5, much less is known about other models of personality. We focus on the morally valued traits, which have been mostly neglected within personality psychology for a long time. This neglect can be dated to the time of Gordon Allport (1897-1967), who claimed in the 1930s that character is merely \\\"personality evaluated, and personality is character devalued\\\" (;, p. 52). Peterson and Seligman (2004) can be seen as a milestone in reviving interest in morally valued traits as a distinct topic of research by proposing the Values-in-Action (VIA) classification of character strengths. Character strengths are a family of morally valued traits that have emerged across cultures and throughout history as important for contributing to a fulfilling life. Character strengths are associated with the good life, or positive life outcomes: studies have shown links between character strengths and positive emotions , academic achievement , healthy behaviors , mindfulness , life satisfaction, and multi-dimensional well-being , and orientation to happiness. Beyond that, character strength interventions have been shown to improve well-being and reduce depressive symptoms and stress (;, Pang & Ruch, 2019b). Supplemental Table S1 outlines the framework of the VIA classification, including an overview of the 24 character strengths. Although there are both conceptual and empirical overlaps between character strengths and the Big 5, such as Agreeableness with kindness and Conscientiousness with perseverance , recent studies also have identified substantial distinctiveness between the two models of personality traits. Park and Peterson (2006) found correlations between VIA strengths and the FFM variables no greater than 0.50 in a group of adolescents. Noftle, Schnitker, and Robins (2011) revealed that the percentage of variance in character strengths explained by the Big 5 domains ranges from 14% (spirituality) to 46% (persistence) with a mean percentage of 33% across the 24 strengths. McGrath, Hall-Simmonds, and Goldberg (2017) demonstrated that spirituality is the least effectively represented by the FFM facet measures (less than 20% of explained variance) and in only three cases (creativity, forgiveness, and perseverance) does their best single predictor account for as much as half the variance in the strength scale. In addition to the direct relationship between character strengths and the Big 5, there is evidence for the incremental validity of the former over the latter from predicting self-reports of well-being , helping behaviors , and other behavioral criteria (e.g., friendliness and erudition;). Therefore, looking at the language of the VIA character strengths on social media in addition to the current findings for the Big 5 would allow us to capture more nuanced individual differences and provide a richer understanding of character strengths. The 24 strength scales are positively intercorrelated, raising the question of an underlying global factor. For example, Ruch et al. (2010) discovered comparable intercorrelations among the scales in both self- (median r = 0.36) and peer-reports (median r =0.38), and McGrath (2014) reported a mean intercorrelation of derived factors of 0.39. The first unrotated principal component (FUPC) alone typically explains about 40% of the variance. This is why Ng, Cao, Marsh, Tay, and Seligman (2017), when identifying the factor structure of the scales, chose to apply a bi-factor model with a separate global positivity factor (GPF) capturing dispositional tendencies toward well-being (rather than a methodological artifact). Thus, we expect that substantial overlap among the language correlates of the 24 character strengths would make it difficult to determine patterns distinctive to each of the 24 strengths. For this reason, we examine the language insights of this GPF separately from the language insights of the 24 individual strengths. We postulate that (a) the GPF, namely the FUPC, will capture increased use of positivity-related words associated with higher scores on character strengths overall ; and (b) each character strength will yield specific language insights when the other 23 strengths, as well as age and gender, are controlled for. Our goal is to identify a unique linguistic profile for each of the 24 character strengths and to provide insights regarding the cognitive, affective, and behavioral concomitants of these morally valued traits. 2 | THE PRESENT STUDY The primary goal of this study is to use Twitter language to illuminate the expression of the 24 character strengths. We use both a dictionary-based approach  and a data-driven open-vocabulary method. We hypothesize that significantly associated words and topics will yield nuanced linguistic cues for the GPF and each character strength. A supplementary goal of the present study is to predict user-level character strengths from Twitter language models, which eventually could serve as a cost-effective and scalable way to assess character strengths. A prediction tool will be useful because the reliable and valid measures of the character strengths, including the original 240-item  and the revised 120-item measures , are quite long and mostly self-report measures. 3 | MATERIALS AND METHODS 3.1 | Participants and procedure From an initial pool of 17,636 self-selected volunteers, 4,423 participants ultimately were analyzed in the current study (see Supplemental Figure S1 for the participant flow and the selection criteria2 ). The initial self-selected volunteers registered on the Authentic Happiness site (www.authe ntich appin ess.sas.upenn.edu) hosted by the Positive Psychology Centre at the University of Pennsylvania and completed the Values in Action Inventory (VIA) of Strengths using their personal devices. Upon registration, participants had the option to provide their Twitter handle for research purposes, after reading and agreeing to an informed consent statement. The final sample consisted of 4,423 participants (64.3% female) ranging from 18 to 65 years in age (M = 32.3, SD = 12.5). The participants were well-educated: 0.9% of them had less than a high school degree (n = 39); 38.9% of them were high school graduates or some college course work (n = 1,722); and 60.2% of them had a bachelor's degree or more (n = 2,262). The sample covered a variety of occupations, including students (29.3%), professionals (13.2%), clerks (8.2%), chief executives (6.1%), manual laborers (5.3%), artists and actors (3.8%), homemakers (0.4%), and people who were retired\\/unemployed\\/invalid (3.8%). Around one-third of them did not report their occupations (29.8%). The majority of the participants came from the United States (n = 2,783; 62.9%). The rest of the participants came from the United Kingdom (n = 383; 8.7%), Canada (n = 280; 6.3%), Australia (n = 256; 5.8%), and other countries (<2%). We used the Twitter Application Programming Interface (API) to query up to the most recent 3,200 tweets from each volunteer. This resulted in 3,937,768 status updates. Respondents were not paid for participating but were provided with an automatically generated summary of their character strengths. All procedures were approved by the University of Pennsylvania Institutional Review Board (protocol #816091). 3.2 | Character strengths measure The character strengths of the participants were measured by the Values in Action Inventory of Strengths (VIA-IS;). It is a self-report questionnaire consisting of 240 items, which measures the 24 character strengths of the VIA classification (10 items for each). A sample item is \\\"I never quit a task before it is done (perseverance).\\\" The reliability of the 24 scales was adequate to high with Cronbach's alpha ranging from 0.74 (prudence\\/honesty) to 0.90 (spirituality) with a median value of 0.80 (see Table 1). 3.3 | Outcome variables Two sets of outcome variables were defined in the present study. First, the FUPC of the 24 character strengths represented the GPF. Second, to derive distinctive linguistic insights for each character strength, we controlled for the influence of the other 23 character strengths using the residual of the character strength from a regression analysis with the specific character strength as a criterion and the remaining others as predictors. As shown in previous studies, age and gender impacted language use (;, Schwartz, Eichstaedt, Dziurzynski, Kern, Blanco, Ramones, et al., 2013), and thus we controlled for these demographics in all analyses by including them as covariates in our regression models. 3.4 | Linguistic analyses 3.4.1 | Closed-vocabulary First, using our Python-based open-source code base, the Differential Language Analysis Toolkit (DLATK;), we extracted 73 dictionaries (\\\"categories\\\") provided by LIWC. Dictionaries included psychological (e.g., positive emotion), life domain (e.g., family and home), and syntactic categories (e.g., pronouns). We also extracted the relative frequency of each dictionary (i.e., the total number of time a word written by the user matches a word in a given dictionary, divided by the user's total number of words). We explored the most positive and negative correlations of the relative frequency of the LIWC categories with the GPF score. In addition, we explored the most positively correlated LIWC categories of each character strength. Our reason for using LIWC was to examine the correlates of the 24 strengths in a variety of domains. Moreover, using LIWC categories had the advantage that results found here could be more easily compared with the existing literature as LIWC has been widely used in psychology research. 3.4.2 | Open-vocabulary Second, again using DLATK, we performed DLA  to identify the most distinguishing language features for our outcomes. We split (\\\"tokenized\\\") the tweets into words, punctuation, emoticons (tokenization;), and we extracted phrases consisting of two or three consecutive words (called 1-3 grams in the present study, for details of the methodology see Kern et al., 2016;). We kept only those two- and threeword phrases with high pointwise mutual information (PMI = 5;), a ratio of the probability of observing the phrase to the probability of observing the constituent words independently. This procedure yielded 11,901 language variables for each user, encoding the use of tokens and phrases. We correlated the GPF and the 24 residuals of the character strengths against all the one- to three-word phrases we extracted from their tweets and shortlisted the most strongly associated words\\/ phrases. As this is an exploratory technique, we utilized the Benjamini-Hochberg procedure  to correct for multiple comparisons and control the false discovery rate (FDR) over correlation tests for 11,901 language features. We selected only Benjamini-Hochberg significant 1-3 grams and topics. Third, we used a set of 2,000 previously created topics , clusters of semantically related words derived through Latent Dirichlet Allocation (LDA), a clustering algorithm akin to factor analysis but appropriate for the statistical distributions of words. For each user, we extracted the relative use of these 2,000 topics. 3.5 | Predictive model based on language We trained and evaluated a ridge regression model  to predict the users' 24 character strengths (the original scale scores) using the 2,000 topics as predictors, using age, and gender as covariates. The statistical model could be summarized as follows: Yi,t referred to the scores of each users' character strengths with i representing the user index (ranges from 1 to 4,423) and t representing the strength index (ranges from 1 to 24). Xp,i referred to the probability of a subject's use of each LDA topic with p representing the topic index (from 1 to 2,000) and Ct referred to the intercept for strength t. Ridge regression models were trained and evaluated using 10-fold crossvalidation (CV). In this procedure, the 4,423 users were split into 10 groups. For each group G, a ridge regression model was trained on the other 9 groups and then used to predict scores for G. For each group, we tested an array of ridge regularization parameters and reported the predictions corresponding to the model with the best performance on G. In this way, we ultimately obtained out-of-sample predictions for all 4,423 users. The predictions were out-of-sample in the sense that the model was trained only on the training set, although the ridge parameter ultimately was tuned on the prediction set. The accuracy of the predictive model was assessed by the Pearson's r coefficients (correlation between the user's character strengths score and their out-of-sample predicted values) and as the mean absolute error (MAE, the absolute difference between the user's character strengths score and their predicted values, in units of the 1-5 original scale). As a baseline, we used age and gender of the users to predict each character strength. After conducting the Fisher's (1915) z transformation, a t test was used to compare the two correlation coefficients. 4 | RESULTS For preliminary analyses, we computed descriptive statistics for all 24 character strengths, the loadings of each character strength on the FUPC and the Pearson's r correlation of each character strength with the other 23 strengths partialled out. Additionally, we included the eigenvalues of the first seven Yi,|t =Ct + |t,Age+ |t,Gender+ | 2,000 p=1 t,pXp,i components as well as their explained variance. The results are displayed in Table 1. 4.1 | The language of the GPF As shown in Table 1, the GPF explained 35.7% of the variance in our sample. Almost all 24 character strengths loaded well on the FUPC with loadings ranging from 0.32 (modesty) to 0.74 (gratitude), and most loadings were close to the median of 0.61. Both closed (LIWC) and open (DLA) vocabulary analyses revealed that the strongest positive correlations with the GPF score were words suggestive of social affiliation, positive emotions, and first person plural pronouns (e.g., love and our, b = 0.13 to. p < 0.001). The strongest negative language correlations with the GPF score were common adverbs, negative emotions, and words related to differentiation and tentativeness (e.g., also, bad, but, and would, b = -0.17 to -0.16, p < 0.001). Table 2 shows the top 10 most positively and negatively correlated LIWC categories (as well as the most frequent words within these categories). In addition, a high GPF score was associated with words that indicated a positive life attitude, such as blessed, patience, moments, and passion, whereas a low GPF score associated with hedging words such as actually, probably, supposed, and apparently. In a similar pattern, the LDA topics that correlated most positively with the GPF score revolved around social connections (e.g., family and friends, b = 0.17), religiousness (e.g., god and lord, b = 0.16), a sense of gratitude (e.g., blessed and thankful, b = 0.16), faith and optimism (strengths and overcome, b = 0.15), an attitude of living in the present (life and cherish, b = 0.14), and positive emotions (e.g., happiness and joy, b = 0.14). By contrast, the topics that correlated most negatively with GPF revolved around negative emotions (bad, b = 0.16), negations (e.g., wasn't and isn't, b = 0.16), a more past-oriented cognitive style (e.g., thought, forgot, and realized, b = 0.14) and hedging (e.g., supposed and apparently, b = 0.14). Figure 1 shows the 100 most distinctive words and phrases as well as the LDA topics for the GPF. 4.2 | The language of the 24 character strengths As discussed above, the 24 character strengths substantially co-vary. To derive the distinctive language insights for each specific character strength, we partialled out the influence of the other 23 character strengths, using the residual as the outcome variable in our linguistic analysis. The residual of the character strengths correlated significantly with the scale scores of the strengths, with median correlation of r = 0.68 (ranging from 0.56 for zest to 0.79 for spirituality, see Table 1). The results of the linguistic analysis for each character strengths are summarized in Figure 2. Additionally, to give a better sense of the context in which the most correlated words\\/phrases\\/topics appeared, we present random selections of tweets featuring these items (see Supplemental Table S2). As shown in Figure 2, the DLA results of creativity showed significant associations with words indicative of people who work in creative professions such as technology (e.g., Facebook and technology) and creative work (e.g., creative, design, and artist). The top three LDA topics also indicated creative professions (Facebook and hacked; computer and program; and art, design, and museum). The language of curiosity suggested an interest in exploring new experiences. The top-correlated LIWC categories were space and relativity (e.g., in, on, and at; indicators of being in new\\/different places), leisure (e.g., twitter, fun, and play; indicators of exploring) and ingestion (e.g., eat, water, and sweet; indicators of trying new food\\/drinks). Similarly, the most correlated words were related to space and relativity (e.g., on); traveling or other cultures (e.g., France and Korean); and leisure and activities (e.g., festival and park). The top-correlated topics which also referenced travel destinations (Paris and London), leisure (lake and boat), and activities (festival and film). The most representative words of judgment\\/critical thinking referenced thinking (e.g., know and think), consideration (e.g., bad and appropriate), and differentiation (e.g., not and but), which were essential for thinking things through (e.g., stop and bad) and examining from all sides (e.g., not and don't). The highly correlated topics likewise revolved around opinions, objective statements, and judgmental comments. Love of learning was associated with syntactic categories that mark more complex language use (e.g., use of articles, the and a), topics concerning school (e.g., school, books, and read), insights (e.g., know and think), and inquisitive language (e.g., why and how). This is consistent with the topcorrelated topics like opinions, books, and reading, as well as political discussions. The language of perspective was in line with a view toward life that emphasizes what matters most. Top-related words and phrases included important, makes me so, so much, life, and statements, similar to the only cluster of topics significantly associated with perspective (e.g., important, life, things, and realize). Bravery seemed to be associated with references to aggression (e.g., hate, kill, and fuck), masculinity (e.g., he, father, and man), freedom (e.g., fight), rights (e.g., rights, racist, and woman), and politics (e.g., vote and political). The top-related topics included swear words, freedom and rights, and politics. The most representative words of perseverance revolved around work (e.g., work), school (e.g., graduation and congrats), achievement (e.g., best and first), and suggested a futureoriented mindset (e.g., when and new), in line with individuals who tended to complete the tasks they set out to accomplish. The highly correlated topics likewise revolved around study (e.g., English and history), graduation (e.g., congrats and graduation), and achievement (e.g., grades and final). The most representative words of individuals high in the honest\\/authentic strength included self-reference (e.g., I and my) and revelations of personal distress (e.g., sleep and head) as well as potentially reduced self-control, indicated by greater use of swear words (e.g., fuck and hell). These also appeared similarly in the top-related topics, such as sleep, gotta, and tired. The most representative words of zest showed positive emotions (e.g., love and good), excitement (e.g., great, passion, and forward), and energetic pursuit of life and work (e.g., work and school). Individuals high in this strength also mentioned more social connections (e.g., we and our) and words related to achievement (e.g., best and work). The highly correlated topics likewise revolved around weekend (e.g., weekend and holiday), positive emotions (e.g., great and awesome), and future-orientation (e.g., forward and hope). The words most associated with love referred to relationships (e.g., my boyfriend and he loves) and gratitude (e.g., thank and thanks so much). Individuals high in this trait also seemed to value close relationships with others (e.g., wish I could and guardian [angels], Supplemental Table S2), and care about sharing (e.g., care about and recourses). Individuals high in kindness expressed support of others (well, hugs, and sorry to hear), were interested or engaged in charity (e.g., raise [money], Supplemental Table S2), and appeared to value close relationships (e.g., best friends and washing [for others], Supplemental Table S2). The most representative words of individuals high in social intelligence tended to be informal (e.g., u, :) and lol), with positive emotional content (e.g., love and good), and social language (e.g., catch up and conversation). The top-related LDA topics showed similar patterns, revolving around social events (e.g., night, town, and carnival) and positive emotions (e.g., love, hugs, and xoxo). The most representative words of teamwork reflected achievement. The top-correlated LIWC categories were reward (e.g., good and great, b = 0.04, p < 0.05), achievement (e.g., best and first, b = 0.04, p < 0.05), and religion (e.g., god and holy, b = 0.04, p < 0.05). In addition, words of support and encouragement (e.g., c'mon [come on] and congrats), future orientation (e.g., future), family (e.g., father), and work life (e.g., office) were highly correlated with the character strength of teamwork. However, we found no significant correlations between LDA topics and teamwork. Fairness was associated with words related to self-reference (e.g., I and my) and more frequent negations (e.g., but, no, and don't). The highly correlated topics likewise revolved around common adverbs (e.g., honestly and anymore), negations (e.g., don't and won't), and common verbs (e.g., talking and suppose). Leadership as a character strength was associated with the language of affiliation (e.g., we and our) and activities and events commonly engaged in by leaders (e.g., challenging, workshops, presentation, and manage). The top-correlated topics suggested further common behaviors of leaders, such as giving charity (e.g., donate and raise), attending events (e.g., event and ticket), and acting future-oriented (e.g., hope and forward). The most representative words of forgiveness were indicative of close relationships (e.g., marriage) and the process of apology (e.g., believed and appeal). Similarly, modesty showed associations with races, which in this context appeared to be running competitions (see Supplemental Table S2 for more details). In addition, the words related to modesty also involved proclamations of effort (e.g., forward to, hustle, and catch up). Prudence was associated with adverbs such as simultaneously, also, recently and apparently. Individuals high in prudence seemed to talk about clicking links (clicked) and also tended to use LDA topics previously shown to be characteristic of introverts , such as suspenseful movies (e.g., sherlock and inception), anime (e.g., anime and manga), and common adverbs and verbs (e.g., apparently and found). The most representative words associated with self-regulation was suggestive of rigorous, healthy lifestyle and selfdiscipline (e.g., life, health, gym, workouts, diet, and weights). The highly correlated topics likewise included workouts (e.g., gym and exercise), diet (e.g., eating and drinking), and losing weight (e.g., lose and pounds). The most representative words of appreciation of beauty and excellence was suggestive of individuals who expressed themselves (e.g., I and my; indicators of expressing oneself) emotionally, liked to observe esthetic work attentively (e.g., see, look, art, and beautiful; indicators of observing attentively), and expressed intensity (e.g., fuck and hell; to address the intensity). These patterns also were revealed in the top-correlated topics, which indicated emotional sensitivity (tears and cry), art (song and music), and positive emotions (beautiful and wonderful). The most representative words of gratitude showed that individuals high in this strength were thankful (e.g., so grateful for and blessed) and experienced positive moods (e.g., impressive and amazing) in social contexts (e.g., congrats and happy birthday to). The most-correlated LIWC categories were male (e.g., he and his) and social processes (e.g., you and love); both indicated objects\\/subjects of gratefulness. The highly correlated topics likewise included male references (e.g., dad and boyfriend), people (e.g., baby and girl), and social processes (e.g., family and friends). Individuals high in hope were future-orientated (e.g., a brand new) and optimistic (e.g., confident, id [I'd], and new products). The topics showed one significant result, namely abbreviation denoting one's mood (e.g., na and sa [concrete example: <USER> when you're ready come and get it la na na, see Supplemental Table S2 for more details]). The language of humor showed that individuals high in this trait tended to talk about themselves frequently (e.g., I and me) and responded to jokes (e.g., jokes and funnier) and funny content (e.g., toilet and dumb). The highly correlated topics likewise included funny content (e.g., toilet, pee, smell and fart) and responses to jokes (e.g., hahaha and laughing). The language associated with spirituality was indicative of individuals who practice their religion actively (e.g., god, church, praying, and lord), positive emotions (e.g., blessed), and were socially connected (e.g., family and mum). The highly correlated topics likewise revolved around religious themes such as god (god and prayers), gratitude (e.g., blessed and grateful), and religious events (e.g., service and church). 4.3 | Predicting character strengths with language As shown in Table 3, the predictive models performed comparably to other models predicting constructs like personality from behaviors, with models for love of learning and spirituality performing excellently (r reaching 0.51) and models for another six strengths (i.e., zest, appreciation of beauty and excellence, gratitude, curiosity, hope and self-regulation) performing relatively well (r greater than 0.30). The strengths that were easiest to predict from Twitter language had also higher relative frequency of 1-3 grams and topics (as indicated by the color of the word clouds and topic clouds). The reason behind the difference in prediction values might be that certain character strengths were more manifest on social media platform, while other strengths were more hidden. For example, love of learning indicates a certain degree of Openness, which is linked to more social media activities (e.g., more \\\"likes\\\" and larger network) and also a tendency to post more content, whereas prudence indicates a degree of introversion which may correlate with less social media use. 5 | DISCUSSION The present study investigates language use associated with the 24 VIA character strengths, extending previous work on the language profiles of the Big 5 to morally valued traits. We demonstrate that each of the 24 character strengths and a GPF are associated with distinctive language profiles and can be accurately predicted by social media language with fair accuracy. The present study expands existing knowledge on the overlaps and distinctiveness of the Big 5 and the VIA character strengths. Consistent with what Peterson and Seligman (2004) point out, our results show that four out of the Big 5 traits have clear counterparts in the virtue domain (see Table 4). Comparing the results of Twitter language on the Big 5 (;, Figure 6 and Figure S2) with our results reveals how the word clouds of the Big 5 differ or coincide the VIA character strengths. For instance, both Extraversion and zest correlate with words related to time for socialization (e.g., weekend) and positive emotion (e.g., great), but the language of zest additionally shows indications of enthusiasm (e.g., passion). In a similar vein, both Openness and appreciation of beauty are related to artistic work (e.g., music), yet the latter further emphasizes esthetic value (e.g., beautiful). This constitutes evidence that the Big 5 and VIA character strengths are complementary measures, with analysis of the VIA strengths contributing to a more nuanced understanding of individual differences. The words\\/phrases that are most positively associated with the GPF suggest positive emotionality, which captures a number of character strengths (e.g., beautiful, love, faith), and language associated with emotional (e.g., happy and passion) and social well-being (family and friend) and accomplishment (e.g., success). This general pattern of results is largely consistent with previous computational linguistic analyses on religious affiliation. We additionally observe language suggestive of mindfulness (i.e., focusing on the current moment, e.g., moment and breaths). This is consistent with previous studies on the association between character strengths and mindfulness  as well as life satisfaction. The linguistic cues in the present study provide support for the potential contribution and association of character strengths to both the hedonic (e.g., happiness or positive affect) and the eudemonic (e.g., a sense of meaning and purpose) aspects of well-being. In the pattern of negative association, we observe a tendency toward more cognition and differentiation but not the use of swear words, which mark disagreeableness and cognitive dysregulation. These observations raise the question of what exactly the GPF is. Is it a method artifact that reflects social desirability or an indicator of positivity? Given the high loadings of all character strengths on the GPF (median loading 0.61, and higher than 0.70 for gratitude, zest, leadership, hope, and perspective), and its associated language profile suggestive of positive emotionality and well-being, the GPF would suggest more of the latter (indicator of positivity) for the following reasons. First, gratitude, zest, hope, curiosity, and love loaded strongly (i.e., >=0.60) on the GPF, which happen to be the five strengths most robustly correlating with well-being across different samples. The higher the loadings of each strength on the GPF, the higher its correlation value with life satisfaction (rank-order correlation ranges 0.63 from to 0.81, computing correlations with Park et al., 2004, Table 3). Second, the overall level of virtuousness that has been ascribed to each character strength (gratitude [3.34], zest [2.72], love of learning [3.06], modesty [3.36];) does not seem to have a systematic impact on whether the character strength loads strongly (gratitude and zest) or weakly (love of learning and modesty) on the GPF. In sum, this suggests that the GPF may capture a sort of dispositional positivity--a trait-like \\\"meta positivity\\\"--that constitutes emotional well-being and emotional health rather than a methodological artifact. Nevertheless, it is possible that the GPF represents a prevalence of positivity-related words because no reversed items are available in the VIAIS. This suggests that a \\\"less fakable\\\" balanced key version of the VIA-IS  may be worth developing and that researchers should additionally try to measure character strengths through peer ratings  or structured interview. The theoretical implications of a general positivity factor observed in psychometric studies and computational linguistic analyses is worthy of further discussion and research. The language insights help to reveal the everyday lives of individuals who are high in each particular strength. For example, it is easy to imagine that a person who scores high in love of learning may love books, read a lot, be interested in history and have the drive to study--and the language results support this prototypical view. We still see grateful and care in the language of love despite the fact that the positive emotionality of the GPF was partialled out, suggesting that these language cues are indicative of love over and above the GPF language profile. If the goal is to have the full picture of each individual strength (i.e., rather than the distinctiveness of each strength), then an analysis of individual VIA-IS scales should be undertaken. As noted, this leads to largely overlapping word clouds given the large shared overlapping variance in the GPF. We avoid this problem in the present study and provide a technique to address this challenge. For studies interested in what differentiates strengths, we recommend using this meta-positivity GPF as a control variable and examining correlates of each strength above and beyond GPF. Our prediction results are comparable to previous work on personality prediction and suggest that language-based assessment of character strengths may one day serve as a cost-effective and scalable alternate measurement system. Social media language (e.g., from Twitter) constitutes a new medium for assessing individual differences which allows insights into other life domains such as well-being, job satisfaction, etc. For example, one interesting idea may be to use tweets from charismatic leaders (who probably are too busy to fill out the VIA-IS) to predict their character strengths and thereby predict firm-level outcomes (e.g., the cognitive strengths might be more related to improvement of revenue, while the justice strength might be more related to sustainable behaviors). All these predictions could contribute to an array of new research interests. 5.1 | Limitations and implications Despite the strong face-validity of the language results, distinctive language insights for each individual strength need to be interpreted with caution because the influence of the other 23 strengths has been removed. For example, the language of bravery includes the connotation of being aggressive, likely because kindness and love are partialled out. This technique can provide a distillation of the unique qualities associated with any given strength but may provide a rather thin or caricatured view of each strength. As each of the VIA strengths contributes to generally positive life outcomes individually and on aggregate, the various strengths are often not only overlapping but mutually supportive and even constitutive of one another. Therefore, the VIA strengths can be viewed in a variety of different ways: as a sum total, as just a few factors, individually without controlling for the others, and individually while controlling for the influence of all of the others. We explore the last of these options because it had not been done previously and in order to provide a more granular and specific view of the linguistic correlates of the character strengths. In addition, we acknowledge that our results are ultimately data rather than theory-driven because we did not have a priori predictions about specific associations between character strengths and the linguistic markers. This approach is exploratory in the sense that it helps with hypothesis generation, in contrast to more traditional hypothesis testing often undertaken in psychology. The words presented in our clouds are the most highly correlated, yet our interpretations are subjective. Readers are welcome to agree or disagree and future research is welcomed to test the hypotheses we generate. Our participants are mainly from an internet source (Authentic Happiness Website) and were not purposefully recruited, possibly resulting in a biased recruitment of people who are interested in positive psychology or who are curious about themselves. As shown in the participants flow chart (see Supplementary Figure S1), more than 3,000 volunteers gave invalid Twitter handles, which could indicate those who are honest were more likely to be included in our sample. These biases could affect the representativeness of the study. The lower amount of variance explained by the GPF compared to other studies (e.g., 41%, McGrath, 2015) might suggest greater homogeneity in our sample, which also could be seen from the characteristics of our sample (the majority of our participants were well-educated, English-speaking Americans). This means that the language features should be understood to describe our sample, not generalize widely. For this very reason, we do not encourage using our findings to estimate character strengths in new samples without prior replication of the findings. Finally, in this study, we predict inter-individual differences in the strengths, not intra-individual differences. Some applications of strengths focus on the \\\"signature strengths,\\\" that is, the 3-7 most highly developed strengths; further research is needed before user-level signature strengths can be reliably predicted from Twitter language. 6 | CONCLUSION The current study demonstrates that social media can be used to further characterize and predict character strengths. The prediction results suggest that language-based assessments of character strengths may well serve as a cost-effective and scalable alternate measurement system. The consistent finding of a general \\\"meta positivity\\\" factor, in this study and in the literature, may suggest that research exploring differences across overlapping constructs (like character strengths) should adopt methods suitable to address this, such as partialling out the shared variance to foreground meaningful differences. The linguistic correlates associated with each character strength provide insights into the behavioral and social components of these morally valued traits, providing a rich set of hypotheses to explore in future research on character strengths.\",\"577240580\":\"Background Schizophrenia and bipolar disorder with psychotic manifestations are two devastating psychosis-spectrum disorders that dramatically affect individuals' quality of life and personal functioning. Indeed, they are strongly associated with lifetime disability, reduced life expectancy, and increased relative risk of suicide. Furthermore, they are both characterized by strong and largely overlapping anomalies in neurocognition and social cognition, as well as by thought disturbances. Given this evidence and the strong and long-lasting disability affecting individuals with psychosis (PSY), especially individuals suffering from schizophrenia or from bipolar disorder with psychotic manifestations, the identification of markers of disease to be targeted in early identification or prevention strategies aimed at softening the PSY-associated burden is becoming more and more crucial in the clinical psychology and personalized medicine fields. In the pathophysiology of psychosis, environmental factors play a crucial role. Amongst those, early adverse experiences that occurred in childhood, especially those related to psychological or physical abuse, are highly prevalent in individuals diagnosed with psychosis (PSY) [9]. Notably, childhood traumatic experiences are associated with more severe clinical profiles and higher functional impairments already in the early stages and risk conditions of mental illness. According to previous literature, adverse childhood experiences are strongly associated with perceived negative parenting, which is in turn associated with attachment insecurity. Specifically, according to attachment theory, an innate motivational system prompts individuals to proximity seeking to alleviate distress. The quality of these experiences of proximity with significant others leads to the generation of individual cognitive-affective representations (i.e. \\\"internal working models\\\") of self and others. These models are key to affect regulation throughout the lifespan, as they guide how information from the social world is appraised, thus in turn potentially affecting social cognition abilities. In adulthood, a secure attachment style, often originating from positive parenting experiences, is indeed associated with high distress tolerance and positive affect regulation. On the other hand, interacting with unpredictable or insensitive attachment figures may make the development of a stable and secure mental foundation more difficultand lead to attachment insecurity, declined either through attachment anxiety (e.g., individuals are strongly vigilant to social threats and rejection experiences, and tend to overestimate the impact of negative emotions), or through attachment avoidance (e.g., individuals disavow the need of being comforted by others, avoid closeness and intimacy, tend to suppress the impact of emotions) [20]. Given its association with affect regulation, it is not surprising that studies have consistently reported a significant association between attachment insecurity and social cognition impairments, a key characteristic of PSY. Furthermore, both negative parenting and attachment insecurity have been associated with reduced coping strategies and increased likelihood of emotional breakdowns. In this framework, low parenting abilities, as well as attachment insecurity, have been previously considered as general environmental factors of vulnerability for mental illnesswhose effects are amplified by other genetic and\\/or environmental factorswithin the psychosis risk pathways. This evidence, therefore, suggests that both perceived parenting and attachment style may have value as potential targets of psychosis in early recognition strategies. Indeed, several univariate studies have reported significant associations between attachment insecurity and clinically relevant aspects of psychosis, e.g. negative symptoms, paranoia, negative beliefs, and social withdrawal. However, univariate studies have characterized main effects or interactions between attachment security and psychosis only at the group-level, thus lacking in generalizability potential and underestimating inter-individual heterogeneity. Furthermore, little is known about the association of perceived negative parenting and attachment insecurity with earlier stages of psychosis, as well as with its risk conditions (i.e., clinical, familial). Thus, the extent of potential of parenting and attachment factors to be targeted in early intervention programs is still unclear. A strategy to circumvent these shortcomings is to employ machine learning techniques. Indeed, machine learning allows quantifying sensitivity, specificity and generalizability of a given set of variables at the single-subject level, rather than just characterizing group differences. Therefore, employing machine learning to deeply investigate the discriminative power of parenting and attachment-related variables in both psychosis and early stages of disease would lead to a better understanding of how negative parenting and attachment insecurity might be considered core vulnerability factors of psychosis and its early stages. From a clinical perspective, this better understanding would potentially allow better tailoring of environmental factors within early identification programs. Specifically, accurate and generalizable machine learning models could potentially lead to the implementation of refined and individualized preventative interventions, and\\/or mental health promotion programs, which would in turn have a greater impact in the reduction of the burden associated with psychosis in terms of symptoms, quality of life, management. Thus, the aim of this study is threefold: (i) to generate a parenting and attachment-based machine learning model which correctly discriminates between PSY and healthy controls (HC); (ii) to test whether this model could track the early stages of psychosis and\\/or risk conditions in independent samples; (iii) to investigate in both psychosis and its early stages the potential association between model's performance and social cognition impairments. We hypothesized that both perceived parenting and attachment-related variables will bear significant PSY vs. HC classification performance. Furthermore, we hypothesized that this model will generalize to early stages of psychosis and familial risk conditions. Moreover, we expected that the discriminative ability of the model will be associated with social cognition abilities. Methods Sample determination A total of 234 individuals, all Caucasians native of the Apulia region, Italy, participated in the study. Inclusion and exclusion criteria are reported in Supplementary Information, section \\\"Background\\\". The discovery sample was composed of 105 individuals, of which 71 HC and 34 PSY on stable antipsychotic treatment for at least one month (Table 1A). Of the PSY, 20 were diagnosed with schizophrenia and 14 with bipolar disorder with psychotic symptoms according to the Structural Clinical Interview for DSM IV-TR. Moreover, 90 individuals were included in the validation clinical sample (Table 1B). Of those, 60 were HC and 30 were individual at the early stages of disease (ESD) compared with PSY. Specifically, 9 were labeled as First Episode of Depression, 9 as First Episode of Psychosis, and 12 as At-Risk Mental State for psychosis. A detailed description of the clinical characteristics of the ESD group is reported in Supplementary Information, section \\\"Background\\\". Furthermore, 26 HC and 13 individuals with a Familial High Risk for psychosis (FHR, i.e., with no DSM IV Axis I diagnosis, but with a first-degree relative affected either by schizophrenia or by bipolar disorder--Table 1C) were included in the validation familial risk sample. Both familial risk and clinical validation samples were used for replication purposes (section \\\"Out-of-sample validation analyses\\\"). ANOVA and kh2 were used to test for group differences in terms of demographics both within- and between- samples (Table 1). Perceived parental bonding and adult attachment assessment All participants completed the Parental Bonding Instrument (PBI) for the assessment of the perceived parental bonding(Supplementary Information, section \\\"Methods\\\"). The PBI is a 25-items self-report questionnaire investigating two main dimensions, \\\"care\\\" and \\\"overprotection\\\", separately for maternal bonding and paternal bonding. The \\\"care\\\" dimension reflects perceived parental warmth, affection, and involvement in contrast to coldness, indifference, and rejection. On the other hand, the \\\"overprotection\\\" dimension reflects perceived parental psychological control and intrusion in contrast to the encouragement of autonomy and independence. The Italian version of the PBI shows good psychometric properties. In this study, we focused on PBI continuous scores of separate maternal and paternal care and overprotection (4 variables in total) for analysis purposes. To assess the adult attachment style, all individuals underwent the 36-items self-report questionnaire Experiences in Close Relationships Scale (ECR) [37]. The ECR allows researchers to investigate feelings and behaviors related to significant relationships in adulthood along two dimensions: \\\"anxiety about abandonment\\\" and \\\"avoidance of closeness\\\". The anxiety factor includes intense concerns for romantic relationships, fear of being abandoned and frequent requests to greater involvement of partners; the second factor, avoidance, includes difficult and uncomfortable feelings in managing emotions and in relying on partners. The Italian version of the ECR shows good psychometric properties. In this study, we focused on continuous scores of ECR anxiety and avoidance for analysis purposes. Two-sample t-tests were employed to assess differences in discovery and validation samples for each of the PBI and ECR variables of interest (Table 2). All significant p values were < 0.05. Social cognition assessment All individuals were administered three tests, each aimed at investigating a different aspect of socio-cognitive abilities: a. to investigate emotion perception, we employed the Facial Emotion Identification Test (FEIT) ([39]; details about the Italian version employed can be found at); we employed a computerized FEIT versionin which individuals were shown 19 individuals' faces each depicting one of six different emotions (happiness, sadness, anger, surprise, fear, shame), shown one at a time for 15 s, with 10 s of blank screen between each stimulus presentation. 15 photographs depict negative emotions (sadness, anger, fear, and shame), while 4 photographs depict positive emotions (happiness, and surprise). After the presentation of each stimulus, individuals were required to select which of the six emotions was depicted on the picture and to mark it on an answer form. The total test score was computed as the percentage of correct answers. b. to investigate theory of mind, we employed The Awareness of Social Inference Test (TASIT) ([42]; details about the Italian version employed can be found at). The TASIT is composed of seven scales (positive emotions, negative emotions, sincere, simple sarcasm, paradoxical sarcasm, sarcasm enriched, lie), organized into three sections: in the section \\\"emotion recognition\\\", individuals undergo 28 video-vignettes of professional actors enacting ambiguous scripts representing 7 basic emotions (happy, sad, surprised, angry, anxious, revolted, neutral), and at the end of each vignette they are asked to choose the perceived emotion of a given actor indicated in the answer form. The second section, social inference (minimal), allows researchers to investigate the understanding of conversational meanings that are determined by paralinguistic cues. Here, individuals are required to watch 15 video-vignettes of either sincere or sarcastic or paradoxical everyday conversational exchanges; for each vignette, individuals are asked 4 comprehension questions, respectively testing the understanding of actors' beliefs, meaning, intentions, and feelings. The last section, social inference (enriched), assesses the ability to use contextual knowledge, like visual and verbal information, to derive meaning. Individuals are asked to undergo 16 video-vignettes, each one including a literally untrue comment (see Supplementary Infor mation, section \\\"Results\\\" for a full description). For each vignette, individuals are asked 4 comprehension questions, respectively testing the understanding of actors' beliefs, meaning, intentions, and feelings. For analysis purposes, we used the total number of correctly answered questions for each of the three TASIT sections. c. To test the ability to manage emotions, i.e. \\\"to be open to feelings, and to modulate them in oneself and others so as to promote personal understanding and growth of regulating emotions in oneself and in one's relationships with others\\\" [43], we employed the Branch 4 (managing emotions) of the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) ([43], Italian version: [44]). MSCEIT Branch 4 allows researchers to examine the ability to manage emotions by asking individuals to read short stories about an imaginary person going through an emotionally difficult situation, and then determine how effective several different courses of action would be for that given person in coping with the difficult emotions of the story. Individuals rate every possible action ranging from <<Very ineffective>>, to <<Very effective>>. MSCEIT scoring was based on the consensus scoring methods outlined in the manual. Two-sample t-tests were employed to assess differences in discovery and validation samples for FEIT, TASIT, and MSCEIT total scores employed in subsequent statistical analysis (Table 3). All p values were <0.05. In the case of significant results, TASIT p values were <0.05, False Discovery Rate (FDR) corrected for the number sections tested (i.e., 3) [45]. Machine learning pipeline The overall machine learning strategy was carried out through the NeuroMiner software, version 1.0 (https:\\/\\/ github. com\\/ neuro miner- git? tab= repos itori es) and consisted in building a multimodal algorithm based on the six perceived parental bonding and adult attachment related-variables (i.e., PBI maternal care, PBI paternal care, PBI maternal overprotection, PBI paternal overprotection, ECR avoidance, ECR anxiety, hereby called \\\"features\\\", Table 2) which accurately discriminates between HC and PSY. With this aim, we implemented a repeated nested cross-validation strategy (CV, Supplementary Information, section \\\"Discussion\\\") [32, 46, 47] to identify models that contributed most to the classification pattern separating PSY and HC at the inner CV level. To enforce an unbiased estimation of classification generalizability, these models were then applied to the test data at the outer CV level, which included individuals that were not used for training the classification algorithm. We obtained a Support Vector Machineensemble model based on the six perceived parental bonding and adult attachment-related variables listed before. Model performance was measured using sensitivity, specificity, balanced accuracy (BAC), positive predictive value (PPV), negative predictive value (NPV), and Area-Under-the Curve (AUC) based on the class membership probability scores generated through ensemble-based majority voting in the repeated nested CV framework (Supplementary Information, section Discussion). We also assigned statistical significance to the observed classification performance of our model through permutation analysis (Supplementary, Information, sect. 5). Furthermore, to understand the importance of the input features for generating decisions (i.e., PSY or HC?), we computed for each feature the probability of being selected for classification purposes within the inner CV loop. A detailed description of our machine learning pipeline is reported in Supplementary Information, section \\\"Discussion\\\". Further, we performed some sanity checks in order to exclude that our machine learning algorithm was associated with any clinical confound. Therefore, in the PSY group, we conducted Pearson's r correlations between subject-specific linear SVM decision scores and, respectively, medication (expressed in chlorpromazine equivalents), level of functioning (as assessed by Global Assessment of Functioning, [52]), and age of onset. A more positive decision score suggests that a given individual is highly prototypical of the HC class, while a more negative decision score suggests that a given individual is highly prototypical of the PSY class. Out-of-sample validation analyses To investigate whether the machine learning model can discriminate between PSY and HC could have a potential prognostic validity, we externally validated the model in the validation clinical sample (Table 1B) and in the validation familial risk sample (Table 1C). With this purpose, we performed out-of-sample validation analysis by applying the SVM ensemble decision model obtained from the discovery analysis without any in-between re-training steps to both validation cohorts (i.e., ESD vs. HC, and FHR vs. HC), as already done in previous publications. Association analysis between decision scores and social cognition To investigate whether the classification performance of our algorithm was associated with social cognition performance as a function of diagnosis, we tested the diagnosis (i.e., PSY vs. HC) x classification rate (i.e., correctly classified vs. misclassified individuals, based on the comparison between observed and predicted labels) interaction on social cognition (as measured by FEIT total score, TASIT total score, MSCEIT standardized total score). Analyses were controlled for age, gender and premorbid IQ (Supplementary Information, section \\\"Background\\\"). All significant p values were p < 0.05. In the case of significant results, TASIT p values were < 0.05, FDR corrected for the number of sections tested (i.e., 3) [45]. Given the validation results (see Results, section \\\"Validation familial risk sample\\\"), this same analysis pipeline has been repeated on the validation clinical sample. As a sanity check, to rule out any possibility that differences in terms of social cognition performance across groups (Table 3) could have influenced the classification performance of our algorithm, we performed correlation analysis on the whole discovery sample between all features which entered the machine learning algorithm, and each socio-cognitive variable of interest used for association analysis. All p values were > 0.05, FDR corrected. Results of this sanity check are reported in Supplementary Table S2 and highlight the absence of any significant association between features entering the algorithm and socio-cognitive variables of interest. Results Discovery sample ANOVA and kh2 analyses revealed that PSY differed significantly from HC in terms of age, WAIS and premorbid IQ (all p < 0.001, Table 1A), as well as on all measures of social cognition (all p < 0.001, Table 3A), with PSY performing significantly worse than HC. The machine learning model based on parental bonding and adult attachment-related variables correctly discriminated PSY from HC with a cross-validated BAC of 72.2% (sensitivity: 82.4%; specificity: 62.2%; Area Under the Curve: 0.71) and was significant at p < 0.001. Detailed statistics of all individual classification models are reported in Table 4. When computing the probability of each feature of being selected for classification purposes within the inner CV loop, we observed that the features with the highest probability (i.e., those that were selected the most for discrimination purposes within the CV framework) were PBI maternal care, ECR avoidance, and PBI overprotection (Fig. 1). In PSY, we found no association between SVM decision scores and, respectively, medication, GAF score and age of onset (respectively, p = 0.25, p = 0.52, p = 0.87). Validation samples Validation clinical sample ANOVA and kh2 revealed that ESD differed significantly from HC in terms of gender distribution (p = 0.007, Table 1B) and of social inference performance (minimal, p < 0.001; enriched, p = 0.011) as measured by TASIT sections \\\"Methods\\\" and \\\"Results\\\" (Table 3B), with ESD performing worse than HC. Results from the out-of-sample validation analysis discriminating between ESD and HC based on the model classifying PSY from HC (Results section \\\"Discovery sample\\\") revealed that the parental bonding and adult attachment-related model discriminating between PSY and HC also discriminated between ESD and HC with a cross-validated BAC of 63.5% (sensitivity: 67.7%; specificity: 59.3%; Area Under the Curve: 0.65, Table 4). Validation familial risk sample ANOVA and kh2 revealed that FHR differed significantly from HC in terms of IQ (p = 0.016, Table 1C) and of minimal social inference (TASIT section \\\"Methods\\\", p < 0.001, Table 3C) and managing of emotions abilities (MSCEIT Branch 4, p = 0.043, Table 3C), with FHR performing worse than HC. Results from the out-of-sample validation analysis discriminating between FHR and HC based on the model classifying PSY from HC revealed that the parental bonding and adult attachment-related model discriminating between PSY and HC did not significantly discriminate between ESD and HC (cross-validated BAC: 44.2%, Table 4). Association analysis between decision scores and social cognition Discovery sample General linear models revealed a significant diagnosis x classification rate interaction on FEIT percentage of correct answers (F = 5.11, p = 0.02, Fig. 2), such that the misclassified PSY (i.e., the PSY individuals that the algorithm wrongly identified as HC) had higher percentage of correct responses than the correctly classified PSY (i.e., the PSY individuals that the algorithm correctly identified as PSY) (p = 0.03), while no differences between the correctly classified and the misclassified HC individuals were found (p = 0.58). We also found a significant diagnosis x classification rate interaction on MSCEIT Branch 4 total score (F = 6.39, p = 0.02, Fig. 3), such that the misclassified HC (i.e., the HC individuals that the algorithm wrongly identified as PSY) had lower MSCEIT total score than the correctly classified HC (i.e., the HC individuals that the algorithm correctly identified as HC, p = 0.04). On the other hand, no differences between the correctly classified and the misclassified PSY individuals were found (p = 0.09). No significant interactions were found on TASIT section-related total scores (respectively, p = 0.45, p = 0.31, p = 0.15). Validation clinical sample General linear models revealed a significant diagnosis x classification rate interaction on MSCEIT Branch 4 total score (F = 4.74, p = 0.03, Fig. 4), such that the misclassified ESD (i.e., the ESD individuals that the algorithm wrongly identified as HC) had higher MSCEIT total score than the correctly classified ESD (i.e., the ESD individuals that the algorithm correctly identified as ESD) (p = 0.02), while no differences between the correctly classified and the misclassified HC individuals were found (p = 0.74). No significant interactions were found on FEIT percentage of correct responses (p = 0.40) and on TASIT section-related total scores (respectively, p = 0.06, p = 0.16, p = 0.51). Discussion In this study, we aimed at discriminating between PSY and HC within a machine learning framework based on perceived parenting and attachment-related variables, at externally validating this classification model in cohorts reflecting early stages of psychosis (i.e., ESD individuals), as well as its familial risk (i.e., FHR individuals), and at understanding across these cohorts whether algorithmic decisions were associated with social cognition impairments. To the best of our knowledge, our results for the first time offer preliminary insights about the machine learning-based classification ability of perceived quality of parenting and attachment style variables, revealing that this set of variables correctly discriminates between PSY and HC with a 72.2% BAC and significance. These findings are in line with previous literature suggesting the role of a perceived negative family environmentand insecure attachmentin the vulnerability for psychosis. Notably, the most reliable features for classification purposes were PBI maternal care, ECR attachment avoidance and PBI maternal overprotection. The PBI results are in line with previous evidence showing that expressed emotions, communication deviance, and rearing within the family have all been previously associated with the prognosis of psychosis. As concerns attachment avoidance, it has been previously associated with both positive and negative symptoms in PSY, and with psychotic symptoms, paranoia, endorsement of delusional experiences, negative affect regulation and social anhedonia in non-clinical samples. Nevertheless, these results should be taken with caution. Indeed, the risk pathways of psychosis are complex and heterogeneous, and reflect complex interplays between multiple genetic, environmental and neurocognitive characteristics. In this framework, our results do not suggest that perceived negative parenting and attachment insecurities are sufficient causative factors for psychosis. Rather, they highlight their potential role as general vulnerability-to-psychosis factors, whose effect may be amplified by other more specific schizophrenia and bipolar disorder-related genetic and environmental factors, possibly leading to cognitive biases in reality interpretation. This view is further corroborated by the fact that our perceived parenting and attachment-related model correctly discriminated also unseen ESD and HC individuals with 63.5% BAC. The 8.7% BAC drop observed in comparison with the discovery sample's performance may be explained by both clinical reasons (i.e., the ESD cohort is composed by individuals either at their first episode of psychosis or depression or by individuals at high clinical risk for psychosis, see Supplementary Information, section \\\"Background) and demographic differences between the discovery and the validation clinical cohorts (Table 1). Despite these differences, these findings show that our model not only achieves high diagnostic performance, but it also carries an intrinsic early identification potential. Thus, they may speak in favor of the possibility of specifically targeting parenting and attachment-related factors into early identification and prevention programs, especially in terms of promotion of attachment security as a key general resilience factor. Nevertheless, future studies in larger and geographically diverse samples are warranted to realistically understand the potential of translation into clinical practice of our results. It should be also noted that the model we generated in the PSY and HC discovery sample discriminated between FHR and HC at chance-level, thus suggesting that the general vulnerability-to-psychosis role of parenting and attachment characteristics might act only in the \\\"late\\\" portion of psychosis risk (as testified by the validation in ESD individuals), but not in \\\"early\\\" portions of psychosis risk. Indeed, FHR individuals do not have any clinical impairment or other risk conditions, but their PSY first-degree relative. This view is supported by previous evidence in which significant avoidance differences in attachment anxiety and avoidance have been found between individuals diagnosed with schizophrenia and their unaffected siblings, but not between their siblings and controls. Another aspect that should be taken into consideration is that the PBI and ECR instruments, from which the features entering the machine learning algorithm were extracted, are self-report questionnaires. Despite evidence testifying that attachment self-reports measures are highly reliable in reflecting individuals' actual attachment dispositions, it can't be excluded that the internal working models of attachment experiences, as well as the perception of the parenting received, are affected by biases in reality interpretation associated with illness experience at any stage (i.e., in both PSY and ESD individuals), which should not occur when only familial or genetic risk conditions are present. Indeed, a previous contributionshowed that individuals with an ongoing depressive disorder had more negative recollections of parenting experiences than those who remitted from a depressive disorder. This evidence corroborates the hypothesis that the experience of a mental illness is associated with biases in the interpretation of memories, although it is difficult to attribute any causality or directionality to this association. Nevertheless, our findings testify the urgency of implementing psychoeducation actions in psychosis, potentially through the combination of social skills trainings and emotional\\/ motivational support to all family members, aimed at improving illness knowledge, resilience and coping skills after adverse childhood experiences. These actions would support not only individuals suffering from psychosis but would also more likely alleviate the illness-related emotional burden of their caregivers. We also found a significant interaction between group and classification rate on social cognition performance in the cohorts in which our models showed good classification power, which speaks in favor of targeting social cognition deficits in early identification and intervention programs through social skills trainings. Indeed, in PSY and HC individuals from the discovery sample, this interaction was present on both FEIT and MSCEIT total scores. For FEIT, PSY that the algorithm misclassified as HC had higher emotion perception and ability to manage emotions, compared with PSY correctly classified as cases, and not as controls. For MSCEIT, HC individuals that the algorithm misclassified as PSY had lower emotion perception and managing of emotions abilities, compared with HC individuals that correctly classified as controls, and not as cases. We found the interaction also in the ESD and HC validation clinical sample, but only on MSCEIT total score, such that misclassified ESD had higher managing of emotions ability compared with correctly classified ESD. In both samples, no group x classification rate interaction was found on theory of mind abilities. Taken together, these results support the view for which parenting and attachment experiences guide how the social world is appraisedto the extent that their association with social cognition abilities is significant. Notably, this association is present on both basic and complex emotional aspects of social cognition as a function of PSY diagnosis, and only on cognitive-emotional aspects in ESD individuals. Indeed, emotion perception (i.e., the conscious recognition of basic emotions on facial stimuli) has repeatedly been found as impaired in PSY, and previous evidence revealed an association between attachment style and both behavioral and brain correlates of emotion recognition. This interaction was significant only in PSY, most likely because this basic emotion ability tends to worsen in the level of impairments along with disease chronicity and symptom severity. Thus, it may be that the lack of interaction on the FEIT score in ESD is due to their more preserved emotion recognition abilities. On the other hand, the presence of a significant group by classification rate interaction in both PSY and ESD on MSCEIT score is consistent with the fact that parenting and attachment-related factors significantly modulate affect regulation at a more complex level since the early stages of psychosis. The ability to manage emotions, indeed, is one of the four emotional intelligence branches and represents the ability of individuals to handle (both at the thought and behavior levels), their own emotions effectively. Consistently, a core concept of attachment theory is that attachment style strongly influences the appraisal of social cues, thus modulating affect regulation from childhood to adulthood. Limitations This study has some limitations. First of all, the small sample sizes across cohorts strongly limit the generalizability of our findings, as they may be overfitted. Second, the use of a retrospective perceived maternal care measure and of a self-report attachment questionnaire could reflect false memories or even re-interpretation of the type of care received. However, previous literature has indicated that both measures are stable over time, suggesting that they may overall constitute reliable measures. Third, the cross-sectional nature of this study does not allow us to give any realistic prognostic insight into psychosis and its early stages based on these findings, or to speculate on whether the experience of a mental illness impacts on the perception of negative childhood experiences, or vice versa. Longitudinal studies are warranted to investigate the prognostic relevance of our model. Conclusions Despite the difficulty in attributing any causality to the association between psychosis and perceptions of negative childhood experiences, our results support the role of perceived negative parenting and attachment insecurity as general vulnerability characteristics in psychosis and its early phases, both because of their good classification power and because of their association with key socio-cognitive psychosis phenotypes. In this view, they improve the understanding of psychosis environmental risk pathways. Furthermore, if externally validated in larger cohorts, they offer intriguing clinical insights about the possibility of promoting positive parenting and attachment security as psychosis resilience factors within early identification and early intervention programs, especially when other more psychosisspecific environmental risk factors (e.g., childhood adverse events, low socioeconomic status, an affected first-degree relative) are present.\",\"577240586\":\"1. Introduction This article explores the potential for measuring the personality of job applicants through their image choices using a novel measure whereby respondents are asked to indicate which image in a pair is more like them. We report on two studies, where the first describes the creation and refinement of the measure and the second creates scoring algorithms and validates them by assessing convergent and discriminant validity and the potential for adverse impact. Although some image-based personality assessments exist commercially (e.g., Traitify and RedBull Wingfinder), there is little literature in this field. Since applicant perceptions of the selection process can influence whether an applicant is likely to accept a job offer , applicant experience is pertinent for both applicants and hiring teams. Image-based formats might increase engagement and thereby applicant perception, as they increase satisfaction and elicit stronger responses compared with questionnaire-based measures. The purpose of this article is to address this lack of validation in the research regarding image-based assessments of personality, particularly those created for use in selection. Our findings provide preliminary evidence that assessments of this type could be a valid and fairer alternative to questionnaire-based selection assessments that use Likert scales. The article begins with a discussion about selection assessments, particularly those measuring cognitive ability or personality, followed by evidence in favor of the use of game- and image-based assessments, such as their shorter testing times. Before we describe the method used in each study and evaluate the performance of the assessment, we outline the need for selection assessments, focusing on those measuring cognitive ability and personality. Specifically, we compare the fairness of measures of these constructs and the scope for assessing them through game- and imagebased assessments. As will be highlighted below, much of the research into gamification has focused on cognitive ability, but there is evidence that image choices can be used to measure personality ; notwithstanding this, a validated Big Five personality measure created for use in selection has not been described in peer-reviewed research. The reported study, therefore, aims to contribute towards the lack of evidence addressing the potential for soft skills, such as personality, to be measured through gamified assessments, particularly those using an image-based format. We do so through an integrated approach, drawing from psychology and machine learning to create and validate the measure. This feasibility study found that all five traits can be accurately measured through an image-based format, with convergent validity similar to that between other traditional measures of personality. While we note that further investigation is needed into the assessment and in particular its lower discriminant validity compared to questionnaire-based measures, our preliminary findings demonstrate that a forced-choice, image-based assessment has the potential to be a valid way of measuring the personality of applicants following further validation. We discuss the implications for recruitment policy and practice and the need for further validation. 1.1. Assessment in Selection Selection methods have been used in recruitment for over 100 years to evaluate candidate suitability and predict future job performance , with around 40 million assessments being completed globally by candidates each year. The most valid predictor of job performance is cognitive ability , with validity estimates of r = 0.51 , a value that increases when combined with integrity tests (r = 0.65), work sample tests (r = 0.63), structured interviews (r = 0.63), or tests of conscientiousness (r = 0.60) . When used alone, the Big Five personality traits--openness to experience, conscientiousness, extraversion, agreeableness, and emotional stability--are also predictive of job performance. While conscientiousness is the most valid trait for predicting personality across multiple job contexts , when predicting performance for jobs that require specific skills, other personality traits are also useful. For example, extraversion predicts the job and training proficiency of those with sales and managerial jobs while openness and extraversion both predict training proficiency across occupations including police officers, sales staff, professionals, managers, and skilled\\/semiskilled workers. For those working in pharmaceutics, openness, extraversion, and conscientiousness predict task performance, while agreeableness and openness predict creativity. The strongest combination is the measures of cognitive ability and personality, which demonstrate incremental validity over each other and predict distinct areas of performance. However, assessments of cognitive ability are often associated with adverse impact , describing differences in selection rates between different groups. Personality assessments, on the other hand, have little to no adverse impact so are a fairer way of assessing candidates while still being predictive of job performance. 1.2. Game and Image-Based Assessments While personality is often assessed via self-report methods using Likert scales, a recent trend in selection is gamification  where elements of a game are added to traditional assessments to increase engagement , including progress bars and visual and audio feedback. Common issues with self-reported assessments are poor response quality  and lack of completion  due to the lengthy nature of scales, resulting in poor data. Game-based assessments (GBAs) can overcome these issues by offering a more engaging experience for participants  and shorter testing times , resulting in them being viewed more favorably by test-takers compared to traditional assessments. GBAs also elicit less test-taking anxiety , are less prone to social desirability bias than questionnaire-based measures, and can be more predictive of future job performance if they are designed to measure job-related behaviors. The algorithms created for GBAs are often tested for and mitigate against adverse impact , therefore reducing the potential for the bias associated with traditional assessments. To date, much of the focus of research into gamification has been on using games to assess cognitive ability , with less of a focus on measurements of soft skills such as personality. However, scores on the Big Five traits have been predicted based on image preferences with convergent validity with the NEO-PI-R  ranging from r = 0.06 for neuroticism to r = 0.28 for agreeableness. In their study, Krainikovsky and colleagues (2019) predicted scores using algorithms based on image preferences from a selection of images tagged with information relating to objects, behavior, emotions, and scenery, although the measure was not fully validated or created for use in selection. An image-based format further reinforces the benefits of game-based assessments and has the additional advantage of being language-neutral, meaning they can easily be adapted for use in other languages  unlike questionnaire-based measures, which often need to be redeveloped in the target language. Language neutrality also has the benefit of increasing the accessibility of the assessment to those with low literacy or those with a learning difference such as dyslexia, reducing the barriers that may prevent them from securing employment  if their underlying ability is obscured by language difficulties which cause them to perform poorly on verbal assessments. While there is a lack of validated image-based measures of all five personality traits, particularly those for use in selection, Leutner and colleagues' (2017) validated imagebased measure of creativity accurately measured openness to experience. Their measure, created for use in selection, presented respondents with images and asked them to indicate which was most like them. Creativity scores on traditional measures were accurately predicted, with good concurrent validity with the traditional scales for all three creativity constructs, namely curiosity (r = 0.35), cognitive flexibility (r = 0.50) and openness to experience (r = 0.50). To build on these findings, the reported study describes an imagebased assessment of all five personality traits with a similar format to that of Leutner and colleagues' (2017) creativity measure, with respondents being asked to indicate which image in a pair is more like them. The scope for developing an image-based assessment of personality for use in selection is explored through the creation of machine-learningbased predictive scoring algorithms and testing adverse impact and convergent validity between the image-based assessment and the questionnaire-based IPIP-NEO-120 . Based on the findings that image preferences can be used to predict personality  and that openness to experience can be predicted through image choices , we expected that our preliminary findings would indicate that there is potential for all five traits to be measured through image choices. 2. Materials and Methods In this section, we outline the methods used in the two studies summarized below. We draw upon methods from industrial\\/organizational psychology and machine learning to develop the measure and create and validate the machine-learning-based scoring algorithms used by the assessment. Overall, the aim these feasibility studies is to explore the potential for creating a valid image-based measure of personality to be used in talent recruitment in the future. An image-based format was chosen as previous findings have indicated personality can be measured through image choices. Two studies were conducted: * Study 1--Item Bank Creation: Study 1 describes the creation of an item pool of image pairs, along with the selection of the 150 best-performing items (image pairs), and the mapping of these items to the Big Five traits; * Study 2--Measure Validation: Study 2 describes the development of predictive machine-learning-based scoring algorithms based on a panel of respondents. This approach, where algorithms are developed that predict outcomes on traditional assessments, is common practice in predictive measures of personality , as they convert binary choices to a more interpretable output that is more reflective of the continuous nature of the Big Five traits. Study 2 also describes the validation of the assessment through measuring convergent and discriminant validity with Johnson's (2014) questionnaire-based IPIP-NEO-120 and tests for potential adverse impact. An overview of the studies, which we expand upon below, can be seen in Figure 1. Figure 1. (a) Study 1 overview: Item creation and selection of the best-performing items for the image-based Big Five measure. (b) Study 2 overview: Creation of scoring algorithms and tests of convergent validity with the questionnaire-based measure and adverse impact. 2.1. Study 1: Item Bank Creation 2.1.1. Image-Based Measure To create the image pairs, statements from Goldberg's (1992) IPIP scales were used to guide image searches and ensure a good representation of the traits. Based on these statements, a team of I-O psychologists brainstormed how they could be represented through images. For example, for the statement \\\"I like to tidy up\\\", the concept was a messy versus tidy email inbox (Figure 2). Images were sourced from Shutterstock due to the wide range of high-quality images available. The image database was searched using keywords related to the conceptualisation of the statements (e.g., searching for 'email notification'). Image pairs, or items, were designed to represent multiple ethnicities, age groups, and genders and the facets of neuroticism associated with mental health (anxiety and depression) were not included in the measure. Although it could be argued that the removal of these facets could alter the structure of the emotional stability construct, this action was taken to prevent discrimination against respondents with mental health issues, particularly since the measure is forced-choice, meaning this could be interpreted as asking respondents whether they have a mental health condition or not. Caution was also taken to ensure that the images would be suitable for professional use, with scenes featuring alcohol, parties, and inappropriately dressed models being avoided. Some images were edited to remove unnecessary text, which would have prevented language neutrality, or to allow the image to be cropped more effectively. The image pairs were either designed to be single-trait, where the images represent high and low levels of the trait, or mixed-trait, where the images reflect high levels of two different traits to determine which trait the respondent identifies with most, with some of these pairs being presented with adjectives to aid understanding. An example of each type of image pair can be seen in Figures 2 and 3. Once the images had been processed (edited and cropped), they were uploaded to the game development platform for the developers to create a functional assessment. The measure is completed on a smartphone device, with image pairs being presented one at a time along with the statement \\\"I am more like. . . \\\", prompting respondents to select the image they identify with most in the pair, thus being forced-choice. Audio and visual feedback was added to gamify the measure , including a progress bar at the top and sound effects when an image was selected, as well as a pause button to allow respondents to pause and resume the assessment. 2.1.2. Questionnaire-Based Measure The IPIP-NEO-120  measures each trait through 24 questions using a five-point Likert scale, with a maximum score of 120 for each trait. Each trait is divided into six facets, with four questions measuring each, e.g., the cheerfulness facet of extraversion is measured by statements like \\\"I radiate joy\\\" and \\\"I love life\\\". Items measuring neuroticism were reversed to reflect emotional stability. Figure 3. Examples of mixed-trait image pairs. (a) is designed to map onto the \\\"I love to help others\\\" statement from the altruism facet of agreeableness (left) and the \\\"I feel comfortable around others\\\" statement from the friendliness facet of extraversion (right). (b) is designed to be mapped onto the \\\"I act comfortably around others\\\" statement from the friendliness facet of extraversion (left) and \\\"I believe in the importance of art\\\" statement from the artistic interests facet of openness (right). 2.1.3. Participants Three hundred compensated respondents were recruited through the online participant pool Prolific Academic (Mage = 31.14, SD = 9.26, 69% female). Respondents completed the questionnaire-based measure along with 100 items from the image-based measure to avoid test-taking fatigue, resulting in each item being completed by an average of 54 participants (95% CI: 38-68). 2.1.4. Item Selection To select the best-performing items and reduce the length of the assessment, Cohen's d values were used to describe the difference in mean IPIP scores for the group of respondents choosing image one versus image two in each pair. This was calculated for each trait. Items that had large Cohen's d values, indicating a large difference in personality scores between those selecting image one versus image two, were considered to perform well. Based on these values, 150 items, or 300 images, were selected to be retained: 132 items with moderate to large values (>0.5 for a trait), and 18 items that showed small to moderate differences (>0.29 on at least one trait) to maintain a balance in the items for each trait. Using the Cohen's d values, items were mapped to the trait that corresponded to the highest value. As can be seen in Table S1, of the 300 images that were retained, almost two thirds (60%) of them were mapped onto the trait that they were designed to measure. The 150 items included in the assessment had a mean highest Cohen's d value of 0.77 (SD = 0.25; range: 0.29-1.77). 2.2. Study 2: Measure Validation 2.2.1. Participants A second sample of 431 compensated respondents were recruited using Prolific Academic. Respondents completed the IPIP-NEO-120 and the full 150-item image-based assessment from study 1. The majority (n = 222) of respondents were female and most (n = 356) were under the age of 40. 209 were White, 73 Black, 66 Asian, 56 Hispanic, and 14 were of Mixed Race. 2.2.2. Analysis A separate scoring algorithm was created for each of the five traits using a machinelearning-based predictive model with scores for the relevant trait on the questionnairebased measure as the outcome variable. The predictors, created by binarizing the 300 images to indicate whether they were chosen by each respondent, were entered into a least absolute shrinkage and selection operator (Lasso;) regression to create the models based on a training portion (70%) of the data. Lasso regression was favored over ordinary least squares (OLS) regression, which is commonly used in behavioral sciences, as it is prone to overfitting and inflating R2 values, leading to overfitting and consequently a lack of generalizability due to variance between datasets. Lasso regression reduces the effects of variance by adding some bias to the model and introduces a regularization parameter, known as l, which decreases the size of all of the coefficients by an equal amount. As a result of l, some coefficients are reduced to zero  and removed from the model, creating a more interpretable model with fewer variables. The removal of predictors also enabled investigation of whether there was crossover in the predictors retained by each model, as well as whether only image pairs mapped to that trait were predictive (See Table S1 for trait mapping and predictor retention). To determine the most appropriate hyperparameters for the models, 10-fold cross validation was used. The remaining 30% of the data acted as an unseen sample, allowing the generalizability of the models beyond the training dataset to be examined. The scores predicted by the model were correlated with the scores on the IPIP to determine the model's accuracy , with the correlations for the test set also being used to determine convergent validity. The potential for adverse impact was determined for age, gender, and ethnicity using the four-fifths rule, the two standard deviations rule, and Cohen's d effect sizes. Group differences in scores were examined based on a pass or fail criteria determined by whether a respondent scored above or below the average score for that trait. According to the four-fifths rule, if the pass rate of a group is less than four-fifths of the pass rate of the group with the greatest pass rate, adverse impact is occurring (Equal Employment Opportunity Commission;). According to the two standard deviations rule, also known as the Z-test , if the disparity between the expected and observed pass rates are greater than two standard deviations, adverse impact is occurring. Finally, Cohen's d can be used to determine the effect size of the difference between the mean scores of two groups, with d = +\\/-0.20 indicating a small effect size, d = +\\/-0.50 indicating a medium effect size, and d = +\\/-0.80 indicating a large effect size. All three types of analysis were used to more robustly test for group differences since the sample size is relatively small. However, group differences in scores are not always indicative of adverse impact and could instead reflect genuine group differences in ability. 3. Results In this section, we evaluate the performance of the scoring algorithms created in study 2. We first present descriptive statistics for both the questionnaire-based measure and the novel image-based measure and subsequently present the metrics used to determine the performance of the models. We assess the convergent and discriminant validity between the questionnaire and image-based measures and test for potential adverse impact. 3.1. Descriptive Statistics The descriptive statistics for scores on the IPIP-NEO-120 and image-based measure can be seen in Table 1. While the skewness and kurtosis values for these scores indicate that there may be a slight divergence from a normal distribution, the values are below the thresholds (two for skewness and seven for kurtosis) to be considered as substantially deviating from a normal distribution. Internal consistency of the questionnaire-based measure, determined by Cronbach's alpha , was high, ranging from 0.83 for openness to experience to 0.93 for emotional stability (see Table 2). This range is consistent with that reported by Johnson (2014), which ranged from 0.83 for openness to experience to 0.90 for emotional stability. The descriptive statistics for both measures are similar, suggesting that there is a similar distribution of scores. Although the Big Five traits are five different constructs, they intercorrelate. The intercorrelations for scores on the questionnaire-based measure, as seen in Table 2, concurred with intercorrelations that would usually be reported, ranging from 0.09 between openness to experience and emotional stability to 0.61 between conscientiousness and emotional stability. One reason for the high level of intercorrelation between emotional stability and conscientiousness could be because of the removal of some facets from emotional stability, which would leave the sub-scales for emotional stability that might be closer to conscientiousness (less neurotic behaviors). 3.2. Model Performance The number of predictors retained in the models ranged from 13 for openness to 32 for extraversion (see Table S2 for coefficients and mapping of the predictors retained by each model), suggesting that personality could be measured through shorter assessments and that they can offer similar insights into personality as longer assessments. Indeed, only 68 of the 150 items were retained across the five models, suggesting that the Big Five can be rapidly measured through a small number of images in around two minutes. The performance for each of the scoring algorithms created for the image-based assessment can be seen in Table 3, where correlations between scores on the image- and questionnairebased assessments indicate model accuracy. While correlations for all models were stronger for the training set, the test set correlations remained strong and significant, suggesting that the models can be generalized to unseen datasets. This is important when creating a scoring algorithm since the models will be applied to datasets other than the ones they were trained on. The test set correlations were also used to assess convergent validity, with convergence ranging from 0.60 for agreeableness to 0.78 for extraversion, indicating that the image-based format can be used to measure personality in a similar way to traditional, questionnairebased formats. To better assess the convergent and discriminant correlations of the imageand questionnaire-based methods, a multitrait-multimethod approach was used. As can be seen in Table 4, in the majority of cases, the discriminant correlations were smaller than the convergent. While for emotional stability in particular, the discriminate correlations were relatively high, the same pattern is seen in Table 2. This result could also be explained by the removal of the anxiety and depression facets from emotional stability since the remaining facets are closer to those of other traits, such as conscientiousness. 3.3. Adverse Impact Analysis The results of the adverse impact analysis for the image-based assessment can be seen in Table 5. Adverse impact ratios below 0.80, medium to large effect sizes, and cases greater than +\\/-2 standard deviations indicated that there were group differences. Group differences were considered to be present when at least two of the metrics were in agreement. Based on these measures, potential for adverse impact was found against male and Asian respondents for the openness model, against Hispanic respondents for the conscientiousness model, and against males, Asians, and Hispanics for the agreeableness model. To examine whether these group differences resulted from the scoring algorithms or scores on the questionnaire-based measure, adverse impact analysis was also conducted for the IPIP-NEO-120. As can be seen in Table 6, the group differences found for the image-based assessment echo those of the questionnaire-based assessment, suggesting that the group differences identified in the image-based measure were due to group differences in scores on the questionnaire-based measure and not due to the image-based format. This highlights the need to examine group differences in the training data, since machine learning algorithms can amplify this bias. The group differences may be due to measurement bias in the questionnaire-based assessment or could reflect genuine differences in ability since group differences are not always indicative of bias. Since group differences were observed for the questionnaire-based measure, measurement bias was investigated by examining whether convergence varied by subgroup. As can be seen in Table 7, there are differences in the convergence for subgroups, and these differences echo the group differences in scores for both the image- and questionnaire-based measures. For example, the convergence for Black and Asian respondents for agreeableness is significantly lower than that of White and Hispanic respondents, with group differences being found in their scores. 4. Discussion In this section, we discuss the performance of the scoring algorithms created for the reported image-based assessment of personality and the possible limitations that could result from the relatively small sample used in this study. Specifically, we discuss the performance of the models and methodological considerations. We also suggest some areas for further research before this assessment can be deployed in practice, and the implications that our preliminary findings may have for the use of image-based measures of personality in selection. This study aimed to create scoring algorithms for and to validate a novel, image-based measure of the Big Five personality traits to explore the potential for such a measure to be used in selection. Study 1 described the creation of an item bank and the selection of the 150 best-performing items. Study 2 described the development of a predictive machinelearning-based scoring algorithm for each trait and the validation of the image-based measure by measuring convergent validity with a validated, questionnaire-based measure and testing for potential adverse impact. 4.1. Model Performance The findings of this study provide preliminary evidence that all five personality traits can be accurately measured via image choices in a similar way to traditional measures of personality. Models were trained on 70% of the data then cross-validated with the remaining test data to assess generalizability. The model's accuracy was assessed by correlating scores on the image- and questionnaire-based measures. Across all five traits, correlations were strong for both the training and test data, indicating good model accuracy and generalizability to unseen data. The convergent validity between the image- and questionnaire-based measures, determined by correlations for the test set, ranged from 0.60 for agreeableness to 0.78 for extraversion. These values exceed those reported by previous non-verbal personality measures; correlations between the Nonverbal Personality Questionnaire and the NEO-FFI  ranged from 0.45 for emotional stability to 0.59 for agreeableness. The convergent validity range for this measure is comparable to the convergent validity between different questionnaire-based measures of personality, with correlations between scores on the IPIP  and NEO-FFI , ranging from 0.50 for agreeableness to 0.76 for emotional stability. However, since some discriminate correlations exceeded the values for convergent ones or were of a similar magnitude, this limits the conclusions that can be made about the validity of the assessment and highlights the need for further investigation. 4.2. Methodological Considerations While this assessment performed well in terms of accuracy and convergent validity, the sample size was small relative to other studies that describe predictive personality measures. Although this could have implications for the model's performance , the use of a train\\/test split is likely to have reduced the potential for the sample size to negatively impact the models. Additionally, although the size of the sample was relatively small, the range of personality scores was large, suggesting that the sample represents a range of personalities. Nevertheless, a larger sample size would likely have been beneficial, with the potential for creating more robust models. Furthermore, while the majority of items were mapped onto the trait that they were designed to measure in study 1, some were not, suggesting that it is difficult to perfectly align text- and image-based measures. This may be because image-based measures rely on personal interpretation of meaning which may vary between people. Despite this, all items were included as predictors for each model to examine which items were retained in the models and whether they aligned with the mapping. 4.3. Future Validation Before this assessment can be used in practice, further validation is needed to more robustly explore bias, generalizability, and how comparable this assessment is to other questionnaire-based measures. We suggest the following: * User experience: To better understand how respondents engage with the measure, future studies could examine user experience, including how engaging respondents found the measure to be. It could also investigate whether the meaning assigned to the items by the team of designers converges with that of the respondents by asking a sample of respondents to assign their own adjectives to the items. This would allow further refinements to be made to the measure which may strengthen its performance; * The potential for shorter measures: Since only a small number of predictors were retained by each model and there was some crossover in the predictors retained by the models, future studies should investigate how shorter versions of the assessment could take advantage of this by examining how effective different item combinations are at measuring the traits. This would result in even shorter testing times for candidates, reducing the time it takes to complete the overall battery of selection assessments; * Bias and transparency: Group differences in scores do not always indicate bias and can instead be reflective of genuine differences in latent levels of traits for different subgroups. However, even when group differences in scores are not due to differences in ability, they do not always lead to adverse impact, especially when the analysis is based on a small sample. Therefore, further validation is needed with a larger sample to more robustly determine whether the reported group differences could result in adverse impact, particularly since the importance of transparency and fairness in the algorithms used in hiring is increasingly a point of concern ; * Mitigating bias: While the potential for adverse impact from this assessment echoes concerns about the fairness of conventional selection assessments , adverse impact associated with algorithmic recruitment processes can be mitigated by removing the items associated with group differences and updating the algorithms , unlike with traditional assessments that use a standard scoring key. Further research exploring the potential for mitigating group differences in the algorithms used by this assessment is needed, particularly since there is evidence of measurement bias in the questionnaire-based measure used to construct and validate the algorithms. Follow-up studies are, therefore, required to investigate the best way to mitigate this; * Generalizability: The samples used in this study may be limited if they did not represent a diverse group of respondents. For example, data referring to the occupation of respondents were not collected, meaning the generalizability of the findings could be limited to a particular industry if respondents are from a similar background. To address this, a future study should recruit an additional sample from another source such as MTurk to validate the algorithm in a different population of respondents who are likely to have different attributes to those in the current samples; * Cultural appropriateness: As only English-speaking respondents were included in this study, a variation in the interpretation of the items was not investigated across multiple cultures or languages. Whilst it is suggested that non-verbal assessments can be applied to any language without redevelopment , it is still important to ascertain whether the images included in this assessment are appropriate in other cultures. The findings of this study indicate that there are potential differences in the interpretation of the images for different subgroups, with convergence being null on some traits for Asian and Black respondents. Therefore, future studies should take a cross-cultural approach to investigate the performance of the measure in different cultures and ethnicities; * Score inflation: Job application contexts have higher stakes as they can affect careerrelated opportunities. Since there is evidence for the inflation of personality scores in high-stakes contexts , a future study could investigate score inflation on this novel assessment in a high-stakes context. The forced-choice image-based format might decrease candidates' ability to fake their responses compared to questionnaire based tests; * Measure reliability: Respondents only took the measure once, meaning that response stability and consistency (test-retest reliability) could not be examined. Thus, it is not known whether respondents are likely to make the same image choices and therefore have similar personality scores each time they take the assessment. Further validation is needed to determine the test-retest reliability of this assessment; * Measure validity: Additionally, further investigation is needed into other forms of validity, including internal validity since items mapped to multiple different traits were used in the models to predict each Big Five trait; * Multitrait-multimethod approach: To better compare this measure to other traditional assessments, a future study using a multitrait-multimethod approach would provide insight into how the measure performs in terms of convergent and divergent validity with multiple other assessments. Such an approach could also investigate whether user experience is greater for the image-based assessment as compared to traditional assessments, as has been previously indicated. 4.4. Implications The reported study contributed towards addressing the lack of validated gamified assessments of personality, particularly assessments using an image-based format. Since GBAs are reportedly more engaging , elicit greater satisfaction , and have shorter testing times  than traditional assessments, this measure could offer a more positive experience for applicants than traditional psychometric assessments. As applicants who view the selection process of an organization more positively reportedly have more positive perceptions of the employer and are more likely to accept a job offer , this has implications for businesses as attractive selection methods can avoid offer rejections from talented candidates. The findings of this validation study provide preliminary evidence that all five personality traits can be measured through image choices, with the novel assessment showing promise for use in selection following further validation. 5. Conclusions Overall, this study found that image-based measures may be a valid and fair alternative form of assessment that could be used in place of traditional assessments using Likert scales. Using predictive scoring algorithms, the image-based assessment of personality described in this study demonstrates convergent validity with a validated, questionnairebased measure comparable with the convergence between other questionnaire-based personality measures, suggesting that the reported assessment measures the Big Five traits in a similar way to traditional measures. Furthermore, this study found that the image-based measure is largely free from group differences which could potentially lead to adverse impact; however, further studies are needed using larger samples to test this more robustly. The measure also needs to be further validated to assess test-retest reliability and score inflation, as well as in other languages and cultures. Further studies could also compare user experience for the image-based assessment and a questionnaire-based measure. These preliminary findings have positive implications for the use of this assessment in selection; however, there is scope for further validation before this measure can be used in practice.\",\"577240618\":\"1. Introduction In recent years, it is feasible to perform everyday activities using the Internet including social media interaction. All of these activities include consumers submitting reviews online. Consumers often use the Internet to launch consumer-related communication. Consumer behavior is any message or social media-based communication that performs reviewing language. Consumer behavior can be classified into various types. Consumer behavior variants can include consumer reviews. New consumer behavior variants utilize various models such as encryption and packing to remain visible to consumer reviews system. We have to detect consumer behavior as soon as it spreads into the social media platform. Consumer behavior prediction is the procedure of investigating review messaging in social media interactions and predicting if it is consumer\\/non-consumer behavior. Consumer behavior can be classified as immediate or future. Identifying consumer behavior requires three steps: (1) Consumer behavior messages and social media interaction are analyzed with proper tools. (2) Dynamic features such as timing are extracted the interaction data. (3) Parameters are assembled in specified sets and are used to differentiate consumer from non-consumer behavior. To enhance the detection rate, different techniques such as data science, cloud computing, deep learning, and computerized learning models are utilized. Various consumer behavior prediction techniques utilize these technologies. These models are signature checking, behavioral analysis and stochastic learning models. Signature-checking models are effective for identifying similar variants of consumer behavior. However, they fail to detect formerly unnoticed consumer behavior. Although stochastic models can detect unknown consumer behavior, they cannot detect more complex consumer behavior clarifications. A deep learning model can be utilized as a standard to eradicate the shortcomings of the current consumer behavior classification models. Deep learning is utilized extensively in different paradigms such as representation processing, human emotion recognition, and action recognition. Nevertheless, it has not been utilized adequately in the retail research, particularly consumer behavior detection. Deep learning is an artificial intelligence model operating on an artificial neural mechanism. Deep learning employs supervision. To enhance the precision, various models have been utilized such as deep belief techniques. Deep learning models have many advantages over customary models: for instance, deep learning models can mine significant data from the input to lessen the training requirements. Deep learning can also use representations resourcefully as well processing big databases while reducing time and enhancing precision. Our research presents a new ensemble learning model for consumer Internet behavior classification. In the ensemble model, consumer behavior data are collected from BIG-D, Facemg, and TwitD databases. Consumer behavior representations are transformed into grayscale and then used as input to the training module. After the data procurement section is complete, the ensemble model extracts high-level consumer behavior features from the consumer behavior representations by utilizing the convolution function of our ensemble model. The model then undergoes supervised training. Several deep models are united to build the ensemble learning model using a number of hidden layer activation functions. The experiments performed prove that the ensemble model efficiently mines unique properties for consumer behavior groups. The performance also presented that the ensemble model predicts various consumer behavior classes with the utmost precision with better performance than recent models. The main contributions of our research are as follows: 1. A new ensemble neural model for consumer Internet behavior classification is defined. 2. The ensemble model utilizes a new merging technique that has two transfer learn ing CNNs. 3. Unique parameters are mined from the consumer behavior data for the specified classes. 4. The ensemble model lessens the parameter dimensionality considerably. 5. The ensemble model evaluates consumer behavior databases. 6. The performance rates outperform similar models. The rest of the article is structured as follows: In Section 2, we clarify the consumer behavior investigation, parameter selection, and classification and survey the current consumer behavior prediction models. The ensemble model is depicted in Section 3. Section 4 presents and discusses the experiment results. Section 5 lists the limitations of the model and future work directions. Conclusions are given in Section 5. 2. Related Work Consumer behavior detection is a long procedure with many phases. Many techniques are utilized in this procedure. The detection of consumer behavior requires analyzing the existing methods. Results are recorded, and parameters are then computed. In the next stage, intelligent methods are utilized to select only the significant parameters. Selected features are used in the training phase of the neural model to distinguish consumer behaviors. Consumer behavior analysis and classification processes are depicted and summarized. To comprehend the reason for a consumer's behavior, more prediction is employed by identifying the groups of consumer behaviors. In order to comprehend the consumer behavior mining methods that are applied, this summary is structured as follows: consumer behavior detection platforms, consumer behavior analysis, consumer behavior feature extraction, and classification. 2.1. Over the Internet Consumer Behavior Detection Platforms Consumer behavior detection models can be performed on computer cloud platforms, mobile, and IoT devices. Smart platforms developed widespread paradigms. Then, these platforms started to become available. In current studies, people widely utilize mobile applications, and IoT platforms are expanding rapidly. Therefore, consumers' interests have moved to Internet computing paradigms. Cloud computing also aids consumer behavior by simple access to different databases. Recent papers on consumer behavior classification models are concerned with IoT and Internet communication. Most published research paved the way to deep learning techniques. 2.2. Consumer Behavior Analysis Model Consumer behavior samples must be investigated to discover consumer behaviors. Consumer behavior investigation is a significant procedure for online retail. Consumer behavior detection is performed to answer questions such as the nature of the consumer behavior structure, the plan and place where the consumer activity will take place, and the spread network for identifying the review scores. Consumer behaviors are split into two classes, local and global. Consumer behavior undergoes static and dynamic analysis. In static analysis, file texts and representation data shared over the Internet are examined by analyzing data without details. To mine these data, numerous tools can be utilized such as PEiD and MD5deep. Static analysis is the initial stage of consumer behavior analysis; to achieve better understanding, advanced static investigation is recommended. In advanced analysis, the representations and social media interactions are inspected in detail. For this reason, complicated representation splitters are extensively used. In the analysis phase, social media interactions and shared representations are inspected in depth to find out features of consumer behavior. Certain consumer behavior maps can be drawn as a result of reversing social interaction networking. However, due to the huge amount of data on the Internet, performing such analysis needs more time. In dynamic analysis, reverse interaction maps are executed, and the behaviors of the involved consumers behavior. The consumer behavior is monitored with process explorer. In advanced analysis, tools such as WinOlly are utilized. Such tools allow consumer behavior and Internet interaction to process for both reading and sharing. 2.3. Consumer Behavior Parameter Setting When individual consumer actions are studied, the interaction timeline is recorded. Records will be analyzed to mine consumer behavior parameters. Learning models use previous knowledge from big data. At this phase, assured patterns in the data and unknown values are extracted. In recent research, data mining methods such as n-gram, bags and the net model are utilized when identifying consumer behavior parameters. There are parameter selection systems that can be presented for consumer behavior prediction. 2.3.1. Feature Extraction Methods N-gram models is utilized in many classification models including consumer behavior classification through sentence analysis. N-gram models can use feature mining to extract learning parameters. Once parameters are selected from the consumer behaviors, it can utilize temporal text analysis. For example, if a sample sentence is S = {1,2,3,4}, 2-gram and 3-gram will be {<1,2> , <2,3> } and {<1,2,3> , <2,3,4> }, respectively. Bag feature extraction models are comparable with n-gram models with high occurrences and are of higher significance than the word location. Although this technique is an operative grouping method, the high growth in the count of the parameters decreases its efficiency. Other versions of this model group consumer behavior features intelligently. 2.3.2. Graph Maps for Feature Extraction Internet interactions gathered while extracting features include strings, friend networks, messages, and interactions and are represented by diagrams, where the map representation is M (N, E) and N is a vertex representing people in a subset of the social media interactions and E are edges representing interaction among the nodes. The problem is that the graph size can grow exponentially over time, and therefore, we use map subsets to approximate the whole map representation s. However, extracting the map subsets is a hard problem in research, and therefore stochastic or greedy algorithms are employed. 2.3.3. Visual Techniques Visual parameter mining techniques have several algorithms to mine the parameters of consumer behavior. In one algorithm, consumer behavior text binaries are used as a representation. In this algorithm, a consumer behavior binary is represented and saved as a one-byte vector. In another algorithm, consumer behavior analysis is performed using tools such as IDA Pro. Then, interaction maps are collected and visualized as representations. In the representation process, models such as graph maps are utilized. In recent research, facts are established that consumer behavior groups and have comparable visual parameters. 2.4. Consumer Behavior Deep Learning Classification Models Consumer behavior features are extracted by utilizing deep learning or experimental models to predict the consumer behavior. We divide review score classification models to different models comprising signature, machine and deep learning models. 2.4.1. Signature Classification Model A bit series that represents the interaction map consists of exclusive bits for structures and are utilized in consumer behavior classification. In the computation, the fixed parameters are mined from the interaction data. The computational process produces outputs by utilizing interaction data and stores them in a database. When the reviewing consumer behavior is marked as actual, the signature is computed and compared with the predetermined signatures as actual. This model is efficient in detecting consumer behavior, but it cannot identify unknown consumer behavior. Additionally, from the results in, consumer behavior classification is not valuable as it is not identifying different consumer behavior types, it is not robust, and it depends on social media interface structure. The bit-series computation model is introduced by the authors in. The ensemble model extracts the signatures utilizing a range of identification-based stochastics. Computed signatures are mostly seen in consumer behavior interaction structures. Therefore, false detection rates are reduced. The authors inpresented a basic regular expressioninduced signature utilizing caterpillars. The presented model induced computed values and entailed several steps: recognizing the highest projecting series utilizing orientation methods, removing noisy portions, and creating regular expression. 2.4.2. Behavior-Based Consumer Behavior Classification Models Behavior-based classification models monitor the behaviors in the interaction network. Based on the monitored behaviors, the reviewing consumer is determined to be actual. This model has three portions: mining behaviors, creating features, and classification through machine learning models. When extracting behaviors, interaction data, shared messages, and posts are utilized. Behaviors are extracted by computing the order of the message sharing and their frequency. Activities are categorized, series are computed, and features can be attained. Granting the interaction structure fluctuates over time, and its overall behavior will not have changed entirely. Hence, numerous consumer behavior types are identified by employing the suggested model. Additionally, consumer behavior has remained formerly unknown, an is predicted. Consumer behavior classification employing the graph method is clarified by the authors in. Consumers are converted into a graph such that each vertex denotes a consumer, and the edge denotes a transition among the posts. Using the result of a consumer wording as an input, the links among social media posts are computed. The graph was mined and associated with the existing graph models. The graphs were defined as consumer or non-consumer behavior. Additionally, new actions that are detected during the social media analysis, were dynamically defined in the graph. The authors inpresented a centric model in which consumer behavior interactions are unlike from non-consumer behaviors. By employing the interaction variances, behavior structures were generated from the graph nodes. Further, by employing these sequences, consumer and non-consumer behavior classes were produced. 2.4.3. Stochastic-Based Consumer Behavior Prediction Model Stochastic classification is a composite classification model that utilizes diverse models together. This classification model uses prior knowledge of rules and machine learning models. Stochastic models employ both sentiment and behavior features to produce rules. Founded on the generated rules, consumer signatures are formed and utilized to regulate diverse consumer behavior as well as formerly unseen consumer behavior. The model learns by employing definite parameters for validation and irregularities. Even though the hit percentage in identifying unknown consumer behavior is in elevation. The authors inpresented a deep learning consumer behavior classification model. The model can detect formerly unseen consumer behavior variants. The authors inclarified a stochastic signature method. In this model, parameters are produced of particles that can be consumer posts, comments, shares, and reviews. 2.4.4. Model Checking Consumer Behavior Prediction Model In model checking classification, malicious parameters are extracted by employing nonlinear temporal models to detect the parameter dependencies, which are defined as disclaimers. Consumer behavior parameters are mined by employing the association rules of different actions that utilize unseen behavior. To mark the input as reviewing consumer behavior, the features that are extracted are compared with the former provisions. This presented model is resilient to secrecy and can identify new portion of the consumer behavior variants. The authors inpresented a confirmation model to identify malicious consumer behavior. In the ensemble model, malicious activities were defined by employing a predicate logic specification model from the assembled social media posts. If the method controller properly identified the specification, the investigated case is defined as consumer or nonconsumer behavior. In this model, consumer behaviors are identified among friends of the same consumer types. Additionally, new, unknown but comparable consumer behavior variants are identified. Agreeing to the method, a checking algorithm detected consumer behavior semantic features more precisely than usual classification models, and this enhanced the precision of the classification. A positive consumer behavior classification model that employed a checking algorithm was introduced by the authors in. The ensemble model can classify various consumers. The model mined features from posts and comments and routinely authenticated them employing the former defined specifications. They employed a novel specification model, NSM. The research depicted good results; the model could classify several types of consumers. A technique is proposed into identify consumer behavior. This model declined the checking difficulty to regulate a Buki pushdown model with figurative factors. Consumer behaviors are transferred into pushdown stacks, and then, by employing the stack predicate logic, the consumer behavior activities are detected. At the final stage, the consumer behavior is mined by comparing it with the pushdown specifications. The presented model is resilient to stealth faulty identification and detects consumer behavior with high precision. 2.4.5. Deep Learning Consumer Behavior Detection Model Deep learning models are extensions of machine learning that are trained from samples and take over from convolutional neural networks. Deep learning models are employed in representation processing, autonomous vehicles, speech recognition, and also consumer behavior classification. The deep learning classification method performs well with high accuracy and decreases the parameter dimension, but it is not resilient to hacking. Moreover, forming hidden layers requires more time; constructing more hidden layers will enhance the accuracy slightly while consuming more time. Deep learning models are not yet widely employed in consumer behavior classification, and thus, more research is required to precisely validate this model. The deep learning consumer behavior classification methodologies that are found in the literature are described below. A deep learning consumer behavior classification model employing two-dimensional consumer behavior features is presented in. The presented model has three central phases: In the first phase, five matching parameters are mined from reviewing consumers and non-consumer cases; in the next phase, three deep and dropout layers are constructed; in the last phase, the outputs are computed by employing the calibrator metric. At this phase, the approximation of whether the social post is consumer behavior or not is classified. From their results, the model performs well, with 96.7% accuracy with high sensitivity and F1-score. The authors inexplicated a deep learning network they defined as a multitasking training model for consumer behavior identification. In the presented model, reviewing samples are trained using dynamic learning. Multitasking training permits the model to pre-train even at shallow layers, and the network utilized optimization techniques to reduce the counts of epochs and decrease errors; the researcher claimed that the model accomplished good accuracy in comparison with other deep models. However, the accuracy of the network cannot have increased by having additional deep layers. The authors inintroduced a mixed deep learning model to identify zero-period consumer behavior. The presented model employed multiple hidden layers using a Boltzmann network and shortterm memory. It has two stages: model learning and parameter tuning. In the learning phase, training is performed by extracting features using a supervised fashion. At this phase, the features of each post are extracted. Then, in the parameter tuning stage, labelling is performed to split consumer from non-consumer behavior. Deep learning classification models are operative when identifying consumer behaviors that can be misled by normal posting with harsh words, which yields miss-identification. For example, argumentative posts can deliver misleading inputs to and generate wrong consumer classification. Additionally, true consumers can be unidentified by shifting some characters in the wording. In our ensemble consumer behavior method, essential constraints are employed to diminish the impacts of such reviews. 3. Material and Methods In this paper, we are presenting our ensemble consumer behavior learning model. This platform is an ensemble deep-CNN model. The ensemble deep-CNN platform as depicted has several stages (Figure 1). In the initial stage, the consumer behavior data are gathered by exhaustive database mining. In the second stage, unimportant and important consumer behavior parameters are extracted employing transfer learning CNN. The final stage starts a supervised learning module. The following subsections describe consumer behavior representation and the deep learning model. In the consumer behavior representation, we will present the ensemble consumer behavior variants in binary. In the model description section, the ensemble consumer behavior classification model is explained. 3.1. Consumer Behavior Representation as a Binary Map Various methods are introduced to transform binary file into maps. This research utilizes the representation of the consumer behavior binary maps. The required goal is to represent binary maps as a binary representation. Based on our algorithm, the consumer behavior binary file is represented as 8-bits vectors of unsigned integers. Then the binary number B is transformed into its number value employing Equation (1). At the end, the value is combined into a 2D array M and construed as a binary representation. The dimension of matrix M depends upon the consumer behavior file size B = '7 + '6 + '5 + '4 + '3 + '2 + '1 + '0 - B = '7 x 27 + '6 x 26 + '5 x 25 + '4 x 24 + '3 x 23 + '2 x 22 + '1 x 21 + '0 x 20 (1) 3.2. The Ensemble Model for Consumer Behavior Prediction The ensemble method defines a platform for consumer behavior prediction. This platform is an deep CNN structure. Our model of the ensemble platform is previously depicted in Figure 1, with consecutive stages: assembly of consumer behavior data, deep CNN structure, training, and testing phases. A flow diagram is depicted where the pre-trained CNN represents a feature extractor module. The first five layers exhibit FC layers for the training module and the Softmax layer. Primarily, consumer behavior data are composed from several databases such as Facemg, BIG-D, and TwitD. The consumer behavior databases are detailed in the following subsection. Then, the ensemble deep CNN model is depicted. There is a pre-processing phase: the procedure of predicting a proper deep learning architecture to embed in it the consumer behavior classifier. It is revealed in pre-processing that the ensemble technique can deliver better precision. An ensemble module that encompasses both DenseNet and Vgg CNNs is constructed employing transfer learning networks. The DenseNetnetwork is a CNN with 50 layers with 5 convolutional layers. The DenseNet CNN has Maxpooling functions, classifier and FC neural layers. The DenseNet network has 25 million hyper parameters as described in Table 1. Vggis a prominent CNN network for large-scale representation recognition. Vgg has an architecture of several deep layers; the primary layers are convolutional neural layers. Vgg has two normalization operations, two pooling layers, and a final classifier, as depicted in Table 1. The description of DenseNet is depicted in Table 2. Transfer learning has been examined for facing the various challenges faced in the classification model such as computational time cost and large data dimension. Transfer learning perform feature extraction process employing pre-trained CNN. Then, the classification procedure is performed with support vector machine (SVM) or with a Softmax classifier. This process is accustomed for the ensemble Deep CNN to aid with the above challenges. The ensemble model associated CNNs with an identical weight to generate a representation map. The learning is then accomplished to attain high precision. The steps are as follows: The transfer learning procedure is completed with the DenseNet and Vgg networks using three databases in the training phase. In the second phase, the parameters extracted by the DenseNet and Vgg networks are joined to produce the feature vector. This produced vector has 2048 dimensions. The features produced by the pre-trained DenseNet and Vgg are extracted from the final FC layer and depicted in T 6 and 7. Then, the joined feature vector is delivered to the Softmax and the FC layers to achieve normalization. Afterwards, the Softmax classifier produces seven outputs that consist of categories of consumer behaviors, and the FC layers encompass 2048 nodes. The last layer targets the enhancement of the learning ability of the ensemble deep-CNN. Finally, the ensemble deep learning model is tested by employing comprehensive databases in the training module. The description of the ensemble model is depicted in Table 3. 4. Results 4.1. The Implementation Process This section describes the implementation process, the experiments, and the evaluation of the ensemble deep-CNN model. The experiments are performed on Intel Core i running at 9.6 GHz with 64 GB RAM. Python language was used to implement the model. Data are partitioned into training and validation databases randomly: 70% of the data for training, 15% for validation stage and 15% for the testing stage. The training process of the deep-CNN model was completed in 29 h for 80 epochs on average. Metrics such as accuracy, sensitivity, specificity, and F-score are used. These metrics are calculated as follows: ACC = TP + TN TP + FP + FN + TN (2) SEN = TP TP + FN (3) SPEC = TN FP + TN (4) D = 2*TP 2*TP + FP + FN (5) where TP denotes the count of true positives, FP denotes the count of false positives, TN denotes the count of true negatives, and FN denotes the count of false negatives. The metrics above are used to infer the performance of the ensemble model. The ensemble model is compared with two deep neural models. Figures 2-4 depict the metric evaluation of the introduced model using an ensemble of Vgg and DenseNet deep models for each database. Table 1 depicts the initial parameters values of the selected configuration for the ensemble deep architecture used for the Facemg, BIG-D, and TwitD databases as depicted in Table 4. 4.2. Benchmark Database Experiments are performed on three benchmark databases. These are Facemg, BIG D, and TwitD. These databases are described below. The Facemg databasehas 9000 consumer behavior instances. Each single con sumer behavior instance in the database fits into one of 20 consumer behavior classes. The count of instances fitting into a consumer behavior class varies across the database. The consumer behavior classes include different types of weapons (13 classes), bombs (5 classes including asking how to make a bomb or purchase materials related to bombs), suicide reviews and killing wordings (2 words). The BIG-D databasecontains 22,000 consumer behavior instances belonging to 9 classes include different types of weapons (7 classes), bombs (one class), and suicide reviews (one class). Similar to the Facemg database, the count of consumer behavior instances over defined groups is not equally spread. A single consumer behavior instance is mapped to one eight-bit map representing a hexadecimal number representing the class number. We used the bytes to form a consumer behavior representation in our simulation. The TwitD databasehas 9000 consumer behavior instances for training and 5000 consumer behavior instances for testing fitting into to 20 consumer behavior classes. Each class has 450 instances for training and variable instances for testing. The consumer behavior classes are the same as the classes of the first database. 4.3. Evaluation Assessment metrics depict the performance of the ensemble consumer behavior clas sification technique. The direct outcome of the classification model is a score to comprehend the accuracy of a model. Accordingly, different performance scores stated in the results are employed to depict the efficiency of the presented models. The performance scores are accuracy and the Dice metric. Figures 2-4 depict the performance of the Vgg and DenseNet deep models and ensemble models for the Facemg, BIG-D, and TwitD databases. In agreement with these charts, it can be specified that the ensemble model performs other deep learning architectures. Our model performance also depicts comparable results for the three databases, while the results for the compared two deep learning models fluctuate considerably for the three databases. The aforementioned settings indicate that our model is more reliable and has higher accuracy in comparison with the other models. Additionally, consumer behavior variants are examined using the metrics. Tables 5-7 depict the confusion matrices for the BIG-D database for nine consumer behavior classes of Vgg, DenseNet, and the ensemble models. Performance rates for each consumer behavior class are verified with the statistics. It is detected that our ensemble model gives higher results for all consumer behavior classifications excluding casual wording class. The DenseNet model delivers a higher classification of the consumer behavior variant compared with other models. We also compared the ensemble model with the state-of-the-art models. Tables 5-7 depict the accuracy results for the Facemg, BIG-D, and TwitD databases for the ensemble model and other models, respectively. It should be clarified that the ensemble model performs better than the state-of- the-art models. 4.4. Discussion In this research, we presented a novel deep learning model to predict different consumer behavior trends using an ensemble architecture This research was focused on the integration of two pre-trained optimized learning algorithms. This model maintains four phases of data gathering, deep modeling, training, and model evaluation. The ensemble model is tested on three social media databases. The experimental results proved the efficiently of consumer behavior trends classification with high precision that outperforms recent models in the literature. Implications This study contributes to the literature on consumer behavior trends classification in a number of ways. Firstly, we present an ensemble deep learning model of the consumer behavior trends such as recurrent purchases and loyalty in replying to the social media marketing content. The main contributions of our model are: defining unique parameters through data mining techniques from the consumer behavior data for the specified classes. Additionally, the parameter dimensionality is reduced considerably for faster learning and classification time. This research also contributes to enterprises who reflect using social media as a marketing channel. The experimental results recommend to digital marketing personal the significance of using social media to influence consumer behavioral trends. Based on the first set of experiments, testing depicted that our scoring model of consumer behavior variants has an important impact on defining consumer satisfaction through social media content sharing. For future research based on our findings, we can encourage and design an interaction marketing model and apply deep learning model on consumer interaction in a more efficient way. The second set of experiments, which computed the true positive and true negative rates as well as testing kappa coefficient, proved the precision of our model compared with the ground truth and that it attains higher sensitivity and specificity than other deep learning models. Designing digital marketing strategies based on our scoring technique (from the extracted parameters), based on the deep learning and mining approaches, the quality of enterprises towards consumers can certainly be developed. 5. Conclusions Consumer behavior classification models effectively identify consumer behavior variants that represent serious consumer behavior in the social media domain that can represent real-life consumers. Unknown people behind the screen with different languages and wordings make the consumer behavior classification a difficult process. Our research ensemble a new merged learning model that efficiently identify consumer behavior classes. The ensemble model employs comprehensive pre-trained models that depend on the transfer learning model. The data on consumer behavior groups were collected by employing several exhaustive databases. Then, the features are mined, and the parameters are computed by employing transfer learning models. Additionally, the ensemble model achieves deep parameter extraction. The central role of our proposed hybrid model is to unite two optimized deep learning models. The ensemble model is tested and validated on Facemg, BIG-D, and TwitD databases. The suggested ensemble model is compared versus the joined models individually. The experiment results established that the ensemble model can efficiently predict consumer behavior with high accuracy and Dice score. It is also found that our model is effective and decreases the feature representation space. The ensemble model was compared against other deep learning models. The experiments attained revealed the improvement and reliability of our model over other models. On the other hand, a few consumer behavior instances were not predicted properly. This is because those consumer behavior variants employed unseen mystification wording and depicted the same features with several consumer behavior variants.\",\"577240636\":\"1. Introduction Of late, cyberspace and social media have become increasingly adjacently parallel to the physical world, especially when it comes to conversations or discourse. Social networks have allowed for easier connections with people right from a person's couch and they have increased the convenience of people's daily lives, but they have also opened people's minds regarding security and privacy concerns that come with the ease of use of these platforms. The industry-academic community has grown significantly interested in studying the potential use that these social networks' data could provide to businesses or the research community. The large quantities of data generated (commonly called \\\"big data\\\") allow researchers to extract information from these data, create new insights into the different domains, and understand users' characteristics, behavior, and decision-making patterns. Literature regarding social media data has studied the demographical characteristics of the users , the users' psychological traits expressed through social networks , stock price predictions , epidemics and pandemics , elections , brand management , information diffusion , public opinion , and healthcare. The domain of emotional intelligence (EI) has been relatively dawdling when it comes to utilizing the potential that social network platforms and their data can play in predicting the users' emotional intelligence. Even though there has been growth in recent times in using social media to understand the facets of EI , these studies have relied on traditional methods of data collection and self-reporting techniques (e.g., survey techniques or interviews) and statistical techniques (e.g., moderation and mediation analyses). Nevertheless, there has been a recent shift in using big data in the area of EI. Even though organizations are perfectly capable of generating big data on their employees, social media platforms (e.g., Facebook, Twitter, YouTube, Reddit, etc.) make the vast majority of contributions towards these big data. A study  encompassing leaders and industry heads found out that onethird of modern organizations are using big data to understand their employees. The organizations also believed that the traditional means of generating big data are competent for effectively managing behavioral and transactional data, but were uncertain about using social media data for generating business intelligence. This was echoed in other industry surveys , where business decisions were being effectively leveraged by these organizations, but less than 1% explained that they were able to use social media for human resource planning and organizational behavior. They believed that social media would transform organizational psychology by effectively studying the emotional intelligence of their employees, but a majority of these organizations had no idea where to even start looking for it. In our study, we concentrated our efforts on one particular social media giant, Twitter. This specific social media platform was chosen over its competitors because of its microblogging capability and the fact that it is arguably the fastest growing social media platform there is. Twitter is increasingly used by its users for multiple reasons, including discussing mental issues, sharing news and personal feelings, or expressing opinions about political and ideological themes in a brand\\/organization\\/celebrity discussion. The industrial community or researchers can access Twitter data through their APIs (application programming interface) to analyze the data for various domains of study. This study was carried out to build upon the understanding of social media in the context of emotional intelligence. This is achieved by using natural language processing techniques to analyze tweets containing contexts of emotional intelligence, and the associated people or users who were discussing EI on the Twitter platform, to develop insights into EI practices and research and the potential role that Twitter can play in this. This thought was echoed in studies on industry experts and organizations , as there is a clear lack of an insightful understanding of the concept, and there is very little literature to support this understanding. Even though there have been a lot of proposed frameworks for understanding emotion expression, textual emotion, and the underpinnings of emotions in real-time data, there is a significant lack in the literature when it comes to creating a framework that understands the contextual understandings of the discourse of emotional intelligence, especially in social media. The motivation of this study was to propose a framework that can be used to better understand how emotional intelligence is discussed in social media and social networks, and how these discussions are driving understandings of the emotional psyche. This study would thus try to create a framework on which future researchers and industries can build to generate knowledge from Twitter data in the EI community. Chae (2015) described Twitter Analytics (TA) as an analytical technique for analyzing Twitter data for a research outcome. They stated that TA is a combination of three analytics--descriptive analytics (DA), content analytics (CA), and network analytics (NA). We have tried to modify this framework to extract the information pertaining to emotional intelligence. These three analytical techniques focus on multiple magnitudes of Twitter data. The collected tweets and metadata covered the discussions of individual users, professionals, and organizations in terms of the concept of EI. Specifically, the findings of the analyses have tried to answer these research questions: (1) Are there any patterns in the characteristics of the information diffusion of emotional intelligence tweets? (2) Are there any dominant topics, content, or discussions that are being shared on Twitter regarding emotional intelligence? (3) Are there any patterns in the characteristics of the Twitter users who indulged in dialogues on emotional intelligence? (4) Are there any patterns in the sentiments of the emotional intelligence tweets, and what are the tweet contents that contain sentiments of emotional intelligence? Accordingly, the research is divided into sections as follows: Section 2 presents the present literature about the use of Twitter in multi-dimensional domains, Section 3 presents the data collection and pre-processing methodologies, Section 4 discusses the framework of Twitter Analytics, Section 5 provides an analysis of the collected tweets using TA, and the final sections conclude the study by presenting the research implications, limitations, and scope for future research. 2. Literature Review Since its inception in 2006, Twitter has become one of the biggest microblogging websites, with 500 million daily and 200 billion yearly tweets , and 150 million monetizable users. A study found out that over 75% of Fortune 500 companies have an active Twitter account, for their corporates and their brands. It has become one of the fastest information dissemination tools that allow for discussions, conversations, and even the spread of information that is true or false, making it one of the strongest assets for anyone with a voice. A tweet , which is Twitter's shared content, contains 280 characters, through which the users share their opinions and have real-time conversations. A tweet can be one of three types: an original tweet, a retweet, or a reply. All of these messages can be traced manually or by using Twitter's application programming interface (API). A popular tweet usually gets a status of \\\"trending\\\", which helps for easier reach and conversations with followers. Due to its increasing popularity, Twitter is being used in varied domains of practical and academic research, including stock market forecasting and predictions , brand management , crisis management , healthcare , finance , information technology and information systems , supply chain management , and psychology. When talking about emotions and emotional intelligence, the domains of psychology, philosophy, sociology, organizational behavior, etc. have been extensively researched over a long time. Initially, it was a part of biological evaluation, but with time, neuroscience has opened avenues for evaluating emotions as a socio-cognitive function. This shift in understanding the influence that emotional intelligence has on the limbic as well as neo-cortex systems, thereby creating a function of the neural system, has allowed the extraction mechanisms to become diverse--from primary and secondary data to more comprehensive experimental, experiential, and real-time big data and natural language processing. Many researchers have proposed their own frameworks for evaluating textual emotions  but these frameworks have a high presence of linguistic and methodological limitations. Moreover, these studies have focused primarily on understanding emotional triggers and have skimmed over the parameters of emotional regulation and intelligence. This study also fights the criticism of previous studies  that the verbal components of emotional discourse are usually missing in the evaluation of emotional intelligence. To address these issues, there have been several studies that have used real-time data to understand the underpinnings of the concept of emotion. Yet, despite the recent increase in the interest in using Twitter as a platform to study the domain of emotions, studies in the area of emotional intelligence are very scarce. One exception  used EI to study the perceptions of political parties. The objective of the present study was to identify the relationship between the tweet contents of emotional intelligence tweets among users, professionals, and organizations. The findings would contribute to the impact that Twitter has when it comes to understanding emotional intelligence in life and the workplace. 3. Data The data collected for the proposed framework required the effort of manual classification of the tweets for the analysis of the extracted tweets. The initial extraction was performed using keywords such as \\\"emotional intelligence\\\", \\\"ei\\\", \\\"eq\\\" etc. and this gave us the understanding that #emotionalintelligence was the most prevalent hashtag that could be used in the study. The tweets were collected from 14th February 2021 to 6th March 2021, and which included 53,361 emotional intelligence tweets and their content. The study was conducted using Python (version 3.8.7) using the Twitter API and the tweepy package of Python via Twitter. The tweets collected were public tweets, and private tweets were excluded from the collection. The privacy of the users was also maintained in the process, as the personal or private information of the users was removed from the analysis. A summary of the dataset is shown in Table 1. Data Pre-Processing The tweets of any user at any given time are usually of three types: textual tweets, visual tweets, and auditory tweets. To make the information in them useful, a great amount of data cleaning was required, also known as data pre-processing. This was achieved by creating data tokens and using only the textual tweets, according to the process explained in Angiani et al. (2016) and Gokulakrishnan et al. (2012). The steps involved in data pre-processing were: * Fixing grammatical, spelling, and punctuation errors; * Fixing slang, acronyms, and colloquialisms; * Removing numbers and digits; * Removing exercising, gym, and workout data; * Removing URLs by searching for http\\/https\\/www and removing the following text; * Removing contractions and negations; * Removing emojis and emoticons; * Removing non-ASCII characters (including non-UTF-8 Unicode); * Removing stop-words and extra spaces; * Converting all the text to lower case; * Stemming and lemmatizing the words 4. Framework for Twitter Text Analytics While data collections from social media platforms such as Twitter rely on APIs, the analysis is usually challenging, as the data have a lot of noise, are unstructured, and are substantially enlarged and enriched  in comparison with their traditional counterparts. An analytical framework is also not readily available , and hence, a framework encompassing the methods that extract and evaluate information from the data is required. The framework used in this study was initially developed in a study by Chae (2015) to analyze the Twitter dataset of supply chain tweets, which has been modified to evaluate the discourse on emotional intelligence. This framework has three analytics: descriptive analytics, content analytics, and network analytics. Figure 1 presents the relevant metrics of the analytical processes. 4.1. Descriptive Analytics (DA) In this process, we focused mainly on the descriptive statistics of the dataset. The descriptive metrics and the user metrics gave us direction into other user-related information that was used in the content and network analytics. 4.2. Content Analytics (CA) The data collected were primarily unstructured in nature, and hence, natural language processing (NLP) was used to pre-process, format, and transform the data for word analysis, topic modeling, and sentiment analysis. 4.3. Network Analytics (NA) With the help of the data and text obtained through the above processes, a network model was created using the GUI tool Gephi. The nodes were the Twitter users and the edges were the relationships between these users. 5. Results 5.1. Descriptive Analytics Descriptive analytics of the data from social media platforms is the initial building block for analyzing the social media data, not only for businesses but also researchers. DA was performed using the Python package Gensim  by using the understandings developed by Bruns and Burgess (2011), along with other statistical techniques. With the help of Python and its package, information about the users and tweets were extracted, and statistical techniques were used to visualize the statistics of the data. 5.1.1. Tweet Statistics Out of 53,361 tweets, unique users accounted for 41.36% (22,895), retweets accounted for 28.29% (15,096), and unique mentions accounted for 7.33% (4056), respectively. In total, 11,910 hashtags were used in these tweets, covering the four important factors of a traditional emotional intelligence model (motivation, self-awareness, empathy, etc.). 5.1.2. User Analysis There were 22,895 unique users in the dataset, indicating that every user, on average, sent 2.33 tweets, 1.52 retweets, and 0.81 mentions. Active users were calculated by using the formula (tweets + retweets + mentions), and visible users by the formula (retweets + mentions received). Figure 2 shows the active users and the visible users. The figure shows that the most active users were also the most visible users, which was expected. One important finding was that amongst the most active and visible users were the speakers that talk about motivation, emotional intelligence, and other soft skill topics. 5.2. Content Analytics 5.2.1. Word Analysis The most popular words found in the tweets were motivation (49,530 times in tweet texts), inspiration (13,383 times), empathy (2872 times), self-belief (1740 times), self-love (1566 times), care (1325 times), inspire (1328 times), emotional intelligence (1200 times), and self-care (1077 times), among others. 5.2.2. Topic Modeling To further classify the clustering of the words, we used the topic modeling technique using Python's Gensim package by creating a corpus and dictionary, according to the algorithm of Cao et al. (2009), and used these as inputs in LDA modeling. We were able to create four distinct topics with eight words in each topic. Table 2 presents the topic modeling outcomes from the word analysis. 5.2.3. Hashtag Analysis In total, 45,859 hashtags were found in the tweets, with the total occurrence of these hashtags being 447,747 times. The most popular hashtags were #motivation, #inspiration, #emotionalintelligence, #success, #goals, #empathy, #positivity, #happiness, #mindfulness, #selflove, #wisdom, #believe, #training, and #selfcare, including others. This showed that there were, on average, 8.39 hashtags per tweet, and the top three hashtags accounted for 15.74% (70,476 times) of the overall hashtag appearance in the tweets. 5.2.4. Sentiment Analysis Sentiment analysis was mostly done using the Python package Textblob , and the tweets were categorized into three major polarities, i.e., positive, negative, and neutral. Table 3 presents the percentages of the polarities from the sentiment analysis of our tweet data. The overwhelming majority of emotions of these tweets were of positive sentiment, with the smallest percentage for positive sentiment being self-awareness, with 88.53%. Neutral sentiments were also higher than negative sentiments in our topics, indicating that when the sentiments were not positive, they were tending towards neutral sentiments. Negative sentiments regarding the topics were very meager, with the largest percentage being 2.95%, attributed to self-awareness. To visually understand the sentiment analysis of the tweet, the first step was to create a tabulated representation of the percentage sentiment analysis of the tweet topics. Figure 3 shows a graphical representation of the percentage sentiment analysis of the tweet topics. Next, word clouds of the positive and negative sentiments were created. This was also achieved using the Textblob package of Python. Figure 4 shows the word clouds of the two sentiments according to their frequencies. It can be seen that motivation, great, life, and dawn were the most frequently occurring positive words, and people, uninspired, impossible, and insomnia were the most frequently occurring negative words. Standardization of these sentimentswas achieved using the SentiStrength package of Python. Even after standardizing the sentiment polarities of the topics, the normal distribution of these sentiments was pushed towards the positive side of the distribution. Figure 5 shows the normal distribution of the sentiment polarities of the dataset. It can be seen from the figure that even though the distribution of sentiment polarity is normally distributed, most of the values of the curve are in the positive region of the curve. A similar finding was observed when the entire dataset of the analysis was tested for sentiment analysis without the topics. Figure 6 shows the sentiment analysis of the entire dataset at the non-topical level. There was an obvious skew of the sentiment polarities towards the positive sentiment, with 68.44% of all the tweets ranging from 0 to +1, indicating an overall positive sentiment within the dataset. 5.3. Network Analytics A topological social network was created to understand the node and network-level metrics of the user data. The network graph was created using the open-source software known as Gephi. The nodes of the graph were the users that had interactions with each other using emotional intelligence as a content topic, and the edges were the mentions that the users gave or received using the @mentions process. There were 4058 nodes and 217 edges. The network diameter was 5, indicating that the network was tight but sparse. Figure 7 shows the network graph of the distribution. The average path length was 2.405, which indicates that all the nodes were, on average, at least three nodes away from each other. This is consistent with the typologies of other networks studied in the domains of emotions. To figure out the node connectedness, the in-degree of all the nodes was calculated to figure out the nodes' popularity. The findings showed that speakers (@DriverClassics, @gvhawtin), doctors (@Gleb_Tsipursky, @denisemose), and businesses (@i_GotQ, @MotivationBytes) were amongst the most popular, with high tweets and mentions. Table 4 presents a tabulation of high in-degree users. The graph also showed that the connectedness had 5% representation on average, further explaining that the network is sparse, with the largest community having 6% of total user representation, with the smallest community having 4% of user representation. 6. Discussion The study was conducted on 53,361 tweets to examine the characteristics of the emotional intelligence tweets and the users discussing the topic. In the preliminary secondary analysis of the topic, it was clear that the literature in this domain is somewhat scarce, as multiple articles have discussed emotions in Twitter users and their tweets , but there is relatively negligible research covering emotional intelligence in the Twitter users. This study was thus carried out to create a branch of findings that can be used to add to the existing literature on the study of emotional intelligence. The rate at which people talk about emotional intelligence on Twitter was contrary to what was expected, as across the data collection timeframe, the discussions were fairly consistent, with almost similar numbers of users, tweets, replies, or mentions being shared and discussed (see Table 1), but the interesting finding was the rate at which hashtags were used in those tweets. Even though a previous study found that users, especially professionals, used 12-23% of hashtags in their tweets , our study found that 85.94% of users used hashtags, with two or more hashtags in their tweets, which is consistent with previous findings. An average of five hashtags was used in the tweets, and a surprisingly high number for #motivation was found. One of the interesting findings of this study was that many popular tweets were shared by people in the motivational speaking business, closely followed by people working in the psychological profession, and employees. Nearly all of the tweets were originally created tweets. It was also apparent that the most active users were the most visible users as well. This also showed that the majority of tweets were made by a very small percentage of users, which was backed by the centrality analysis of the network diagram. The first 9% of the users accounted for 56% of the tweets, which is consistent with previous studies. In analyzing the sentiments of the tweets, the findings suggested that the overwhelming majority of the emotions were positive (as shown in Figure 4), which was consistent after standardizing the values of the sentiments. Good topic modeling performance was achieved in the study using the LDA model, which provided us with four distinct topics on which topic modeling could be carried out and the sentiment analysis could be modeled around. The four topics found were emotional intelligence, self-awareness, empathy, and motivation. The highest positive sentiment was found for motivation and the lowest for self-awareness; this is not saying much, as the difference between the highest and the lowest positive sentiment was 6.08% (highest = 94.61%; lowest = 88.53%). Negative and neutral sentiments were very low--almost marginal--when it came to their positive sentiment counterparts, suggesting that when people were talking about emotional intelligence and the topics related to it, the driving factors were positivity in their text. The findings showed that most of the people using the topics for their discussions of emotional intelligence were using Twitter to share their dealings in their personal, professional, and daily lives. These findings are consistent with previous studies which pertain to other fields. These were followed by EI advertisements, events, reports, and studies. As at the time of the study, the world was going through the distressful time of the pandemic, the platform of Twitter was being used as a tool to generate awareness about the advantages of being emotionally intelligent, which would explain the overwhelming positivity in the sentiments of the tweets, and the sparse but high user connectivity in the network analysis. Our findings on pandemicrelated emotional discourse are partially aligned with similar studies. This may also explain the disproportionately high diffusion of topics of motivation, empathy, self-awareness, and emotional intelligence, even though other prominent factors contribute to the understanding of emotional intelligence. Analyzing these discussions on Twitter provides new avenues in understanding the affective undertones of the users talking about a soft skill such as emotional intelligence. This study also shows that studying social media adds an invaluable source of information for professionals or academic researchers, and this information can be used to make more informed decisions and effectively deal with the issues attached to it. 7. Study Implications Twitter has been used by professionals in a myriad of ways  and the findings of this study indicated that professionals having conversations on emotional intelligence are more conversational and information-focused than ideological. This is important for people pursuing careers in the domain of emotional intelligence because the business environment is dynamic and ever-changing, and it is adamant for these professionals to continually upgrade themselves with changes in the sources of knowledge on emotional intelligence. It is also important to network and promote one's skill set and expertise on online platforms , not only for business professionals but also academic researchers. Professionals can take advantage of these metrics for learning, networking, and promotion. Sentiment analysis and topic modeling can provide a general consensus of the important topics that are being discussed, and what the overall sentiment of these discussions is. Similarly, network analysis can show the communication patterns amongst the users of interest and measure the intensity of these communications. This can also be used to understand the type of people one is about to hire in an organizations. Because organizations are swiftly becoming global and competitive, social media could become one of the better tools for hiring professionals who are not only experts in their domain of expertise, but are also socially adept enough to be intelligent in their adaptive behavior. Mining of user information can allow for a better perspective on these potential candidates through their tweets, retweets, followers, or other engagements. Some important research implications can also be derived from this study. The amount of data shared under each of the main hashtags of each topic was astonishing over the month-long study. They were also significantly diverse, as seen in the network topology. Emotional intelligence is also very relevant to other areas of research, such as behavior, personality, leadership, mental health, etc. This is also consistent with the findings of empirical studies in the area. The interdisciplinary nature of EI opens up new sets of diversions and cohesion for the future. 8. Limitations and Future Research Professionals and academic researchers in the area of emotional intelligence must use Twitter as an important source of information. However, the EI world has been tediously slow in figuring out the importance of effectively using social media and the prominence it plays in developing research, academia, and the industry. One limitation of this study is the data collection. The timeframe of collecting the data was very small--less than a month--and thus, a longer time duration would allow for a better understanding of the contextual nature of the conversations regarding emotional intelligence. Another thing was the abundance of the words and hashtags of emotional intelligence. Further studies can look into multiple words or hashtags, preferably the topics identified in this study, which would further provide a more comprehensive picture of the domain of study. Another way could be to use the preferential keywords that could be identified using analytical techniques to mine tweets that cover other factors influencing emotional intelligence that have been identified by other prominent researchers. It is important to build an understanding of how social media plays a prominent role in emotional intelligence. This has to be built through academic investigations and practical applications. This can be achieved by creating guidelines that allow an understanding of the diverse practices of emotional intelligence and innovation in academia, and also through measuring investments into the performance of emotionally intelligent individuals and teams. There is an increasing need in the area of emotional intelligence, and big data and social media can provide an incentivized breakthrough. 9. Conclusions In the present scenario of the dynamic environment, the study of emotional intelligence has increasingly become a significantly key research topic. The study of social media, big data, and natural language processing allows for analyzing, processing, and summarizing the information by using the affective and emotional spectrum of the content, which, in turn, allows measurement and analysis of the emotional competency of the users of online media. In the current domain of analysis of microblogging on emotional intelligence, there is an increasing need and demand for analyzing the role that emotional intelligence plays in users' interactions. This study also analyzed the hidden features and characteristics of the users and their shared content, and opens avenues for establishing the connections between the users and their shared content. This study aimed to explore the issues and evaluate the role that emotional intelligence plays in the users' interactions on the microblogging giant Twitter, by analyzing the content through a framework that studies the descriptive, content, and network analytics (DA, CA, and NA). However, due to the limitations and other objective factors, there are still some areas that can be improved in future studies. For future work, researchers could conduct a more holistic emotional expression analysis that could comprehensively study the individual users' characteristics, behavior, and personality, and how these can affect their judgments and decision-making. This is especially imperative for business organizations, as they are amongst the top contributors to the content on emotional intelligence on Twitter, and are amongst the most active and visible handles, which makes it clear that they are increasingly trying to use the platform and the theory of emotional intelligence to further better their business policies. This study thus opens up an avenue that can help both research academicians and industry professionals to increase their engagement and attention to further improve their workplace performance and outcomes.\",\"577240647\":\"1 | INTRODUCTION Behavioral finance has emerged as a key field of study in the area of investment management. It examines the rationally inexplicable beha vioral aspects of individuals and institutions transacting in financial markets. An increase in the investment activity of retail participants during the recent past has made understanding such behavioral aspects all the more critical. In this regard, scholars have observed that two distinct sets of factors drive retail investors' decision to invest: (a) rational factors related to traditional finance  and (b) irrational factors that come under the domain of beha vioral finance. Traditional finance assumes that investors consider the available information and act rationally while making a decision. In comparison, behavioral finance examines the effect of emotions and cognitive errors in financial decision-making. Behavioral finance explicates investor behavior by drawing upon insights not only from finance but also from psychology, sociology, and other related areas to examine behavior in varied markets that deviate from standard assumptions. Taking the debate further, the proponents of market efficiency and its as sumption that markets are rational use theories of traditional finance to explain the anomalies in the market. In com parison, behavioral finance supporters explain market anomalies through behavioral biases. In addition, the role of sentiment and emotions in investment decision-making is well recognized in the literature. For instance, Pineiro-Chousa et al. (2016) examined investors' social media activity to confirm that sentiments related to social media use impact stock markets. Similarly, scholars have noted that emotional stress associated with loss from invest ments may impact the quality of future investment decisions. Behavioral biases have attracted researchers' attention over the past several decades , with different biases being proposed and examined to explain retail investors' decision-making. In this regard, some of the recently examined behavioral biases are the disposition effect in the options market , overconfidence and underdog bias , home bias tendency while trading , and myopic loss aversion. While the accumulated knowledge is rich, the literature is deficient in terms of three aspects: First, there are limited prior studies on investor behavior in the face of an external stressor, such as a global health crisis, even though the existing scholarship has noted the impact of events, such as the Ebola virus outbreak  and various disasters  on stock market returns. It is important to acknowledge here that although such events wreak havoc, they provide natural experimental settings to assess investor behavior, in addition to offering opportunities for assessing the pricing and reaction of funds. The COVID-19 pandemic offers another such natural setting for ex amining investors' behavior to address the gap in the related findings. In concurrence, some recent studies have discussed the upheaval caused by the pandemic in the financial markets and its impact on investors, thereby underscoring the need to understand investor behavior better. In this regard, Bansal (2020) has suggested that the extreme volatility and market crash during COVID-19 should be analyzed through the lens of behavioral biases and related cognitive errors. Responding to these calls, the present study examines investors' behavioral biases and their influ ence on trading activity during the COVID-19 pandemic. Second, although scholars have acknowledged the effect of de mographic variables on the decisions of retail investors , no studies have focussed on millennials' (also called Generation Y) behavioral biases. Millennial investors are those born between the years 1981 to 1996 . Since they can be anticipated to be active in the stock market for the foreseeable future, given their age, a deeper understanding of millennials' beha vioral biases could help managers plan their future strategies. Due to this, the present study investigates millen nials' behavioral biases and their influence on trading activity during the COVID-19 pandemic. Third, although it is known anecdotally that trading in equity markets is driven largely by peers' and professionals' recommenda tions, no prior study has examined the association between beha vioral biases and recommendation intentions in this context. The link between investment and recommendation intentions has also been highlighted by the academic research. In this regard, scholars have observed that in addition to seeking economic gains, investors have certain social motives associated with their investment activity that may manifest as word of mouth or recommendation intentions. Given that recommendations can impact in vestment decisions, it would be quite useful for the concerned sta keholders to understand how various behavioral biases can influence the recommendation intent of retail investors. We propose to ad dress the paucity of insights in this context by examining the asso ciation between the behavioral biases and recommendation intentions of millennials during the COVID-19 pandemic. Cognisant of the gaps in the extant literature and the need to address them, the present study thus utilizes a mixed-method ap proach to elucidate the association between millennials' behavioral biases on the one hand and their trading activity and recommenda tion intentions during the pandemic on the other. Specifically, the study addresses two main research questions (RQs): RQ1. How do behavioral biases predict the equity trading activity of millennials in the face of an external stressor (i.e., COVID-19) that has both psy chological and economic implications? RQ2. How do behavioral bia ses predict the recommendation intentions of millennials related to equity trading under the impact of a health crisis? Scholars have examined different sets of behavioral biases in various contexts in the recent past. In addition, behavioral scientists have classified these biases into different categories for ease of un derstanding and evaluation. We have referred to one such classifi cation\\/taxonomy, proposed by Montier (2002) and simplified from Hirshleifer (2001) and Yalcin et al. (2016), to identify the biases to be examined in the present study. Accordingly, we developed a con ceptual model comprising eight biases as exogenous variables and millennials' trading activity and recommendation intentions as out come variables. We analyzed survey data collected from 351 Finnish millennials to test the theorized associations. Furthermore, drawing upon the past literature observing the presence of nonlinearity in behaviors, investment outcomes, and business phenomena , as well as the contention that biases are associated with irrational decision-making that goes against the postulates of the Theory of Expected Utility , we expect the association of behavioral biases with the proposed outcome variables to be nonlinear. Scholars have observed that in a given relationship, nonlinearity occurs when the marginal impact of an additional unit of change is not the same as that of the preceding one. However, the associations can be linear as well. In sum, recognizing that the re lationship of biases with outcome variables may be more complex than simple linear variations, the present study analyzed the data using the artificial neural network (ANN) method, which accom modates both linear and nonlinear associations. Finally, we followed this empirical investigation with a post hoc qualitative study to evaluate if the biases that manifested at the pandemic's beginning persisted as it advanced. The novelty of the present study comes from the following: (a) It investigates behavioral biases and investor behavior during the COVID-19 pandemic, thereby clarifying the effect of unprecedented outliers, such as global lockdown and the subsequent economic ra mifications, on investment decisions, (b) it highlights millennials as strategically important generational cohorts to be focused upon in the contemporary research on investment decision-making, (c) it examines recommendation intentions, which have remained under-explored in the prior literature on biases and investment decisions despite being recognized as a key influence in equity markets, and (d) it recognizes the possibility of the existence of both linear and nonlinear associa tions between biases, trading activity, and recommendation intentions. To our knowledge, past studies on investors' behavioral biases have not captured this possibility. Given that retail investors may have low acumen in investment matters , the possi bility of nonlinear associations cannot be ignored. 2 | BACKGROUND LITERATURE 2.1 | COVID-19 and the stock markets The COVID-19 pandemic made its first appearance in 2019 in China, and soon after, it spread around the world. Due to the scale of its impact, the World Health Organization (WHO) de clared it a global pandemic in March 2020. Countries responded to the pandemic by unveiling various control measures, including complete domestic shutdown, social distancing, travel restrictions, and provi sions of quarantine for those exposed to the virus. The massive scale of the lockdown produced severe economic chal lenges due to the complete stoppage in commercial activity and tourism and other consequences, such as job loss, lower growth, and damage to the supply side , which also adversely affected the financial system. For instance, the restrictions imposed in the wake of the pandemic increased the banking sectors' systemic risk in many affected economies initially , though the pandemic is expected to have high economic costs in the future as well. Even before the onset of the COVID-19 pandemic, scholars had raised concerns about the susceptibility and vulnerability of economies to health pandemics. Past studies have observed that, in the same way as economies, the stock market returns are also susceptible to major events. This contention was confirmed when the markets worldwide registered a substantial fall in their value, as presented in Figure 1, when the health crisis was declared a pandemic in March 2020. To elaborate, Figure 1 indicates that the stock markets worldwide showed a declining trend during the said period, losing 15%-20% of their value. Notably, scholars expressed that the financial crisis unleashed by the pandemic has been more perilous than the 2008 crisis , with financial markets reaching close to collapse in its wake. However, it is not entirely correct to compare the crisis caused by the COVID-19 pandemic with the 2008 crisis. The 2008 crisis was triggered by institutional structures and practices in the financial sector , beginning with the collapse of the subprime mortgage market in the United States , thereby making it easier to contain and counteract. In comparison, the pandemic has impacted all aspects of human life, which has made it much more difficult to control. Fur thermore, the 2008 crisis was largely a financial turmoil that affected the global economy, leading to a contraction in the gross domestic product. As the 2008 crisis unfolded, central banks in various countries introduced a slew of monetary policy measures to stabilize prices and financial markets. In contrast, in response to the COVID-19 pandemic, the governments focused on public health measures before initiating any economic recovery plans. In addition to its macro influence at the national and global level, the pandemic has also produced a micro effect at the individual level. Apart from impacting the movement and livelihood of individuals, the panic created by the life-threatening element of the pandemic could have impacted the psyche and behavior of retail investors, thereby causing them to make suboptimal investment decisions. The asso ciation between panic and stock market activity has been docu mented in the past literature, revealing the role of sentiment and irrational thought-process in investment decisions. For instance, scholars have observed that the panic created by terrorist attacks has caused temporary declines in stock markets in the past. This revelation is pertinent in the present context since scholars have compared the pandemic to ter rorist attacks due to the fear, panic, and uncertainty it has incited. The movement of stock markets worldwide confirmed the drastic impact of the pandemic on investors. One after the other, all markets registered a steep fall in March 2020. The US market, which had hit circuit breaker only once before, in 1997, hit it four times in the space of ten days in March 2020 . Parti cularly, the returns of the S&P 500 were severely impaired. The story in Europe and Asia was no different, with FTSE (UK's main index) plunging more than 10% within a day and Japan losing more than 20% of its peak reached in December 2019 . Such volatility not only eroded market capitalization at an aggregate level but also diminished retail investor wealth, potentially impacting their short- and long-run investment decisions and choices. Retail investors' decisions and choices can produce serious repercussions for the markets since retail trades can move stock prices , and the short-selling activity of retail investors can predict negative stock returns in the future. Given their potential to impact the market as a whole, we contend that it is important to examine retail investors' behavior in un precedented times, such as the COVID-19 pandemic, to better un derstand the aggregate market response. Moreover, since investors' preferences may be influenced by trends , the behavior of a set of retail investors causing a downtrend in prices may trigger panic in other investors. This bodes ill for broader market valuations and underscores the need to understand the drivers of investment\\/trading behavior. Furthermore, from the perspective of the investors themselves, they may make suboptimal investment decisions by becoming more risk-averse, low ering their growth expectations, expecting higher market risk premium, and reducing their trading activity , as observed in post-terrorist attack periods. This supposition is corroborated by the finding that there has been a general reduction in individual risk tolerance during the pandemic. Due to this, the present study proposes to examine the investment behavior of retail investors during the COVID-19 pandemic. 2.2 | Behavioral biases The debate around the existence of biases in decision-making is rooted primarily in Kahneman and Tversky's (1979) Prospect Theory. Examining behavioral biases can be expected to yield a better understanding of investors' decision-making process  as these biases represent errors of judgment in decision making  and suggestions regarding how investors process the available information to make decisions. Scholars have categorized these biases in many dif ferent ways. Shefrin (2000) classified the biases into two categories: heuristic-driven biases and frame-dependent biases. Pompian (2011) categorized them as cognitive and emotional biases. Montier (2002), meanwhile, gave a comprehensive taxonomy of biases, spanning three broad categories: self-deception, heuristic simplification, and social interaction. These three categories comprise nearly twenty biases. The self-deception bias captures overconfidence, self attribution, and hindsight bias. Heuristic simplification includes re presentativeness and anchoring\\/salience. In comparison, the last category, which is social interaction, includes biases like herding. In the present study, we have selected a set of biases to represent each of the three categories proposed by Montier (2002). 3 | CONCEPTUAL MODEL The present study examines how behavioral biases influence the trading activity and recommendation intentions of retail investors in the face of crises, such as the one posed by the spread of the COVID 19 virus (Figure 2). As mentioned above, the study has identified biases from Montier's taxonomy (2002). This taxonomy was pre ferred for two reasons: First, it is very comprehensive, since it in cludes nearly twenty biases that cover various cognitive errors manifested by investors, and, second, because it provides a clear delineation of biases into three categories, which makes it easy to understand why a particular bias is manifested. Furthermore, only some biases representing the three categories given by Montier were selected since it is not possible to examine all the biases in one study. These biases were selected in consultation with a panel of three professors (specialized in Marketing and Finance) and three practi tioners. The biases include overconfidence and self-attribution bias , hindsight bias, over-optimism, representativeness, anchoring, loss aversion, mental accounting, and herding. Although mental accounting is not a part of Montier's classification, it was included based on the suggestion of the panel. The variables used in the study are described in Table 1. 3.1 | Self-deception biases and trading activity Self-deception poses limits to learning that may prevent individuals from training their minds towards alternative thought processes. We have examined self-deception through over confidence, self-attribution, hindsight, and over-optimism biases. Overconfidence causes under- or overreactions in the financial markets  and may be exacerbated by an increase in experience that, in turn, results in the deterioration of returns from investment. Self-attribution is a manifestation of over-confidence, due to which investors attribute their success to the self and failure to others. Prior empirical studies have revealed that overconfidence can affect in vestment decision-making adversely and cause losses by decreasing risk-aversion , increasing market volatility , and instigating investors to indulge in trading excessively. Hindsight bias has been confirmed to exist in financial markets and is known to produce financial consequences. Individuals with hindsight bias tend to be lieve, albeit falsely, that they had foreseen the possibility of an event, such as the volatility of a particular stock, and accurately predicted its actual outcome. Such a tendency to over estimate their ability to predict movement in stock prices may cause investors to over-react by potentially indulging in heightened trading activity. Over-optimism represents individuals' tendency to think that the likelihood of their experiencing positive outcomes is more than that of them experiencing adverse outcomes, particularly if the related event is perceived to be controllable by them. Such optimism has also been found to exist in investment decision-making. Over-optimism related to investment decision-making may lead to a better-than-average effect in financial markets, which is linked with a high volume of trading. Scholars have also linked over optimismwith overconfidence , which again provides a basis for associating it with increased trading activity. Based on the preceding discussion, we expect the over confidence and self-attribution, hindsight, and over-optimism biases of millennial investors to influence their trading activity. 3.2 | Heuristic simplification and trading activity Heuristic simplification represents errors in processing information, in cluding the effect of emotions due to the limited mental capacity of humans. We have examined heuristic simplification in the present study through representativeness, anchoring, loss aversion, and mental accounting biases. Representativeness is a cognitive bias that causes investors to make forecasts based on the analogues they associate with something. For instance, investors may consider the realized returns as representations of expected returns in the future and act to sell or buy accordingly. Prior scholars have confirmed the influence of representative bias in investment decision making , revealing age and gender based differences in its manifestation. Such bias can influence the quality of investment made by individuals and adversely impact markets in the long run. For instance, during the COVID-19 pandemic, a tendency to compare the 2020 crash of the stock market with the 2008 crisis has been observed, which is a manifestation of representativeness bias. However, experts warn that the decline in the markets due to the COVID-19 crisis may continue longer than the duration observed in 2008, similar to the prolonged recovery witnessed after the 1929 crash. Anchoring bias causes investors to begin their estimates with a certain initial value before adjusting it thereafter. It can potentially influence decision-making and lead to poor in vestment choices based on limited and subjective information. Due to this, the bias can affect the market as well. For instance, anchoring bias can lead to a 52-week high momentum, especially when the investor sentiment is high. Fur thermore, it can influence buying and selling activity by causing investors to expect earnings to conform to past trends or shares to trade within a range. This can lead to under-reaction to changes in trends. Such bias is also present in market participants' estimates of the profitability of a firm. Loss aversion represents the investors' decision to sell stock that has risen in price and keep holding stock that has fallen. Emotions drive this decision and manifest less when the decision is being made for others. Loss aversion influences selling deci sions since investors react differently to the possibility of losses compared with assured profits. There are findings relating loss aversion to demographic profile as well, with investors who are unmarried exhibiting more loss aversion than married ones. Mental accounting causes investors to compartmentalize their in vestments separately into mental categories. To maximize returns and minimize risk, investors apply different policies in managing these categories and consider each to have a specific aim. Such cognitive error can adversely impact the portfolio selection process since it is lar gely driven by emotions. Scholars have also discussed mental accounting along with other biases as risk factors that can lead to suboptimal investment decisions at both the retail and in stitutional levels. Mental accounting has also been examined in the larger context of consumer behavior to explain aspects like inaction inertia. The above discussion confirms that heuristic simplification biases can influence investment decisions. Accordingly, we extrapolate the same outcome to the millennial investors and suggest that these biases will influence their trading activity. 3.3 | Social interaction and trading activity Social interaction represents human beings' intrinsic social nature, which results in the horizontal transmission of information across social groups. In the present study, we have examined the impact of social interaction on millennials' investment decision-making through herding bias. The tendency to trust others' judgment, manifested as herding bias, has been cited as a key cause of bubbles, volatility, and crashes in financial markets. Other studies have also confirmed that herding bias produces adverse con sequences for markets and causes financial losses for investors (; 2019). Focusing on demographic factors, Lin (2011) observed that men exhibit lower herding bias than women, and younger investors ex hibit this bias more than older ones. Taking these findings forward, other scholars have also examined the effect of demographic characteristics, including age, on herding bias. Based on the preceding discussion, we expect herding bias to influence millennials' decision to imitate others and trade more if others are doing it. 3.4 | Behavioral biases and recommendation intentions Recommendation intentions or positive word of mouth is a well researched form of consumer behavior and has been examined in various contexts, including online products and services, mobile wallets, hospitality, and so on. Although the prior studies on behavioral bias have not investigated the association between these biases and recommendation intentions, the existing literature does in dicate its importance. Montier's taxonomy itself, which is the basis of the present study, has identified social interaction and transmission of information across social groups as a potential source of bias. Scholars have also argued that, in the future, the individual decision-making process might be better understood by evaluating group decision-making processes. Re commendation intentions are particularly important to understand in the case of millennials as they represent generational cohorts exposed to information technology at a young age , which enables them to stay connected with others. In fact, this gen eration has been called digital natives as they are known to maintain a continuous online presence. Furthermore, millen nials are more likely to consult others while making decisions. In the general context of consumer decision-making, scholars have also noted the rising importance of re views available online , which under scores the importance of consumer recommendations. Given their connectivity and dependence on others for decision making, including investments, it is plausible to assume that millen nials would be inclined to share their knowledge about investment with their social groups. Since the recommendations made may in fluence the investment decisions of such groups, it is important to understand how millennials' behavioral biases shape their re commendation intentions. This can help understand how contagion and panic in the stock market may cascade at the retail level through the communication of investors with each other. 4 | DATA AND METHODS 4.1 | Data collection We used a mixed-method approach to achieve the objectives of our study. The first part comprised quantitative data collection, and the second part comprised a post hoc qualitative study. We collected data for empirical analysis through a cross sectional survey conducted in Finland. The respondents were re cruited by specifying three screening questions: (a) They should have been born between 1981 and 1996, (b) their gender should be male, and (c) they should have experience with trading in equity. These screening criteria enabled us to recruit respondents from our target sample, which comprised Finnish millennial males active in equity markets. Finland was chosen to collect data based on recent reg ulatory developments. In March 2019, the Parliament in Finland ac cepted the law related to equity savings accounts to encourage equity investment. In this regard, millennials constitute 50% of the accounts opened so far, with the majority being males. The questionnaire was developed by adapting pre-validated scales from prior studies in the area of behavioral finance. The fol lowing scales were derived from Baker et al. (2019). Overconfidence and self-attribution, hindsight bias, representativeness, anchoring, mental accounting, and herding. Over-optimism was operationalized by adapting the scale from Barrafrem et al. (2020), while the loss aversion scale was adapted from Chun and Ming (2009). For trading activity, items were developed based on Milgrom and Stokey (1982) and Barber and Odean (2000), and recommendation intentions were captured through a scale adapted from Riquelme et al. (2016). Before finalizing the questionnaire, we followed the due procedure to ensure the face and content validity of the survey items, in consonance with recent studies. To this end, we first presented the preliminary questionnaire to a panel of three pro fessors (specialized in Marketing and Finance) and three professionals experienced in advising retail investors. Corrections suggested by them were used to modify the questionnaire, which was then pilot tested with ten respondents representing the target sample. These respondents' feedback was then used to change the language of the items further as required. This process culminated in the preparation of the final questionnaire. The survey was conducted during May 2020 to capture the re sponses during the ongoing COVID-19 pandemic. Due to this, mul tiple touchpoints were used to solicit a response. The online survey link was shared on WhatsApp and Facebook groups and different Facebook pages related to equity investment. To control for self response bias, we informed the participants that their anonymity would be maintained, and no personal information except basic de mographic details would be collected. Furthermore, we did not dis close that the purpose of the study was to measure behavioral biases. This was done to prevent biases in response that may manifest if the respondents knew the purpose of the study. No incentive was offered for responding to the survey. A total of 380 responses were received, of which 351 were taken forward for analysis after the removal of incomplete responses and outliers. We followed the empirical analysis with a post hoc qualitative study conducted through open-ended essays. To this end, we re cruited 19 male millennial investors as participants through Prolific Academic. The objective of this study was to evaluate whether the biases that manifested immediately after the onset of the pandemic persisted as it advanced. Such insight can be useful to understand how the panic and uncertainty associated with a crisis drive biases initially and how coming to terms with the unfolding situation changes them over time. 4.2 | Methods We considered three potential methods for data analysis, covariance based structural equation modelling (CB-SEM), variance-based structural equation modelling (VB-SEM), and ANNs, based on the nature of the data and research questions. With regard to the nature of the data, Henseler et al. (2009) suggested that the suitability of an analysis method depends on its requirements related to the size of the sample, outliers, normality, multicollinearity, linearity, and homoscedasticity. In this context, if the research questions are grounded in a conceptual model based on a strong theoretical framework, and the collected data meets the requirements mentioned above, then CB SEM is suitable. Similarly, if the research questions are related to theory-building, and the collected data does not conform to some of the requirements mentioned above, VB-SEM is suitable since it is lenient about sample size, outliers, and normality. In comparison, if the research questions mandate the detection of both linear as well as nonlinear relationships to uncover the predictive capacity of exogenous variables, then ANN is better suited for data analysis, as suggested by Hew et al. (2019) and M. Talwar et al. (2021). ANN method utilizes artificial intelligence to generate a solution and is lenient on the various data-related assumptions mentioned above. In the present study, we have used ANN to analyze data since our objective is to detect both types of relationships and determine the predictive power of biases in the context of millennials' trading ac tivity and recommendation intentions. We performed the ANN analysis using sklearn and multi-layer perceptron in Python and SPSS. ANN uses neurons distributed in multiple layers, grouped as in put, output, and hidden. The network is based on the principle of how the human brain works and learns new information. This learning occurs through a training process that in volves forward iterations and backward propagation of information to fine-tune the output. The learned in formation is stored in the model as synaptic weights. These weights are then adjusted using activation function (sigmoid function in this study) and through the propagation of errors in the backward di rection. The key idea here is to reduce the gap between the actual and the desired output over numerous iterations to reach the level where bias is minimized. We employed a cross-validation process for analysis, wherein the data were bifurcated into two parts, training and validation, as sug gested by scholars. Moreover, we utilized 70% of the data as training data and the balance 30% for validation to avoid over-fitting. To assess the model's prediction ac curacy, we evaluated the root mean square error (RMSE) value, as recommended by Samuel et al. (2014). The relative influence of each exogenous variable was gauged through sensitivity analysis, in which we first calculated the relative importance of each variable and then expressed it as the proportion of the highest value. 5 | RESULTS 5.1 | Validity and reliability of the instrument The present study computed Cronbach's a and the composite reliability (CR) to confirm the reliability of the scale, in line with prior literature. Both criteria for reliability were met, in line with the recommended cut-off value of 0.70 , except in the case of over-optimism, due to which it was excluded from analysis (Table 2). Next, the instrument's validity was assessed through con vergent and discriminant validity, as suggested by prior methodological literature. In this context, the average variance ex tracted (AVE) for all of the constructs taken forward was greater than 0.50, thereby confirming the convergent validity (Table 2). Similarly, square roots of the AVEs were generally higher than the pairwise cor relations, confirming discriminant validity (Table 2). 5.2 | Data diagnostics 5.2.1 | Normality, multicollinearity, linearity, and homoscedasticity We used the Kolmogorov-Smirnov (K-S) test to evaluate if the data under the study were normally distributed. Since the null hypothesis of normal distribution was rejected (p > .00), we concluded that the data followed a non-normal distribution. This outcome provides another justification for using ANN. Next, the multicollinearity issue was examined by computing values of toler ance and variance inflation factor (VIF), as suggested by prior studies. All tolerance values were greater than 0.1, and VIFs were below the threshold value of 3, which confirmed the ab sence of collinearity issues in the data. The absence of multicollinearity was also confirmed by the value of the correlations between the exogenous variables being less than 0.90  (Table 2). In consonance with Hew et al. (2019), we conducted the ANOVA test to confirm the type of relationship (linear vs. non linear) between the variables. The test confirmed that both trading activity and recommendation intentions have a nonlinear relationship with representativeness, anchoring, and herding biases. The existence of these relationships yields further support for the use of ANN. We also examined the data under study to understand whe ther it was homoscedastic or heteroscedastic. To this end, in consonance with Hew et al. (2019), we generated a scatter plot to visually examine if the residuals were evenly distributed around the fitted line. As seen in Figures 3 and 4, the trading activity data is homoscedastic, whereas the recommendation intentions data is not. However, the heteroscedasticity of the data was not a con cern in the present study since the feedforward neural network estimates are better than those of the weighted least square re gression in the instance of deviation from homoscedasticity. 5.2.2 | Common method bias (CMB) The issue of CMB may exist in the present study since the data for all of the variables were collected through a single instrument in one wave. Due to this, we used a two-pronged approach to control and assess the issue of CMB, as suggested by Podsakoff et al. (2003). Due attention was paid to the questionnaire design and protection of the respondents' anonymity. Next, we conducted Harman's single factor test, which revealed that one factor explained only 26.82% of the total variance. Being less than the suggested threshold of 50%, this value confirmed that CMB was not an issue in the data under study. 5.3 | Validation of ANN Alternative ANNmodels were generated with seven input neurons, three hidden neurons, and two output neurons (Figure 5). Based on the RMSE values presented in Tables 3 and 4, we can conclude that these models have a high prediction accuracy. This is further confirmed by the fact that the mean values of the RMSE of the training and validation data are 0.1455 and 0.1484 for trading activity and 0.1972 and 0.2197 for re commendation intentions. We have reported RMSE values in consonance with recent studies that have applied ANN for data analysis. 5.4 | Sensitivity analysis The present study conducted a sensitivity analysis to assess the comparative influence of biases by computing their normalized im portance. This was calculated by expressing each value as a percentage of the highest value. As presented in Table 5, in the case of trading activity, herding is the most important bias, followed by hindsight, overconfidence and self-attribution, representativeness, and anchor ing. As presented inTable 6, in the case of recommendation intentions, herding is the most important bias, followed by hindsight, over confidence and self-attribution, representativeness, anchoring, mental accounting, and loss aversion. 5.5 | Post hoc qualitative study We followed our quantitative empirical investigation with a post hoc qualitative study to understand whether the biases that manifested during the early part of the pandemic were still playing a role in influencing millennials' trading activity and recommendation inten tions. To this end, we collected and analysed data from 19 male millennials with varying degrees of experience in equity investment. The relevant details of the participants are presented in Table 7. We developed the questions for the open-ended essay by using key descriptors of each bias (as given in Table 1). To elaborate, we formulated eight questions, one for each bias, with two subparts. One subpart was related to the influence of the concerned bias on trading activity, and another was related to the influence of the concerned bias on recommendation intentions. The questions and sample re sponses are presented in Appendix A. Each author independently assessed the responses and prepared a matrix recording whether a given bias influenced the participants' trading activity and re commendation intentions. Since the responses were quite clear and distinct, there was no inter-coder disagreement. We consolidated the independent codes into a single table, using a tick mark to indicate the presence of a bias. The results are presented in Tables 8 and 9. 6 | DISCUSSION OF RESULTS AND IMPLICATIONS Using Montier's (2002) taxonomy as a reference, we identified eight behavioral biases (i.e., overconfidence and self-attribution, hindsight bias, over-optimism, representativeness, anchoring, loss aversion, mental accounting, and herding) to examine their relative influence on millennials' trading activity and recommendation intentions during the pandemic. All biases except over-optimism were found to exist among this population. The absence of over-optimism implies that millennials do not feel that financial advisors are redundant. It also indicates that they do not live in the moment. This result is not as anticipated in light of the prior extended literature. A potential reason for the absence of over optimism could either be due to the mental make-up of the millen nials or the effect of the pandemic. However, before drawing any firm conclusion about the absence of over-optimism in millennials, further analysis with a larger sample size drawn from different geo graphies is required. The extent of the influence of other biases on the endogenous variables and the potential reasons are discussed below. 6.1 | Biases and trading activity The results revealing the relative influence of the biases on trading activity are presented inTable 10. Herding is the most important bias predicting trading activity as its relative importance is 100%. The association between the two is nonlinear and positive, as observed from their correlation. The outcome indicates that a higher herding bias increases the trading activity in the face of external stressors like a pandemic. This also implies that millennials tend to consult others and are impacted by their stock market reactions. Accordingly, they are likely to buy and sell more stocks during a crisis if others are doing the same. Such heightened trading activity when markets are volatile can result in financial losses for individual investors and impact the market's volatility further, as argued by prior scholars. This finding is in consonance with past extended lit erature. Hindsight is the second most influential bias for trading activity, with the value of influence equal to 87%. In addition, the relationship between the two is linear and positive, as seen from the correlation between the two. This implies that millennials with hindsight bias, believing that they had been able to predict past collapses in stock markets and even the initial crash during the COVID-19 pandemic, would indulge in heightened trading activity in the face of an external stressor. Such a tendency can be extremely risky since it might lead to adverse financial consequences for them, as argued by the existing scholarship. Put differ ently, individuals with hindsight bias tend to believe, albeit falsely, that they had foreseen the possibility of an event, such as the vola tility of a particular stock, and accurately predicted its actual outcome. Such a tendency to overestimate their ability to predict is an indicator of the high level of overconfidence, which may cause them to over-react by potentially indulging in heightened trading activity. The third bias that influences trading activity is overconfidence and self-attribution, with a relative importance equal to 62%. Fur thermore, the relationship between the two is linear and positive, implying that the millennials who think that their actions, knowledge, and opinions are responsible for the increase in the value of their investments and that their skills can help beat the market are likely to trade more in situations like a pandemic. The result aligns with our anticipation based on the prior findings. This bias makes the millennials vulnerable to making investment\\/trading decisions that may not be in their favor. In other words, overconfidence and self-attribution bias is potentially pre carious since it may decrease investors' risk-aversion, as discussed in past studies , and increase their market volatility as well. Representativeness is the fourth most influential bias for trading activity, with the value of influence equal to 48%. Moreover, the relationship between the two is nonlinear and positive. The finding confirms that millennials who estimate future prices based on current stock price and depend on past prices and earnings for decision making are likely to trade more during a crisis. This result is in con cordance with our expectations based on prior studies. The risk, in this case, is that such cognitive errors can influence the quality of investments and impact markets negatively in the long run. The next bias in the order of importance is anchoring, with the value of influence equal to 12%. Furthermore, its relationship with trading activity is nonlinear and negative. The influence of anchoring is relatively small in magnitude. The presence of this bias implies that the millennials who take their stock buying and selling decisions by keeping past prices and purchase price in mind and tend to hold a falling stock until it returns to its purchase price are unlikely to in dulge in heightened trading activity during a crisis. Since anchoring bias causes under-reaction, especially at the time of high investor sentiment , the presence of this bias may cause these millennials to miss out on some good investment opportunities by under-reacting when the markets are moving, especially during a crisis. The relative importance of the remaining two biases, loss aver sion and mental accounting, is zero, indicating that they do not in fluence millennials' decision to trade during a crisis. This implies that the tendency of retail millennial investors to hold on to falling stocks and their nervousness about their investments in the presence of an external stressor does not predict their trading activity. Similarly, their tendency to put different investments in different mental ca tegories and not look at their portfolio as a whole also does not influence millennials' trading activity during a crisis. These results are contrary to prior findings that revealed the impact of loss aversion and mental accounting on retail individuals' investment behavior. The reason behind this unanticipated outcome could be situational or cultural. Due to this, further studies based on diverse samples, settings, and multiple countries are required to fully understand why these two biases do not play any role in influencing millennials' trading activity during a crisis. 6.2 | Biases and recommendation intentions The results revealing the relative influence of the biases on re commendation intentions are presented in Table 11. Herding is the most important bias predicting recommendation intentions as its relative importance is 100%. In addition, its relationship with inten tions is nonlinear and positive, as seen from the correlation between the two. This outcome implies that higher herding bias increases millennials' recommendation intentions in the face of an external stressor like a pandemic. The finding confirms that the millennials, who are likely to consult others and be impacted by their reactions in the market, are more likely to recommend equity investments. This could be detrimental for the efforts to stabilize the markets since the recommendations may get translated into actual buying or selling, without much knowledge, by those who have received the re commendations, thereby increasing the market's volatility, as con tended by prior studies. We had anticipated millennials' behavior to exhibit recommendation intentions given their tendency to consult others while making decisions  and be part of an online world of con stant connectivity. Hindsight is the second most influential bias for recommendation intentions, with the value of influence equal to 25%. Moreover, its relationship with intentions is linear and positive, as seen from their correlation value. This indicates that millennials with hindsight bias, having the tendency to believe that they had been able to predict past collapses in stock markets and even the initial crash during the COVID-19 pandemic, would show positive intentions to recommend equity investment during such a crisis. Such tendency can have ad verse financial consequences for their peer group who might act on millennials' recommendations, as argued by prior extended literature. In other words, re commendations given by millennials can potentially lead to im prudent investments by their social group, causing losses and panic in the market. In sum, our finding indicates that a belief in their hind sight, which makes them overconfident , causes millennials to feel that they know enough to advise others. The third bias that predicts recommendation intentions is over confidence and self-attribution, with the value of influence equal to 17%. Moreover, its relationship with intentions is linear and positive. The result indicates that the millennials who feel that their knowledge of investing and ability is superior to that of financial analysts are more likely to recommend equity investments to others in situations such as a pandemic. Additionally, in believing that they can exceed market returns, such millennial investors would be quite keen about making equity investment-related recommendations to others. This tendency of making recommendations can be quite detrimental for others since the advice received may cause them to act in a less circumspect and less risk-averse manner, as contended by past stu dies , and have a cascading effect of increasing market volatility. Representativeness is the fourth most influential bias for re commendation intentions, with the value of influence equal to 12%. In addition, its relationship with intentions is nonlinear and positive. This finding indicates that the millennials who decide to buy a stock based on its past price and firm performance and who forecast price changes based on recent prices will show a higher tendency to re commend equity investments during a crisis. Although there is no a priori basis in the literature to support this finding, a plausible reason behind such behavior could be millennials' strong belief in their de cisions based on the available information. To express this differently, millennials may believe that analyzing relevant data can help in su perior stock selection, making them feel more confident about re commending the same to others. However, such recommendations are quite risky since they might be based on misjudgements, as dis cussed by existing scholarship. Such mis judgements may result from the fact that retail millennials investors may identify naive patterns in stock price changes and trade in less effective ways, as observed in the case of other individual investors. By sharing their misjudgements with others, these millennials might transmit cognitive errors that can potentially harm the markets and adversely impact the quality of investments of the recipients of such suggestions. The remaining three biases: mental accounting, loss aversion, and anchoring, have a very low relative influence on the recommendation intentions of millennials, with the value of influence being 2% for mental accounting and 1% each for the other two. Furthermore, while the relationship of mental accounting with intentions is linear and positive, both loss aversion and anchoring have a negative re lationship with it. However, the relationship of loss aversion is linear, whereas that of anchoring is nonlinear. Since the present study is the first empirical endeavor to examine these relationships, there is a need to examine them in various contexts and evaluate the variances in the outcomes before drawing any conclusive inferences regarding the reasons behind the weak influence of these biases on re commendation intentions of millennials during a crisis. It can also be observed that the influence of biases is greater in the case of trading activity than recommendation intentions, in dicating that trading activity is predicted to a greater extent by the biases than recommendation intentions. Since the findings of our study are based on the analysis of data collected from millennial investors in the initial phase of the pandemic, we followed them up with a post hoc qualitative study. The purpose of this study was to evaluate if the manifestation of biases and their influence on trading activity and recommendation intentions have changed with the advancement of the pandemic or not. The results of this study, presented in Tables 8 and 9, reveal that the manifestations and the impact of biases on the two endogenous variables have changed, though not drastically, in the later phase of the pandemic. Specifically, most participants have accepted herding to have played an important role in influencing both their trading activity (74%) and recommendation intentions (47%). Our quantitative analysis had also in dicated the same outcome. This implies that consistently throughout the pandemic, millennials made their stock buying and selling decisions based on the actions and judgment of others, a dependence that also motivated them to make recommendations to their social group to some extent. A potential reason behind this could be that, like all others, millennials also depend more on their social groups in times of crisis, resulting in in creased interaction during the pandemic. Another reason could be that a reliance on social media has increased with enforced social distancing norms during the pandemic, making it easier to send and receive information. Interestingly, many participants have not considered hindsight bias to be influential, with 47% expressing its effect in the case of their trading activity and 21% in the case of recommendation in tentions. This outcome contradicts the findings of our quantitative study, wherein hindsight was the most prominent after herding. This result indicates that the belief that they could have predicted the movement already occurred in some stocks during the pandemic did not play as significant a role later on in influencing millennials' trading activity and recommendation intentions, as compared to the initial phase of the pandemic. A reason behind this could be that global stock markets scaled new heights quickly after crashing extensively at the beginning of the pandemic, taking millennials, like most other investors, by complete surprise. No one could have predicted or anticipated such a recovery, with the pandemic still impacting eco nomic activity. In comparison, although less than herding, most participants agree that two biases, overconfidence and self-attribution and representative ness, have been influential in impacting their trading activity (nearly 58% each) and recommendation intentions (42% and 47%, respectively). The result in the case of overconfidence and self-attribution indicates that past knowledge about investing and past successes caused millennials to buy and sell more stocks as the pandemic progressed. It also made them feel more confident to make recommendations to others. The gains made after the markets' recovery could have played a major part in the in creased manifestation of this bias. Similarly, the influential role of representativeness indicates that as the pandemic progressed, the millennials felt that it was better to anticipate the movement in future prices based on the past prices of stock while undertaking trading and recommendation decisions. A potential reason behind this could be the faster recovery of stocks that were trading high before the onset of the pandemic, which may have reinforced the perception that past prices provide a reliable cue for future movement. In the case of anchoring, 63% of participants confirmed its in fluence on trading activity, and 37% confirmed its influence on re commendation intentions. This finding implies that millennials tended to forecast future stock prices as the pandemic progressed by fo cusing more on certain information. This also encouraged many of them to make recommendations to their social group. Such behavior is quite understandable since public and media debates have been spotlighting key sectors and activities that could gain more promi nence with reference to the new normal enforced by the pandemic. These discussions could have attracted the attention of millennial investors, making them focus on specific information related to cer tain stocks and sectors. Our quantitative analysis had revealed that loss aversion and mental accounting do not influence trading activity at all and have a very small effect on recommendation intentions. However, the post hoc qualitative study indicated that these two biases influenced the recommendation intentions of a substantial number of participants (47% and 32%, respectively). In comparison, in the case of trading activity, 32% of participants indicated the influence of loss aversion, and 37% confirmed the influence of mental accounting. The influ ential role of loss aversion implies that millennials experienced regret for incurring losses in stocks during the pandemic, which caused them to consciously focus on avoiding future loss and regret. Such experience also impacted their recommendation intentions to some extent. A potential reason behind this could be that some suboptimal investments made during the initial phase of the pandemic did not turn out as anticipated, making the millennials experience regret and subsequently cause loss aversion to influence their decisions as the pandemic progressed. In comparison, the influence of mental accounting on trading activity and recommendation intentions indicates that millennials exhibited the tendency to make separate investments for different purposes, such as education, buying a home, travelling, etc., as the pandemic progressed, and advised others to do the same. A potential reason behind this could be the personal experience of a reduction in income, the loss of a loved one, or the general environment of in security prevailing during the pandemic that could have made mil lennials seek safety and become more risk-averse. Finally, in consonance with the results of the quantitative data analysis, the results of the qualitative data also indicated that over optimism bias does not have much effect on trading activity and recommendation intentions, with only 26% of participants confirming its influence on trading activity and only one participant indicating its influence on his recommendation intentions. This implies that mil lennials do not think it is better to focus on good news only while trading or making stock recommendations during the pandemic. A potential reason behind this could be the milieu in which the mil lennials have grown, wherein they have been exposed to multiple aspects of any event and know better than to focus on limited factors while making any decision. In addition, a comparison between Tables 8 and 9 indicates that the biases play a more prominent role in influencing trading activity than their influence on millennial investors' recommendation inten tions. This is consistent with the findings of our quantitative analysis. 6.3 | Theoretical implications The study makes five theoretical contributions: First, it answers the re search call to empirically examine the impact of behavioral biases on retail investors' stock market behavior, thereby underscoring the importance of behavioral finance and the related asset pricing models. In addition, the empirical findings of our study are based on data collected through a primary survey, an approach that has seldom been used to examine biases. Furthermore, although beha vioral finance has emerged as a key part of mainstream finance to explain biases that impact investment decisions , past studies have focussed more on the existence of biases and their asso ciation with financial decision-making\\/investment decision-making in general. In comparison, a specific focus on trading activity and recommendation intentions has remained under-explored, even though trading activity in the form of buying and selling can affect prices and subsequently cause market volatility. At the same time, the recommendations of peers and social groups can cause retail investors to make misjudgements and make suboptimal de cisions that can adversely affect their investment portfolios. Clearly, un derstanding the drivers of both trading activity and recommendation intentions is very critical. Notably, this is the first study to empirically examine recommendation intentions as the outcome of behavioral biases, even though recommendations and mutual consultations are quite pop ular in practice. Thus, by revealing the impact of biases on retail investors' trading activity and recommendation intentions, our study generates useful insights to explain irrationalities, as represented by biases, that can affect stock prices and lead to market inefficiency. In sum, our findings strengthen the accumulated knowledge available for the reference of key stakeholders by exploring novel associations. More im portantly, the continued persistence of most biases with the advance ment of the pandemic, as revealed by our qualitative study, indicates that they were not manifested under the impact of the fear and panic incited by the pandemic alone; rather, these biases are a deeper part of investors' psyche, making them all the more critical to examine and understand. Second, our choice of millennials as the target segment yields new theoretical findings related to biases and their effect on trading activity and recommendation intentions. The insights related to millennials are quite important since they represent a group that is likely to remain active in the market for a long time. In addition, an enhanced understanding of their behavior can be useful for future research in the area. The focus on millennials as the target group is also in concordance with the growing trend of examining consumer be havior in the context of generational cohorts. Moreover, scholars and investment consultants worldwide have noted certain aspects of the milieu in which millennials have grown that makes them and their financial decision-making a significant consideration. For instance, according to a survey from asset manager BlackRock, millennials save more than Baby Boomers in the United States and have become more interested in investing during the past few years. This makes them an important segment to examine since their decisions and biases can impact market volatility. In the specific context of this study, a report published by Deloitte in 2018 revealed that Nordic millennials, including Finns, in vest more than their generational counterparts in other countries. Therefore, our findings explicating their behavioral biases can be of use to multiple stakeholders. Furthermore, being the digital natives that they are, millennials tend to use technology-based approaches and social media tools to make their investment decisions and remain connected. Due to this, they are in a position to influence the investment decisions of others as well. Third, our study captures retail investor behavior during the COVID 19 pandemic, a crisis that has placed the economic and social structures worldwide under severe stress. Since the prices of stocks are considered to move in response to several anticipated, un anticipated, and unknown factors, academic research has continually examined various aspects of financial markets, including investor beha vior, to evaluate the impact of a variety of events. The ongoing thought process is to have accumulated learnings based on the investigation of dynamic factors, which would help formulate strategies, design educa tional courses, develop risk-hedging products, and so on. The pandemic is another interesting context in this regard since it represents a crisis and an extrinsic stressor. By explicating investor behavior in the presence of such extrinsic stressors beyond the control of individuals, firms, and regulators, our study provides useful theoretical insights that can serve as the basis for predictive modelling to forecast the related outcomes in future crisis events. Fourth, since heuristics offer a way of simplifying the decision making process in a dynamic environment, one way to prevent the influence of biases is to develop algorithms that can be applied un emotionally and with ease, as discussed by Otuteye and Siddiquee (2015). The findings of our study can thus serve as a basis for de veloping such algorithms by researchers in the area. Lastly, the study contributes to methodological advancement in the area by applying an advanced data analysis technique, ANN, which addresses the deficiencies of the popular structural path analysis methods' specific data-related requirements. At the same time, ANN is not challenging to apply since it is a part of SPSS and is a well-recognized, albeit less used, method. Moreover, ANN is ac knowledged to be a suitable prognostic method to analyze data in situations where other statistical tools are not applicable. The key advantages of this method are its ability to learn through a training process, its fault tolerance, its versatility in ana lyzing nonlinear data, and its ability to generate optimal results through updating weights continuously. This intelligence, inspired by the human brain and biological neurons, makes ANN a more robust choice for analyzing nonlinear data as compared to nonlinear structural equation modelling. 6.4 | Practical implications The study findings offer five practical inferences for regulators, millen nials, and firms. First, from the regulatory perspective, the confirmation of the existence of behavioral biases among millennials indicates a need for investor education efforts directed towards this generation to reduce such biases. This is important since biases lead to irrational decisions, which can potentially cause losses in retail investors' equity portfolios. It is also important to address these biases since various stock market bubbles and depressions that have harmed the interests of individuals and economies have been attributed to investor irrationality and sentiment. Furthermore, the findings of our study provide inputs for formulating and revising investor protection policies, in line with the past contributions in the area. Second, by uncovering the effect and relative importance of key behavioral biases influencing the trading activity and recommenda tion intentions of retail investors, our study provides useful in formation for firms offering investment advisory services to such investors. Knowing the effect of behavioral biases on investors' de cisions can help these firms offer advice in line with their expecta tions, perceptions, and thought processes. This is particularly crucial since most countries have some guidelines for investor protection, which mandate that potential investors should be offered investment products aligned with their objectives and risk appetite, as also dis cussed by scholars. Third, from the perspective of the millennials themselves, the study reveals the presence of behavioral biases that can affect their trading activity and recommendation intentions irrationally, which, in turn, can adversely impact their equity returns. Since, more often than not, these biases manifest sub consciously, awareness about their role can guide retail investors to consciously try and overcome them and be as rational as possible in their investment-related decision making. Fourth, the findings of our study can help firms and academics engaged in retail investor education and training to design useful and effective content, courses, and programs. In addition, in formation about biases that influence investment-related decision making and the efforts of trainers to overcome such biases can enhance millennials' financial sophistication and make them ap preciate the benefits of diversification, thereby helping to increase the robustness of markets. In this regard, it is important to note that the existing scholarship has contended that sophisticated investors are less likely to manifest these biases. Lastly, our findings with the COVID-19 pandemic as the context provide insights for policymaking, investor education, and the strength ening of the financial system in preparation for future challenges. As a result, all stakeholders can be better fortified to deal with crises in the future, which can ultimately help reduce losses for investors at an individual level and volatility in the market at an aggregate level. For instance, the finding that herding has the highest relative influence on trading activity during a crisis can be used to educate retail investors in strategies, such as asset allocation, to counter such bias. In more specific terms, the findings of the present study can be used to prepare simulation software for training, through which the retail investors can then be exposed to simulated health or other crises\\/disasters wherein they can practice strategies that can counter the effect of behavioral biases. Such training imparted regularly can prepare retail investors to make more informed trading decisions when faced with any crisis. 7 | CONCLUSION, LIMITATION, AND FUTURE RESEARCH AREAS The present study investigated the influence of behavioral biases on millennials' trading activity and recommendation intentions during the COVID-19 pandemic through two research questions, which queried about the predictive capacity of the selected behavioral biases, namely, overconfidence and self-attribution, hindsight, representativeness, anchoring, mental accounting, loss aversion, and herding on trading activity and recommendation intentions of millennials during a pan demic. To address these questions, we applied the ANN approach to analyze data collected from 351 millennials in Finland. Thereafter, we conducted a post hoc qualitative study to examine if the biases that manifested at the pandemic's beginning persisted as it advanced. The findings of the empirical analysis revealed that herding, hindsight, overconfidence and self-attribution, representativeness, and anchoring influence both trading activity and recommendation intentions, but to a varying degree, with the values of influence being higher for trading activity and only anchoring having a negative influence. In comparison, loss aversion and mental accounting influence only recommendation intentions to a very small extent as well, with loss aversion having a negative influence. Furthermore, the relationship of the two en dogenous variables is nonlinear with herding, representativeness, and anchoring and is linear with the rest. The findings of the post hoc qualitative study indicate that most biases observed at the beginning of the pandemic continue to manifest with its advancement, giving us a reason to contend that they were not manifested under the influence of panic caused by the crisis alone. Ra ther, they are an ingrained part of the psyche of millennial investors. 7.1 | Limitations and future research areas The contribution of our study needs to be evaluated in light of three limitations: First, our study is based on a cross-sectional data col lection approach that may allow for certain respondent-related biases to manifest. Being aware of this issue, we followed the laid down processes to minimize the self-response biases, thereby increasing the robustness of the results. We also conducted a post hoc quali tative study to evaluate the implications of our findings. Second, we collected the data for analysis from only one country, which might restrict the broader generalizability of the findings. Nevertheless, our study being the first to investigate millennials in the said context contributes by laying a basis for future replication studies in various geographies to provide the regulators and firms with relevant decision-making inputs. Lastly, our study is focused on a narrow sample of male millennials that might again restrict the general izability of the findings to a broader population. However, since gender differences have been acknowledged to impact the mani festation of biases , we consciously decided to base our analysis on an all-male sample. Future researchers can expand the findings of our study by testing the influence of gender and other demographic variables of behavioral biases of millennials, as suggested for investors, in general, by prior studies. Furthermore, future researchers can test biases, such as moods, cognitive dissonance, and so on, by referring to Montier's (2002), Pompian's (2011), and other classifications.\",\"577240671\":\"Decades of research in the organizational sciences has demonstrated the importance of job attitudes and work perceptions, as they are related to important outcomes such as job performance, withdrawal, and life satisfaction. As such, organizations often engage in considerable efforts to understand how employees perceive their jobs and company. A whole industry of employee attitude surveying exists for this purpose, and many organizations have human resources (HR) teams devoted to measuring employee attitudes. Employee attitudes and perceptions are typically captured using closed-end numerical ratings (e.g., Likert scales), which allow for standardization and adhere well to psychometric models. These have been referred to as \\\"traditional numerical ratings\\\"  or \\\"closed questions,\\\" and they require that respondentsmake judgments from a list of predetermined responses. Although such ratings have dominated organizational research, it is also common to include open-ended comment boxes within surveys. The resulting narratives allow for rich and contextualized perspectives, which can produce insights beyond the rigidness of traditional survey items , and unearth actionable information that can further serve to broaden or create new theories. Likewise, employees express themselves using a variety of other open-ended mediums such as social media, blogs, forums, and via text, chat, or email. Such data can contain valuable insights regarding employee attitudes and work perceptions. Unfortunately, there are practical challenges when trying to use narratives to understand employee attitudes and perceptions. In fact, some of the features that make narratives so beneficial, such as how the unstandardized format allows for the provision of diverse types of information, also make them challenging to use within traditional analytical frameworks. Qualitative text analysis traditionally involves training people to code and interpret narratives according to various themes. This process is time-consuming and expensive, hampering attempts to leverage narratives in any practical way for large organizations or samples. This said, recent advances in natural language processing (NLP) have opened the door to scoring narrative text more efficiently. The organizational sciences have seen a recent explosion in NLP applications. Although this has furthered measurement science and organizational theory, it is not currently easy to use NLP to score text regarding the many work attitudes and perceptions important to organizations. For example, suppose an organization wanted to automatically score qualitative exit interview responses according to levels of job satisfaction, diversity climate, and perceptions of autonomy. In that case, there is no easy way to do this without collecting a massive amount of data and training custom algorithms. That is costly, time-intensive, and not possible in most situations. It also is not entirely clear if these constructs can be accurately assessed via NLP (i.e., are scores valid), and under what conditions these and similar constructs will be most accurately measured via NLP. The overarching goal of the current research was to build and validate NLP algorithms capable of scoring text according to important employee attitudes and work perceptions (e.g., engagement, job complexity, turnover intentions). Algorithms to score important work constructs do not exist beyond narrowed construct domains. Furthermore, although existing algorithms are useful in some cases, they do not apply state-of-the-art NLP algorithms to derive meaning from organizational text. In this research, we hoped to create reliable, valid, and cutting-edge algorithms to assess 28 commonly measured constructs from the organizational sciences and make these freely available. The algorithms are referred to as the Text-Based Attitude and Perception Scoring (TAPS) dictionaries. As part of this research, there are several high-level objectives and contributions to the literature. Our primary goal was to develop, validate, and freely provide NLP algorithms capable of assessing work attitudes and perceptions. These algorithms were designed to measure the topics a particular text discusses --similar to Linguistic Inquiry Word Count  but developed explicitly for work-related attitudes--as well as the degree to which a topic is discussed favorably. By relying on nonwork-related NLP dictionaries such as LIWC, researchers are unable to automatically assess which work attitudes and perceptions are discussed in text. Such algorithms do not exist, and researchers must go to considerable lengths to develop and validate any customized algorithms to meet these purposes. Likewise, there are no known valence algorithms to assess work attitudes and perceptions. TAPS addresses both of these voids, and in developing these algorithms, we applied rigorous validation techniques to support their use in new contexts. This included examining reliability via split-half and test-retest designs, convergence with aligned Likert composite scores, discriminant validity, and criterion-related validity in terms of correlations with important work outcomes (e.g., organizational citizenship behaviors). Second, we sought to examine the benefits of using more advanced NLP algorithms. Specifically, we compared scoring via bag-of-words (BOW), which has been frequently applied in the organizational sciences , to more contemporary NLP scoring methods. The BOW framework ignores the order of words and simply considers the presence of word phrases occurring in text as the focus of analysis, often then applying machine learning procedures such as elastic net  or random forests  to predict a target variable. Although these methods generally perform well for NLP classification tasks, the science of NLP has advanced in recent years, and in this study, we focus on transformer NLP models. Transformer models better incorporate the context and sequence of language via deep neural networks. Transformers have become commonplace for most NLP tasks and have replaced alternative NLP frameworks, such as long-short-term models, as the dominant NLP architecture. Transformer models will be compared to BOW random forests scoring (i.e., labeled bag of words--machine learning [BOW-ML]) to test whether more advanced NLP methods offer meaningful improvements in algorithm accuracy. Finally, this study tests the boundary conditions that affect the validity of NLP scores. Although NLP is an exciting method to derive meaning from text, it is not perfect. Unlike traditional Likert questions, which are standardized across respondents, the type of text that respondents write is likely to vary from person to person. Furthermore, the prompts that elicit text (e.g., instructions of questions for what text to provide) are likely to impact the quality of text and what is discussed. For example, survey respondents might be presented with a vague open-ended comment box to broadly describe whatever information they wish, or they might be asked explicitly about some work topic, such as diversity perceptions. In psychological terms, different prompts will elicit different degrees of trait activation potential , such that certain prompts will be more capable of deriving information necessary to accurately measure constructs. As such, this study will investigate how instructional prompts impact the information provided (e.g., themes discussed) and the validity of NLP valence scores. This investigation will thus provide critical information to ensure that future surveys are designed in a way that elicits useful narrative information and whether the developed algorithms from this study are likely to generalize. Targeted Work Attitudes and Perceptions The current research focuses on attitudes and general work perceptions.Work attitudes are evaluations of one's job, occupation, or employer that reflect beliefs, feelings, and attachment. The long history of work attitudes has coincided with the accumulation of many work attitude constructs. This makes it challenging to include all possible attitudinal constructs within the current research endeavor. Instead, the intent was to capture a broad range of work attitudes and perceptions that are likely to be useful to researchers and might reasonably be captured from comments in work surveys. To this latter point, while it is possible to ask respondents to provide their perceptions regarding any potential construct, the intended algorithms and scoring schemes are intended to generalize across most surveys. With these general considerations laid out, the current research focused on commonly researched attitudes, as well as various perceptions and motivational states. To determine which constructs to target, we reviewed seminal articles in the field of organizational sciences. From those articles, we identified a set of commonly discussed constructs that we felt could be measured via text. Second, we consulted with subject matter experts (SMEs) and sent this list of constructs along with definitions to SMEs in the field. Six industrial & organizational psychology professors were surveyed, with three responding. The SMEs reviewed the constructs, indicated whether they would be relevant and useful for the current research, and provided suggestions for major gaps they felt were missing. After doing this, a total of 28 constructs were identified, which can be found in Table 1, along with construct definitions. Although there are additional constructs that could be incorporated or might be interesting to also include, the list balances comprehensiveness and practicality. Narrative-Based Scores Created in This Study Two sets of scores were created for each TAPS construct: theme scores and valence scores. Theme Scores Theme scores indicate whether a construct is discussed or mentioned in text. The most well-known example of theme scores comes from LIWC , which counts the number of construct-specific words used in a document and controls for the length of the document (i.e., number of words). Higher values indicate a text is more likely to have discussed a topic or construct. In this research, we identified a set of word phrases relevant to each of the 28 TAPS constructs.We then derived theme scores from those dictionaries to indicate whether a text is relevant to each of the 28 TAPS constructs. Valence Scores Valence scores indicate a text's favorability regarding a specific topic or construct. For example, a valence score would indicate whether a person is satisfied with their job based on their written text, as opposed to simply indicating whether that text was relevant to the construct of job satisfaction. Thus, valence scores are particularly useful for understanding employees' attitudes and perceptions, serving as a barometer of construct level. Valence scores were empirically determined by training an algorithm to reproduce Likert composite scores from text, and thus valence scores were developed via supervised machine learning. A separate algorithm was formed for each of the TAPS constructs, which can then be applied to new data to estimate the valence of attitudes and perceptions. Research Questions and Psychometric Tests It is unclear whether work attitudes and perceptions can accurately be derived using NLP, and there are no existing comprehensive algorithms that researchers can use to score text automatically. The current research developed such algorithms and then examined the psychometric properties of the resulting TAPS scores. Reliability Measures must be reliable to justify use. The reliability of valence scores was estimated in two ways. First, a measure of split-half reliability was calculated by dividing the narrative comments into two groups (text from odd numbered and even numbered question prompts) for each person in the sample, scoring the text, and then correlating the two vectors and using the Spearman-Brown Prophecy formula to estimate the reliability of the composite of all text for each person. Second, we also estimated reliability using a test-retest reliability design, collecting data one to 2 weeks following the first data collection. Research Question 1: What is the reliability of the derived narrative scores? Convergent Correlations Successful NLP algorithms will produce scores that correlate strongly with the target variable scores, which in this case are the aligned Likert composite scores. Such a correlation indicates that the two sets of scores assess similar constructs. Research Question 2: What is the correlation between NLP valence scores and Likert composite scores? Comparisons of Natural Language Processing Algorithms There are many ways to transform text into predicted target scores, and we compared two NLP approaches for the valence scores. In each case, Likert composite scores were used as target variables, and algorithms were trained to reproduce those Likert composite scores. For the first calculation method, we used a bag-of-words (BOW) framework , where the simple presence of word phrases occurring in text is the focus of the analysis. BOW approaches have been the norm in the organizational sciences. After forming a document term matrix that represents the frequency of word phrases, we conducted supervised machine learning to predict the Likert composite scores via random forests , similar to past research. The second method applied deep neural network transformer models  to better encapsulate the contextual sequence and meaning of language. Transformer models  handle sequential text input with contextual embeddings and specialized attention algorithms via deep neural networks , allowing for better utilization of text information and often serving as the foundation for NLP prediction tasks. Transformer models are composed of dense, multilayered neural networks, with the layers capable of capturing meaning in language. These perform better than previously popular NLP architectures such as long-short-term-models and have achieved widespread use for NLP tasks, such that most language models now use transformer architectures. In prediction tasks, a top neural network layer is stacked above the language layers--language layers capture the meaning of language and allow for prediction and generation of language--and the top layer is designed to take language layer output and translate it into predictions of target variables. In this case, an additional layer was added to translate the text representations into predicted attitude scores, with self-report attitude composites as the target variables. Because transformers are more capable of handling the complex ities of language, we expected that transformer scores would exhibit more favorable psychometric properties. Hypothesis 1: Transformer-based valence scores will exhibit higher reliability and exhibit stronger correlations with aligned composite Likert scores than BOW-ML-based valence scores. Discriminant Validity Ideally, convergent correlations will be larger than heteroconstruct-monomethod (i.e., different construct same method) correlations, per principles outlined by Campbell and Fiske (1959). However, using NLP to derive construct estimates from text for multiple constructs may result in inflated intercorrelations between scores. Unlike Likert scales, where specific items are designed and scored for one and only one construct, the whole narrative response is used to estimate NLP construct scores for all measured constructs. As such, machine learning scores might exhibit higher heteroconstruct-monomethod correlations than Likert scores. Establishing that NLP valence scores exhibit similar intercorrelations within method as Likert scores is important to establishing evidence in support of NLP to score work attitudes and perceptions. Evidence of discriminant validity is also established if the monoconstruct-heteromethod correlations (i.e., convergent correlations; NLP valence scores correlated with aligned Likert composite scores) are larger than the heteroconstruct-heteromethod correlations (i.e., NLP valence scores correlated with nonaligned Likert composite scores). Because such correlations do not reflect the same constructs or methods, heteroconstruct-heteromethod correlations should be the weakest of all comparisons. Research Question 3: Do NLP valence scores exhibit discriminant validity in the form of (a) heteroconstruct-monomethod correlations and (b) heteroconstruct-heteromethod correlations? Criterion-Related Validity Narrative scores should also predict future work outcomes that are organizationally valued. There is evidence that many of the included work attitudes and perceptions are related to various work outcomes, but that research is predominantly based on closed-end survey items. The question becomes whether narrative-derived estimates are correlated with important work outcomes and whether those scores predict important criteria after controlling for variance from other measurement methods (i.e., Likert composites). When combined with traditional numerical ratings, narrative text is likely to increase measurement bandwidth by allowing raters to contextualize their responses and elaborate beyond response scales. This is also likely to increase overall measurement reliability, as each data source should be influenced by the same latent construct, and each contribute true score variance. Together, this should lead to larger composite correlations with criterion variables when Likert and NLP scores are combined. The performance criteria examined within this study include selfreported organizational citizenship behaviors (OCBs) and counterproductive work behaviors (CWBs). OCBs are behaviors that go beyond traditional task performance to contribute to the overall welfare of the organization (supporting others, supporting the organization). CWBs are intentional behaviors that harm or intend to harm organizations, employees, or customers. Tomake for a more parsimonious investigation, we focused on attitude\\/perception and criterion relationships with stronger alignment, basing these judgments on meta-analytic evidence showing at least moderate correlations for alignment. The studies used to make these decisions include LePine et al. (2002), Meyer et al. (2002), and Berry et al. (2012). Research Question 4: (a) What is the criterion-related validity of NLP valence scores and how does it compare to the criterionrelated validity of Likert composite scores? (b) Do NLP valence scores explain unique variance in criterion outcomes after controlling for Likert composite scores? Information Elicitation From Narratives NLP is often described as a panacea to measurement, capable of unearthing hidden nuggets of information that traditional measurement methods are incapable of assessing. However, the quality of derived NLP scores will be dependent upon the soundness of the measurement design. This study thus investigated how robust the developed TAPS algorithms are across various survey prompts and how prompt characteristics impact the validity of TAPS scores. Open-ended text is a unique medium because of its unstructured format. Such a format allows for diverse information to be provided, much of which is likely to be contextualized and offer elaboration beyond traditional numerical ratings. At the same time, the openended nature decreases standardization and therefore increases variance in the quality of information. This feature makes the concept of situational press, or how situational factors influence behavioral responses, quite important. For Likert items, there are clear prompts indicating how a person should respond. Items are presented, and instructions indicate what the respondent should do (i.e., choose one of the available options). In comparison, situational press is typically not very strong in providing information pertaining to specific psychological constructs when responding to survey text prompts. For example, a common prompt such as \\\"describe what you like about your job\\\" is likely to result in narratives that vary in discussed topics, ranging from supervisor attitudes to perceptions of job autonomy, among many others. On the other hand, a narrative prompt specifically asking about pay will exhibit more consistency in the information provided by respondents, with responses being more relevant to pay perceptions. According to trait activation theory (TAT;), consistency in behavior (e.g., writing within a comment box) is a function of the relevance and strength of situational cues for specific behaviors. If narratives are embedded within an organizational survey, it is likely the textual information provided will be workrelated, but beyond that there is considerable leeway in how a person responds. This will result in variability in what themes are written about, the number of themes discussed, and how much text is provided. On the other hand, targeted, narrow prompts (e.g., describe your satisfaction with pay) necessarily have more trait activation potential to assess targeted constructs, and as such will be more likely to provide information regarding those targeted constructs. This has several implications. First, we expect that prompts can be a priori evaluated for their activation potential to elicit information specific to certain constructs. Prompts with higher activation potential will have a greater proportion of comments relevant to specific constructs. Hypothesis 2: Prompts with higher activation potential produce a higher frequency of construct-relevant comments (i.e., higher theme scores). Second, when there is more narrative information regarding a construct, NLP scores should be more valid. Speer (2020) indirectly found support for this with performance appraisal narratives, such that targeted narrow prompts produced more favorable psychometric properties than broad, less-focused prompts and that narratives containing theme-specific words were generally more valid. However, this has not been tested at the individual prompt level, therefore clouding the degree to which prompt specificity and existent theme information impact the validity of NLP valence scores. We expected that theme scores could be used to filter text based on relevance, and that text responses with higher theme scores would exhibit higher convergent correlations with aligned Likert composite scores. Hypothesis 3: NLP theme scores will moderate the relationship between NLP valence scores and convergence with aligned Likert composite scores, such that the correlation between NLP valence scores and aligned Likert composite scores will be stronger when aligned theme scores are high. Method Participants The sample consisted of 1,506 currently employed adults who responded to a survey of work attitudes and perceptions that included various narrative prompts. Participants were recruited from Prolific, which is an online labor market used for academic research. Participants were required to have an approval rate of 98% or greater, live in the United States, and currently, be employed at an organization. The sample was diverse, spanning various job types. The sample was varied in gender (51% female), race (79% White, 7% Black, 6% multiracial, 4% Asian Pacific, 2% Hispanic), age (M = 34.95, SD = 10.83), and education (43% bachelor's degree). The 1,506 respondents were segmented into two conditions that contained different narrative prompts. In one condition, 805 respondents answered 16 specific, targeted prompts (see Narrative Prompts section, Table 2). In the second condition (i.e., a separate survey), 701 respondents provided narratives for two broad prompts (Table 2). Across all responses, a total of 14,282 unique comments existed. This represents the primary sample. In addition to the primary sample, two additional within-person data collections were conducted. In the first, a random subsample of a thousand respondents was surveyed to provide data regarding selfreported OCBs and CWBs, with 840 respondents completing the survey. Additionally, several hundred respondents were surveyed to respond to the narratives a second time to establish test-retest reliability, with 217 responding. These data collections occurred 1-2 weeks after the first data collection. Measures and Scoring Likert Measures A total of 28 constructs were assessed using Likert scales and used as target variables. For simplicity, information pertaining to these measures can be found in Table 1, including the source of the methods used to assess the constructs and reliability estimates. Example items and the source of those items can be found in Appendix. All measures exhibited acceptable evidence of reliability. Narrative Prompts As discussed, participants were exposed to one of two conditions: one with 16 targeted, narrow prompts and one with two broad prompts. The intent was to collect a diverse set of narratives and train algorithms upon that diverse set, so that the trained algorithms would generalize better in new contexts. Although 28 constructs were targeted, only 16 targeted narrow prompts were used because providing an open-ended text box for every single construct was not feasible, as that would be tedious for respondents. Instead, some prompts were designed to measure several constructs. These were developed by the first author and then revised with several of the other study authors after judgments were made regarding the degree to which the prompts were capable of assessing the targeted constructs. The intent was that each construct had at least one prompt likely to elicit information regarding that construct. The 16 text prompts can be found in Table 2 (Supplemental Materials also contain detailed information about each prompt). For example, \\\"Describe your level of engagement at work. Are you energized, dedicated, and absorbed in work\\\" was designed to measure engagement and satisfaction with work itself. Text was restricted to 75-500 characters for each narrow comment. The two broad prompts are also included in Table 2. These asked about work more generally and are therefore less targeted in assessing specific constructs. Broad prompts were restricted to 400-3,500 characters each. Construct Activation Ratings. To help determine the training sample for the NLP algorithms and to later test the effects of construct activation potential, two of the study authors rated each prompt for how likely the prompt would elicit information relevant to each construct. These construct activation ratings were made on a 1-5 scale ranging from 1 = very unlikely to elicit information regarding this construct (fewer than 5% of cases) to 5 very likely to elicit information regarding this construct (over 75% of cases).\\\" Interrater agreement between the two judges was good (intraclass correlation coefficient 2,2 = .88), and after making ratings the judgments were averaged to represent construct activation potential for each prompt, done separately for all constructs. High scores indicate that a prompt's responses are likely to elicit information regarding the construct in question. Construct activation scores are provided in the Supplemental Materials. Training Valence Scores Valence scores were developed by training algorithms to transform text into predicted Likert composite scores. This was completed using two different approaches, with transformer valence scores serving as the focal method. Transformer Valence Scores. We expected that transformer based valence scores would perform best, given their ability to better understand the complexities of language and their widespread adoption within the computer sciences. As discussed, transformers are typically composed of deep neural networks that encapsulate the contextual sequence and meaning of language. In this case, a top neural network layer was stacked to predict Likert composite scores. Transformer scores were computed via Python in Google Colab and using the HuggingFace interface , which streamlines the use of transformer models. A separate transformer model was created for each TAPS construct, one to predict each of the Likert composites. To train the models, only a subset of the narrative responses was used as inputs for each construct. Specifically, for each construct, narratives from prompts with construct activation potential scores greater or equal to 3.5 were included when training the algorithm. Additionally, responses to at least one broad prompt were included when developing algorithms for each construct, assuming one of the two broad prompts had an activation potential score greater than two. If after considering these two criteria a construct still only had responses from one type of prompt, then responses from the prompt with the next highest activation score were added. This strategy ensured each algorithm was developed based on responses to multiple types of prompts in attempts to increase the generalizability of the algorithms. To avoid overfitting and to produce scores independent of model training, k-folds cross-validation was used to train the models and then compute holdout NLP scores. Specifically, the relevant batch of narratives for each construct was split into five 20% groups, and a single fold was held out at a time while the other four folds (80%) were used to estimate a model. That model was then used to create predicted scores in the holdout fold, and this process was repeated until holdout scores existed for the entire calibration sample. Once complete, a final model was estimated on all the calibration data, and this model was used to produce estimated scores for the remaining narrative comments (which were also independent of the training sample). Data were analyzed at the narrative response level (total possible N = 14,282). To avoid data dependency and overfitting, the entirety of each respondent's narrative text (i.e., all their narrative responses) was included in a single fold so that when estimating a model, a person's responses were either fully in the calibration set or fully in the holdout set. Once completed, the entire data set (N respondents = 1,506, number of narratives = 14,282) had NLP scores for all constructs, and these scores were independent of model training. We ran Bidirectional Encoder Representations from Transfor mers (BERT) models from HuggingFace  to develop transformer scoring algorithms using text responses as input to reproduce Likert composite scores. BERT is built off the transformer architecture described by Vaswani et al. (2017), and it consists of an embedding layer that translates the raw text (we did not perform any text preprocessing outside of the standard BERT tokenizer) into pretrained embeddings that encode positional and relational information of how tokens are located within context and relate to other words. A token is a string of characters such as a word, and each token from a document is assigned an embedding vector of numeric values that capture textual meaning (e.g., the correlation between word vectors will be higher for words that are more conceptually related). Transformer models also incorporate positional information that reflects where a token is located within the larger sequence of words (i.e., positional embedding information). These embedding scores will differ based on which words are contained in the document via a mechanism known as \\\"self-attention\\\"; attention allows embedding information from surrounding words to influence other embedding weights, allowing transformers to model long-term dependencies between words in a document (can detect that for the sentence \\\"The man ran to the store but then got tired,\\\" that \\\"tired\\\" is in reference to \\\"The man\\\"; article, for more details on self-attention). The embedding layer sits below a series of 12 hidden layers and attention heads that were trained to predict text, therefore, capturing the meaning of text (i.e., \\\"language model\\\"). The parameter weights within the model have been pretrained on massive data sets, thus mitigating the need for large primary samples. These weights fully link each hidden unit with the hidden units in the direct subsequent layers. Having already been trained on large samples of data, we then applied transfer learning  by minimally tweaking the weights on the newly collected data from this study. This prevents drastic forgetting of past learned associations but allows for customization of the model to the work narratives provided. Additionally, we stacked an extra layer on top of the pretrained language layers that was used to predict the target variable (i.e., aligned Likert composite scores), as is the practice when transformers are used for sentiment analysis. \\\"Stacking\\\" means we added a set of weights on top of the last hidden unit scores to predict the aligned Likert composite scores. To prevent forgetting of well-established pretrained parameters from the other model layers , we fixed the embedding layer and the bottom six language layers of BERT when training, thus holding those parameters constant and only allowing for parameter updates to the remaining upper layers. Given the computational burden of deep neural networks and the need to create 28 models (one for each construct), we used a partial nested k-folds approach to determining hyper-parameters for the transformer models. A hyper-parameter is a parameter that influences the model training process; in other words, it establishes the settings for the larger model. For neural networks, there are many possible hyper-parameters. Some of the more common hyperparameters to consider for neural networks include learning rate, number of epochs, and weight decay, among many others. The reader can refer to Goodfellow et al. (2016) for more details. Upon examining initial results, very minimal changes from the default hyper-parameter settings were made. More specifically, we ran a small grid search of hyper-parameters once (i.e., using only one 20% holdout) for three of the constructs (task variety, satisfaction with work, workload). Holding batch size to eight, we varied weight decay, learning rate, and the number of epochs. We found that tweaking weight decay from.00 did not produce meaningful changes in model performance, that a small learning rate of.00005 was superior, and that three epochs were ideal with those settings. These optimal hyper-parameter settings were consistent across the three sampled constructs. Thus, we retained the default HuggingFace setting for weight decay, used a value of.00005 for learning rate, a batch size of eight, and trained for three full passes through the data (i.e., three epochs) on the upper six layers, plus the classification layer. These hyper-parameter settings were applied to estimate models for all 28 constructs. BOW-ML Valence Scores. The same k-folds settings and narrative responses were used for both sets of valence scores, thus allowing for a fair comparison between the scoring methods. As is typical with BOW approaches, we began by cleaning the text by removing unhelpful information and standardizing information across narratives. This included lowercasing, removing select punctuation (everything except for question marks and exclamation marks), replacing contractions, replacing abbreviations, replacing ordinal values and numbers, replacing commonly used symbols, controlling for negation (e.g., \\\"not great\\\" becomes \\\"no_great\\\"), removing a custom set of stopwords, and lemmatizing (i.e., reducing words to their base; e.g., \\\"working\\\" would become \\\"work\\\"). Once done, document term matrices were formed with term frequencyinverse document frequency weighting, consisting of unigrams, bigrams, and trigrams, and with extremely uncommon word phrases removed (i.e., those occurring in less than.002% of documents). Valence scores were formed via random forests. To be consistent with the hyper-parameter tuning of the transformer models, we only performed tuning for the same three constructs and only on a single holdout fold. Within the training fold for the hyper-parameter tuning, the number of predictor vectors was first reduced based on correlations with the target variable (>=|.03|). Then, the number of variables sampled per node and the number of trees were tuned (i.e., by varying different numbers of trees and variables sampled per node). Upon doing this, we found that the optimal settings were 2,500 trees with five variables sampled per each node, with a default node size of five. These were thus used for all 28 constructs. With hyper-parameters set, the same k-folds crossvalidation strategy used for transformers was applied to create BOW-ML valence scores for all comments. Developing Theme Scores Theme scores were created for the same set of constructs, with a few exceptions. Rather than create separate theme scores for all types of organizational commitment (affective, continuance, normative) and organizational justice (distributive, procedural, informational, interactional), single theme scores were created for both organizational commitment overall and organizational justice overall. We also combined physical demands and work conditions into a single theme score, given their conceptual similarity. To create theme scores, we developed a dictionary of word phrases for each construct. Unigrams (single words, e.g., \\\"depart\\\"), bigrams (two words, e.g., \\\"new opportunity\\\"), and trigrams (three words, \\\"in the zone\\\") were all included, though the majority of included phrases were unigrams. Then, theme scores were computed by counting the usage of those words and controlling for document length, similar to LIWC. The theme dictionaries were created using a rational-empirical method. First, the study authors reviewed each construct definition, discussed the meaning of the constructs, and then rationally generated words that were relevant to each construct. An example word for autonomy is \\\"discretion,\\\" and an example word for diversity climate was \\\"inclusion.\\\" A rule was adopted that a word was deemed relevant only if it was likely to be relevant to a construct at least one-third of the time it was used. Multiple researchers reviewed each word list, reviewed synonyms for chosen words, and revised the lists accordingly. After generating a list of rationally determined words for each construct, we applied an empirical approach to identify additional words that share similar meanings in text. Specifically, and based on recent work by Li et al. (2021), we identified a set of highly relevant \\\"seed words\\\" for each construct. After calculating word embeddings for all words, we identified word vectors that correlated highly with those seed words. If the identified words made logical sense, they were added to the theme dictionaries. Thus, we were able to mesh top-down rational methods with data-driven identification of words occurring within our corpus. An example set of finalized word phrases for engagement can be found in Table 3. Dependent Variables Outcome data were collected at Time 2 for 840 respondents (65% from the narrow Time 2 condition, and 35% from the broad condition). OCBs were self-reported and measured using the Dalal et al. (2009) 12-item scale and conceptualized as overall OCB, with an internal consistency estimate of.90. CWBs were selfreported and measured using Bennett and Robinson (2000) 19-item scale, with an internal consistency estimate of.89 for overall CWB. Results Reliability Estimates and Convergent Correlations by Machine Learning Algorithm Table 4 provides convergent correlations and reliability coefficients for all valence scores. These analyses were performed at the person-level (N = 1,506) by aggregating valence scores across all prompts within-person. First, we examined the reliability of the valence scores. As previously mentioned, reliability was estimated in two ways: using a split-half estimate of reliability and a test-retest design. Split-half reliability was calculated by dividing the narrative comments into two groups (odds, evens), scoring each set of text, and then correlating the two vectors and adjusting using SpearmanBrown Prophecy formula to estimate the composite reliability of all text for each person. As shown in Table 4, split-half reliability was satisfactory for all constructs for the transformer scores (Research Question 1), with an average reliability coefficient of.84 and a range from.70 (workload) to.89 (organizational commitment--affective, engagement, task identity). In comparison, BOW-ML scores generally exhibited acceptable reliability, with an average coefficient of.69. However, this was significantly lower than split-half reliability estimates for transformer scores (Steiger z = 12.21, p < .01, Hypothesis 1). Next, we examined test-retest reliability. Recall that the second survey administration occurred 1-2 weeks following the first survey. This gap in time should not have resulted in drastic changes in attitudes and perceptions. However, it is likely that \\\"testing\\\" effects occurred as a result of writing narratives in the first session, and it was not expected that respondents would write identical text for each survey sitting. In other words, because of the nature of writing narratives, test-retest reliability was expected to be lower. Despite this, test-retest reliability values were generally good (Research Question 1), particularly for transformer scores. The average testretest reliability coefficient for BOW-ML scores was.52. In contrast, it was.66 for transformer scores, and the difference between these correlations was significant (Steiger z = 7.94, p < .01). As an aside, it should be noted that neither scoring method was strongly related to response length (i.e., number of words), with BOW-ML scores correlating.06 on average with number of words and transformer scores correlating -.02 on average. Next, convergent correlations between valence scores and Likert composite scores were examined (Table 4, Research Question 2). As seen, average convergence was moderate-to-strong for BOW-ML scores, averaging.36 and ranging from.12 to.55. Although this evidence was generally in support of the BOW-ML scores, average convergence was consistently stronger when evaluating the transformer valence scores. The transformer scores had an average convergent correlation of.48, with values ranging from.23 to.65. The average convergent correlations for BOW-ML and transformer scores were significantly different from one another (Steiger z = 5.80, p < .01). Thus, once again the more complex transformer model produced better psychometric evidence than the simpler BOW approach. Given the superior psychometric performance of transformer scores (Hypothesis 1), for the remaining study analyses, we focus on the transformer scores and refer to them solely as valence scores. It is also worthwhile to examine the results in Table 4 for which constructs exhibited higher and lower convergence. Generally, constructs reflecting broad, generalized attitudes toward the job or organization had higher levels of convergence. For example, organizational commitment--affective (r = .62), engagement (r = .64), satisfaction with work (r = .65), and turnover intentions (r = .64) all had higher levels of convergence when analyzing results across combined text. More nuanced topics such as task identity (r= .27), physical demands (r = .34), work conditions (r = .30), and organizational commitment--continuance (r = .23) had lower convergence. These findings will be explored more in later sections by examining convergence for specific narrative responses that differ in relevance to the assessed constructs. Theme Scores Theme scores identify what a narrative is about. To examine whether theme scores could differentiate text according to its relevance to the targeted constructs, we used the prompt activation ratings made by SMEs to sort the narrative comments into low, moderate, or high activation potential for each construct. High activation potential indicates a high probability that respondents will discuss the particular construct. We then calculated average theme scores within each category of prompt activation, done for each construct. As seen in Table 5, theme scores were low for prompts with minimal activation potential. For narratives written in response to low activation prompts, the average theme scores were.59, meaning.59% of words were, on average, construct-relevant. On the other hand, theme scores were much higher for narratives written in response to prompts with high activation potential, with an average theme score of 5.02. Theme scores from high activation potential prompts were above 1.0 for all constructs besides variety (theme score = .87) and went as high as 8.16 (organizational commitment). The standardized mean difference (d) between theme scores from low and high activation prompts averaged 2.60 (p < .01), which is a substantial effect size. Table 5 also contains the correlations between construct activation ratings and theme scores. The average correlation was.42 (p < .01). Taken together, these results indicate that theme scores differentiated narrative responses based on how likely they were to target specific constructs. Hypothesis 2 was supported. Convergent Correlations by Prompt Activation and Theme Scores Valence scores exhibited moderate-to-large correlations with aligned Likert composite scores for combined text across all narratives (Table 4). However, valence scores will likely be more accurate in narratives relevant to the targeted construct. In other words, it should only be possible for text to accurately measure a construct if the writing provides information specific to that construct. This was shown in two ways. First, Table 6 shows that when prompts were unlikely to elicit construct-related text (i.e., low activation potential), valence scores had lower convergence, and convergence improved as prompts increased in construct activation. The average convergent correlations were.27, .38, and.58 in low, moderate, and high activation potential prompts. Interestingly, even in prompts that were unrelated to targeted constructs, the valence scores still exhibit some evidence of validity. However, the convergent correlations are generally capped for such responses, and as prompt relevance increases, validity jumps, with convergent correlations as large as.76 (satisfaction with pay) for high activation potential prompts. These findings suggest that if researchers or practitioners wish to measure the targeted attitude and perception constructs via text, the use of relevant, targeted narratives designed to assess focal constructs is important. Another way to demonstrate the impact of narrative relevance on the validity of valence scores is by simultaneously examining theme and valence score validity. It was expected that valence score validity would be stronger for narratives with higher theme scores, as higher theme scores imply that the narrative contains information about the targeted construct. Indeed, we found that convergent correlations were higher when theme scores were very high (Table 7), with an average convergent correlation of.56 when theme scores were highest but only a.25 average correlation when theme scores were zero. As theme scores increased, validity did as well. In fact, for certain constructs, validity reached values as high as.80 (turnover intentions and satisfaction with work, respectively). Overall, these findings suggest that theme scores can be used to filter text when interpreting construct levels (i.e., interpret valence scores only when theme scores are at least moderate), supporting Hypothesis 3. Discriminant Validity We next examined the discriminate validity of the valence scores by looking at the pattern of correlations within method (NLP, Likert composites). Table 8 presents these results. The average absolute intercorrelation among the single stimulus composite scores was.31, and a single factor explained 35% of the variance. Given that these constructs share a good deal of conceptual overlap, it is not surprising that a strong general factor pervades the data. Due to this and because the NLP valence scores were developed to recreate the same Likert composite scores, the created valence scores should likewise exhibit strong intercorrelations, and this was the case. The average absolute correlation between NLP valence scores was.54, and a single factor explained 52% of the variance. This is noticeably higher, possibly indicating that the NLP scores show less discrimination from one another than the original Likert composite scores. Although such a finding would not be ideal, it aligns with past NLP research. Unlike Likert surveys, where different items uniquely align to one and only one construct, the NLP scores were all formed based on the same text (i.e., all responses aggregated), and thus stronger intercorrelations are expected. This would be similar to if the same Likert items were used to assess a handful of constructs, just with different item-level weighting applied to form composite scores. If that were performed, intercorrelations between constructs would be higher because all construct scores would be based on the same inputs. However, a closer look at the data reveals that discriminant validity is not so problematic. The confound of using the same text for all NLP scores can be circumvented by using the theme scores to first filter the text. More specifically, if for each construct, the text is filtered to only text that is likely relevant to the construct (i.e., higher theme scores) prior to calculating correlations, discriminant correlations should lower to more acceptable values. Indeed, after restricting the text in this way, the average heteroconstruct-monomethod correlation falls to.24 for valence scores, which is in fact lower than that found for Likert composites (.31). Likewise, a general factor only explains 30% of variance after making this change (also lower than single stimulus scores, 35%). This finding shows that NLP scores can exhibit evidence of discriminant validity and further highlights the importance of properly filtering text based on construct relevance. We also examined the heteroconstruct-heteromethod correlations, which are the valence score correlations with nonaligned Likert composites. Discriminant validity was generally favorable when interpreting these findings, as these were the lowest set of correlations ( -r = .31), in line with multitrait-multimethod principles. Once again, we suspected that theme scores would impact the heteroconstruct-heteromethod correlations for valence scores. Thus, we filtered the valence scores to only those where the corresponding theme score was high and then recalculated the heteroconstruct-heteromethod correlations. As seen, this led to a further improvement in discriminant validity ( -r = .23). As a formal comparison of convergent to discriminant validity and after filtering to comments with high theme scores, the average convergent correlation was.56, compared to values of.24 and.23 for heteroconstruct-monomethod correlations and average heteroconstruct-heteromethod correlations. The NLP scores showed stronger convergent correlations than discriminant correlations, and they showed better discriminant validity than that found for Likert composite scores after the text was filtered based on theme scores (Research Question 3). Criterion-Related Validity Practitioners and researchers may also be concerned with whether the TAPS scores are useful in predicting employee outcomes. To this effect, we examined how valence scores related to OCBs and CWBs. Valence scores computed at time 1 were correlated with OCBs and CWBs at time 2, with the results shown in Table 9. As mentioned previously, we limited our focus to only attitude and perception constructs with stronger alignment to these outcomes based on meta-analytical evidence. First, both Likert and valence scores were correlated with OCBs. Across constructs, the average correlationwithOCBswas.34 for Likert scores and.33 for valence scores. Thus, each method of measurement exhibited similar relationships with the dependent variable. Of the constructs, engagement, organizational commitment-affective, and satisfaction with work exhibited the strongest relationships. To determine whether NLP-based valence scores were useful beyond traditional Likert scores, hierarchical regressions were performed for each construct. In Step 1, the Likert composite score was entered as the independent variable, and in Step 2, the aligned valence score was entered. Findings for these results across all 28 constructs can be found in Table 9. As seen, valence scores explained significant variance in OCBs above and beyond Likert scores for all constructs. The average DR was.04, showing a meaningful increase in the prediction of OCBs. Although scores were related to OCBs, the same trend did not hold for CWBs. In general, both Likert and valence scores were weakly to moderately related to CWBs. Across constructs, the average correlation with CWBs was -.16 for Likert scores and -.08 for valence scores, and these values were significantly different from one another (Steiger z = 2.29, p < .05), meaning that Likert scores exhibited stronger correlations with CWBs. Furthermore, the only construct that produced a significant increase in variance explained when the valence score was added was satisfaction with work (DR = .02, p < .05). Taken together, NLP valence scores exhibited comparable validity for OCBs but slightly weaker validity for CWBs, and NLP valence scores only explained significant incremental variance for OCBs (Research Question 4). Final TAPS Scoring Algorithms The final developed TAPS algorithms (theme scores, transformerbased valence scores) can be found on Open Science Framework (https:\\/\\/osf.io\\/9dx6n\\/?view_only=1d64b8126df54a1ba17d9f7306d 7dd5e) and are freely available to researchers. The provided syntax allows researchers to score new organizational text according to all of the TAPS constructs, requiring no model development or advanced work beyond simply running the provided code. Discussion Prior to this research, there was no easy way to automatically score employee text according to targeted work-relevant attitudes and perceptions without engaging in the time-consuming and data-intensive process of developing customized algorithms. As such, the major goal of this research was to build and validate NLP algorithms capable of automatically scoring organizational text according to important employee attitudes and work perceptions. In addition to building theme and valence scores capable of identifying what work topics are discussed and how favorably they were discussed, this research also tested different methods of NLP scoring. Furthermore, this article highlighted just how important it is to consider how text is elicited (i.e., prompted), showing that certain text prompts are likely to yield responses with different psychometric properties. Finally, we provided the developed TAPS scoring algorithms so that researchers can automatically score organizational narratives for any of the TAPS constructs. Finalized Text-Based Attitude and Perception Dictionary The TAPS algorithms generally demonstrated strong evidence of reliability and validity. However, after examining psychometric evidence across the constructs, we decided to revise TAPS by removing constructs with poor psychometric properties. Most notably, neither organizational commitment--continuance, task variety, nor task identity exhibited favorably psychometric properties, particularly in terms of convergence. Given the lack of psychometric support for these constructs, they were removed from TAPS. After removing these three constructs, the remaining 25 TAPS constructs displayed favorably psychometric evidence. First, theme scores were substantially higher for comments coming from narrative prompts more relevant to the targeted construct ( -d = 2.60). Second, the median split-half reliability was.84 and the median test-retest reliability coefficient for valence scores was.67. Third, the median convergent correlation between valence scores and Likert composite scores was.63 when text was taken from construct-relevant narrative prompts. Additionally, when using the theme scores to filter text based on relevance, the median convergent correlation was.60. All told, the developed algorithms exhibited strong reliability and validity evidence, and the result is a set of algorithms that can be applied to organizational data to understand employee attitudes and perceptions from text. Implications There are several major implications to this research. First, the TAPS algorithms are made freely available (https:\\/\\/osf.io\\/9dx6n\\/? view_only=1d64b8126df54a1ba17d9f7306d7dd5e), thus allowing researchers to automatically score organizational text according to any of the TAPS constructs. No such algorithms existed prior to this work, and the TAPS algorithms fill this void in hopes of facilitating greater methodological flexibility in understanding employee attitudes and perceptions at work. Rather than needing to collect large amounts of data and develop algorithms from scratch, researchers can now quickly and easily leverage the TAPS algorithms in research or applied practice. These can be used to target a specific construct from the TAPS dictionary, or to broadly assess many TAPS constructs, based on research needs. Second, this study demonstrated the benefits of using more sophisticated NLP algorithms. Namely, transformer models produced more valid valence scores than scores trained via supervised BOW (i.e., random forests), with transformer valence scores having average convergent validity values 33% higher than the supervised BOWmethod. This difference is seen because BOWmethods fail to capture nuance in the data. For example, a BOW model will not adequately distinguish that \\\"This job is great\\\" and \\\"This job is great if you like working for Satan\\\" reflect drastically different levels of sentiment. BOWmethods work fine in some instances, but they can be tripped up easily, and it is an outdated NLP method. Transformer neural networks were trained to understand and predict language, and therefore, they develop a more accurate representation of text when applied to recreate target scores in supervised machine learning tasks. Given the superiority of transformers found here, we encourage researchers and practitioners to explore transformer models for other NLP endeavors, which can easily be implemented via user-friendly interfaces such as HuggingFace. Third, this study highlighted the importance of scoring text according to both theme and valence scores. This is particularly important when scoring diverse sets of text that may or may not be relevant to constructs of interest. Just like with other forms of assessments (e.g., interviews, Likert surveys), reliable construct information is unlikely to be obtained unless that information is explicitly prompted (i.e., asked about). Because of the open-ended nature of employee comments and text, many comments will fail to provide relevant information about constructs of interest. However, theme scores serve to determine whether a comment is relevant to a construct, and therefore allows for comment filtering. For example, even if a survey only contains a generic open-ended prompt such as \\\"tell us what you like about your job,\\\" the responses can be filtered to those pertaining to constructs of interest. This curates valence scores to only those likely to be accurate. As such, practitioners and researchers should use theme scores as a required hurdle before interpreting valence scores for a particular construct. Additionally, theme scores help summarize what employees are talking about, which helps realize the benefits of open-ended text in the first place. For example, theme scores can be used to identify employee concerns that even the survey creator had not considered. Whereas Likert questions are deductively driven, open-ended narrative comments yield attitudinal information in a more inductive manner. As an illustration, in a follow-up project to this work, we used TAPS to score narrative responses within an operational survey and found 3-5 topics that employees discussed frequently but which had not been considered prior to survey implementation. The organization had not considered these topics as relevant, which led to consideration of ways to address employee concerns regarding role conflict, promotional bottlenecks, and pay inequity. Theme scores can therefore help summarize employee concerns, serving as a powerful analytical tool for researchers. Limitations and Areas for Additional Research Several additional limitations and areas for future research should be noted. First, although this study used a large and diverse sample, concerns regarding the generalizability of the developed algorithms are still pertinent. For example, it is unclear howwell the developed algorithms will generalize to different types of narratives than those used in this study. We utilized a mix of narrative prompts and intentionally varied them in level of specificity in hopes algorithms would then be more generalizable, a strategy that has been shown to increase algorithm generalizability in other settings. However, for stronger conclusions of generalizability, evidence must be obtained showing the valence scores generalize to other types of narrative prompts, including to nonsurvey data (e.g., blogs, internal communications). We suspect the algorithms will work well when used to score open-ended responses from attitude and perception surveys. However, they are likely less appropriate for scoring text that differs more in intent, such as employee emails. Not only is the language structure different for such communications, but topic relevance to the targeted constructs is also less likely. In other words, construct activation potential would likely be low for such mediums, and this would be reflected by lower theme scores. Future research should examine this. In relation, we also encourage researchers to apply TAPS to new organizational data and to examine how TAPS scores relate to organizational outcomes and other variables within the constructs' nomological networks, including the degree to which TAPS scores predict future work behaviors (e.g., job performance, turnover) above and beyond traditional Likert scores. Given that the method of measurement impacts the variance captured , it may be beneficial to investigate how NLP-based scores relate to outcomes such as these. Open-ended comments should allow greater contextualization and elaboration than more rigid survey measures, and it is therefore possible that text-based scores capture unique variance beyond traditional Likert scores. For example, employee work comments may contain nuggets of information pertaining to a wide variety of perceptions that are casually related to turnover. Organizations are generally unable to include closed-ended items to assess all possible work attitudes and perceptions, but open-ended comment boxes can assess a hypothetically unlimited number of constructs. Analysis of the narrative comments may reveal that employees are discussing several themes with high frequency (i.e., theme scores are high for certain constructs), perhaps which the organization researchers had not considered. Likewise, the researchers may discover that certain constructs (e.g., diversity climate perceptions) have low valence scores for employees who ultimately turnover. Such analyses could be quickly performed using TAPS, and TAPS may enable increased prediction of important work outcomes like job performance or turnover when paired with more traditional predictors. This is just one example, and TAPS can be applied to myriad other research domains to help advance theory. Because NLP scores might capture different types of variance than traditional self-report measures, using TAPS could possibly help avoid common method variance within research designs. Given the vast number of constructs assessed by TAPS, the possibilities are beyond this discussion section. More generally though, we believe TAPS is a powerful method for exploratory research, given that it quickly measures dozens of work constructs from openended text. We also believe it has notable methodological potential, including in multitrait-multimethod investigations , and we encourage researchers to leverage TAPS in future qualitative work. Second, although TAPS measures a wide array of organizational constructs, there were necessarily some constructs that we could not assess given limitations to survey length in this study. That said, the procedures applied in this study could easily be adapted to train text to predict most any organizational construct, and future researchers may do so if there is a need. Third, we wanted to discuss a methodological choice that often dictates convergent correlations for machine learning scores, and that is the source of the target variable. When assessing constructs based on text, two designs are traditionally used to establish target scores: SME-based ratings of the input text and self-report scores on the targeted constructs. For SME-based ratings, humans review the text and make ratings of the targeted constructs. Thus, each document is reviewed by a rater(s), and that rater infers construct levels based on the text. On the other hand, self-report target scores are not based on the text but rather established via another measurement medium (e.g., Likert items). This was the approach taken for this study, such that target scores were self-reported Likert composites. When correlating the established NLP scores (i.e., valence scores) with target scores, correlations will be larger when the target scores are based on observer ratings. This is because the input predictor data (i.e., text) is identical to the data used to infer target scores (i.e., text). Similar to the discourse regarding the traitreputation-identity model , the text data may ormay not be relevant to a person's internal perceptions regarding their construct standing. Only if the narrative instructions elicit an abundance of relevant text information pertinent to the targeted construct will the developed scores accurately converge with selfreported construct scores. Respondents self-reporting their scores have access to their thoughts, emotions, and perceptions, some of which may not be expressed in the open-ended text. Only when there is strong convergence between the written text and the person's internal thoughts will convergence be high, with this applying to either human judgments or NLP-derived valence scores. Given this, there are pros and cons to using either of the approaches to derive target scores. SME-based target scores will exhibit stronger convergence with NLP scores, and this design makes it easier to compare the reliability of NLP scores versus human-rated scores. On the other hand, self-reported scores likely provide a better reflection of true respondent construct values. Ultimately, in this study, we used self-report scores as the target variables because it was the most feasible design given the large sample of data collected. Coding narratives for 28 constructs would have been laborious with such a large sample. Nonetheless, there are benefits for future research to compare NLP convergence with SMEbased target scores. Furthermore, it is probable that the captured variance will differ based on which method is used to train the NLP algorithms. Little research has directly examined this issue within NLP or machine learning contexts (;, for an exception), but we believe such research is needed to better understand each design and better compare findings across NLP studies using different designs. Conclusion Although employee attitudes and perceptions are typically captured using closed-end numerical ratings (e.g., Likert scales), openended comments can provide unique, nuanced, and contextualized information. This study created and then empirically tested the largest set of organizational sciences NLP algorithms to date. The TAPS Dictionaries exhibited evidence of reliability and validity, though valence score validity differed across types of narrative prompts and based on the construct relevance of responses.We believe that scoring systems such as TAPS offer organizations a powerful toolkit to better understand employees, and we hope future research tests the limits of employee attitude measurement via natural language processing.\",\"577240741\":\"A wealth of evidence suggests that human personality traits differ across geographical regions. Such geographical variation in personality has been shown to predict a broad array of psychological, political, economic and health outcomes. However, one important question remains: What generates these geographical differences in personality? Humans constantly experience and react to ambient temperature. Because temperature varies markedly across the world, it is conceivable that temperature shapes the fundamental dimensions of personality by affecting the habitual behaviours that underlie personality traits. Temperature may shape personality directly by influencing individual behaviours (for example, exploring outdoors versus staying indoors), and less directly by influencing collective activities (for example, agriculture) that guide individual behaviours7. Consequently, regions with different ambient temperatures may result in different patterns of personality traits. Personality is defined as \\\"the interactive aggregate of personal characteristics that influence an individual's response to the environment\\\"8. The hundreds of personality traits used to describe humans are largely captured by five broad dimensions, often called the Big Five: agreeableness, conscientiousness, emotional stability, extraversion, and openness to experience9. These five personality factors can be further aggregated into two higher-order factors: 'Alpha' (agreeableness, conscientiousness, and emotional stability), which represents a socialization and stability factor, and 'Beta' (extraversion and openness to experience), which represents a personal growth and plasticity factor. We propose that ambient temperature clemency is a key factor that relates to personality. This proposition is rooted in the fact that, as a warm-blooded species, humans have the existential need for thermal comfort. Clement (that is, mild) temperatures encourage individuals to explore the outside environment, where both social interactions and new experiences abound; by contrast, when the ambient temperature is either too hot or too cold, individuals are less likely to go outside (for example, to meet up with friends, or to try new activities). This perspective is consistent with attachment theories, which state that individuals are more likely to explore their environments when they feel psychologically secure. Based on this reasoning, we hypothesize that individuals who grow up in more clement temperatures will be higher on both the socialization factor (Alpha) and the personal growth factor (Beta). Regarding the socialization factor Alpha, research has found that personality traits develop partly through social interactions. More clement temperatures facilitate social contact, for which agreeableness, conscientiousness and emotional stability are important. Moreover, clement temperatures have been shown to enhance positive mood and lead individuals to behave more prosocially. Regarding the personal growth factor Beta, more clement temperatures promote a wider range of activities, which may lead individuals to become more extraverted and open to new experiences. A study of 49 cultures revealed that the mean temperature of a cultural region was positively related to people's perception of how extraverted and open a typical person in that culture is; however, this study did not examine how temperature was related to actual personalities beyond these stereotypical perceptions. In another study of 1,662 Chinese residents, individuals from provinces with more clement temperatures scored higher on individualism -- a cultural value dimension that is positively correlated with extraversion. Our temperature clemency perspective adds to several key theories that speak to geographical variation in personality. First, subsistence style theory posits that different subsistence strategies can produce geographical differences in personality-related cultural constructs7,28; for example, members of farming and fishing communities, which value harmonious social interdependence, show greater holistic tendencies than members of herding communities, which value individual decision making and social independence. Second, the selective migration theory of personality proposes that selective migration patterns can produce geographical differences in personality. According to this view, people selectively migrate to regions that fulfill and reinforce their physical and psychological needs3. Third, the pathogen prevalence theory of personality suggests that, as a self-protective mechanism, individuals exhibit lower extraversion and openness to experiences in regions with a higher prevalence of disease-causing pathogens. There is ample evidence that ambient temperature affects agricultural activities (when and what to farm) individuals' migration decisions3, and pathogen prevalence. Thus, ambient temperature likely has important explanatory power for geographical variation in personality. Overall, our temperature clemency perspective of personality offers a mechanism for why and how macro-level environmental forces might shape individual-level personality. Several methodological problems have plagued the few investigations into geographical differences in personality. Most notably, previous research has largely focused on personality differences across broad geographical regions (for example, across countries) which makes it difficult to eliminate the confounding effects of other variables, such as between-country cultural differences. To isolate the effects of ambient temperature from such confounding variables, the use of subject samples from within a single country is preferable. Of course, that single country must be of substantial geographical size to capture sufficient variance in temperature. A related issue with examining personality at the country level is that analyses at such a broad level may obscure meaningful withincountry regional variances in temperature and personality. Thus, it is important to analyse the effects of temperature on personality at the lowest geographical levels feasible -- that is, at the city level or even the ZIP-code level -- within which variances in both temperature and personality differences will be minimal. To overcome these methodological concerns, we conducted two separate, large-scale studies within two geographically large yet culturally distinct countries: China and the United States. Given that the period from birth to adulthood is crucial for personality development, for each participant, we collected meteorological data of the geographical location where he or she grew up. In study 1, a total of 5,587 university students (42.4% females, mean age = 22.07 years, s.d. age = 2.05) who were born and raised in 59 Chinese cities completed a personality survey online in return for individualized feedback (see Supplementary Information for details). These cities covered all provincial-level administrative divisions in continental China (Fig. 1; Supplementary Table 1). To preclude reverse causality, in which certain personalities may cause individuals to migrate to cities with certain temperatures, we limited our sample to students who had spent their pre-college youth in their birthplace. To rule out another alternative explanation -- that parents with certain personalities chose to migrate to a certain city and then gave birth to children who resemble their personalities -- we further limited the sample to participants whose birthplace matched their ancestral home (that is, jiguan, the home of their patrilineal ancestors). Importantly, all results remained substantively unchanged without these exclusion criteria. In line with past research, we computed a 'temperature clemency' variable, -|mean temperature - 22 degC|, which measures the extent to which a city's ambient temperature is close to the psychophysiological comfort optimum of 22 degC (about 72 degF). Thus, the further a city's temperature is from 22 degC, the less clement it is. At the city level, temperature clemency was positively correlated with both Alpha and Beta, as well as with each of the Big-Five personality factors (P < 0.05 for all personality factors except agreeableness, for which P = 0.160; see Supplementary Table 5). Figure 1 maps the temperature clemency of each city with Alpha and Beta, respectively. By contrast, air pressure or wind speed was not significantly correlated with Alpha, Beta or any of the Big-Five personality factors (P > 0.23 for all personality factors). Because the 5,587 participants (level 1) were nested within the 59 cities (level 2), we conducted multilevel analyses to account for the statistical dependence within each city and the fact that different cities had different sample sizes (see Supplementary Tables 6-12). Consistent with the city-level correlational results, temperature clemency was positively associated with Alpha, Beta, and each of the Big-Five personality factors, even after accounting for individual -level control variables age, gender, and acquiescent response style (P < 0.05 for all personality factors) and city-level control variables population density, gross domestic product (GDP) per capita, average annual rice-farming area, average annual wheat-farming area, influenza incidence, and the standard deviation of the mean temperature (P < 0.01 for all personality factors). As a robustness check, we also computed another version of temperature clemency using -(|minimum temperature - 22 degC| + |maximum temperature - 22 degC|)13,14. All results remained substantively unchanged when we used this measure as the predictor in multilevel analyses (P < 0.01 for all personality factors) (see Supplementary Table 13). Figure 2a compares the effect sizes (calculated by t-to-r transformation) of all the predictor variables and highlights the importance of temperature clemency in the Chinese sample. In addition to multilevel analyses, we conducted machine-learning analyses to explore which of the variables were likely important predictors of personality (see Supplementary Figs. 8-12). Consistent with the results of multilevel modelling, conditional random forest analyses reliably identified temperature clemency to be an important predictor of each of the seven personality factors. For analytical details, see Supplementary Information. In support of our temperature clemency perspective of personality, study 1 revealed a relationship between ambient temperature clemency and personality among Chinese participants: individuals who grew up in cities with milder temperatures scored higher on both the socialization factor (Alpha) and the personal growth factor (Beta) of personality, as well as on each of the Big-Five personality factors. Study 2 sought to extend study 1 in several important ways. First, we investigated whether the effects of temperature clemency on personality factors would replicate in the United States, another geographically large yet culturally distinct country. Second, we more closely scrutinized these effects by collecting data at the lowest geographical level feasible: the ZIP-code level. Third, to examine the robustness of these effects, we used another well-validated measure of the Big-Five personality factors (see Supplementary Information). Fourth, we used an even larger participant sample (N > 1.6 million) that was representative of the general US population in terms of age, social class, and education levels (age range = 16-60 years, as opposed to only university students). Study 2 involved 1,660,638 Americans who participated in return for a customized personality evaluation (65.3% female, mean age = 27.05 years, s.d. age = 11.00, 17.0% with a college degree, 9.44% with a graduate degree) (for details, see Supplementary Information). Participants reported the US ZIP code where they spent most of their youth (12,499 US ZIP codes in 8,102 cities). As in study 1, we operationalized ambient temperature as being 'more clement' to the extent that it is closer to 22 degC. Replicating the Chinese data, multilevel analyses (see Supplementary Tables 17-23) revealed that temperature clemency was positively associated with Alpha, Beta, and each of the Big-Five personality factors after accounting for individual-level control variables age, gender, education, and acquiescent response style (P < 0.015 for all personality factors) and ZIP-code level control variables humidity, wind speed, population density, GDP per capita, the percentages of civilians employed in the primary sector (for example, agriculture), in the secondary sector (for example, construction, and manufacturing), and in the tertiary sector (that is, service) (P < 0.001 for all personality factors). Importantly, temperature clemency was the only meteorological variable that was consistently associated with each of the seven personality factors; for example, neither humidity nor wind speed was significantly associated with emotional stability (P > 0.25 for both humidity and wind speed). Figure 2b compares the effect sizes (calculated by t-to-r transformation) of all the predictor variables and highlights the importance of temperature clemency in the sample from the United States. Consistent with the results of multilevel modelling, machinelearning analyses using conditional random forest again reliably identified temperature clemency to be an important predictor of each of the seven personality factors (see Supplementary Figs. 13-17). In summary, two large-scale studies from China and the United States found that the ambient temperature during an individual's youth was related to the key dimensions of personality: individuals who grew up in more clement regions scored higher on both the socialization factor (Alpha) and the personal growth factor (Beta) of personality, as well as on each of the Big-Five personality factors. These effects were robust when controlling for various factors that might affect personality-related constructs: selective migration, individual response style, demographic factors (age, gender, and education), socioeconomic factors (population density, GDP per capita, rice-farming area, and wheat-farming area), ecological factors (pathogen prevalence), and other meteorological factors (air pressure, humidity, and wind speed). It is particularly telling that our large datasets from two geographically large yet culturally distinct countries provided converging evidence. Taken together, these findings are consistent with our temperature clemency perspective of personality: growing up in temperatures that are close to the psychophysiological comfort optimum encourages individuals to explore the outside environment, thereby influencing their personalities. The present research adds to past theories and findings on how socioecological factors (selective migration, subsistence strategies and pathogen prevalence) are associated with human personality. Theoretically, we point to a probable antecedent of these factors: ambient temperature. Empirically, we explicitly controlled for selective migration, subsistence strategies, and pathogen prevalence in the Chinese sample. Moreover, whereas past studies focused on broad geographical levels (for example, countries), we examined the effects of ambient temperature on personality at the lowest geographical levels feasible (city and ZIP-code levels). Although our temperature clemency perspective of personality offers a mechanism for why and how macro-level environmental forces might influence individual-level personality, we note that temperature clemency is one of many factors associated with human personality. In addition, standardized partial effect size plots (Fig. 2) and variable importance plots (Fig. 3; Supplementary Figs. 8-17) suggest that temperature clemency might be more associated with the personalities of Chinese individuals than the personalities of American individuals. Future research could further examine such cross-cultural differences. In light of the present findings, it is also important to highlight social thermoregulation theory, which posits that people seek 'social warmth' in a cold environment because an important function of social relationships is to facilitate the regulation of body temperature. For example, a recent study found that, compared with residents of warmer climates, residents of colder climates reported a wider variety of social ties. Importantly, social thermoregulation theory and our temperature clemency perspective of personality do not necessarily oppose each other. Although social thermoregulation theory suggests that a cold climate compels people to seek social warmth, our findings suggest that a clement climate encourages people to explore the outside environment to engage in more social activities and new experiences that are conducive to socialization (Alpha) and growth (Beta). While much is known about the effects of temperature on human health and performance, the present research examined its relationship with personality. Our findings offer insights into why people in different regions of the world exhibit different personality traits and behaviours. As climate change continues across the world, we may also observe concomitant changes in human personality. Of course, questions about the size and extent of these changes await future investigation. Methods For analytical details of multilevel and machine-learning analyses, see Supplementary Information. Study 1. This research was approved by the Institutional Review Board of Peking University. All participants completed the 40-item Mini-Markers Scale that assessed the Big-Five personality factors, each of which consisted of eight items (ot[agreeableness] = 0.79, ot[conscientiousness] = 0.88, ot[emotional stability] = 0.85, ot[extraversion] = 0.83, ot[openness to experience] = 0.88) (see Supplementary Information). Based on the personality literature, we then further aggregated the Big-Five personality factors into the higher-level factors of Alpha (agreeableness, conscientiousness, and emotional stability) and Beta (extraversion and openness to experience), both of which demonstrated high levels of internal consistency (ot[Alpha] = 0.89, ot[Beta] = 0.90). For each of the 59 cities, the China Meteorological Administration provided us with city-level data of five meteorological indices across the latest available 40 years (1971-2010): average annual mean ambient temperature (2.2 to 23.3 degC), average annual minimum ambient temperature (- 3.0 to 20.8 degC), average annual maximum ambient temperature (7.8 to 27.8 degC), average annual air pressure, and average annual wind speed. In line with past research, we computed a 'temperature clemency' variable, -|mean temperature - 22 degC|, which measures the extent to which a city's ambient temperature is close to the psychophysiological comfort optimum of 22 degC (about 72 degF). Thus, the further a city's temperature is from 22 degC, the less clement it is. In light of prior research on geographical differences in personality (for example, subsistence style theory and pathogen prevalence theory), we also collected pertinent city-level control variables for at least 10 years that overlapped with participants' childhood: GDP per capita, population density, average annual rice-farming area, average annual wheat-farming area, and influenza incidence (see Supplementary Information). To reduce the effect of idiosyncrasies of any particular year, we computed the mean value across those years for each of these control variables. Moreover, we computed an 'acquiescent response style' score for each participant based on their responses to the personality items of the Mini-Markers Scale; this variable controlled for individual differences in response style, because individuals from regions with more clement temperatures might consistently agree (yea-saying) or consistently disagree (nay-saying) with questionnaire items regardless of their content. Finally, because temperature fluctuation might also affect personality (over and above mean temperature), we also controlled for the standard deviation of mean temperature for each city. Study 2. This research was approved by the Institutional Review Board of the University of Texas. Each participant completed the 44-item Big-Five Inventory (ot[agreeableness] = 0.84, ot[conscientiousness] = 0.86, ot[emotional stability] = 0.88, ot[extraversion] = 0.91, ot[openness to experience] = 0.84) (see Supplementary Information). As with the Chinese data, we aggregated the Big-Five personality factors into the higher-level Alpha and Beta, both of which demonstrated high levels of internal consistency (ot[Alpha] = 0.90, ot[Beta] = 0.86). In addition, participants reported the US ZIP code where they spent most of their youth (12,499 US ZIP codes in 8,102 cities). For each ZIP code, we collected the following meteorological variables: average annual mean ambient temperature, average annual humidity, and average annual wind speed (see Supplementary Information). As in study 1, we operationalized ambient temperature as being 'more clement' to the extent that it is closer to 22 degC. To control for pertinent economic variables, we also collected ZIP-code level data on GDP per capita, population density, the percentages of civilians employed in the primary sector (for example, agriculture), in the secondary sector (for example, construction and manufacturing), and in the tertiary sector (that is, service). As in study 1, we again computed an 'acquiescent response style' variable to control for individual differences in response style.\",\"577240786\":\"Introduction Social media are generating new spaces for public debate. Its growing presence in our society is causing citizens to transfer many of their activities to the digital environment, including political conversation. Its facility to promote connectivity and promote communication flows is generating relevant transformations in political communication. One of the key questions is whether social media can alter the power relations from the mass communication era. During the second half of the 20th century, political actors were decisive actors in determining the issues, contents, and limits of public debate in conjunction with journalists. That made them great influencers, endowed with the ability to persuade citizens and condition public opinion. However, the rise of social media is reconfiguring its position. In this context, it is important to identify the factors that determine and condition the influence and digital authority of politicians in the public debate developed on this digital platform. To do this, we use a big data sample of 127.3 million tweets, obtained using the machine learning technique and analyzed using the social network analysis methodology. The Reconfiguration of the Influence of Political Actors on the Public Debate on Twitter Along with journalists, political leaders and parties have traditionally been configured as a crucial actor in political communication. In this conception, the media and politicians are central and indispensable for the creation and articulation of the public sphere. This has placed them as the main protagonists of the political debate in our society and has given them a high capacity to establish the agenda and the predominant frames of the issues capable of capturing the attention of the majority of citizens. As a consequence, both actors have enjoyed a high capacity to influence and condition public opinion. This classic scenario is being deeply altered by the emergence and consolidation of social media. Its ability to introduce new communication dynamics enabling access to the political debate in the digital environment to anybody is transforming the dynamics since the era of mass communication. On the other hand, the growing penetration in different fields and social spheres is providing them a central position in our daily lives. These digital media are increasingly essential to the development of our routines to the point that they have generated a platformization of society. This process is causing a reorganization of the social and political practices around digital platforms. Also, it is placing these devices as a vector of social transformation in our societies in a context of deep mediatization. A consequence of this is that social media are emerging as new places for political debate for a growing number of social actors. The increased use of these platforms to consume news is making them a relevant forum for public conversation. In this sense, Twitter plays a prominent role because it is the digital medium most oriented toward the dissemination, and discussion, of information and news. Also, it is the favorite place for political actors to communicate. As a new scenario, it imposes a new logic and conditions on the political debate and introduces new dynamics that can potentially redefine who and how exerts social influence. The emergence of Twitter as a place for political debate offers opportunities and risks for political actors. The first advantage is enabling these actors to create and share messages autonomously, without needing the media to communicate to many in an interpersonal way, while reaching a wide audience with a single click. Politicians can thus speak directly to their constituents without relying on third parties. The immediacy, speed, and brevity of this microblogging platform facilitate self-communication focused on the political proposals that make up their electoral program. Likewise, it offers a channel to express their opinions regarding social issues. Political actors have found on Twitter a way to reach a mass audience using a more personal or intimate side. This strategy is mainly deployed in the candidates' profiles, who take advantage of this platform to assume prominence, build their brand and gain public attention, to the detriment of their parties. This can create a sense of connection between politicians and citizens, giving way to a communicative strategy based on personal leadership. In this attempt to reach out to their voters, political actors have seen a way to reach a wider audience and even mobilize and persuade their supporters. Although parties and politicians do not employ Twitter to promote and encourage dialogue and direct interaction with citizens , previous literature suggests that social media play a significant role in influencing the behavior of voters. Twitter enables the construction of a potentially broad community that identifies with a project and political values. Authors such as Tsatsou and Zhao (2016) even point out how the political commitment created within social media encourages users to become more actively involved and feel part of the political project. Gaining influence in the digital environment is therefore a very valuable resource for political actors. Parties and politicians aspire to condition the online political conversation to influence the agenda and public opinion. Consequently, political actors have incorporated Twitter as a common tool to share their messages, especially during electoral campaign periods. Therefore, this platform has become a reference to articulate the communication strategies of political actors. These actors have found in this medium a way to maximize their ability to approach citizens, directly disseminate their opinions and comments, and improve their public image, thus expanding the range of instruments that they can employ for their selfpromotion. However, Twitter also entails challenges for political actors. The main one is the increase in competition when articulating and conditioning the political debate. As opposed to a restricted and limited scenario characteristic of mass communication, in which political actors assumed a dominant role, social media have opened the public discussion to new actors, becoming a more open and competitive scenario. The ease of accessibility and low economic cost enabled the entry of new participants to these platforms, many of them previously located on the periphery of the political system capable of producing and disseminating content and information to take part in the conversation. This rise in competition challenges both the place and the influence capacity of political actors in the political debate on Twitter. Some research affirms that this phenomenon is causing a breakdown of the monopoly of the political elite, and also of the conventional media, on political discussion and the articulation of public opinion. This fact would lead to a weakening of their authority and social influence that would lead to the appearance of more difficulties to persuade and convince citizens. However, other authors dispute this thesis of the response of the traditional actors. Taking into consideration the prominence of the rules of the attention economy, on Twitter, although all users can access the political conversation, not all have the same options to effectively condition it. On digital platforms, it is easy to claim influence, but not so easy to exercise it successfully. Consequently, from this perspective, social media, and Twitter, in particular, tend to reinforce the authority of those actors, such as politicians, who already occupied a central role in the discussion networks. Method Sample and Data The main objective of this research is to identify the determining factors that condition the digital authority obtained by political actors in the conversation generated on Twitter. Specifically, we want to know if factors such as (a) Digital Popularity; (b) Political Ideology; (c) Political Initiative; (d) Political Career; and (e) Electoral Support condition the digital authority that political actors acquire in the political debate on Twitter about the formation of a government after the elections. The methodology applied in this research is based on social network analysis. This approach is appropriate for analyzing the interactions and relationships between political communication actors. The machine learning technique is applied blending automatic coding with a manual one. Employing specifically designed algorithms, we have identified the messages published on Twitter concerning the negotiation process of Government formation after the elections in Spain. Subsequently, the tweets obtained were reviewed and refined to guarantee the relevance of the sample. To measure digital authority, the eigenvector centrality has been used as an indicator of measurement. It measures the centrality of a user within a particular social media. To do this, the software assigns scores between 0 and 1 to all users of a network according to their degree of direct or indirect connection with other users. Those with higher scores have a greater capacity to condition the conversation since they are better connected in the network. The sample refers to a highly relevant political event produced after the elections: the negotiation process for the Government formation. The case of Spain between 2015 and 2016 has been marked as the object of study. The analyzed period runs from December 20, 2015, the day the elections were held, to the dissolution of the Spanish Parliament, and the calling of new elections on May 3, 2016, after the failure of the negotiations. Thus, 133 days have been analyzed. To obtain the data, 145 Twitter profiles from the political and journalistic fields were used. The selection was determined considering their relevance in Spanish political life. Therefore, it is a strategic sample. In a second phase, the followers of these 145 profiles, which together account for 24 million profiles, have been incorporated into the analysis. The size of the sample of tweets derived from this set of profiles made its study unfeasible. For this reason, we decided to limit the analysis to the 30,000 most influential profiles by applying a PageRank calculation. After manual review, duplicate profiles and bots have been removed. Thus, the total of Twitter profiles analyzed has been 24,389 accounts. The tweets generated by these users during the period studied were a total of 127.3 million messages, which is configured as the final sample of this research. The sample has been collected from three networks or political communities, corresponding to three Spanish cities. This choice is based on the number of the population and its relevance to Spanish political life and its different relationship with the centers of power. Madrid, the capital of Spain, with 3.1 million inhabitants in 2016, is the headquarters of the political institutions of the State such as the Parliament or the Government. Madrid is the center of political life. Second, the city of Barcelona has been selected, with 1.6 million inhabitants, which in recent years has experienced the rise of an independence process that aspires to create a new country separate from Spain. Finally, the city of Valencia has been included, with 787,000 inhabitants, which occupies a peripheral position with respect to the political, economic, and media centers of power. In each of these networks, the most influential users have been analyzed in terms of their digital authority. In some cases, the same Twitter user manages to be present in two or all three networks simultaneously, while in others, they only manage to access one with sufficient digital authority. Despite being a large variety of profiles in the sample, we have focused only on political actors. Within this group, three types have been defined: (a) candidates, who correspond to the leaders of the parties that have stood in the elections; (b) other politicians, which incorporates the rest of the individual political actors; and (c) political parties, which includes the profiles of political organizations. Subsequently, a classification has been made to separate individual actors (candidates and other politicians) and groups (parties). To meet our objectives, a specific sample comprised of the 250 political profiles with the highest eigenvector centrality has been extracted from the general sample. To determine the factors that most decisively condition the political debate on Twitter, Pearson correlations have been made, whose values range between -1 and 1, between the digital authority of the political profiles included in the sample and five variables: digital popularity, political ideology, political initiative, political career and electoral support (votes). Measurements Digital Popularity. Digital popularity is related to the visibility that a specific user acquires within a network. The main element of this indicator is the number of followers. Thus, the larger the number of followers of a user, the greater its impact on the circulation of information and its popularity on the Internet. However, when we talk about Twitter, some previous research  has highlighted that having a very high number of followers does not necessarily imply influencing the conversation. The popularity measure applied in this research is the Follower Rank, the standardized version of the traditional in-degree measure. The formula to calculate the Follower Rank is the following: number of followers of the user \\/ number of followers of the user + number of followed by the user. This indicator ranges from 0 to 1. The closer to the maximum value, the higher the popularity of a user. Political Ideology. Political ideology is the set of ideas, values, and fundamental postulates that define a political party regarding the functioning of a society and its institutions. In this research, the analyzed political actors have been classified according to the following ideological divisions: (a) left-wing, (b) right-wing, or (c) independentists. This last category includes political actors who seek to create their state and separate from Spain. Political Initiative. The degree of political initiative refers to the position taken by the analyzed political actors regarding the negotiation for the formation of a Government after the elections. In this analysis, the political initiative was taken by the Socialist Party (PSOE) and its leaders, in charge of leading the negotiations to obtain the support of the rest of the political forces present in the Spanish Parliament to try to govern. In this case, the political actors have been classified according to whether they led the negotiation (1, in the case of PSOE and its members) or not (0, the rest of the parties and their members). Political Career. The political career refers to the parties' historical path, and it is determined by their year of creation. In the Spanish case, until the 2015 elections, only those political organizations from the period of recovery of democracy at the end of the 1970s, endowed with a long tradition, had entered the Parliament. However, in 2014, new political parties were created and obtained seats in the Parliament. Podemos emerged on the left, and Ciudadanos on the right. Therefore, depending on their history, political actors have been classified as (a) traditional or (b) emerging. Electoral Support (Votes). Voting for a political option refers to the level of support a political option gets from the public in elections. The measure used to calculate this indicator is the percentage of the vote obtained by Spanish political parties in the elections held in December 2015. For this, official data provided by the Ministry of Home Affairs of the Spanish Government have been used. Spain as a Case of Study The general elections held in Spain in December 2015 were significant as two new political parties emerged: Podemos and Ciudadanos. The appearance of these new parties marked the end of the dominant bipartisanship in the past 40 years, embodied by the Popular Party and the Socialist Party. The elections held in December 2015 resulted in a highly fragmented Parliament where many political options won representation. This aspect forced all the parties with representation to open a negotiation process to elect a Prime Minister. In this context, the Socialist Party (PSOE) took the lead in the talks to form a new government. However, no agreement was reached, and the socialist candidate, Pedro Sanchez, did not obtain the necessary support to become the Prime Minister. This fact led to the dissolution of Parliament in May, and new elections were called. New elections were held on June 26, 2016. Meanwhile, since 2012, Catalonia has experienced a complex political process in which a large percentage of Catalan society supports the independence of the territory from the Spanish State. From that moment onward, a political and civic movement was launched to create a state of its own. This has led the Catalan proindependence parties to incorporate this issue as a priority on their political agenda. In consequence, this issue has transcended to the national political agenda. Results: Conditioning Factors of the Conversation About the Formation of a Government After the Elections on Twitter The main objective of this research is to know what factors are determining for a political actor to achieve greater digital authority in the debate generated on Twitter about the formation of a Government after the elections. To do so, Pearson correlations have been made between eigencentrality and five variables: (a) Digital Popularity; (b) Political Ideology; (c) Political Initiative; (d) Political Career; and (e) Electoral Support. The correlations of these factors with digital authority have been developed in each of the political communities studied: Madrid, Barcelona, and Valencia (Table 1). Likewise, individual actors (candidates and other politicians) have been grouped in a single category and groups (parties) in another. The results obtained demonstrate different operating dynamics between the three networks. Digital Popularity Popularity, measured with the Follower Rank, is not a determining factor to condition the conversation on Twitter according to the data obtained (Table 1). However, slight differences are detected depending on the context of the digital debate in which it is inserted. In other words, the characteristics of each network studied and articulated around one of the three cities that conform the sample, slightly determine the results. The Barcelona network is conditioned by the prominence of the independence movement. Here, we found out that a high level of Follower Rank does not involve a high digital authority on Twitter. This is more significant in the case of the parties (r = -.750; p < .01), since the independentist organizations, despite having a smaller number of followers, obtain high values of eigenvector centrality. This fact determines its important ability to influence the political discussion about the formation of the State Government on Twitter. On the other hand, the Spanish parties, which obtain a higher Follower Rank, have a lower incidence in the political debate on this network. The analysis of the candidates and the other politicians in Barcelona (r = -.134; p < .01) offers the same pattern. Although the correlation is less intense, greater popularity is not associated with greater digital authority. In the cases of the networks of Madrid, the political center of the country, on the one hand, and Valencia, which occupies a peripheral political position, on the other, the results reveal the absence of correlation between popularity and digital authority concerning candidates and other politicians (Table 1). However, the case of political parties presents a difference. Both in Madrid (r = .436; p < .01) and Valencia (r = .476; p < .01) it is observed that the relationship between popularity and the parties' digital authority is relevant, although it assumes moderate values. Therefore, there is a connection between popularity and authority in these actors to condition the debate, although at a low level (Table 1). Political Ideology Data show that the ideology of political actors is a key factor in determining their digital authority on Twitter (Table 1). This is especially relevant in the Barcelona network, where local parties propose a political project based on independence from Spain. The proindependence parties (r = .966; p < .01) have the largest capacity to condition the debate about the formation of a Government on Twitter. The case of Esquerra Republicana de Catalunya (ERC) stands out, as it obtains an eigenvector very close to one (EC = 0.990440785) within this political community. In contrast, the Socialist Party (EC = 0.174360535), which nominates its candidate as a potential president for the Spanish Government, has a low level of digital authority. The data reveal that in this network, the digital authority and the ideology of the candidates and other politicians (r = .660; p < .01) are also correlated, but with a lower intensity than the parties. In this context, the proindependence politicians obtain a greater eigenvector centrality. This is the case of Oriol Amoros (EC = 1), ERC deputy in the Parliament of Catalonia, or Oriol Junqueras (EC = 0.8782900236), leader of this party. Consequently, despite the fact that the discussion revolves around a Spanish event, the proindependence parties and politicians have the highest influence on the conversation on Twitter in this political community. On the other hand, in the networks of Madrid and Valencia, left-wing political options have the highest capacity to condition the debate on the formation of the Government on Twitter (Table 1). In Madrid, the relationship between digital authority and ideology is larger in the case of the parties (r = -.483; p < .01) than in the candidates and the rest of the politicians (r = -.332; p < .01). The case of the Socialist Party (EC = 0.945235956) and Pedro Sanchez (EC = 0.833164395) is particularly noteworthy. These are the left-wing political actors who achieve greater digital authority. However, among the actors with a higher eigenvector centrality in the Madrid network, there are also other left-wing parties such as Podemos (EC = 0.538953733) or Izquierda Unida (IU; EC = 0.692618237), and their leaders, Pablo Iglesias (EC = 0.72669794) and Alberto Garzon (EC = 0.70632856), respectively. On the Valencia network, the pattern is similar. Left-wing politicians and parties obtain the highest values of influence in the political debate on Twitter about the formation of the Government after the elections. Here, the relationship between digital authority and ideology is especially notable concerning candidates and other politicians (r = -.630; p < .01). In this political community, the politicians who obtain greater authority are members of the Socialist Party (PSOE), the organization that leads the formation of the Government. Thus, the first positions are held by Angeles Alvarez (EC = 0.952002703), deputy for Madrid; Carmen Monton (EC = 0.944740604), at that time Minister of Health and Public Health in the Valencian Community; and the socialist candidate Pedro Sanchez (EC = 0.942920373). In the case of the parties (r = -.366; p < .01), the correlation reaches lower values, although it is also observed that the left-wing parties have the highest digital authority on Twitter. Particularly noteworthy is the Socialist Party, whose eigenvector centrality is 1, as well as IU (EC = 0.440900275) and Compromis (EC = 0.368310262), a local left-wing party. Political Initiative The degree of political initiative, that is, the position adopted by political actors in the negotiations for the formation of a Government in the political sphere is also a key factor that determines the digital authority in the conversation on Twitter (Table 1). This correlation is especially strong with respect to political parties in the networks of Madrid (r = .902; p < .01) and Valencia (r = .969; p < .01). In both cases, the Socialist Party (PSOE), who took the initiative in the negotiations on the formation of a Government, obtained the highest digital authority on Twitter both in the political community of Madrid (EC = 0.945235956) and Valencia (EC = 1). In this last network, we also observe that the correlation values are high with regard to the candidates and the other politicians (r = .826; p < .01). In the political community of Barcelona, in which the proindependence political options have a greater presence, the effect is the opposite: they are the parties (r = -.592; p < .01) and the politicians (r = .360; p < .01) that have a secondary role in the negotiations for the formation of the Government, those that have a greater digital authority in the debate on Twitter (Table 1). Thus, proindependence political actors, such as ERC (EC = 0.990440785) and Junts pel Si (JxS; EC = 0.738721906), can condition the digital discussion around a relevant aspect at the state level such as the formation of a Government after the elections. Political Career The data show that the political career is related to its digital authority on Twitter (Table 1). Political actors with a broader history have more influence on the debate generated on this platform regarding the formation of a government. By contrast, emerging parties and their candidates achieve less digital authority. A dynamic that is repeated in a very similar way in the three networks analyzed. In the Barcelona and Valencia networks, we find similar results both in the case of parties and politicians (Table 1). In the political community of Barcelona, the parties (r = -.523; p < .01) more consolidated in the Catalan political system, and their leaders (r = -.344; p < .01) have more capacity to condition the digital conversation (Table 1). In this case, there is a coincidence. In addition to the Socialist Party (PSOE), the parties with the longest tradition in Catalonia are those with a proindependence tendency such as ERC (EC = 0.990440785) and JxS (EC = 0.738721906), formerly called Convergencia i Unio. Besides, candidates and politicians who belong to these parties obtain higher levels of digital authority. In the network of Valencia, digital authority is also related to the career of parties (r = .447; p < .01) and politicians (r = -.326; p < .01). Once again, the most consolidated political options in the Spanish political system obtain the greatest capacity to influence the discussion on Twitter. Two examples of this are the traditional political parties in the Spanish case: the Socialist Party (PSOE; EC = 1) and the Popular Party (PP; EC = 0.462457444). In the political community of Madrid, the correlation between the career and the digital authority is the most accentuated, primarily in the case of political parties (r = -.745; p < .01). In this political network, it is the traditional parties, the Socialist Party (PSOE; EC = 0.945235956), the PP (EC = 0.639033026), and IU (EC = 0.692618237), which have greater digital authority and dominate the conversation on Twitter. On the contrary, the emerging forces play a secondary role in the debate generated in this social media. In this sense, Podemos (EC = 0.538953733) and Ciudadanos (EC = 0.505002104) achieve lower eigenvector centrality values than those achieved by parties with a longer historical path. However, in the case of the candidates and other politicians (r = .028; p < .01) of this political community we do not have enough evidence to consider that their political career is a conditioning factor of digital authority on Twitter. Electoral Support Finally, the percentage of votes obtained in the elections is not a determining factor of the digital authority on Twitter in the networks of Madrid and Valencia. However, in the political community of Barcelona, the data indicate that there is a relationship (Table 1). In this last network, the result of the correlations shows that the electoral support received has a moderate influence on the digital authority of political actors on Twitter. The data show that both parties (r = -.515; p < .01) and candidates as well as other politicians (r = .315; p < .01) with a lower percentage of the vote have greater digital authority on this platform. In this case, the proindependence parties such as ERC (15.98% of votes) or JxS (15.08% of votes) are the ones who acquire stronger relevance in the debate generated on Twitter about the formation of a government after the elections. In the Madrid network, the data reveal that the least voted political options (r = -.182; p < .01), and also their candidates and other politicians (r = -.144; p < .01), are the ones that obtain a higher eigenvector centrality (Table 1). In this sense, the winner of the elections was the PP (33.46% of votes), which obtained a significantly higher percentage of the vote than the rest of the political options. However, it was the Socialist Party (PSOE; 17.88% of votes), Podemos (20.86% of votes), or Ciudadanos (18.80% of votes) who achieved the highest digital authority on Twitter. In the political community of Valencia, the effect of the percentage of the votes on the digital authority is similar to the case of Madrid. The less voted candidates and the other politicians (r = -.380; p < .01) obtain a higher digital authority in the debate on Twitter (Table 1). The Socialist Party (PSOE) and its members (19.84% of the votes) have the largest capacity to influence the political conversation on this platform, despite obtaining a much lower percentage of the votes than its competitors, such as the PP (31.30%) or Podemos (25.09%). Therefore, getting more votes in the elections does not ensure more possibilities to condition the political discussion on Twitter generated after the elections. Conclusion and Discussion Our findings offer several original contributions to the research field of social media, public debate, and digital political communication. Our first contribution shows that digital popularity, measured through the Follower Rank, is not a factor capable of conditioning the political conversation on Twitter. The correlation data indicate that owning more popularity or followers in the digital environment does not lead to having greater digital authority. The case of the Barcelona network is especially noteworthy, characterized by the political process that demands independence from Spain. In this political community, the independentists, whose Follower Rank is lower, acquire a priority role in the debate on the formation of a Government after the elections on Twitter. A second relevant finding is that the ideology of political actors is a central factor in gaining influence in the political debate on Twitter. The data suggest that left-wing political actors, on the one hand, and proindependence parties, on the other, obtain greater digital authority than right-wing political actors. Previous investigations have obtained similar results in the case of the media. This indicates that ideology is a variable with a powerful capacity to condition digital authority in the political conversation of the different actors of political communication. Third, the political initiative is also related to the ability to condition political debate in the digital environment. Thus, the political actors who lead the negotiations for the formation of the Government (Pedro Sanchez and the Socialist Party) obtain a greater centrality in the debate generated on Twitter on this issue. Consequently, actors capable of leading political action, in our case those trying to reach a Government pact, acquire more digital authority and more resources to condition the conversation on this platform. The fourth contribution of our analysis is the following idea: The career of political actors is related to their digital authority. The political actors with more history within the Spanish political system have a more prominent role in the debate on Twitter about the formation of Government, therefore achieving stronger influence. On the other hand, emerging political parties and leaders are at a second level. The historical career emerges as a determining factor of the centrality of the network. However, concerning the media, the pure players, whose appearance is more recent, manage to surpass the legacy media in the digital political conversation in digital authority. Finally, our findings show that the percentage of the vote is not a determining factor to attain a greater digital authority to direct the political conversation about the formation of the Government on Twitter. However, these results are not conclusive since the strength of the correlation is low or moderate in some specific cases. Lastly, the data show that the sociopolitical context decisively conditions the political debate on Twitter. Both the actors with more digital authority and the factors that determine it are decidedly affected by what occurs outside the digital environment. The best example is the Barcelona network. The results about this political community are affected by the process that pursues the independence of Spain that began in 2012. As a result, the most influential political actors on Twitter are the independentists, unlike those actors from other networks such as Madrid or Valencia. Likewise, this also affects the factors that determine the digital authority. Consequently, we can affirm that in the dynamics of influence within this digital platform, the external context is highly important. These results permit us to advance in the knowledge about the influence capacity of political actors on the political conversation in the digital environment. Our research enables us to identify three key factors that determine digital authority on Twitter: ideology, political initiative, and the career of political actors. Also, the external sociopolitical context must be added as an explanatory factor, because it has a remarkable impact on the digital political discussion. These findings open up new ways to understand the functioning and consequences of the digital political debate and enable us to expand the existing knowledge about the role that political actors play in the dynamics of digital political communication.\",\"577240798\":\"Whether moral judgment is a product of reason or emotion has been an ongoing debate among philosophers and psychologists for decades. When moral psychology separated itself from moral philosophy, it almost exclusively focused on reasoning rather than on affective aspects of morality. The first empirical attempts in moral psychology started by examining cognitive-developmental components of understanding fairness and rules. But subsequently, as the field expanded, there was an increasing interest in the affective components of morality. Accumulating evidence suggests that emotion can ensue from, amplify, or directly cause moral judgment. Irrespective of the exact nature of the relationship between emotion and morality, distinct emotions are known to be associated with specific moral concerns as well as moral violations. Emotions are neural and somatic events that have the evolutionary function of preparing an organism to respond adaptively to a change in social or physical circumstances. Once emotions are induced, individuals can consciously experience them by constructing a feeling, that is, generating a conscious mental experience. Constructing feelings of emotions depends on brain systems that map and regulate body responses. Both classic and modern theories of emotion postulate that interoception--the sensing of physiological feedback from the body and its visceral organs--is essential for emotional experience. The link between interoception and emotion continues to be supported by various studies. For example, Barrett, Quigley, BlissMoreau, and Aronson (2004) found that arousal focus, the extent to which individuals emphasize the changes of feelings in their verbal reports of experienced emotion, is related to interoceptive sensitivity. Individuals who were sensitive to their heartbeat change in response to emotion-arousal images reported more intense emotional experiences compared with less sensitive individuals , supporting the association between body feedback and emotional states. Even though emotions have been studied for a long time in psychology, the topographical distribution of the emotion-related body sensations has been identified only recently. Nummenmaa and colleagues (2014) mapped the \\\"feeling space\\\" for different emotions, demonstrating that consciously felt emotions are represented in the human body by topographically distinct maps with partial overlaps. Particularly, some emotions are associated with \\\"activations\\\" in certain body parts, whereas other body regions might be \\\"deactivated\\\" in the same emotional experience. For example, feelings of fear are paired with activations in the chest and head area, whereas feelings of sadness are represented by exorbitant deactivations in lower limbs and slight activations in the chest. To our knowledge, topographical representations of moral emotions ensuing moral violations have not yet been studied. As mentioned, distinct emotions are known to be associated with various violations of moral norms. Two decades ago, Rozin, Lowery, Imada, and Haidt (1999) proposed the contempt, anger, and disgust (CAD) hypothesis, indicating that the \\\"other-condemning\\\" moral emotions of contempt, anger, and disgust correspond to violations of the moral codes of community, autonomy, and divinity. Consistent with the CAD hypothesis and based on the intuitionist perspective on moral judgment, the moral-foundations theory (MFT;) was developed by searching for the best links between anthropological and evolutionary accounts of moral intuitions across cultures. This framework suggests that moral intuitions derive from innate psychological mechanisms that coevolved with cultural institutions. Each moral system produces fast, automatic, gut-level reactions of like or dislike when certain phenomena are perceived in the social world, which in turn guide moral judgments of right and wrong. These systems, according to the MFT, have evolutionarily adaptive underpinnings present in individuals across cultural norms: care, fairness, loyalty, authority, and purity. Violating the norms of care, fairness, loyalty, authority, and purity does not necessarily produce a uniform emotional response. Research suggests that specific predictions can be made regarding the types of response that violations of distinct moral norms may elicit. For instance, the specific-correspondence model  posits that specific moral emotions map onto specific violation types. Therefore, any action in which an entity was harmed should reliably elicit anger, and any action in which a bodily norm was violated should elicit disgust. The specific-correspondence model thus suggests that witnessing harmful actions should uniquely activate a desire to confront the violator, whereas witnessing impure actions should uniquely elicit avoidance. By contrast, constructionist models of moral emotion posit that there are no exclusive links between moral-content domains and elicited emotions. Rather, contextual cues (e.g., framing language) and conceptual knowledge (e.g., who was harmed;) inform the interpretation of moral violation, and no specific stimulus would predictably elicit the same emotion across all moral contexts. Recent research suggests that individuals express distinctively high levels of desire to avoid (vs. confront) violators of purity norms. Violations of other moral norms, however, do not similarly elicit unique patterns of avoidance or confrontation. Thus, behavioral responses to moral violations depend in part on the norm that was violated, with impure acts eliciting a uniquely strong avoidance response. Therefore, it stands to reason that the topography of different moral violations would reveal different maps on the human body. Here, we aimed to examine how emotions associated with violations of moral concerns are topographically represented in the body. We preregistered our specific research questions, all of which were written in the descriptive mode, an approach termed \\\"informed curiosity\\\" by Rozin (2001, p. 2). Specifically, we addressed four preregistered questions in Study 1. First, are body-sensation maps distinct for the five moral concerns posited by MFT? Second, can machine-learning techniques be used to distinguish body-sensation maps associated with different moral violations? Third, can we differentiate the bodysensation maps of people with different political ideologies (e.g., do liberals feel \\\"purity\\\" in the same regions of their bodies as conservatives) and individual differences in moral concerns (e.g., do people who score high on purity concerns feel \\\"purity\\\" in the same parts of their body as those who score low on purity concerns)? Finally, can the topographical body-sensation maps be predicted from the textual descriptions of moral-violation vignettes? In Study 2, we replicated our findings in a nationally stratified sample in the United States. Study 1 Method Participants. Nummenmaa et al. (2014) suggested using at least 40 participants in each group for studies examining self-reported perceptions of feelings. Following our preregistered intention as well as Nummenmaa et al. (2014), we aimed to recruit 600 participants in total, with 120 participants in each condition (i.e., each moral foundation). We requested 300 liberals and 300 conservatives from TurkPrime , and to compensate for potential attention-check failures, we recruited 630 participants overall. Following our preregistration, we excluded participants who failed attention checks, resulting in a total of 596 individuals (age: M = 36.5 years, SD = 11.8 years; gender: female = 355, male = 237, other = 3, unknown = 1). Participants were randomly assigned to five experimental conditions and completed the target task and several individualdifferences measures, explained below. Measures. Moral-violation scenarios. Participants were randomly assigned to five moral-violation conditions based on MFT, and each read a vignette about violation of a particular moral foundation. For each foundation, we selected vignettes from a larger pool of stimuli provided by Clifford et al. (2015). For each foundation (care, fairness, loyalty, authority, and purity), we included four vignettes, matched on average perceived wrongness, arousal, and frequency. A sample vignette is \\\"You see a woman clearly avoiding sitting next to an obese woman on the bus.\\\" After presenting the scenario, we asked participants to respond to the following questions on a 5-point Likerttype scale: \\\"How morally wrong is the action depicted in this scenario?\\\" (1 = Not at all wrong, 5 = Very wrong; M = 3.30, SD = 1.18) and \\\"How strong was your emotional response to the behavior depicted in the scenario?\\\" (1 = Not at all strong, 5 = Very strong; M = 2.90, SD = 1.12). Body-sensation task. Bodily topography of feelings was mapped using the emBODY tool. The online data-acquisition package is publicly available at https:\\/\\/version.aalto.fi\\/gitlab\\/eglerean\\/ embody. Yet some parts of our analytic framework were different from that used by Nummenmaa et al. (2014). Before engaging in the task, participants were shown a brief tutorial video to make sure they fully understood the task. Participants were then asked to color where activations (regions whose activity became stronger or faster) and deactivations (regions whose activity became weaker or slower) were felt in their body. The subtraction yielded a valid operationalization of pixel-level activation to be used in statistical analyses. Specifically, participants viewed two silhouettes, one on the right side and one on the left side of the screen. After reading the moralviolation scenario, participants were asked to draw on the portions of the silhouette where they felt activation (left silhouette) and deactivation (right silhouette). Individuallevel bodily topographies were then computed on the basis of the difference between the left and right silhouettes. Final body-sensation maps were represented by 48,954 pixels. Political orientation. All participants rated their political affiliation with the Republican party or the Democratic party along a 7-point scale ranging from 1 (Strong Democrat) to 7 (Strong Republican). Another item asked participants to rate their political conservatism on a scale ranging from 1 (Very Liberal) to 7 (Very Conservative). We averaged these two items to create a political-orientation score, on which higher scores indicated more conservative political orientation. A similar method was used in previous work for assessment of political ideology. The internal consistency of these two items was relatively high in the current sample (a = .94). We labeled participants who scored 1 standard deviation (SD = 1.86) lower than the average political conservatism (M = 3.83) as liberal (n = 91) and those who scored 1 standard deviation higher as conservative (n = 126). Moral concerns. Individual differences in moral concerns were measured using the 30-item Moral Foundations Questionnaire (MFQ;). The MFQ assesses the degree to which participants deem different considerations as relevant when making moral judgments (1 = Not at all relevant, 5 = Extremely relevant) and their agreement with statements germane to morality (1 = Strongly disagree, 5 = Strongly agree). These items were used to create foundation-level scores for care (a = .69), fairness (a = .63), loyalty (a = .76), authority (a = .76), and purity (a = .83). Participants who scored at least 1 standard deviation above the average of the respective MFQ subscale were considered to have high levels of that moral concern, and those who scored at least 1 standard deviation below the average were considered to have low levels of that moral concern. Procedure. This study was approved by the institutional review board of the University of Southern California (UP-16-00695-AM003). Potential participants were invited to take part in a psychological study on Amazon's Mechanical Turk (MTurk) for monetary compensation. Participation was on a voluntary basis, and participants were compensated $0.50 for their time. Each participant completed the body-sensation task after reading a vignette about a particular moral violation and then completed a set of self-administered measures of political ideology and moral concerns. This study's hypotheses, predictions, and analyses were preregistered on the Open Science Framework (OSF; https:\\/\\/osf.io\\/zbv6e\\/), and all data and materials have been made publicly available at https:\\/\\/ osf.io\\/4tdx5\\/. Data-analysis strategy. As discussed in our preregistration, we examined the topographic representation of each moral scenario across conditions, using the method described in Nummenmaa et al. (2014). Specifically, for each participant, a single map comprising 48,954 pixels was obtained by subtracting the activation and deactivation maps to obtain 48,954 variables per person in a given condition. The differences between activations and deactivations were then assessed using 48,954 univariate t tests: A one-sample t test against zero was performed for each pixel within a condition, resulting in a statistical t map. We then produced effect-size maps based on the t maps and sample sizes in each condition. In order to classify the condition to which each participant was assigned, we used support-vector machines (SVMs;), which are a class of supervised machine-learning algorithms considered to be robust to overfitting in classification settings. In the SVM training phase, a hyperplane is chosen that maximizes the margin of separation between the classes while also allowing for some data points to be misclassified. On the basis of the model built from training data, a new data point can then be classified using its position relative to the hyperplane. Further, nonlinear classification can be performed using SVMs by projecting the data into high-dimensional space using the so-called kernel trick. SVMs can also be used in high-dimensional settings in which the number of features (in our case, 48,954) is larger than the number of participants. Therefore, we preregistered and used SVMs that perform well under such constraints and are computationally efficient. In all of our analyses, we balanced the data points between the classes and performed 10-fold crossvalidations 100 times to obtain estimates of variance in the performance of the models. We then used Fisher's one-sample permutation test, with 5,000 permutations, to assess whether the classification algorithm performed significantly more accurately than would be expected by chance. Averaged accuracies (across the 100 folds), their bias-corrected, bootstrapped 95% confidence intervals (CIs), and p values associated with permutation tests are reported for each model. To examine the role of political orientation in expressed mental representation of felt emotion, we divided the data by political orientation and investigated whether the maps in each condition were significantly different for liberals and conservatives (e.g., do liberals feel \\\"purity\\\" in the same part of their body as conservatives do?). Similarly, for each condition, we balanced the data points between the classes (i.e., liberals and conservatives) and, for each concern, performed 10-fold cross-validations 100 times to obtain estimates of variance in the performance of the models. We then compared the average accuracy against chance (50%) using Fisher's one-sample permutation test, with 5,000 permutations, to investigate the reliability of the differences between the maps generated by liberals and conservatives. We also broke down the MFQ responses to examine whether having high (vs. low) levels of a particular moral concern had a similar effect on self-reported body sensations in matching conditions. For each condition, we balanced the data points between the classes and performed 10-fold cross-validations 100 times to obtain estimates of variance in the performance of the models to examine whether high scorers and low scorers felt moral violations in different body regions. We then compared the average accuracy in each condition against chance (50%) using Fisher's one-sample permutation test, with 5,000 permutations. In a preregistered exploratory analysis, we explored whether the topographical maps for different moral concerns can be predicted from the textual description of that moral violation as stated in the vignette in natural language. Whereas typical approaches for the analysis of moral language  estimate the \\\"moral loadings\\\" of a piece of text on the basis of the moral words (as measured by a prespecified dictionary), here the vignettes described events that are potentially, but not necessarily, moral. Therefore, we applied the InferSent1 methodology  to generate vector representations of each vignette. InferSent learns sentence-level representations by first training a supervised bidirectional recurrent neural network with long short-term memory (LSTM) on the Stanford Natural Language Inference data set. The sentence-encoder function provided by InferSent retrieves pretrained word embeddings of each token in an input sentence and feeds them to the pretrained bi-LSTM model, described above, with maximum pooling. The output is a general-purpose sentence embedding that captures generic information useful for a broad set of tasks. These representations have been shown to achieve superior results compared with most available models. Specifically, the sentence-level embeddings produced by InferSent have been evaluated on 12 transferlearning tasks and outperformed sentence embeddings learned by models trained in unsupervised conditions or on other supervised tasks. Therefore, we used the InferSent-trained model to obtain sentence encodings as the latent representation of the vignettes. What made InferSent appropriate for our study was that (a) it had been proposed for acquiring generic sentence-level semantic representations of natural-language data and (b) had achieved better results compared with available models described in the literature for sentence-level representations. Using the best-performing predictive model, we encoded each moral vignette into a vector length of 4,096. We then ran 48,954 ridge regressions using InferSent vectors as predictors of activation of each pixel and then calculated R2 for each model to build an R2 map to visualize which parts of the body's activation or deactivation can be explained by the semantic representation of textual stimuli. We ran all analyses in the R (Version 3.4.1;), Python (Version 3.6;), and Octave (Version 4.4.1;) programming languages. Results Bodily sensation of moral violations. Figure 1 displays the body-sensation maps associated with each moral violation. For each condition, we normalized the difference between activation pixels and deactivation pixels subject-wise. Then, we performed 48,954 one-sample t tests against zero to get the resulting t maps for each moral-violation condition. Next, we transformed these t maps into effect-size maps by dividing each pixel's t value by the square root of sample size in that condition (Cohen's d). Effect-size maps are visualized in Figure 1. As can be seen, in each condition, the moral violations are associated with slightly distinct distributions of body areas where activation was felt to either increase (or speed up) or decrease (or slow down). Classification of moral violations. As mentioned previously, we ran binary SVM classifiers with 10-fold cross-validation 100 times for each condition; in each fold, the target concern and a randomly selected subset of data from the other conditions (equal in size) were classified. For care, the average classification accuracy was 49.0% (95% CI = [47.96, 49.83]), which was not significantly higher than chance (p = .991). For fairness, the average classification accuracy was 51.5% (95% CI = [50.55, 52.45]), which was slightly greater than chance (p = .002). For loyalty, the average accuracy was 49.0% (95% CI = [47.95, 49.82]), which was not significantly higher than chance (p = .983). For authority, the average accuracy was 49.0% (95% CI = [48.04, 49.79]), which was not significantly higher than chance (p = .993). Finally, the mean classification accuracy for purity was 51.0% Care Fairness Loyalty Authority Purity (95% CI = [50.14, 51.68]), which was slightly higher than chance (p = .009). Individual differences and moral violations. Bodysensation maps of moral violations for liberals and conservatives are presented in Figure 1. We ran binary SVM classifiers with 10-fold cross-validation 100 times for each condition; in each fold, the political orientation of the left-out participants was predicted. The average classification accuracies for care (n = 45, M = 66.7%, 95% CI = [66.67, 66.67], p < .001), fairness (n = 36, M = 62.3%, 95% CI = [59.33, 64.00], p < .001), loyalty (n = 49, M = 74.0%, 95% CI = [72.00, 74.50], p < .001), authority (n = 48, M = 66.7%, 95% CI = [66.67, 66.67], p < .001), and purity (n = 39, M = 72.0%, 95% CI = [69.33, 74.67], p < .001) were significantly greater than chance. Therefore, for all moral concerns, we could classify political ideology on the basis of body-sensation maps, indicating that liberals and conservatives feel moral violations, especially perceptions of feelings of loyalty and purity, in different parts of their body. Although we preregistered our analysis, these findings should be interpreted with caution because the numbers of liberals and conservatives included in the study were small (36 <= n <= 48 in each group), and middleof-the-road individuals were not included. We also examined classification of high (vs. low) moral concerns based on body-sensation maps. We ran binary SVM classifiers with 10-fold cross-validation 100 times for each condition; in each fold, high scorers and low scorers on the MFQ subscales were classified. The average classification accuracies for care (n = 39, M = 66.3%, 95% CI = [64.33, 66.67], p < .001), fairness (n = 37, M = 61.0%, 95% CI = [56.67, 63.00], p < .001), loyalty (n = 37, M = 50.5%, 95% CI = [45.50, 54.50], p = .504), authority (n = 42, M = 49.8%, 95% CI = [48.25, 50.00], p = .999), and purity (n = 43, M = 58.5%, 95% CI = [53.25, 63.25], p < .001) were relatively high, except for loyalty and authority. Therefore, individual differences in moral concerns were associated with where in the body individuals report perceptions of feeling associated with moral violations. Body-sensation maps of moral violations for low scorers and high scorers are presented in the Supplemental Material available online. Of note, high scorers and low scorers formed small subsets in each group (37 <= n <= 43 per group), and these findings should be viewed with caution until further replicated. Semantic representation of moral violations. As men tioned in our data-analysis strategy, we represented each vignette onto a 4,096-dimensional sentence embedding. We used these vectors to predict activation or deactivation of each pixel using a ridge regression with 10-fold cross-validation. We then computed an R2 map, indicating the explained variance in each pixel on the basis of the semantic representation of the textual stimuli. Results of these ridge regressions are presented in Figure 2. It can be seen that semantic representation of the texts used as experimental stimuli can predict larger proportions of variance in activation of the gut area. In the second study, we addressed two limitations of Study 1. First, we collected a nationally representative sample to generalize our findings to American citizens in all demographic layers of the society. Second, we used a longer and more reliable measure to assess political orientation of the participants. Study 2 Method We recruited a nationally stratified sample (in terms of age, ethnicity, gender, and political orientation) consisting of 300 participants (age: M = 46.1 years, SD = 16.7; gender: female = 152, male = 148) through Qualtrics Panels (https:\\/\\/www.qualtrics.com\\/online-sample\\/). Our sample size was preregistered and included more participants per condition than Nummenmaa et al. (2014) suggested. Participants were randomly assigned to the five experimental conditions and completed the emBODY task. In addition to completing all individual-differences measures used in Study 1, participants filled out the 12-item Social and Economic Conservatism Scale (; a = .86). In addition, internal consistency coefficients were acceptable for care (a = .75), fairness (a = .69), loyalty (a = .74), authority (a = .75), and purity (a = .81). Our data-analysis strategy and procedures were the same as those in Study 1. Study 2's hypotheses, predictions, and analyses were preregistered on the OSF (https:\\/\\/ osf.io\\/5hfcs\\/), and all data and materials are publicly available at https:\\/\\/osf.io\\/4tdx5\\/. Results In each condition, moral violations were associated with slightly distinct distributions of body areas, mirroring our findings in Study 1. The visualizations can be found in the Supplemental Material. For care, the average classification accuracy was 53.3% (95% CI = [50.25, 56.00]), which was slightly higher than chance (p = .011). For fairness, the average classification accuracy was 51.33% (95% CI = [49.46, 53.17]), which was was not different from chance (p = .067). For loyalty, the average accuracy was 49.1% (95% CI = [46.00, 51.80]), which was not significantly higher than chance (p = .747). For authority, the average accuracy was 50.3% (95% CI = [47.33, 52.92]), which was not significantly higher than chance (p = .414). Finally, the mean classification accuracy for purity was 48.3% (95% CI = [45.20, 51.10]), which was not higher than chance (p = .883). We labeled participants who scored 1 standard deviation (SD = 19.03) lower than the average political conservatism (M = 64.99) as liberal (n = 50) and those who scored 1 standard deviation higher as conservative (n = 55). Body-sensation maps of moral violations for liberals and conservatives are presented in the Supplemental Material. The average classification accuracies for care (n = 26, M = 75.0%, 95% CI = [75.00, 75.00], p < .001), fairness (n = 20, M = 66.7%, 95% CI = [66.67, 66.67], p < .001), loyalty (n = 22, M = 65.3%, 95% CI = [62.00, 66.33], p < .001), authority (n = 19, M = 66.7%, 95% CI = [66.67, 66.67], p < .001), and purity (n = 18, M = 66.7%, 95% CI = [66.67, 66.67], p < .001) were significantly greater than chance. Therefore, for all moral concerns, we could classify political ideology on the basis of body-sensation maps, indicating that liberals and conservatives feel moral violations in different parts of their body. These results fully replicate those of Study 1. We also examined classification of high (vs. low) moral concerns based on body-sensation maps. The average classification accuracies for care (n = 15, M = 66.3%, 95% CI = [64.33, 66.67], p < .001), fairness (n = 27, M = 85.7%, 95% CI = [85.71, 85.71], p < .001), loyalty (n = 26, M = 85.7%, 95% CI = [85.71, 85.71], p < .001), authority (n = 17, M = 100%, 95% CI = [100.00, 100.00], p < .001), and purity (n = 32, M = 91.7%, 95% CI = [91.67, 91.67], p < .001) were high. Therefore, individual differences in moral concerns are associated with where in the body individuals feel moral violations, replicating our findings in Study 1. Finally, results of these ridge regressions based on sentence embeddings are presented in the Supplemental Material. General Discussion The social-intuitionist model of moral cognition  suggests that moral judgments are caused by emotional responses to a person, an action, or a violation. Drawing on intuitionist models of human morality  and recent research on body maps of emotions , we conducted two preregistered examinations of body sensations associated with violations of different moral concerns. In Studies 1 and 2, we demonstrated that moralviolation scenarios are associated with changes in activation and deactivation of specific body regions. The topographic maps associated with moral violations manifested more commonalities than differences (see Fig. 1). These similarities are consistent with the findings of Kemper and Newheiser (2018), who did not uncover evidence for a specific-correspondence mapping of behavioral response to moral violations based on MFT. Hence, our results are more in line with the constructionist model of moral emotions, indicating that there is no clear correspondence between foundation-level moral violation and self-reported perceptions of feelings associated with those moral foundations. It can be seen that across moral violations, the head and face area was highly activated, paired with varying levels of activation in the chest. The consistent activation observed in the head area suggests that people subjectively associate moral violations with high-level cognitive processing \\\"in their head.\\\" This is not surprising, because the moralviolation scenarios require a high level of cognitive and emotional processing, as well as an evaluation of personal states in relation to standards of right and wrong. The patterns of activation and deactivation observed for the head area consistently occurred across moral violations, and they may reflect subjective changes not only in the brain but also in the face. On the basis of the current data, we cannot infer whether these activations reflect perceived mental processing, facial blushing, or a combination of both. Deactivation of the limbs represented a consistent pattern across all moral scenarios. Of note, the chest area was activated in response to violations of care, fairness, loyalty, and authority, but less so in response to violations of purity. Instead, people reported higher activations in the abdomen area in purity. Over the past decade, a body of research has shown that liberals and conservatives rely on different moral foundations and react differently to different moral violations. Accordingly, we trained classifiers that could reliably predict political ideology on the basis of body maps. This is the first work indicating that political orientation influences where and how moral violations are felt in the body. These findings contrast null effects of political orientation on the link between moral transgressions and moral emotions. Therefore, it is possible that, as we found, liberals and conservatives feel moral violations in different body regions, interpret them as distinct complex feelings , and subsequently make different moral and political judgments. This seems to be a robust effect, because it was fully replicated in our nationally representative sample, which had the same breakdown of political ideology as in the United States. We also trained classifiers that were able to reliably predict people's moral concerns (high vs. low in moral foundations) in both MTurk and representative samples. It stands to reason that participants who score high on purity, for example, may have different reactions to impure actions than their counterparts who do not endorse purity values as strongly. Those who are more concerned with purity are more sensitive to cues of degradation and purity violations; thus, such individuals are more likely to report stronger subjective activations or deactivations (or both) when primed with stimuli that violate relevant norms. As mentioned, these results are promising but should be viewed with caution until further replicated in future studies. Together, these results suggest that moral violations may evoke a \\\"moral upset\\\" that cannot be differentiated across moral foundations but can differ between groups (e.g., liberals vs. conservatives). Semantic representation of textual stimuli  predicted activation or deactivation of different body parts, especially in the abdomen or gut area. Research in cognitive linguistics and social cognition suggests that individuals construe the world in large part through conceptual metaphors, which enable them to understand abstract concepts using knowledge of superficially dissimilar but more concrete phenomena. Interestingly, the vignettes that we used did not include highly moral words; rather, they simply described a social scenario in which a particular moral norm was violated. These vignettes did differ from each other in their semantic representation in important ways to predict activation or deactivation of different body parts. We found that the semantic representations can locate body representation of moral violations, suggesting that the semantic space of natural language describing moral violations can be coupled with emotional states and their body-sensation maps. Although we used different populations in Studies 1 and 2, we consistently found that (a) body maps of moral foundations were not substantially different from each other, (b) body maps associated with moral violations were reliably different between liberals and conservatives, (c) body maps associated with moral violations were associated with self-reported moral values, and (d) semantic representation of stimuli could predict activation or deactivation of body regions--especially the abdomen or gut area--in response to moral violations. These studies have limitations worth noting. First, we collected self-report data regarding where activations and deactivations were felt in the body because our main objective was to examine representation of moral transgressions in the body; however, we did not collect data with regard to change in physiological states. Second, we collected data on two 2-D silhouettes to represent activation and deactivation of different body parts. Future research can use more accurate silhouettes, including 3-D ones, to better disentangle different body parts (e.g., occipital and frontal parts of the head). Third, we used textual stimuli to evoke body sensations; however, more vivid evocative stimuli (e.g., videos of moral violations) might evoke stronger responses. Fourth, we used vignettes that were matched on frequency and wrongness, representing only the moral foundations suggested by MFT. A good next step would be to include other morally controversial scenarios (e.g., transgressions from norms of honesty or humility) with varying levels of wrongness, frequency, and weirdness. Finally, we mention two constraints on generality of these findings for replication and followup studies. First, our study included only Western participants, and our findings cannot be generalized to other cultures. Second, we used a limited number of moral-violation vignettes. Therefore, in follow-up studies, researchers are encouraged to test boundary conditions of the present findings and use alternative sets of validated stimuli.\",\"1135745713\":\"Personality measurement is central to the study of individual differences and the prediction of behaviors, attitudes, beliefs, and outcomes. Most personality tests use questionnaires composed of natural language items--words and sentences--that describe common traits. Participants are asked to rate themselves (or others, such as acquaintances) on the trait descriptions, and the resulting data are projected onto a small number of dimensions through statistical techniques like factor analysis. These methods provide quantitative insights into the structure of variability in traits across individuals and are used to motivate influential and highly predictive theories of personality. Despite their successes, traditional methods of personality assessment are constrained to making inferences over the respective set of participants and items in a survey data set. In other words, factor analysis on standard questionnaire data does not provide any information about the responses of individuals who have not taken the questionnaire or about participant responses for questionnaire items (and thus traits or constructs) that have not been surveyed. This is particularly relevant for research on understudied populations and traits. Researchers have introduced new tools for addressing the first of these limitations: that of generalizing to out-of-sample individuals. These tools rely on large-scale digital data, such as social media activity, to quantitatively represent thousands of individuals. Researchers give a subset of these individuals a personality questionnaire and, using their responses, build machine learning models capable of predicting the personalities of other individuals using only their digital data. In this article, we examine whether digital data and machine learning can also provide a solution to the second limitation: That of generalizing survey data to out-of-sample items, that is, items that have not been surveyed previously (including completely new items that are not part of existing questionnaires). In order to solve this challenge, we need a way to quantitatively represent the language used in a personality item, so that machine learning models trained on a participant's ratings for one set of items can generalize and make predictions about ratings for a completely new set of items. Recently, a class of deep neural networks, known as transformer models , has been shown to accurately represent natural language sentences as embeddings. Sentence embeddings (also known as sentence vectors) are points in a high-dimensional space, whose structure captures the linguistic properties of sentences. Linguistically similar sentences have similar embeddings and can thus be seen as occupying nearby points in the embedding space. Transformer models are typically trained on large amounts of text data, and the embeddings they extract from this data contain high-quality representations of the linguistic properties of sentences. For this reason, sentence embeddings from transformer models are highly predictive in a variety of natural language processing tasks, including text summarization, sentiment analysis, question answering, and translation. High-quality sentence embeddings are also responsible for the success of new Artificial Intelligence models like the Generative Pre-trained Transformer 3 , which generate human-like language based on their embedding representations of the user's linguistic input. One type of transformer model is Sentence-BERT (SBERT;). SBERT is specialized for creating embedding representations of sentences that capture their semantics, so that sentences that have similar meanings are given similar embedding representations. SBERT is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture  and is trained on a very large language data set, as well as a large data set of sentence pairs annotated with linguistic entailment relations. Subsequently, embeddings produced by SBERT have outperformed many other methods in the SentEval evaluation set of tasks. We take advantage of this model in our study and use SBERT to extract embeddings for questionnaire items (Figure 1A). This allows us to describe personality item sentences as points in a high-dimensional semantic space (Figure 1B), in which items with similar meanings are located close to each other. Since SBERT can be used to obtain embedding representations for any possible personality item, we can use it to generalize from a small set of rated items to thousands of new unrated personality items. This can be accomplished using several standard machine learning techniques that use the similarities between representations for generalization. The K-nearest neighbors regression , for example, predicts the rating of a new item by averaging the ratings assigned to the K nearest items in the embedding space (Figure 1B). Theoretically, our approach draws on the lexical hypothesis in personality psychology , which proposes that personality traits that are important to a group are expressed through words and sentences in their language. This hypothesis has motivated many advances in personality research and is the foundation for leading personality theories, such as the five-factor model, which uses natural language to describe and measure the core dimensions of variation in people's personalities. The lexical hypothesis also implies that quantitative representations of language obtained from deep neural networks should make it possible to quantify, and subsequently predict personality traits, since these models are based on the statistics of everyday language. In other words, traits that co-occur with each other should have similar linguistic descriptors, and deep networks trained to quantify these descriptors should be able to generate similar representations for the traits. Our approach is also inspired by recent work that has used the semantic similarity of the individual words in questionnaire items to measure the similarity of items. This line of research leverages word embeddings (high-dimensional vectors for individual words) for item selection, for example, finding related items for a new scale or avoiding redundancies. We extend this idea to predict participant responses, a problem that can now be solved as deep language models provide high-quality representations for sentences that are based not just on the individual words in the items but also syntax and word order in the sentence (in this way, sentence embeddings capture nuances in sentence meaning that cannot be captured by word embedding models). In our analysis below, we also consider a version of our approach applied to word embeddings, a method similar to that of Rosenbusch et al. (2020), to test whether sentence embeddings from deep neural networks provide superior representations and predictions for personality items. We evaluate the applicability of our approach for personality prediction using a series of empirical tests. In Study 1, we apply crossvalidation to a 100-item NEO Personality Inventory-Revised (NEOPI-R) data set to assess our model's ability to predict out-of-sample, that is, we train our model on ratings for a subset of items and use it to predict ratings for held-out items (Figure 1C). We also contrast our models' accuracy rates with those of human judges that are given an identical task and are incentivized to make accurate predictions. To ensure that our approach is robust, we replicate this analysis pipeline for three additional personality questionnaires in Study 2A-C. In Study 3, we extend our tests to a large new data set of over 3,000 existing personality items taken from hundreds of different scales and constructs, thus allowing us to test the cross-domain predictive accuracy of our approach. Finally, in Study 4, we use SBERT's assessment of item similarity to infer the personality dimensions and constructs associated with personality items. This allows us to test whether established constructs are explicitly reflected in the structure of the underlying SBERT embedding space.We also attempt to cluster novel, unlabeled personality items with already established and labeled ones, in order to infer personality constructs associated with these items. If successful, the methods outlined in this article would provide a powerful set of tools for personality researchers. By predicting the responses of participants to thousands of personality items, researchers would be able to describe each individual in terms of a large variety of distinct traits, scales, and constructs, and in turn build richer models of personality (and associated behaviors, attitudes, beliefs, and outcomes), without the need for extensive data collection. Additionally, new personality items could be tested using our trained models instead of human participants. This would offer researchers a cost-effective and scalable resource for psychological measurement and theory development. Transparency and Openness Promotion We report how we determined our sample size, all data exclusions (if any), allmanipulations, and allmeasures in the study, andwe follow Journal Article Reporting Standards. All data, analysis code, and research materials are available at https:\\/\\/osf.io\\/sxg8n\\/?view_only=762b6188d54246c0a4c1c7e6218e33c3. Data were analyzed using Python, Version 3.9 , and the packages scikit-learn, Version 0.23 , statsmodel, Version 0.13 , and matplotlib, Version 3.5.0 . Study 1's depth analysis experiment was preregistered on https:\\/\\/aspredicted.org\\/dx4dn.pdf. Ethics Statement All studies involving human data collection were approved by the institutional review board of The University of Pennsylvania under approval number 823184 (\\\"Everyday Judgments and Decisions\\\"). Computational studies without human data or with only retrospective analysis of anonymized human data from public data sets were exempted from the need for approval. Computational Methods We begin by summarizing the computational methods used in our studies. These methods entail the generation of questionnaire items' embeddings (i.e., SBERT and baselines), training, validation, and selection of the predictive models, as well as evaluations of model performance. Any deviations from and extensions of these methods are detailed in the respective studies' sections. SBERT Embeddings We created the questionnaire item embeddings by feeding each item's text through the SBERT neural network, then averaging the embeddings in the last layer across the whole sentence to get a representation for the item. See Figure 1A for a visual overview of this procedure. We used the SBERT version called \\\"nli-roberta-large,\\\" based on the pretrained RoBERTa-Large model  with 24 layers and 1,024 hidden vector dimensions, and trained on the MultiGenre Natural Language Inference data set. RoBERTa is based on the BERT  architecture and outputs a 1,024-dimensional embedding for each of the questionnaire items. For the implementation of the embedding extraction, we used the Python library sentence transformers, provided by the original authors. Note that the item responses for the personality prediction task were not reversed-coded and no information about items' construct or direction they load onto a construct (positive\\/negative) was provided to the model, meaning our model based its representations solely on the items' raw text. Baseline Embeddings We considered two alternative embeddings as baselines to SBERT, Word2Vec , and Linguistic Inquiry and Word Count (LIWC;). Word2Vec was trained on English Wikipedia texts and generates 300-dimensional embeddings for millions of common English words. These embeddings have been shown to accurately capture the similarity relationships between individual words, so that similar words have similar word embeddings. To generate Word2Vec embeddings for each item, we averaged the word embeddings across all words in the item text. Importantly,Word2Vec is only sensitive to the individual words from the personality item and, unlike SBERT, ignores the ordering of the words and syntactical structure of the sentence. Therefore, this model is similar to prior attempts at measuring item similarities using the similarities of their individual words. LIWC provides sets of words (called \\\"lexicon\\\") for 73 common psychological variables. For example, the Anxiety variable contains words like \\\"worried,\\\" \\\"fearful,\\\" and so on. To obtain LIWC embeddings for our personality items, we simply counted the number of times words in each of the 73 LIWC lexicons occurred in the personality item, giving us 73-dimensional LIWC embeddings. The counting and scoring of personality items on LIWC dimensions were done using the Differential Language Analysis ToolKit. Predictive Models We applied standard machine learning techniques, such as ridge , K-nearest neighbor (K-NN;), and support vector (SVC;Hearst et al., 1998) classification and regression, to map each item's embeddings onto each participant's rating of the item. Ridge methods made predictions by estimating a regularized linear function on each embedding dimension, K-NN methods made predictions by averaging the ratings on the K most similar items to a target item, and SVC methods made predictions by estimating a potentially nonlinear function on the embedding dimensions with a kernel trick (i.e., mapping the input into a further high-dimensional space). We applied the regression and classification methods to all three types of embeddings: SBERT, Word2Vec, and LIWC. This gave us a total of 5 x 3 = 15 models (e.g., KNN regression with SBERT embeddings, SVC with LIWC embeddings). Regarding hyperparameter tuning, for the ridge models, we tested five different a-values, corresponding to the weight on the regularization penalty. For the KNNmethods, we tested five differentK values, reflecting the number of nearest neighbors used for prediction. For the SVC method, we tested five different C values, corresponding to the strength of the regularization penalty. To ensure consistency across our studies, we determined the best-performing embedding and hyperparameter combination in Study 1 and used it in all subsequent studies. See Table A1 in the Appendix for the specific values used and their effect on performance in Study 1. Cross-Validation We trained and evaluated models using 10-fold cross-validation. In particular, we divided each participant's data into 10 equally sized groups or folds (with 10% of ratings in each fold), then fitted each model on the 90% of items in the first nine folds (the training data) and evaluated its predictions on the 10% of items in the held-out fold (the test data). This was repeated nine more times with each fold serving as the test data once. See Figure 1C for an illustration. All models were estimated in the Python's scikit-learn library. To evaluate model performance, we calculated the correlation of a model's predictions with the observed ratings for each participant. Specifically, for the ith participant and the kth test fold (k = 1, 2, ... , 10), we calculated a model's predictions for the 10% of items in the fold, when that fold was in the test data. We then concatenated the predictions across all testing folds into one list containing our model's out-of-sample predictions for each item offered to the participant. This list was then compared with the observed ratings of the participant using Pearson correlation, to obtain ameasure of ourmodel's accuracy for that participant. Study 1 In Study 1, we tested whether our approach, as described in the Computational Methods section, accurately predicts participant responses to out-of-sample items in an established personality questionnaire: the NEO-PI-R. We also evaluated our approach against previous models in the field and against incentivized human judges. Method NEO-PI-R Data Set We used data collected by Stillwell and Kosinski (2012) in order to train and test our predictive model. This data set has responses from N = 2,749 individuals who used the myPersonality Facebook application between 2007 and 2012. Personality was measured using the NEO-PI-R five-factor model , and for this reason, we will refer to this data set as the NEO-PI-R data set for the rest of the article. The five-factor model classifies each participant along five personality dimensions: Openness to experience (O), Conscientiousness (C), Extraversion (E), Agreeableness (A), and Neuroticism (N). The questionnaire contains 100 natural language items (20 per personality dimension) from the International Personality Item Pool (IPIP;). Each item asks participants to indicate their agreement with a description on a 5-point Likert scale. Participants in the NEO-PI-R data set completed all 100 items. See Table 1 for a summary of data set characteristics. Comparison With Human Judges We compared our model's performance with that of incentivized humans that were given an identical prediction task (i.e., predicting a participant's NEO-PI-R ratings on 10 test items using their ratings on 90 training items). There were 2,749 participants and 10 crossvalidation folds per participant in the NEO-PI-R data set, generating 27,490 distinct prediction tasks for our model. Since collecting human predictions for so many tasks was not feasible, we compared model versus human performance on a subset of participants from the original study. Specifically, we conducted two human rating experiments, which prioritized depth (i.e., a detailed analysis of target-level human accuracy that reduces noise on the target-level estimates) and breadth (i.e., covering a broad range of model performance to maximize representativeness), respectively. Depth Experiment. For our first experiment, we selected three target participants out of the 2,749 participants from the NEO-PI-R data set based on the predictive accuracy of our main SBERT model (details in the Results section). Specifically, we selected one target participant for whom our model performed well (75th percentile accuracy out of all participants), one participant for whom our model performed moderately (50th percentile accuracy), and one participant for whom our model performed poorly (25th percentile accuracy). The 100 responses of each of these three target participants were divided into the same 10 folds as used in the cross-validation analysis for model training and evaluation, resulting in a total of 30 tasks for our human judges. We collected a sample of 600 human judges (41.83% female; Mage = 36.17) from Prolific Academic for this part of Study 1. Each human judge was randomly assigned to one task leading to 20 judges per task, reducing noise and allowing for rigorous tests of target-level human accuracy. In the training phase of the task, the human judge first viewed a target participant's responses to 90 NEO-PI-R survey questions one at a time and then in the testing phase, they predicted the target participant's responses to the remaining 10 NEO-PI-R survey questions in the testing fold. Each participant received $2 for completion. We also incentivized participants by giving a bonus payment of $1 to those whose predictive accuracy was in the top 10% among all judges. We preregistered this study at https:\\/\\/aspredicted.org\\/dx4dn.pdf. Breadth Experiment. Our second experiment obtained human ratings for 60 target NEO-PI-R participants. We chose the target participants corresponding to the 0th-100th percentiles of model predictive accuracy in equidistant steps (e.g., 0th, 1.5th, 3rd, 4.5th percentile accuracy). This way, our sample of target participants covered a broad range of our model's predictive accuracy allowing for a more representative comparison of the model and human performance (i.e., comparing human predictions to several bad, average, and good model predictions). We collected a sample of 600 human judges (45.5% female;Mage= 38.85) from Prolific Academic. All other aspects of the design were identical to the depth experiment, except for the fact that there were 600 tasks (60 target participants x 10 cross-validation folds) leading to only one human judge per task. Results Model Performance We found the best-performing model using SBERT embeddings to be the K-NN regression with K = 5 (see Table A1 for a full model comparison). Intuitively, this model finds the K = 5 training items that have the most similar embeddings to the test item and then averages the participant's rating on these items to predict their rating of the test item. For Word2Vec and LIWC embeddings, we found the best-performing model to be SVC with C = 10 and ridge regression with a = 10, respectively. The best-performing SBERT model achieved an average correlation of.45 in predicting out-of-sample ratings, one-sample t test against zero means: t(2,747) = 158.09, p < .001, 95% CI [.45, .46]. As can be seen in Figure 2A, the models with theWord2Vec and LIWC embeddings achieved much lower performances than the model with the SBERT embeddings, with average correlations of.13, t(2,747) = 51.68, p < .001, 95% CI [.12, .13], and.10, t(2,747) = 35.43, p < .001, 95% CI [.01, .11], respectively. This indicates SBERT's superiority in quantifying item representations. In Figure 2B, we present the distribution of correlations across participants. Here, we can see that the SBERT model achieved a significant (p < .05) positive correlation for the vast majority (94.2%) of participants. By contrast, the Word2Vec and LIWC models achieved significant positive correlations for a much smaller proportion of participants (30.2% and 28.0%, respectively). Comparison With Human Judges To contrast our model's performance against human judges, we calculated the judges' performance as the correlation between the predicted item responses and the observed item responses for each test fold on each target participant. Specifically, for the ith target participant (i = 1, 2, 3) and the kth fold (k = 1, 2 ... , 10), we calculated the human prediction for the 10 items in the fold. These 10 predictions were then compared with the observed ratings of the target participant to generate a fold-level Pearson's correlation. We calculated the K-NN SBERT model's performance analogously. Figure 3A shows that our method's performance, in our depth experiment, was on par with that of human judges. The average correlation across 10 testing folds of our model for the 75th percentile target was.61, one-sample t test against zero mean: t(9) = 9.75, p < .001, 95% CI [.47, .75]; 50th percentile target was.46, t(9) = 4.86, p < .001, 95% CI [.24, .67]; and 25th percentile target was.38, t(9) = 4.07, p = .003, 95% CI [.17, .59]. Meanwhile, the average correlation of 200 human judges for the 75th percentile target was.43, t(199) = 17.51, p < .001, 95% CI [.38, .47]; for the 50th percentile target was.58, t(199)= 31.54, p< .001, 95%CI [.54, .62]; and for the 25th percentile target was.39, t(199) = 17.8, p < .001, 95% CI [.34, .43]. In total, the average correlation across all three targets was slightly better for our model: .48, t(29) = 9.57, p < .001, 95% CI [.38, .59], versus.47, t(599) = 36.09, p < .001, 95% CI [.44, .49], though not statistically distinguishable; Welch's two-sample t test, independent samples: t(32.92) = -0.32, p = .751. Notice that the error bars were larger for our models relative to the human judges as its correlations were averaged across only 10 values (one value for each of 10 testing folds), instead of 200 values for human judges (20 judges for each of 10 testing folds). Figure 3B shows the SBERT model's performance and the human judges' performance for each of the 30 tasks in the depth experiment (10 distinct testing folds, for three target participants). Here we see that the model performance was roughly equal to the human performance across the tasks. Note that there was one outlier condition for the 50th percentile target, for which our model achieved a correlation of -.26, r(8), p = .47, whereas human judges achieved an average correlation of.57, one-sample t test against zero mean: t(19) = 10.53, p < .001, 95% CI [.46, .68]. The outliers seem to be driven by contradictory statements in the conscientiousness training items (high and low), which led to neutral predictions of these items. It could be that the human judges were less sensitive to these contradictions because they ignore contradicting facets (e.g., using only the most relevant facet) or use their expectations regarding social faking. This explains the slight underperformance of our model for the second (50th percentile) target. Regarding the second, breadth-focused experiment, Figure 4 shows that our method's performance was, again, on par with that of human judges and that the overall distribution of prediction performance across experimental conditions was similar. Specifically, the average correlation across all experimental conditions was.47, t(598)= 38.39, p < .001, for our model, and.49, t(598) = 36.375, p < .001, for the human judges. The difference between model and human performance was statistically nonsignificant, Welch's two-sample t test, independent samples: t(1183.06) = 1.08, p = .28. Importantly, Figure 5A and 5B shows that the performances of both our model and the human judges were not driven by outliers. For example, Figure 5A shows that most of the model and human performance is positive (>90%), with the majority being larger than.50. Figure 5B shows that the performances on most targets are very similar (close to the dashed line\\/equality). The figure further shows that model and human performance strongly correlated, r(58) = .76, p < .001, indicating that the prediction problems were similar for both model and humans (i.e., targets that were easy to predict for our model were also easy to predict for humans). Discussion Study 1 tested our approach to predicting participant responses on out-of-sample personality items, using a large existing data set of NEO-PI-R ratings. It found that sentence embeddings obtained from SBERT (a leading deep language model optimized for encoding semantic similarity between sentences) were able to predict out-ofsample participant responses with an average correlation of.45. To interpret our accuracy rates, we can contrast our results with previous predictive models of five-factor responses. L. Liu et al. (2016) achieved a maximum correlation of.19 using social media profile picture features; Golbeck (2016) reported a correlation of up to.24 using texts from participants' public Facebook posts; and Youyou et al. (2015) showed an average correlation of up to.56 using a large amount of a person's Facebook likes. Our approach differed from these tests in two ways. First, we predicted responses for out-of-sample items, whereas all prior tests predicted responses for out-of-sample individuals. Second, we predicted responses at the item level, instead of the average personality scores on a construct (e.g., an Openness score). There is much higher variability in responses on the item level, making our prediction problem significantly harder. Finally, it should be noted that the performance metrics for L. Liu et al. (2016) and Golbeck (2016) reported above were the metrics for the best-performing constructs (e.g., Agreeableness for Golbeck (2016)) and not the average performance across all items as reported for our method. For this reason, we can conclude that our prediction exercise was quite successful, especially relative to past work on personality prediction. We also tested various machine learning models for predicting responses from SBERT embeddings. Here we found that a K-nearest neighbors regression with K = 5 performed the best. This model predicts the rating for an out-of-sample item by finding the five training items with the highest similarity and then averaging the participant's ratings for those items. The superiority of the K-NN algorithm over alternate models like the ridge regression indicates that the mapping between item embeddings and participant ratings may be nonlinear. K-NN can successfully capture these nonlinearities, as it uses only the local structure of input space (five nearest neighbors) to predict responses. We also found that SBERTprovided better item representations, and subsequently higher predictive power, than embeddings obtained from Word2Vec (a prominent word representation model) and LIWC (a common approach to extracting psychological variables from text). The superiority of SBERT over Word2Vec and LIWC indicates that the context and sentence structure (e.g., word order, syntax) of a personality item plays an important role in specifying its meaning and that averaging embeddings for the words in the item (as with Word2Vec) or counting up the words associated with various psychological variables (as with LIWC) is not enough for capturing the psychological richness of the item. Prior work has used word embeddings for scale creation, for example, tasks such as item selection. Our results show that it may be possible to improve on this work by using SBERT embeddings to measure item similarity. Finally, and perhaps most importantly, we ran two experiments in which we elicited incentivized personality predictions from human judges. The first of these was a depth experiment, which obtained a large number of human predictions for each cross-validation task performed by our model but used only three NEO-PI-R target participants. The second was a breadth experiment that used a much larger set of NEO-PI-R participants but obtained a smaller number of human predictions per cross-validation task. Both these experiments showed that our model performed equivalently to human judges, indicating that it is as good at out-of-sample personality prediction as humans. Additionally, we found that human and model predictions were highly correlated with each other (r= .76), indicating that target participants that were easy or difficult for humans to predict were also easy or difficult for the model to predict. This provides strong evidence for the capabilities of our model and, in particular, the quality of its representations for the items that make up the NEO-PI-R survey. Studies 2A-C Overall, Study 1 shows that deep language models provide highquality quantitative representations for NEO-PI-R personality items and that the similarities between these representations can be used to predict responses for out-of-sample items with a human level of accuracy. In Studies 2A-C, we tested whether our model's performance generalizes beyond the NEO-PI-R to three other personality questionnaires: 16PF (16 Personality Factors;) in Study 2A, RIASEC  in Study 2B, and HSQ  in Study 2C. This diverse conceptualization and structure of personality allowed us to test the generalizability of our model. Method Data Sets We used data collected from the Open Source Psychometrics Project (https:\\/\\/openpsychometrics.org\\/_rawdata\\/) for the 16PF questionnaire (N = 49,159 participants), RIASEC questionnaire (N = 135,764 participants), and HSQ questionnaire (N = 590 participants). These questionnaires had 163, 48, and 32 items, respectively, and each participant rated each item on a 5-point Likert scale. See Table 1 for a summary. We selected these three data sets (and not others in the Open Source Psychometrics Project), as they are all multiconstruct Likert-scale questionnaires with at least 500 responses, covering diverse topics and involving diverse constructs. For instance, the 16PF questionnaire is another hierarchical personality model based on the lexical hypothesis , similar to the NEO-PI-R. However, the 16PF hierarchical structures were designed using a bottom-up structure (identifying 16 primary factors and then five higher level dimensions) as opposed to the NEO-PI-R's top-down approach (identifying five higher level dimensions and then 30 lower level facets). The 16PF questionnaire contains the following constructs: Warmth (A), Reasoning (B), Emotional stability (C), Dominance (E), Liveliness (F), Ruleconsciousness (G), Social boldness (H), Sensitivity (I), Vigilance (L), Abstractedness (M), Privateness (N), Apprehension (O), Openness to change (Q1), Self-reliance (Q2), Perfectionism (Q3), and Tension (Q4;). The RIASEC questionnaire describes personality through preferences and aversions that influence the choice of work environments (and environments through typical work activities and demands placed on individuals). The questionnaire contains six personality dimensions (and parallel environments): Realistic (R), Investigative (I), Artistic (A), Social (S), Enterprising (E), and Conventional (C), collectively called RIASEC. Finally, the HSQ describes personality through different styles of using humor, containing the dimensions of Self-enhancing, Affiliative, Aggressive, and Self-defeating. As such, the HSQ uses a conceptualization of humor as a stable multidimensional aspect of personality. Validation studies have further shown that the HSQ dimensions (humor styles) correlate with other established personality measures, such as the NEO-PI-R dimensions. Comparison With Human Judges Analogous to Study 1, we compared our model's performances on the above questionnaires with that of human judges incentivized to make accurate predictions. We collected a sample of 600 judges for each of the questionnaires (16PF: 62.00% female,Mage = 39.71; RIASEC: 51.33% female, Mage = 42.41; HSQ: 50.83% female, Mage = 40.60). All test procedures were the same as the breadth experiment in Study 1. In particular, human judges predicted the responses of 60 target participants from the original questionnaire data sets (i.e., target participants for which our model performance ranged from the 0th percentile to the 100th percentile). Each judge was given one of the model's cross-validation tasks, in which they used 90% of the target's responses (training fold) to predict the heldout 10% of their responses (test fold). We incentivized participants by giving a bonus payment of $1 to those whose predictive accuracy was in the top 10% among all judges. Results and Discussion Figure 2C-H shows that the out-of-sample predictive performance of the SBERT method persisted for these questionnaires, with average correlations of.39, t(49,158) = 585.49, p < .001, 95% CI [.39, .39] for 16PF in Study 2A; .35, t(135,763) = 594.91, p < .001, 95% CI [.35, .35] for RIASEC in Study 2B; and.34, t(589) = 33.21, p< .001, 95%CI [.32, .36] for HSQ in Study 2C. Overall, our SBERT model achieved significant (p < .05) positive correlations for 93.3%, 51.4%, and 64.7% of participants in the three studies, respectively. Consistent with Study 1, our method outperformed alternative baseline models that use the Word2Vec and LIWC embeddings. These models achieved much lower correlations than SBERT in all three studies (though note that the performance of the Word2Vec method was very close to that of SBERT in Study 2B). Furthermore, Figure 4 shows that the average out-of-sample predictive performance of our method was, for all questionnaires, on par with that of human judges incentivized to make accurate predictions. The average correlation across all experimental conditions for the 16PF was.40, t(598) = 37.02, p < .001, 95% CI [.37, .42], for our model, and.39, t(598) = 33.60, p < .001, 95% CI [.37, .42], for the human judges. For the RIASEC, it was.38, t(598) = 18.29, p < .001, 95% CI [.34, .42], for our model, and.32, t(598) = 14.72, p < .001, 95% CI [.28, .37], for the human judges. For the HSQ, it was.42, t(598) = 15.30, p < .001, 95% CI [.37, .48], for our model, and.42, t(598) = 15.86, p < .001, 95% CI [.36, .47], for the human judges. Our model performed slightly better than the human judges, albeit statistically nonsignificant for the 16PF;Welch's two-sample t test, independent samples: t(1152.59)= -0.08, p = .94; RIASEC: t(1060.20) = -1.95, p = .05; and HSQ: t(995.83) = -.18, p = .86. Figure 5C-H shows that most of the model and human performance is positive. The scatterplots show no outliers in model\\/human performance, except a single outlier for the 16PF, for which our model performs significantly worse than human judges (-1.0 vs. .02). This outlier did not influence any of our results and model and human performance strongly to moderately correlated for all questionnaires; 16PF: r(58) = .70, p < .001; RIASEC: r(58) = .53, p < .001; HSQ: r(58) = .46, p < .001, indicating that the prediction problems were similar for both model and humans. Overall, the results of Studies 2A-C show that the success of our approach is not specific to the NEO-PI-R but instead generalizable to other common and less common questionnaires used in personality research. Study 3 Although Studies 1 and 2A-C provide evidence for the power of our approach, each of these studies uses a single personality questionnaire, composed of a small set of curated items. As a more challenging test, Study 3 examined whether our method could achieve good performance on a large and unconstrained collection of items, spanning a diverse set of domains and constructs, as well as grammar and phrasing structures. For this, we collected a new data set of personality ratings of over 3,500 different items taken from the IPIP. Method One hundred sixty-one participants (62.30% female; Mage = 19.88) were recruited through the university's undergraduate subject pool. We selected the sample size for this study to ensure that we have data from at least 150 participants to evaluate our approach. Participants indicated at the beginning of the study if they wanted to answer one, two, or three blocks of approximately 300 items (each for a fixed amount of study credit). These items were sampled from a larger set of 3,563 personality items, all available items at the time of data collection in 2019, taken from the IPIP. The IPIP is a very large, broad collection of personality items across a multitude of different scales and constructs. It covers items from 36 distinct scales, involving 242 distinct constructs. For example, the IPIP includes constructs such as Tolerance, Adaptability, or Toughness with items such as \\\"I believe in equality between all races,\\\" \\\"I adapt easily to new situations,\\\" or \\\"I remain calm under pressure.\\\" Note that the IPIP lists two item sets, one \\\"total\\\" list with 3,320 items and one \\\"assigned only\\\" list with items that have information about scales and constructs provided. We obtained 3,563 unique items after merging both lists. Also note that the number of scales and constructs referred to in this article was taken from the items that have scale and construct information provided (see https:\\/\\/ipip.ori.org\\/ItemAssignmentTable.htm for more information). Each item involved responses on a 5-point Likert scale. See Table 1 for a summary of data set characteristics. Model predictions for Study 3 participants were obtained using the same methods as in Studies 2A-C. Results and Discussion Figure 2I and J shows that the out-of-sample predictive performance of the SBERT method persisted for Study 3, with an average correlation of.37, t(160) = 44.24, p < .001, 95% CI [.35, .38]. Additionally, our SBERT model achieved significant (p < .05) positive correlations for 99.4% of participants. Finally, as in Studies 1 and 2A-C, our method outperformed alternative baseline models that use the Word2Vec and LIWC embeddings. Overall, these results demonstrate the ability of our method to generalize across constructs and questionnaires, even beyond the standard questionnaire format. In other words, our approach can be used to make accurate predictions for thousands of different personality items using only a small set of participant ratings. Study 4 Studies 1-3 have shown that our approach is able to predict participant responses to out-of-sample personality items andmoreover do so with a human-level of accuracy. This is likely because the SBERT semantic space captures the meanings of items in a manner that corresponds to the distribution of personality traits in the population. In other words, traits that correlate in the population are likely to have SBERT embeddings that are highly similar to each other. In Study 4, we rigorously tested this assumption by using item embeddings to predict the item's construct and its direction of loading on the construct. Note that this test is quite challenging (especially for the IPIP data), as the model has to learn how to classify items into a very large number of constructs. Method The items for Study 4 were taken from Studies 1-3 and consisted of the NEO-PI-R, 16PF, RIASEC, HSQ, and IPIP questionnaire (see Table 1 for a summary). Model training and prediction were done using a procedure that was similar to that in Studies 1-3. However, instead of learning to predict participant responses using item embeddings, our models learnt to predict the questionnaire construct that the item belonged to. This is a multinomial classification problem in which the number of categories corresponds to the number of constructs in the questionnaire. Thus, for example, for the NEO-PI-R questionnaire, our model attempted to predict whether a held-out item would fall into the Openness, Conscientiousness, Extraversion, Agreeableness, or Neuroticism categories. Importantly, it attempted to do so using only the items' texts and not human responses to the items. We also considered a second model type that was trained to predict the direction (positive or negative) on which an item loaded onto a construct (e.g., \\\"I feel comfortable around people\\\" vs. \\\"I keep in the background\\\" for the Extraversion construct in NEO-PI-R). We trained this model in a manner that was identical to the first model but used the items' directional loading as labels during classifier training. Note that we did not train this model on the RIASEC or IPIP data because these questionnaires do not have item direction codes (RIASEC has only positive directional loadings; the IPIP data do not provide directional loadings for most items). To keep the model training procedure consistent across studies and tasks, we again identified the best-performing machine learning technique and associated hyperparameters on the NEO-PI-R data set for each embedding type (SBERT, Word2Vec, LIWC) and then applied the respective models and hyperparameters on the remaining data sets. Note, since the labels in this task reflect a nominal scale (item construct), we only considered classification and not regression algorithms from Study 1 (ridge classification, K-nearest neighbor classification, support vector classification). To get robust estimates of our model's out-of-sample performance, we again applied crossvalidation. Analogous to Studies 1-3, we then compared our predictive model against two alternative embeddings (Word2Vec and LIWC). To evaluate model performances, for both construct and direction prediction, we calculated the classification accuracy as the percentage of correct classifications across all predictions. Note that the prediction classes, personality constructs, and directional loadings are balanced for all questionnaires, except the IPIP which has a nonequal number of items for the different personality constructs. Results Semantic Similarity Before building predictive models of construct classification, we first examined whether the embedding similarity of pairs of items from the same construct was (significantly) larger than that of items from different constructs. We measured the similarities between items by calculating the cosine similarity of their SBERT embeddings. We did this for all pairs of items in a questionnaire, then regressing pairwise item similarity scores onto a binary independent variable describing whether or not the items came from the same construct. We also included fixed effects for each of the constructs in these regressions. The regression showed a significant positive coefficient for the in-construct variable for all data sets, except the HSQ: b = 0.03, t(491) = 1.581, p = .114. Details are presented in Table 2. A visualization of the results is presented in Figure 6, for the NEO PI-R data set (6A), the 16PF data set (6B), the RIASEC data set (6C), and the HSQ data set (6D). The IPIP data set is omitted from Figure 6 since it has too many constructs (242 in total). The figure shows the average similarity scores for item pairs grouped by their constructs as a heat map and indicates that, on average, items that load onto a given construct are closer to other items that load onto that same construct relative to items that load onto other constructs. In other words, personality items, across multiple distinct questionnaires, cohere together in stable constructs not only in human data but also in linguistic meaning. Figure 6 also shows some high cross-construct similarities. For example, items from the Agreeableness and Extraversion constructs of NEO-PI-R are highly similar to each other, as are items from the Aggressive and Self-defeating constructs of HSQ. However, it should be noted here that (a) the same-construct similarity is still higher and (b) our predictive models (in the subsequent analysis) use high-dimensional item representations and not a single similarity score to make predictions. As such, these models can use additional information, such as the exact positions or directions relative to other items in the embedding space, for predicting an item's construct and distinguishing it from other constructs that have semantically similar items. Construct Prediction We found the ridge classification model with a = 1,000 to be the best-performing model on the NEO-PI-R and applied it on all other data sets as well. Figure 7A, C, E, F, and H illustrates the results of the construct prediction task for each questionnaire. Here, we see that our model achieved very high accuracy rates, outperforming both a random baseline (which would achieve an accuracy equaling the proportion of the most frequent construct) as well as the Word2Vec and LIWC baselines. This indicates that SBERT embeddings are indeed able to distinguish between same- and cross-construct items. Interestingly, our model achieved a 25% accuracy for the IPIP data set, despite the very large number of constructs (a random model would achieve only 2% accuracy for this test). Figure 7B, D, and G shows similar results for the direction prediction task. For this task, we found the ridge classification model with a = 10 to be the best-performing model on the NEO-PI-R and applied it on all other data sets as well. We achieved high accuracy rates, of up to 100%, across all questionnaires, outperforming every baseline. We also analyzed the performance of our models for individual constructs. Our main model's prediction accuracy is high across all constructs with only few exceptions. For instance, the performance on four of the 16PF constructs (Emotional stability, Social boldness, Apprehension, and Tension) is low (20%-50%) compared to the remaining constructs (70%-80%). This might be due to the high intercorrelation of these 16PF constructs, which were explicitly designed to be nonorthogonal. For the IPIP, our model performed well over a variety of constructs with an overrepresentation of Interest-related items in the top-performing constructs and Expressivity-related items in the bottom-performing constructs. It should be noted here, however, that the IPIP items stem from a multitude of scales and as such from a wide range of potentially very fine-grained constructs with strong conceptual overlap (see, e.g., constructs, such as Tolerance, Compassion, Forgiveness, Mercy, etc.). Therefore, a lower performance of our model on some constructs does not necessarily indicate a systematic weakness in capturing these meanings. Indeed, a closer look into the misclassified items reveals that many were classified as closely related constructs. For instance, \\\"I find it hard to forgive others\\\" and \\\"I try to forgive and forget\\\" were misclassified as Forgiveness\\/Mercy instead of Compassion, and \\\"I express my affection physically\\\" and \\\"I have difficulty showing affection\\\" were misclassified as Romantic disinterest instead of Positive expressivity. Similarly, some items that belong to overarching, higher level constructs are misclassified because there are suitable lower level constructs and facets that have a closer semantic match. For example, \\\"I break rules\\\" and \\\"I continue until everything is perfect\\\" were misclassified as Norm-violation and Perfectionism instead of Conscientiousness. However, this is not captured by the accuracy metric, which ignores the closeness of the model's misclassifications to the true constructs. Yet, our model was still able to achieve accuracies several times above baselines indicating our approach's broad ability to capture psychological meanings in item texts, which it can then leverage to make more accurate predictions. Clustering In our final analysis, we attempted to interpret the latent structure of our SBERT item space by clustering embeddings for all 3,653 IPIP items into a small set of clusters. For this, we used the K-means clustering algorithmwithK= 5, meaning that the items were grouped into five clusters. Note that this parameter is unrelated to the K in K-NN regression, which is a regularization parameter expressing how many responses to similar items (the Kmost similar ones) are used to estimate the response to another item. For K-means clustering, K represents a top-down assumption on the number of clusters in the data and can be used for theorizing (i.e., by setting the number of clusters based on theoretical considerations).We choseK= 5 to show the feasibility of clustering large item sets into small sets of clusters and for ease of presentation. However, we are not bound to any specific clustering solution. The resulting clusters are presented in Table 3, which reports the most frequent traits measured by the items in the respective clusters. Here, we can see that the clusters reflect reasonable psychological topics. For instance, Cluster 1 refers to traits associated with attention problems (e.g., adhd and conscientiousness), with exemplary items containing \\\"I felt like I was dreaming when I was awake,\\\" \\\"I have been told I am not listening when others are speaking,\\\" and \\\"I make careless mistakes.\\\" The remaining Clusters 2, 3, 4, and 5 refer to traits related to mental health (e.g., depression and anxiety), integrity (e.g., honesty and humility), leadership qualities (e.g., organization and leadership items), and sociality (e.g., sociability and sensation-seeking items), respectively. Table 4 provides an overview of exemplary items in each cluster. Although this demonstration is undoubtedly only a preliminary qualitative analysis, it shows that our approach can be used to meaningfully cluster large sets of items using only the item texts (;, for a similar test using word embeddings rather than sentence embeddings). Thus, this method can be used as a basis for future theoretical work that attempts to synthesize the very large set of items and behaviors that describe human personality. Discussion While Studies 1-3 evaluated the utility of SBERT embeddings for predicting participant ratings, Study 4 attempted to use SBERT item embeddings to classify items into constructs. Using the questionnaires from Studies 1-3, we found that items from the same construct were significantly more similar to each other in the SBERT embedding space than items from different constructs. Additionally, we found that our machine learning models could accurately predict both the construct and the loading (positive or negative) of items onto the construct in a questionnaire. Again, models based on SBERT embeddings outperformed those based on Word2Vec or LIWC, showing that SBERT provides a superior representation of item meaning. Finally, we provided a simple demonstration of the utility of our SBERT embeddings for construct classification, by clustering all 3,653 IPIP items into a small number of categories based on their positions in the SBERT embedding space. Together, the tests conducted in Study 4 show that item embeddings are able to capture the psychological content of questionnaire items using only their text. General Discussion The lexical hypothesis proposes that personality traits that are important to a group are encoded in words of its everyday language. This hypothesis has provided a theoretical basis for the use of linguistic descriptors in personality research and, in this way, has guided and constrained personality research for decades. Expanding on the lexical hypothesis, we have used a recent deep language model, SBERT , to obtain representations for items in personality questionnaires. SBERT is trained on large amounts of natural language data and can accurately represent the meaning of sentences in high-dimensional embeddings that are sensitive toword order and syntax.We have used SBERT embeddings, combined with various machine learning models, to predict participant responses to out-of-sample personality items from several existing questionnaire data sets: NEO-PI-R  in Study 1, 16PF  in Study 2A, RIASEC  in Study 2B, and HSQ  in Study 2C. Across these studies, we have found that the SBERT approach achieves high accuracy rates, greatly outperforming accuracy rates obtained from baseline models that represent questionnaire items using either Word2Vec word embeddings  or LIWC dimensions. Word2Vec can capture word meaning but does not take into account the structure and word order in personality items. Likewise, LIWCuses word frequency statistics to categorize items on several psychological variables but does not encode additional nuances in the meanings of the items. The superior performance of SBERT relative to Word2Vec and LIWC highlights the importance of sentence embeddings for capturing the meanings of personality items and the overall value of recent advances in deep learning and natural language processing for the prediction of human personality. Studies 1 and 2A-C have used existing personality questionnaires to measure model accuracy. To test whether our approach can also apply to a larger unconstrained set of personality items, we collected new data in Study 3, in which we offered over 3,500 personality items from the IPIP  to human participants. As with our previous studies, we found that our approach successfully extrapolates personality ratings and predicts people's responses even when test items involve diverse domains and different types of constructs. This illustrates the broad applicability of our method for personality prediction. In Studies 1 and 2A-C, we also compared the performance of our approach to incentivized human judges that were given an identical prediction task. We found that our approach achieved similar performance to human judges, indicating that the accuracy levels documented in this article match the accuracy levels that can be obtained by humans. Additionally, there was a high correlation between human accuracy and model accuracy in all studies, indicating that target participants that were easy or difficult to predict by human judges were also easy or difficult to predict by the model. This provides strong evidence that our model is able to represent the meanings of personality items in a human-like manner. In Study 4, we directly tested whether the SBERT assigned similar representations to closely related personality traits. We found that this was indeed the case, with items belonging to the same construct being given similar embeddings. For this reason, our approach was able to predict the construct that an item belonged to and could even describe how an item loaded onto its construct. Again, SBERT was much better at construct prediction than Word2Vec and LIWC. SBERT was also able to generate interpretable clusters of IPIP items in a bottom-up manner, indicating that it can be used for scale construction and construct delivery. The success of our approach has implications for the representation and measurement of personality. Prior research has represented a person's personality using a collection of linguistic descriptors. For example, a person may be described as being high in extraversion, where extraversion is specified as a set of sentences or trait words.We show that these linguistic descriptors can themselves be assigned quantitative representations. Thus, we are able to represent an individual using a collection of points (corresponding to the personality items they rate highly) in a high-dimensional space. Thus, even though the embeddings used in our analysis are the representations of the language models, since these language models represent the meanings of sentences, and sentences are (in accordance with the lexical hypothesis) used to represent personality, the embeddings by proxy also become representations of personality. The advantage of these quantitative representations is that they also inherently express the relationship between items and constructs (e.g., via distance metrics in the embedding space), as shown in the construct prediction and the clustering tasks. Testing whether our approach extends to other psychological variables is an important topic for future work. For example, life satisfaction is a variable associated with several personality traits (;, Steel et al., 2008). The study of the interplay between life satisfaction and personality currently involves regressing participants' ratings of life satisfaction on a small handful of personality dimensions (e.g., those from the five-factor model). Instead, using our approach, we could use a participant's position in our language space as a predictor, by treating each individual as the sum of the embeddings of the items that they rate highly. The high dimensionality of this representation could lead to more accurate predictions and shed light on the specific personality items (i.e., regions of embedding space) that correlate with life satisfaction (;, for a related analysis). Of course, similar analyses could be attempted with other important psychological variables, as well as socioeconomic, neural, and even genetic variables. Attempting such an analysis is a fruitful topic for future work. Our approach is complementary to recent work that attempts to model personality using high-dimensional feature vectors for individuals obtained from their social media activity. By featuring individuals, these researchers can successfully predict responses to personality surveys for out-of-sample individuals. For example, it is now possible to determine, based only on a person's social media activity, whether a person will be high or low on extraversion. While social media analyses focus on predicting out-ofsample individuals, we show that it is also possible to predict out-ofsample personality items. By combining the two approaches, it may even be possible to predict responses of out-of-sample individuals to out-of-sample items. If successful, this would help personality researchers characterize arbitrary individuals on arbitrary traits and thus open the door to several new types of quantitative behavioral analyses (;, for recent work demonstrating the feasibility of this analysis). However, these approaches need to be carefully balanced with ethical considerations. Researchers applying our methods must ensure strict observance of informed consent, optout of data usage, and especially data anonymization to prevent out-ofsample participant predictions from being applied on unwilling participants (e.g., by collecting their online data) and out-of-sample itempredictions frombeing applied on sensitive items that participants refuse to answer (e.g., questions on sexual orientation). Quantifying personality items, as we have done in this article, also has value beyond prediction. For example, we have shown that our methods can be used to infer high-level constructs associated with unlabeled personality items and even cluster large sets of items into a smaller set of constructs. Importantly, this method does not require participant data, making it easily scalable to tens of thousands of personality items. This method has particular value for survey design. For example, it would be possible to optimize the set of personality items used in a given survey, by selecting personality items from distinct regions of language space, thereby maximizing the information acquired in a survey. Similar optimal experimental design methods have been proposed in other areas of psychology  and can be extended to personality research, as we are now able to quantitatively represent the stimuli used in personality experiments. It should be noted here that our approach relies on data exclusively collected from Western samples. The overreliance on so-called WEIRD (Western, educated, industrialized, rich, democratic) samples for psychology research has been criticized and previous research found that many fundamental cognitive and affective processes differ across populations. Similarly, the language models used to create quantitative item representations are based on the language use of mostly WEIRD populations in English. Different populations and languages might have different linguistic conceptualizations of personality that, following the lexical hypothesis, should map better on these populations' personalities (;, for difficulty accessing personality traits using WEIRD-based questionnaires). As such, our models could be WEIRD-skewed and mask potential group-specific differences in personality traits. This gives reason for caution against an unreflective use of ourmodels and should motivate researchers to consider language models trained on non-WEIRD corpora. For instance, ParsBERT  and ARBERT\\/MARBERT  are BERT-based monolingual language models trained on Persian and Arabic corpora and subsequently outperform multilingual language models usually used for these languages, such as multilingual BERT  or Cross-lingual Language Model based on A Robustly Optimized BERT Pretraining Approach. However, the cultural bias in the language models is also an opportunity to quantitatively study group differences using this framework. For instance, to what extent do item representations based on different languages or language use of different populations differ? Do cultural perceptions and connotations of personality in language map onto measurements of personality? Another potentially fruitful line of research could incorporate current debiasing efforts of language models  into this approach. Reducing cultural biases in language models could help increase model performance and generalizability to non-WEIRD populations. Finally, the applicability of our approach is not limited to personality research. For example, it would be possible to apply an identical research pipeline to predict emotion and well-being, using the thousands of items across hundreds of survey questionnaires and well-being dimensions, that have been proposed in prior research. A similar approach could be applied to the prediction of risk preferences, organizational attitudes, and health judgments. More generally, survey-based research is a central component of many areas in the behavioral and cognitive sciences, including health, clinical, consumer, and managerial psychology. We show that it is possible to accurately quantify the language in surveys with deep networks, providing researchers with mathematical representations for verbal constructs and items. We look forward to future work that extends our approach to alternate domains in the behavioral sciences, thereby facilitating not only more accurate prediction but also more rigorous scientific theorizing.\",\"1135745859\":\"INTRODUCTION Stress and adversity are an inevitable part of the human experience. However, not everyone is equally successful at overcoming potentially negative events. The construct of resilience offers one explanation for this. While there is no general agreement on what constitutes the psychological resilience, most researchers agree that it entails two core concepts: (1) the presence of a potential stressor, and (2) positive adaptation (; for a review). There is a growing body of research investigating different trajectories or divergent pathways of adjustment in response to acute and chronic stressors. These trajectories can include thriving, recovery, surviving, and succumbing. However, little empirical evidence exists for these trajectories in response to a clearly referenced, acute stressor. Such research, however, can provide evidence for the positive adaptation, the key process suggested to underlie mental resilience (; for a review). Well-designed interactive technology (e.g., games, simulations), also referred to as Virtual Performance assessments (VPAs) show promise as a means to measure complex noncognitive constructs including resilience, and have been referred to as the next-generation of assessment. A recent scoping review found interactive technology can deliver effective interventions to increase resilience. The primary aim of this paper is to detail the design, development and validation of an immersive simulation-based assessment methodology to measure resilience, focusing on different trajectories within an acute stressful event. The assessment framework draws from principles of evidencecentered design  and embedded stealth assessment , and is informed by well-established theories of resilience. We created a dynamic simulation environment, where players had to adapt to and overcome various unexpected and challenging events to complete the task objective. Performance was reflective of different trajectories, and these pathways were validated against an existing gold-standard self-report measure of resilience, the Connor-Davidson Resilience Scale (CD-RISC;). The present research makes several novel theoretical and empirical contributions. First, we advance research on how game design elements, coupled with an evidence-based assessment framework, can make powerful assessment tools. Second, we give evidence of patterns of trajectories in response to adversity, emphasizing a holistic approach to resilience. Third, we contribute to recent advances in Machine Learning as a technique to analyze complex datasets and predict individual differences. Practically speaking, our work addresses the potential for gamified assessments as a supplementary method for measurement. Assessment of Resilience Given different conceptualizations of the resilience construct in the literature (as a trait, a process, and\\/or outcome; for reviews), varying methods have been used for its assessment. Common methods for the assessment of resilience include: (1) self-report scales and situational judgment tests , (2) indirect based on the presence of risk and positive adaptation , and (3) measuring resistance or adaptation to negative life events, everyday stressors, or experimentally created stressors. Despite the wide range of assessment formats, they are all evaluated against fundamental reliability and validity criteria. The psychometric properties of self-report methods of resilience are well-established, with some measures demonstrating excellent validity and reliability (e.g., CD-RISC;). Nevertheless, these scales are designed to capture resilience as a trait, although research has moved toward a broader process view, depicting the process through which internal and external factors interact to influence one's response to adversity. These scales also tend to approach resilience in basic terms of presence or absence of psychopathology. However, this approach neglects the distribution of individual differences in resilience, that is, the different types or variations in responses to adversity. Researchers have also highlighted the need to develop multimodal methods of resilience, including moving toward objectively verified assessments, rather than a sole focus on resilience as a personality-like variable. Some self-report measures of resilience also have problems with discriminant validity as they fail to separate from related constructs , prompting some authors to advocate for an integrative process model of resilience. Game-based and simulation assessments offer a potential supplementary method for assessing the process of resilience, capturing responses to adverse or stressful events to produce an outcome. Integrating such an approach with the use of traditional self-report scales such as the CD-RISC to capture stable individual differences, would provide a more holistic depiction of the resilience process. Self-report measures can also be affected by faking such as malingering, self-deception and impression management, especially in high-stakes contexts. This can impact both construct and criterion-related validity. Response distortion can negatively affect the factor structure of measures through differential item functioning , and lead to misinformed decisions and predictions. Different methods have been developed to detect response distortion. A common method is to correct scores using social desirability or lie scales. Yet, researchers have questioned the construct validity of these scales as they may confound response style with trait measurement. Other methods in detecting intentional response distortion include forced-choice response options , eye-tracking , response latencies , and sophisticated mathematical algorithms to adjust scoring. However, these methods do not remedy the problem of reducing multiple sources or opportunities to distort responses. More recently, psychophysiological markers measuring responses to external stimuli (e.g., skin conductance) have been employed to indicate or predict resilience. For instance, Walker et al. (2019) found individuals who rated themselves higher on trait resilience habituated quicker to acoustic startle stimuli (i.e., showed a reduction in the amplitude in skin conductance on repeated presentations). Although such approaches show promise for supplementing traditional methods, these studies tend to be resource-intensive, resulting in relatively small sample sizes typically below 40 participants. Using Games and Simulations to Assess Psychological Constructs Modern technologies have allowed for the development of dynamic games and simulation environments. These open-ended discovery spaces take many forms, including entertainment games, serious educational games, simulations, and virtual\\/augmented reality (; for reviews). Gamification is referred to as the use of game design elements in non-gaming contexts. Games and simulations are defined as \\\"a system in which players engage in an artificial conflict, defined by rules, that results in a quantifiable outcome\\\" (;, p. 80). In addition, Prensky (2001) includes goals, interaction, feedback, and representation as game design elements. In other words, a game necessarily involves creating an environment, objects, connections, rules and choices that allow the player to identify with the virtual character and immerse themselves into that game. Game-Based Assessment The commercial use of game-based assessment has increased substantially in recent years. For example, personality  and non-cognitive constructs such as resilience, adaptability and flexibility  have been assessed for personnel selection via games. However, the rapid expansion of commercial gamified assessments has attracted reservations about their psychometric properties. The lack of transparent and publicly available information about the design and data gathered from these instruments leave researchers with little evidence to evaluate their utility. Other criticisms include that they: lack a theory-driven design, tend to not be vetted in terms of scientific rigor, and have yet to demonstrate validity and reliability comparable with existing measures. Game-based assessments are also frequently used in educational settings, and their design is typically well-informed by evidence-based assessment frameworks. This type of assessment can support learning objectives and outcomes. For instance, Shute et al. (2013, 2015) have demonstrated how games, coupled with evidence-based embedded assessment, can validly assess hard-to-measure constructs in educational contexts such as persistence , problem-solving, and creativity. Through log files, games allow us to record both the player's final choices and the decisions made before arriving at that choice (i.e., product and process data). This gives a rich bank of data that typically cannot be captured by closed-ended assessment tools. Simulation-Based Assessment Simulations are defined as \\\"any artificial or synthetic environment that is created to manage the experiences of an individual with reality\\\" (;, p. 415). They share similar characteristics to games, however, can be distinguished in that they attempt to represent real-life situations and environments. The level of realism or fidelity of simulated worlds can range from low to high, depending on how well the system represents reality. For instance, artificial fantasy in games vs. hyper-realistic 3D audio-visual rendering and motion platforms in large scale simulations. Similarly, user response options can range from purely symbolic such as keystrokes and mouse clicks, through to more realistic controls such as joysticks, steering wheels and pedals producing physically plausible changes, and all the way to locomotion in free-roam virtual reality applications. Representational and physical fidelity are important design considerations, and the cost-fidelity trade-off is a well-known problem. Whilst high fidelity may appear to be desirable, meaningful play actually comes from the interaction between players and the system of the game. Thus, both low-and high-fidelity systems can foster meaningful interaction. Using a combination of simulated environments and embedded performance assessment techniques, early systems measured behavioral task performance with minimal interference attributable to measurement itself. For instance, The Strategic and Tactical Assessment Record (STAR;), produced a comprehensive array of perceptual and information processing parameters comparable with common laboratory measurements. One of its key advantages was that \\\"all measurement procedures were embedded within the operations required to play a computer game\\\" (;, p. 643). Shute (2011) describe the concept of stealth assessment, which involves embedding assessment unobtrusively and directly into the fabric of the gaming or simulated environment. Another strength of simulation technologies is the ability to continuously gather complex behavioral or performance data at a fine grain size, dynamically and in real-time. For instance, a player can be immersed in a 3D augmented reality experience and presented with complex objects that move and rotate in space. In addition to recording their responses, it is also possible to capture their body and gaze movements, and how they rotate the objects around them. This richness of data collection makes simulated scenarios very powerful assessment tools. In this regard, simulated environments can exceed the usual physical and cost-prohibitive boundaries of space and time. Nevertheless, well-designed simulations have many challenges and require substantial effort in the design and iteration phases. Some of these challenges include: (1) crafting appropriate and accurate digital environments to elicit the constructs of interest; (2) making valid inferences about the individual's behavior without disrupting the \\\"free-flow\\\" feel of the simulation; and (3) processing, synthesizing, extracting and interpreting the large quantities of data captured. The following sections detail how these challenges were addressed in designing the present simulation using an evidence-centered design framework. Applying Evidence-Centered Design in Simulation-Based Assessment For a simulation to be valid, we must consider psychometric principles from assessment design frameworks. Evidencecentered design (ECD;) provides an excellent point of departure. ECD builds an evidentiary chain of reasoning  and dates to Messick (1994, 1995), who lays out a series of questions for assessment design: (1) what knowledge, skills, or attributes should be assessed (are they valued by society)? (2) what behaviors reveal those constructs? and (3) what tasks or scenarios should elicit those behaviors? The ECD framework builds on the vision of Messick by formalizing these questions through three central models in the Conceptual Assessment Framework: Student (or Competency) Model, Task Model, and Evidence Model. The Student Model answers the question of What are we measuring? It defines the variables related to the knowledge, skills and abilities to be measured. The Task Model answers the question Where do we measure it? That is, what tasks elicit behaviors to produce the evidence? It describes the environment or scenarios in which individuals say or do something to produce evidence about the target construct. Finally, the Evidence Model bridges the Student and Tasks Models and answers the question How do we measure it? That is, what behaviors reveal different levels of the targeted construct? It analyses a player's interaction with, and responses to a given task. An evidence model consists of two parts: evidence rules and statistical model. Evidence rules take work product (e.g., a sequence of actions) that comes from the individual's interaction with the task as input, and produce observable variables (e.g., scores) as output, which are summaries of the work product. The statistical model expresses the relationship between the constructs of interest in the Student Model and the observable variables. The Student Model: Defining Resilience The rationale for investigating resilience as the focal construct is because stressful and aversive events continue to plague humans at each stage of the lifecycle. How one responds to setbacks and uncertainty is critical for well-being and survival in a constantly evolving world. Resilience entails two core concepts: (1) the presence of a potential stressor, and (2) positive adaptation. The term \\\"potential\\\" is important as there are differences in how individuals react to a certain event; some people are overwhelmed by daily hassles whereas others thrive in testing experiences. This means that people do not uniformly perceive a potential stressor as stressful. Recent research also emphasizes the broad term stressor, which encompasses a range of demands that necessitate resilience. Earlier definitions used the narrow term adversity, which only captures significant negative life events. Davis et al. (2009, p. 1638) argued that \\\"for most of us, the adversities we encounter do not constitute major disasters but rather are more modest disruptions that are embedded in our everyday lives.\\\" Thus, this study focuses on modest short-term stressors, rather than severe longterm hardship. By integrating Carver's (1998) responses to adversity framework and Richardson's (2002) metatheory of resilience, four potential trajectories can result after experiencing a stressor: (1) resilient reintegration, where the individual returns to a higher level of homeostasis (thriving), (2) homeostatic reintegration, where the individual returns to their baseline level after decline (recovery), (3) reintegration with loss, leading to a lower level of functioning (surviving), and (4) dysfunctional reintegration, leading to maladaptive behaviors (succumbing). This is presented in Figure 1. In the present study, the model is applied to a discrete, time-bound adverse experience (although it can also be applied to a prolonged period of adversity;). The Task Model: Design Elements As mentioned earlier, the most common method to measure resilience is via self-report. A less commonly developed method is measuring resistance or adaptation to experimentally created stressors. Thus, we employed the latter method by aiming to capture responses to a clearly referenced adversity, where an individual is surrounded by high levels of emotional (and potentially physiological) stress. This \\\"reactivity\\\" approach involves measuring behavioral, subjective, or physiological responses to stimuli, whether naturalistic or experimental. Adaptive reactivity would indicate resilience. The challenge was to foster high enough levels of stress and demand to elicit responses (not necessarily in magnitude, but in nature) to those experienced in realworld situations. To foster meaningful play, designing the formal system structures of the present simulation took into consideration core game elements, including objectives, procedures, rules, players and player interaction, conflict, and outcomes. The scenario was a driving simulation; however, it is important to note that the driving task is merely a medium or means to demonstrate the method of assessment (i.e., embedded evidencebased design framework), which is the focus of this research and described in detail below. That is, the assessment methodology is based on an ECD framework and intended to measure the target construct, thus should be independent of the medium. The driving scenario does not intend to measure driving or gaming experience per se, but rather, an individual's trajectory in response to stressors, which would demonstrate pathways of resilience as hypothesized in previous theories of resilience. Indeed, any type of simulated task could be used (e.g., flight, racing simulators), so long as it follows and is grounded in a strong theoretical assessment framework and methodology. A driving task was chosen out of practicality because they are middle ground in representational and physical fidelity, with physically consequential responses captured via steering action and pedals for acceleration and braking. Players actively partook in a simulated training exercise for drivers of emergency vehicles (e.g., ambulance). Taking on the role of an emergency driver created a sense of urgency and timepressure. Players drove around a metropolitan area designed as an urban grid, until they reached a final predetermined destination. This allowed players to be continuously directed along new routes in a relatively easily rendered space. Unbeknownst to them, they completed five different \\\"laps\\\" within the metropolitan area. Laps began and ended at the same location but took different paths around the city (see Figure 2). Procedures, which are specific instructions of what actions to take, involved the player driving in a direction given by green arrows at every intersection. Directional arrows allowed the ability to create standardized and identical testing conditions. Hence, all players experienced the same events at approximately the same time. They could deviate from the arrows; however, they were instructed to stay on the desired path. Even if left unfollowed, participants could eventually return onto the directed path, although it would take longer for them to complete the task. If they did not follow the arrows, red no-go signs appeared, reminding them to make a U-turn which would lead back to the arrows. The objective, which aims to increase motivation and engagement, was to reach the destination as quickly and as safely as possible (i.e., maximize speed and minimize collisions). If present, another player controlled an Unmanned Aerial Vehicle (UAV) with a birds-eye view, in which they communicated information to the driver to help them drive faster and safer. For instance, instructing the driver when it was safe to use an incoming traffic lane. Depending on the helpfulness of the information, player interaction facilitated or inhibited the drivers' actions. Rules, which define conditions, restrict actions and determine effects on players, specified what players could and could not do. Players could break road rules (e.g., speeding, driving in the opposite lane), and could only drive on the road (e.g., could not drive on footpaths or through parks). Small road guards were added to prevent cars leaving the road. Figure 3A shows how this was managed via large invisible colliders. Conflict, which emerges to prevent players from achieving their goal, was another core element. The type of conflict were physical obstacles encountered throughout the drive-what we term as \\\"event probes.\\\" This event-based methodology is a systematic approach to designing simulated scenarios that are linked to the target constructs for assessment. Events must have clear start and end points to demarcate windows of meaningful data. Within these windows, the conditions players are exposed to are comparable. In line with stealth assessment, events were created and embedded with an emphasis on the scenario. Event probes appeared unexpectedly, disrupting players' actions such that a decrease in speed and increase in collisions was likely. Five events were designed: a falling lamp post, animals crossing the road, a stationary car blocking the driving lane, dense fog, and black ice. For the first three events, drivers needed to slow down and\\/or change lanes to avoid collisions. For the dense fog, drivers needed to reduce speed as long-distance vision was compromised. For the black ice, there was a loss of friction causing the car to slide, meaning drivers had to persist in accelerating and maneuvering the position of the car to avoid collisions (see Figure 3B). Each event was presented once in each lap, meaning participants had five encounters with each event throughout the entire simulation. The insertion of event probes changed task demands in which participants were to adapt and overcome. The onset of each event was without warning, and embedded at a different location and order on each lap, in order to mitigate associating an event with a certain location. This is known as the \\\"task-change paradigm\\\" . Event probes also provoked stress and frustration, requiring players to maintain composure. Emotional regulation is needed to redirect situational attention toward task demands. When no event probes took place, this was referred to as \\\"probe-free periods.\\\" Nevertheless, participants still navigated heavy and changing traffic conditions. They also did not have an opportunity to practice as the aim was to capture dynamic responses under stressful and unpredictable conditions. In simulations aiming to capture other behaviors, it might be reasonable to inform players about the obstacles they are about to face and to provide them with the practice opportunities. This, however, would have compromised the purpose of the simulated environment. The Evidence Model: Individual Performance Trajectories Using Slope Analysis Log files record players' progress throughout the game or simulation. Analyzing log files involves parsing for relevant information and extracting performance indicators. Identifying evidence that connects performance to the target construct requires well-structured log files and analysis methods. Three types data were collected: 1. A tab-separated values file for session-level information (e.g., start time of the simulation, session ID, player ID); 2. A tab-separated values file for time-stamped actions (e.g., collisions with other cars); 3. A directory of tab-separated values files, one for each value time-stamped and recorded frame-by-frame (e.g., angle of the steering wheel; position of the vehicle). Evidence of the target constructs need to reflect how participants respond to the event probes, and how their responses change over the course of the simulation with each encounter. Speed and collisions were included as the key output indicators. They were chosen as variables of interest as they give measurable, unambiguous and objective outcomes of performance level. A response-time in complex simulation-based tasks is generally suggested to be in scoring. Speed was defined as an arbitrary digital miles per hour, and collisions were measured as the number of times a participant's vehicle collided with external objects. Below summarizes the process of how work product was taken as input (e.g., raw time-stamped data indicating a sequence of actions) and how observable variables (e.g., lap-level scores\\/measurements) were produced as output. This is known as the evidence rules. Step 1: Lap-level measurements. The raw timestamped data included logs of collisions, speed, frame-by-frame recordings of the vehicle position, and steering wheel angle, accelerator and brake values. These data were used to create the following lap-level scores: collision frequency, average speed, time taken, distance traveled, and the mean and standard deviation of steering wheel, accelerator, and brake values. That is, each driver had five estimates for each of these variables corresponding to each of the five laps. Average speed and collisions were also estimated overall (i.e., across the entire drive), during event probes, and during probe-free periods. Figure 4 shows the average speed and number of collisions of all drivers for each lap. Step 2: Slope analysis. Derived variable analysis takes a collection of measurements (in this case, the five lap-level scores) and collapses them into a single meaningful summary feature (in this case, the slope) . Slope analysis was used to determine individual performance trajectories. That is, we analyzed an individual's rate of change over time, with the slope as the key outcome. For each individual, the magnitude and direction of performance changes (Y) were estimated over baseline (intercept), linear, quadratic, and cubic slopes (see Equation 1). The model intercept (a) and slopes (b1Xi linear, b2X2i quadratic and b3X3i cubic) were derived for each participant on each of the five estimates for both speed and collisions to represent a player's starting point and change over time. The statistical significance of slopes (beta weights) capture the strength of changes. The performance trajectories are indicative of individual differences in responses to stress (defined in the Student Model): thriving, recovery, surviving, or succumbing. This is known as the statistical model. Y = a + b1Xi(linear) + b2X2 i(quadratic) + b3X3 i(cubic) +- e (1) Slope Analysis: Linear, Quadratic, and Cubic Slopes For speed, thriving is indicated by a strong positive linear slope (see Figure 5A). For collisions, thriving is indicated by a strong negative linear slope (see Figure 5B). These trends indicate the player continuously improved their performance (faster speeds and lower amount of collisions), despite obstacles encountered throughout the drive. These individuals adapted quickly to the changing situation; there was no or a relatively small loss of performance following obstacles or changes in the simulated environment in a lap 1, and they quickly relearned the changed situation. Thriving reflects decreased reactivity and faster recovery to subsequent stressful events, and a consistently high level of functioning. The strong the betas, the stronger the improvements across the five laps. For speed, recovery is indicated by a strong positive quadratic beta, or a strong positive cubic beta (see Figures 6A,B). For collisions, recovery, is reflected by a strong negative quadratic beta, or a strong negative cubic beta (see Figures 6C,D). These trends demonstrate the ability to bounce back or return to former levels of functioning, after a decline in performance. After a downturn, they either return to baseline levels or continue toward an upward trend and function at a higher level than previously. After repeated experiences with the events, they are able to recover, should the stressor recur. Weak or non-significant linear slope indicates merely surviving (i.e., strength of betas is relatively low). Whilst no significant improvements nor declines, these participants maintained homeostasis and were able to \\\"just get past\\\" the challenging events. Examples of surviving are displayed in Figures 7A,B for speed and collisions, respectively. These participants are able to withstand the challenging events, with no major deterioration nor improvement. The ability to withstand stressors is argued to be commonplace, emerging from the normative functions of human adaptation systems. Finally, succumbing is indicated by various slopes. For speed, strong negative linear, quadratic, or cubic slopes indicate a poorer performance level relative to the initial baseline level (see Figures 8A,B). For collisions, strong positive linear, quadratic, or cubic slopes indicate succumbing (see Figures 8C,D). These participants show a continued downward slide (slower speeds and greater amount of collisions). They have a relatively large loss of performance following unexpected events, and they slowly or are unable to adapt to them. This leads to eventual succumbing after experience with repeated stressful events. Validation With an Existing Resilience Measure Following the approaches of Ventura and Shute (2013) and DiCerbo (2014), the individual performance trajectories were validated against an existing measure of resilience, the CDRISC. Connor and Davidson (2003) adopted Richardson's (2002) metatheory of resilience to develop their widely validated scale. It captures several trait-like aspects of resilience: the ability to adapt, to deal with stressors, to stay focus under stress, to handle unpleasant feelings, and the ability to stay on a task in the face of failure. The scale demonstrated strong psychometric properties. Reviews Windle et al. (2011) and Pangallo et al. (2015) have given the CDRISC the highest rating in terms of quality criteria (validity and reliability), compared to other resilience scales. For validation, individual performance trajectories (linear, quadratic, and cubic slopes derived from lap-level measurements) are taken as inputs, and scores from the CD-RISC are taken as outcome criteria for predictive models. The experiment was conducted in a lowstakes environment to foster genuine responses on the CDRISC, as the motivation to distort responses is more likely to occur in high-stakes situations such as job assessments. Research Questions and Aims The overarching goal of the present research was to design a simulation-based assessment methodology to measure psychological resilience and provide data to evaluate its merits. Given the same experiences with the challenging simulated scenario and events, which people thrive and which are impaired? This study focuses on two key research questions to assess the validity of the simulation-based assessment: 1. Can slope analysis (i.e., rate of change over time in the simulated task) give empirical evidence of resilience theories hypothesizing different individual trajectories of responses to stressors (thriving, recovery, surviving, succumbing)? 2. What is the relationship between individual trajectories and scores on an existing resilience measure (CD-RISC)? METHODS We report how we determined our sample size, all data exclusions, and all measures in the study. Participants 109 undergraduate students participated and acted as drivers in the simulation, in return for course credit (59 females, 50 males; mean age = 20.10, SD = 4.72). Two participants were excluded because they did not complete the simulation. Four participants were excluded because they ignored instructions to follow the arrows directing their course, thus missing multiple events (more than 7). An additional 13 participants were excluded due to a hardware error which caused their teammate to observe different traffic conditions to what they experienced, making the teammate's instructions inaccurate. The final sample was composed of 90 participants who acted as drivers in the simulation (50 females, 40 males; mean age = 20.40, SD = 5.11). Machine Learning analyses (MLA) were utilized to quantify the accuracy of predictions. There are no agreed upon normative rules about how much data is needed to validate predictive models using MLA. Bzdok et al. (2018) recommends these analytics \\\"when one is dealing with 'wide data,' where the number of input variables exceeds the number of subjects, in contrast to \\\"long data\\\" (2018, p. 233). The decision is typically based on the complexity of both the research question and the learning algorithm used in training and prediction, and the number of classes, input features, and model parameters used. Given the simplicity of the research questions and predictive model (based on eight main features used separately for two metrics--speed and collision), and standard algorithms used, this sample size is more than satisfactory for the preliminary validation of the simulation before proceeding with a cross-validation of these results. This sample size was also adequate to examine a baseline linear regression model (with more than 10 people per feature\\/variable). More details are provided in the Machine Learning Analysis section of the results below. Measures Simulation and Related Measures Driving Simulation The simulation is described above in the introduction. NASA Task Load Index (NASA-TLX;). This is a 6-item measure of workload. It was administered immediately after, and in direct relation to the simulation. An example item is, \\\"How mentally demanding was the task?\\\" which was rated on a 7-point scale from (1) very low to (7) very high. Simulator Sickness  This questionnaire measures 16 symptoms of simulator sickness and cybersickness. There were three symptom clusters including Oculomotor (e.g., eyestrain, headache), Disorientation (e.g., dizziness, difficulty concentrating), and Nausea (e.g., stomach awareness, burping). Symptoms were scored on a 4-point scale from (0) None to (3) Severe. This questionnaire has demonstrated good internal consistency estimates (0.87;). Driving Experience and Gaming Intensity Driving experience was recorded as the number of years driving a car, and gaming intensity was measured as the average number of hours spent playing video games in a single session. Validation Measure Connor-Davidson Resilience Scale (CD-RISC;). This measure consists of 25 items assessing the ability to cope with stress and adversity. Items (e.g., \\\"I am able to adapt to change\\\") were rated on a 5-point Likert scale from (1) not true at all to (5) nearly always true. A higher total score indicates greater resilience. This scale has demonstrated good internal reliability (0.89;). Procedure The driving simulation was hosted on Unity game engine platform, and presented on LG flat-screen monitors (43inch screen size). The driving station consisted of Logitech G Driving Force Racing Wheel, Pedals and Playseat. All other measures were computerized and hosted on Qualtrics, a survey software platform. Sixty-seven (74.4%) drivers did the simulation with a teammate and twentythree (25.6%) drivers completed it alone. Participants did not know each other before the study. They first completed demographic information (age, gender) and the simulation-related measures. They then completed the driving simulation (30 min), followed by the CD-RISC. Ethics approval was granted by Australian Defence Science and Technology Group Low-Risk Human Research Ethics Review (Protocol Number 07\\/415). RESULTS We begin by presenting descriptive statistics, providing a comprehensive examination of simulation-derived metrics, selfreported resilience, and evaluation of the simulation. Next, we present results of the slope analyses, including the proportion of individuals showing different performance trajectories. Given the complex distributional properties of the simulation-derived metrics and possible non-linear relationships between variables, we then used Machine Learning Analysis (MLA) to quantify the accuracy of predictions. MLA was employed instead of traditional correlational analyses, because it can achieve relatively greater sensitivity compared to conventional techniques, which would likely deflate and\\/or fail to capture relationships between the variables. As MLA in psychological sciences is concerned with predictive accuracy, to optimize prediction we compared algorithms across degrees of complexity through the use of different linear and non-linear algorithms (including ridge regression, support vector machines, boosting, random forests) to examine relationships in the data (; for reviews). Accuracy of these algorithms were also compared to a baseline linear regression model to test whether they outperform the baseline. Descriptive Statistics Simulation-Derived Metrics Figures 9A,B display the frequency distributions for overall speed and collisions in the simulation. Speed was normally distributed whilst collisions were positively skewed. Table 1 presents descriptive statistics and Cronbach's alphas for average speed and number of collisions overall, during probe-free periods and during event probes. Internal consistency for speed and number of collisions overall across the five laps was high, and acceptable to good during probe-free periods and event probes. Paired sample t-tests were conducted to examine differences in speed and collisions during event probes vs. during probe-free periods. Participants had more collisions during event probes (t = 4.05, p < 0.001, Cohen's d = 0.43), however, there were no significant differences in average speed (t = -1.42, p = 0.16, Cohen's d = -0.15). Simulation-Related Measures Table 2 presents the descriptive statistics for NASA-TLX, driving experience, and gaming intensity. It appeared that participants were highly engaged and motivated to try their best in following the instructions to achieve the goal. Participants reported the highest score for effort, followed by mental and temporal (timepressured) demands, then frustration, physical demand, and finally for performance. Table 3 summarizes paired sample t-tests comparing pre- and post-simulation for 16 simulator sickness symptoms. Participants reported changes to half of the symptoms. Cohen's d were below medium effect sizes (d < 0.50) for 75% of affected symptoms. The most potent differences were sweating and fullness of the head with d-values over a medium effect size. The Nausea cluster of symptoms was most affected (in order of effect sizes): sweating, general discomfort, salivation, and nausea. The second most affected cluster was Disorientation (in order of effect sizes): fullness of head, and dizziness (eyes open). Two symptoms in the Oculomotor cluster (eyestrain, headache) were affected but their effect sizes were small. Participants reported no differences in fatigue, difficulty focusing and concentrating, blurred vision, dizziness (eyes closed) vertigo, stomach awareness, or burping. An overall simulator sickness score was computed both preand post-simulation. The difference between the two scores was standardized. Majority of drivers (83.3%) were within 1 SD of the mean difference symptoms score. Four drivers (4.4%) reported an improvement in symptomatology in the size of 1 (3 drivers) and 3 (1 driver) SDs, respectively. Four drivers (4.4%) were 1 SD below the mean of the standardized difference score. Five and two drivers were 2 and 3 SDs below the mean, respectively. Thus, these seven people reported a notable detrimental change in symptomatology post-simulation. Validation Measure The mean score on the CD-RISC was 3.63 (SD = 0.40, Range = 2.20-4.84) and internal consistency was good (a = 0.86). The frequency distributions represented a good spread of variance, rather than a skewed distribution (see Figure 10). In low-stakes situations such as that of the present research, people are less inclined to \\\"fake good\\\" or \\\"fake bad,\\\" thus a normal distribution was observed, with an expected proportion of people reporting relatively low or high resilience levels. Slope Analysis: Individual Performance Trajectories Table 4 summarizes the percentage of people with different slopes for speed and collisions, during both the event probes and probe-free periods. Significant positive and negative slopes were investigated for the linear, quadratic, and cubic slopes. To allow for marginal error, the significance level was set at p = 0.20. Several findings are worth noting. During the event probes, a small proportion of people had a positive linear slope for speed and a negative linear slope for collisions, which is indicative of thriving. With each encounter with an event, performance improved (speed increased, collisions decreased). These participants displayed a consistently high level of functioning despite constant challenges. During the event probes, a small proportion of people displayed positive quadratic and cubic slopes, respectively, for speed, and negative quadratic and cubic slopes, respectively, for collisions. These trends suggest the ability to recover--these people experienced a brief downward slide in performance but were able to bounce back from the embedded stressors and returned to their previous levels of functioning. A large proportion of participants (around 6070%, with the percent varying for speed and collisions during events and probe-free periods) displayed non-significant trends (i.e., relatively weak betas or plateau-like slope). This suggests they were merely surviving--their performance level neither substantially increased nor decreased, but they maintained homeostasis. Finally, during the events, a minor proportion of people showed a negative linear slope for speed and a positive linear slope for collisions. This may suggest succumbing--these participants failed to adapt their behavior to each recurring event and could not recover after initial poor performance. With each encounter with events, there was a downslide in performance. Machine Learning Analysis One of the major applications of Machine Learning in psychological research is the development of models focused on predicting human behavior. These analytics are recommended when non-linear relationships with well measured predictors are being modeled, which is a case for this research. Key machine learning techniques were considered in this study, including feature selection, crossvalidation, and models\\/algorithms used. Feature selection is selecting specific variables from a larger dataset to enhance accuracy and generalizability. Eight speed features and eight collisions features were selected to train models to predict scores on the CD-RISC for validation. The eight features were: first lap value, best lap value, worst lap value, overall average lap value, intercept term, linear term, quadratic term, and cubic term. For each of these features, there were estimates for during the event probes, during probe-free periods, and throughout the simulation overall. In order to mitigate the risk of overfitting, we used a train-test split to derive train and test subsets (7:3 ratio of train:test). The hyperparameters were only trained in the train subset, and the results were for the test subset only. A baseline regression model was first conducted using a linear regression; the outcome variable was separately regressed onto the eight features. A baseline model provides a reference point to which to compare different machine learning algorithms, including the extent to which these algorithms add improvement over and above the baseline. Then, three common linear and nonlinear Machine Learning algorithms were applied: Random Forest, Bayesian Ridge regression and Support Vector Machine. Jacobucci and Grimm (2020) have recommended to compare predictive accuracy across algorithms with different degrees of complexity. The models were fitted and tested with predictions, and the Symmetric Mean Absolute Percentage Error (SMAPE) was used to quantify prediction accuracy. SMAPE is a widely used measure of accuracy, due to advantages of interpretability and scale-independency. These estimates are presented in Table 5. Each algorithm performed substantially better than the baseline linear regression model. While all methods produced good SMAPE estimates, attesting to the stability of the predictions, Bayesian Ridge regression algorithm produced the highest predictive accuracy (lowest error, with SMAPEs below 5%). To demonstrate that the methodology was an assessment of resilience, rather than driving or gaming ability, both driving experience and gaming intensity were investigated as outcome variables in the machine learning models. The bottom of Table 5 also presents the SMAPE estimates using the best performing algorithm, Bayesian Ridge regression (see Supplementary Material for results of other algorithms and baseline). Individual performance trajectories were weakly associated with driving experience, as it was predicted with relatively high error rates between 36.84 and 46.04%. Similarly, weak association was observed with gaming intensity: predictive error rates were high (between 36.57 and 50.15%). This suggests the simulation was assessing the target construct, resilience, and provides evidence of discriminant validity. DISCUSSION As modern technologies continue to progress, game and simulation design has correspondingly expanded for learning, assessment and training purposes. The present study aimed to detail the development, design and initial validation of a simulation-based assessment methodology to measure psychological resilience. The development of the simulation was guided by well-known resilience theories , in addition to an evidence-centered design framework  and embedded stealth assessment. This study took a \\\"reactivity\\\" approach to measuring resilience, which involved systematic and deliberate exposure to stressful conditions. These tests of reactivity to acute stressors have been proposed to assess different levels of adaptive or maladaptive responding. The findings demonstrate how game design elements, such as objectives, procedures, rules, and conflict, can be applied to make powerful assessment tools. Majority of players accomplished the objective of following arrows to reach the end destination. The success of this objective varied, with prominent individual differences in both absolute performance (speed and collisions) and trajectories (rate of change over the five laps). Conflict, crafted from the event probes, increased task difficulty by impeding players in reaching the objective of maximizing speed and minimizing collisions. Indeed, players collided more often during event probes, compared to probe-free periods. Players also reported the task being effortful, mentally demanding, and temporally demanding (fast-paced). Performance in the simulation was recorded into log files unobtrusively--a key component of stealth assessment. Evidence extracted from these log files was used to identify different individual performance trajectories. That is, using derived variable analysis , data from log files were transformed into lap-level measurements (i.e., average speed and collisions for each of the five laps), which were then collapsed into a meaningful summary feature--the slope. An individual's slope indicated their rate of change throughout the simulated task. Using slope analysis, different trends were observed, including linear, quadratic, and cubic slopes in both positive and negative directions. The strength of the slopes (i.e., betas) also held important information. One research question we aimed to investigate was whether these slopes indicated different responses to stressors, which represents a holistic approach to the resilience process. We used MLA to build predictive models of resilience based on the simulationembedded metrics. These analytics are especially recommended when non-linear relationships with well measured predictors are being modeled. Both are the case in this research. For the simulation-embedded metrics, this indicated the stability of speed and collision tendencies of drivers within the simulation. Secondly, it appeared that a small proportion of participants demonstrated thriving, indicated by a positive linear slope for speed and a negative linear slope for collisions. Under stressful conditions, they were able to maintain emotional regulation and composure, and had no or minimal performance decrements. Another small percentage of people displayed recovery, shown by positive quadratic and cubic SMAPE, symmetric mean absolute percentage error. The best performing algorithm is bolded. slope for speed and negative quadratic and cubic slopes for collisions. These people recovered from an initial setback in performance and returned to their previous level of functioning. The majority of people showed survival, as they had no significant slope (e.g., plateau-like slope). They were able to withstand the stressors and showed adaptive behavior to maintain their performance level. This process of survival and minimal impact has been argued to be commonplace and arises from the basic, normative functions of human adaptation systems. Hence, it is not surprising that a large proportion of people showed the ability to maintain homeostasis. Finally, a small proportion of people showed trends indicative of succumbing. They were unable to adapt to the changing conditions or bounce back after initial poor performance, possibly indicating maladaptive reactivity. The findings have practical implications for simulationbased training to support resilience, particularly for those showing a trajectory of succumbing, or those who seek to improve their resilience in the face of adversity (;, for a review). Existing resilience training programs have shown promise in contexts such as defense, workplace, and medical (; for reviews). There is potential to employ the present simulationbased assessment methodology in conjunction with resiliency programs--a randomized controlled design with follow-up measurements can ascertain the effectiveness of such training programs on raising performance levels to a point of thriving. Few studies have investigated this approach of building adversity into resilience training programs by systematic exposure to realistic simulations. To illustrate, in studies by Arnetz et al. (2009) and McCraty and Atkinson (2012), first-responders participated in realistic simulation scenarios (e.g., high-speed car pursuits). Compared to a control condition, those in resilience training programs reported less psychophysiological stress and better performance in the simulation. Indeed, moderate exposure to adversity with appropriate challenges may help individuals develop resilience, particularly for future stressful situations. Thus, combining simulation-based assessment and training may be a promising paradigm to building resilience. Another research question we aimed to answer was how these individual slopes would relate to an existing self-report measure of resilience (CD-RISC), for purposes of validation. Using machine learning techniques, the individual response trajectories were predictive of CD-RISC scores with high accuracy, provides evidence of construct validity. Error rates were below 5% for the best performing algorithm, Bayesian Ridge regression; and, importantly, were similar for other algorithms used, attesting to the stability of the predictions. Each of the machine learning models used outperformed a baseline linear regression model also tested in terms of predictive validity. Hence, as recommended by Jacobucci and Grimm (2020), machine learning approaches are more sensitive to modeling non-linear relationships, which can complement traditional statistical analysis techniques. To demonstrate discriminant validity, driving experience and gaming intensity were also placed in the models as outcome variables. These variables were predicted with relatively high error rates (above 35%), implying that behaviors in the simulation was not necessarily sensitive to the reports of driving or gaming ability. Thus, we reiterate that it is the design and validation of the assessment methodology, and not the driving task, which was the focus of this research. No predictions about the actual driving skills and abilities can be made based on the simulation. The driving scenario was merely a convenient medium to demonstrate how ECD could be applied to assess different resilience trajectories in response to stressors. It is of course possible and needed for future research to develop and test other simulated tasks using a variety of mediums (e.g., flight simulators), embedding the methodology presented to demonstrate that the method of assessment is independent of the medium. These findings advance the way we construe resilience by demonstrating the dynamic process through which individuals adapt to stressors. The prediction of CD-RISC scores of trait resilience from the trajectories supports the role of stable individual differences in shaping one's adaptation to adverse events. However, the simulation goes beyond capturing these stable traits to assess real-time responses to stressors, i.e., the in-lab window into a process of resilience. We propose that capturing both is necessary to deepen our understanding of the psychological resilience construct. Challenges, Limitations, and Future Directions Despite holding promise as an alternative or supplementary assessment method, gamified assessments are still in its early stages. The present study is a step in the direction toward a \\\"nextgeneration\\\" of assessment. However, there is a need for further validation with other well-validated measures of resilience, as well as with measures of similar constructs in the nomological network (e.g., adaptability), and related real-world outcomes. This newly developed simulation methodology must also be tested and replicated across multiple samples and contexts to determine the generalizability of the findings. A significant challenge encountered in this study was the need for multidisciplinary expertise. For example, development of the simulation required software developers, game developers and 3D modelers; data management and analysis (particularly predictive modeling) required data scientists and programmers; and understanding and implementing the theory and constructs required psychologists and cognitive scientists. Not only must these personnel have expertise in their respective areas, but they must also develop their work output with consideration for other experts. For example, the software developer must program the simulation to output data logs which capture the target constructs as defined by the research psychologist. These data logs must also be suitably formatted for use by the data scientist. Another challenge relates to issues of accessibility and feasibility, including the need for specialized equipment (both software and hardware). The present simulation used Unity development platform, however there are many other game engines such as Bohemia Interactive Simulations, Unreal Engine, and Godot. Specialized hardware (e.g., driving equipment, virtual reality headsets and equipment) can also increase costs and the need for dedicated space. Since game- and simulationbased assessments are more expensive and can take greater time to develop and validate, researchers must weigh the costs and benefits about their needs and goals. Thus, it remains a future research direction to explore ways to create gamified assessment protocols that are accessible, accurate, and costeffective for both researchers and end-users, so their full benefit can be realized. Specific to driving and other motion-based simulations, the potential impact of simulation sickness on performance must be fully investigated to limit its severity. High levels of simulator sickness can affect performance by confounding data and influence participant dropout rates. Indicated by the simulator sickness questionnaire measured pre- and post-simulation, majority of the sample (83.3%) were not affected by the simulation. However, 7.8% of the sample reported a notable increase in symptoms. About 50% of the 16 symptoms showed significant changes, but only 25% of them were of notable effect size. Moreover, while five laps were sufficient to examine rate of change over time, the stability of the slope analysis could be strengthened by increasing the number of laps. However, longer exposures can produce more symptoms. Perhaps in future iterations, the lap length could be reduced and consequently, the number of laps could be increased, without increasing the duration of the task. Additional research is also needed to determine the optimal length of a single exposure. While we focused on speed and collision metrics, other indicators could be used to measure resilient responses. This could include, for instance, intentional lane-changing strategies (data points that collect the location of the vehicle), and maintaining emotional composure (i.e., modulating the level of control over one's responses to match environmental demands). Recording psychophysiological (e.g., skin conductance, heart rate variability) whilst completing the simulation may also give valuable information. Several studies have investigated indicators of physiological arousal whilst participants completed a stressful laboratory task (;, for a review). For instance, Hildebrandt et al. (2016) placed participants in a threatening and changing immersive virtual environment while recording skin conductance, and found self-rated resilience predicted arousal during the sustained experience of threat. Other studies have investigated regulation and recovery from stressors via startle responses  or matching emotional responses to changing stimuli. Employing biomarkers in conjunction with game- and simulation-based assessment protocols can act as an additional source of validation. It would be interesting to determine whether those with performance levels indicating thriving or recovery show better regulation of psychophysiological arousal (e.g., lower skin conductance)--this may provide evidence that resilient people can regulate and change their affective and physiological responses to match the demands of changing environmental circumstances. This study recruited university students in a low-stakes context. While the sample size was appropriate, a larger sample is recommended to replicate these results. The promising aspect of this research is the stability of predictions across different ML algorithms; thus we anticipate that these results will replicate on a larger sample. Also, whilst there are limits of generalizability due to the sample characteristics, the results still show promising utility of the simulation-based assessment. Future studies should examine specific samples where resilience is critical for success (e.g., elite athletes, defense personnel, business managers). On a related note, the incremental and criterion-related validity of this methodology is yet to be established, above and beyond existing measures of resilience. Iterative validation of game- and simulation-based assessments includes determining their utility in predicting real-world outcomes (and being implemented in high-stakes environments;). These outcomes could be subjective or objective, for instance, attrition rates and posttraumatic stress trajectories in military personnel ; game performance consistency and injury rehabilitation in competitive athletes ; or job performance and burnout in employees such as healthcare professionals. Finally, Machine Learning approaches typically require large sample sizes to train the data. Future research is recommended to obtain larger samples to strengthen model predictions. CONCLUSION This work is contributing to the growing literature on gamified psychometrics, and to the theory of mental resilience, integrating the process model of resilience to its measurement. Gameand simulation-based assessment is a nascent research area, with promising progress being made toward their theory, design, validation, and implementation for end-users in various contexts (e.g., education, defense, organizational). Well-designed games and simulations provide opportunities to assess \\\"hard-tomeasure\\\" constructs, particularly those regarded as twenty-first century skills ; not to replace, but to supplement traditional measures and methods. Data can be collected continuously and unobtrusively (stealth assessment), providing a rich bank of information about individuals' skills, abilities, and attributes. However, iterative, and rigorous validation is necessary for the utility of gamified assessments to be fully achieved. We look forward to the continued investigation of gamified methods that may change how we think about assessment.\",\"1135745966\":\"INTRODUCTION Nowadays, multimedia learning environment, learning management system, intelligent tutoring system, and massive open online course (MOOCs) provide great opportunities to generate big data in education. Researchers from various disciplines have conducted many interesting studies in the fields of educational data mining and learning analytics. Most researchers paid much attention to analyze student data that were generated from different kinds of learning platforms. It helps to address personal learning demands of students and improve the quality of individualized learning. However, teaching is an important part of education as well. If the data of instructors in various teaching platforms can be fully applied, the educational data mining can provide instructors with service and further benefit students. Among various learning platforms, MOOCs has obviously become a popular way to learn for many students around the world. MOOCs provide students with opportunities to a personalized learning environment  and enables them participate in the cooperative learning through the discussion forum and peer evaluation. Many scholars have conducted studies about MOOCs from the perspective of the characteristics of the learners, learning effect, and course design , but few scholars analyzed the teaching complexity and the instructors in MOOCs. Teaching in traditional classes is different from the teaching in MOOCs in many aspects, such as the size of class, prior knowledge of students, and the expense of the course. Nowadays, most courses in MOOC platforms are xMOOCs, which are based on the traditional instructiondriven principle. Course lecture (i.e., course videos) is still the key component of the course. Hence, analyzing the lectures of instructors of xMOOCs would be helpful to evaluate the course quality and provide feedback to MOOCs instructors, which will further benefit learning of the students. One straightforward way is to describe large-scale MOOC lectures through natural language processing. For example, what semantic characteristics do these MOOC lectures have? Does any potential and valuable pattern exist among these semantic characteristics? Do these potential patterns associate with the learning of students? Here we define these semantic patterns that emerged from MOOC lectures as the \\\"lecture style\\\" of the current study. Specifically, the operational definition of lecture style is as follows: the results of cluster analysis based on the semantic features of a given MOOC video (for more details, see section Data Analysis). When it comes to the quality of MOOCs, researchers have summarized some evaluation systems. For example, Yousef et al. (2014) conducted a large-scale survey of the learners and instructors who have the experience of MOOCs and summarized an evaluation standard of MOOCs. They found that the lectures of instructors play a vital role in the quality of MOOCs. Quality matters rubric is also a widely used evaluation rubric of online courses. This rubric makes raters mark the courses from the eight dimensions of learning objectives, namely, interactivity, usability, etc. . Integrating with the survey investigation and focus groups interview, Poce (2015) evaluates MOOC through the clarity and comprehensibility of the lecture, course design quality, etc. In the evaluation of traditional classes, the classroom instruction or course videos were often evaluated by the trained observers or experts using the mature rubric (;, which is complicated and cannot avoid the subjectivity in questionnaire investigation. To address this issue, some people tried to use natural language processing to evaluate the lectures instructors of math classes. They extracted the semantic features from the lectures of the instructors and established several classifiers to automatically predict whether a specific category ofmath content (e.g., factions) or teacher practice (e.g., reasoning or immediate feedback) was covered by instructors. The results of the classifiers were compared with the experts who were invited to rate the course videos of math classes. They found that the agreements between classifiers and the raters were satisfactory. This may be a new method to evaluate the course quality. It inspires us to evaluate the lectures of the instructors in MOOCs by using natural language processing, and explore the effects of different lecture styles on the learning of students. In their study, the linguistic inquiry and word count (LIWC) was used to count word categories related tomathematics content and teacher practice. With the assumption of the words people use in daily life reflect who they are and the social relationships they are in, Pennebaker et al. (2001) developed LIWC, which mainly focus on analyzing the language people use from the perspective of word frequency. Psychologists have conducted many studies in different fields by using LIWC. For example, Rude et al. (2004) found participants who are experiencing physical and emotional pain tend to have their attention drawn to themselves and subsequently use more firstperson singular pronouns. Gunsch et al. (2000) found that more self-references (e.g., \\\"I\\\") were present in positive political advertisements compared with mixed and negative political advertisements, whereas more other-references (e.g., \\\"she\\\") were present in negative advertisements compared with positive and mixed advertisements. Researchers also applied LIWC in the field of education; Pennebaker et al. (2014) analyzed more than 50,000 essays from 25,000 students and found that word use was related to the grades of students over all 4 years of college. Robinson et al. (2013) tested whether differences in the use of linguistic categories in written self-introductions at the start of the semester predicted final course performance at the end of the semester, and the results supported their hypothesis. Based on these empirical studies, it is reasonable to use LIWC to analyze the different language use of the lectures of instructors in MOOCs. Although LIWC is a powerful transparent text analysis program that counts words in psychologically meaningful categories, deeper discourse characteristics are still needed to analyze the lectures in MOOCs. Researchers in the field of discourse analysis proposed a multilevel theoretical framework for discourse processing. They identified six levels from the shallower to the deeper, including words, syntax, explicit textbase, situation model, discourse genre and rhetorical structure, and pragmatic communication. Our study relates at least to the first three levels of this theoretical framework. The first two levels (i.e., words and syntax) were addressed by LIWC. The third level in our study is textbase, which contains explicit ideas in the text that preserve the meaning. The basic units of meaning in the textbase is proposition. Proposition includes a predicate and one or more arguments. Cohesion is considered an important theoretical construct that measures the overlap between propositions in the textbase. It provides linguistic clues to make connections between an adjacent pair of sentences. Higher level of cohesion in text has been found to facilitate comprehension for many readers  and is particularly important to low-knowledge readers. When there is a lack of cohesion, an idea, relationship, or event must often be inferred by the leaner. Learners with low prior knowledge lack sufficient ability generate the inferences needed to meaningfully connect constituents in low cohesion texts. Cohesion is important to the lectures in MOOCs as well. Just like reading comprehension, a lecture with greater cohesion may help students to connect the discourse constituents and construct coherent meanings. In fact, the coherence assumption was one of the central theoretical constructs in the constructivist theory of discourse comprehension. They assumed that students routinely try to construct coherent meanings and connections among text\\/discourse constituents unless the text\\/discourse is poorly organized. Therefore, cohesion is an essential discourse feature in the present study. CohMetrix will be used to extract the cohesion of the lectures in MOOCs , and one of its central purposes is to examine the role of cohesion in distinguishing text types and in predicting text difficulty. Many studies have suggested that Coh-Metrix can be used to detect subtle differences in text and discourse , and it has been widely applied in the studies of education. For example, the previous study has demonstrated that the increase in cohesion can help the students with low prior knowledge to understand the meaning of texts , but the increase in cohesion does not work for the students with higher prior knowledge. As a matter of fact, students with higher knowledge can benefit from low cohesion texts because they were forced to fill in the conceptual gaps in the texts and they have sufficient knowledge to do that. On the basis mentioned above, we proposed three research questions for the current study, which are as follows: (1) Can the lectures styles of MOOC instructors be portrayed by using natural language processing? (2) What are the semantic characteristics of different lecture styles in different discipline? (3) How are the lecture styles of MOOC instructors in different disciplines associated with learning engagement (e.g., discussion posts and notes taking) and course satisfaction? To address these questions, we collected 129 course transcripts from Coursera and edX (including humanities, social science, and science), and extracted the semantic features of the lectures the instructors in MOOCs by using LIWC and Coh-Metrix. Then, cluster analysis was used to detect different lecture styles of MOOC instructors. Finally, we used ANOVA to explore the effects of different lecture styles on the learning engagement of students and perception of the course, including the number of discussion posts, notes taken, and overall course satisfaction. METHOD Data Collection The datasets in the current study are course-level data, which consist of two parts, namely, text data and student data. The first part of the data was collected from the two major MOOC platforms (i.e., Coursera and edX). Convenience sampling was conducted to collect a total of 129 course transcripts (in English), and each transcript includes all sessions of MOOC. These courses cover three disciplines (humanities: 24.8%, social science: 38%, science: 37.2%), and the proportion of different discipline is relatively uniform. The average number of words per course is around 100,000 words, which ensures the robustness of the analysis results. The second part of the data (i.e., student data) was collected from MOOC College of Guokr.com, one of the largest MOOC learning communities inMainlandChina. This community offers online learners a platform where they can voluntarily evaluate MOOCs and share their opinions with fellow online learners. The community also provides various learning assistance tools, including a service for learners to take notes while taking a MOOC, as well as study groups and discussion boards for individual MOOCs. We collected the student data of the 129 courses. The student data refer to the ratings and learning engagement of the student (i.e., the number of notes taken per course and the number asynchronous discussion posts per course). Student ratings involves four dimensions, which are as follows: the amount of knowledge gained, teacher participation, interest, and curriculum design. The items include \\\"Is the course substantial and valuable?\\\" (The amount of knowledge), \\\"Does the teacher participate in communication or interaction?\\\" (Teacher participation), \\\"Is the course interesting and attractive?\\\" (Interestingness), and \\\"Is the structure of the curriculum reasonable and sufficient?\\\" (Curriculum design). A 10-point Likert scale was used, and the average of these four ratings was calculated to indicate overall course satisfaction. Extracting Semantic Features Linguistic inquiry and word count 2015 and Coh-Metrix were used to extract semantic features from the course transcripts of instructors in 129 MOOCs. LIWC provides texts summary information (e.g., text length, sentence length, analysis style, etc.), function words (e.g., pronouns, articles, prepositions, etc.), cognitive processes (e.g., see, hear, and feel), emotional words (e.g., positive emotions, anger, anxiety, and sadness), biological processing (e.g., body, health, sex, etc.), drive (e.g., power, affiliation, etc.), grammatical features (e.g., verbs, adjectives, quantifiers, etc.), and informal words as the first-class semantic indices. Each first-class semantic index involves several secondclass and third-class indices. To test the cohesion of the lectures, Coh-Metrix was chosen as a supplement to LIWC. We chose referential cohesion as the index of cohesion in the present study. It refers to the degree to which there is an overlap or a repetition of words or concepts across sentences, paragraphs, or the entire text. Referential cohesion was widely investigated in the psychological studies of discourse processing. Previous studies have found that lexical sophistication, syntactic complexity, and cohesion were related to the quality of writing. Pronouns, emotional words, and other indices of LIWC were also found to be important in the psychological studies of discourse processing.More importantly, these semantic features could be mapped to the multiple levels of the theoretical framework of discourse analysis by Graesser et al. (2011). Thus, the following semantic features were extracted in the present study: 1. Self-reference (i.e., I, me, my, and we); 2. Emotional words, including positive emotions and negative emotions (i.e., anger, anxiety, and sadness); 3. Sentence length (the number of words contained in each sentence); 4. Cognitive words (including causality, comparison, certainty, insight, and other dimensions); 5. Big words (words with more than six letters are considered as complex words or big words in English); 6. Tone (a high number is associated with a more positive, upbeat style; a low number reveals greater anxiety, sadness, or hostility); 7. Cohesion (i.e., coreference cohesion local, the proportion of adjacent sentence pairs in the text that shares a common noun argument). Data Analysis As Coh-Metrix can only analyze the texts with a length <10,000 words, the transcript of each course was sliced into several fragments with a length of 8,000-9,000 words. Then we aggregated the semantic indexes of all fragments. All data preprocessing was completed in R 3.4.3 and Microsoft Excel. Cluster analysis (cluster package, https:\\/\\/cran.r-project.org\\/ web\\/packages\\/cluster\\/index.html) was conducted on the selected seven semantic indices to portray the lecture styles of MOOC instructors in different disciplines. We transformed all the semantic features into Z-score to avoid the effect of a different variable scale. Then we performed k-means algorithm with Euclidean distance. The k value was assigned with a value from 1 to 15. Due to the sensitivity of choosing the initial center points in the clustering method, 25 initial center points were set for the configuration. Subsequently, ANOVA was conducted to explore the effects of different clusters (i.e., lecture styles) within different disciplines on the number of asynchronous discussion posts, notes taken, and course satisfaction. These results would help us to understand how different lecture styles in MOOCs influence the students learning and the perception of the courses. RESULTS Descriptive statistics of student ratings and semantic features between the three disciplines have been conducted. Please see the results in Appendix. Here, we mainly focus on the results of cluster analysis and inferential statistics. Results of K-Mean Cluster Analysis In all the disciplines (humanities: 38 courses, social science: 49 courses, science: 48 courses), the within sum of squares showed a significant downward trend when the number of clusters changed from one to three, and this decreased trend became slighter when the number cluster changed from four to 15 (see Figure 1). It suggested that three clusters would fit the data well in the present study. Then we conducted three K-mean cluster analysis within different disciplines (i.e., each discipline has three clusters). For humanities, 14, 10, and 8 courses were classified as Cluster A to Cluster C, respectively. For social science, 13, 14, and 22 courses were classified as Cluster D to Cluster F, respectively. As for the science, 10, 13, and 25 courses were included from Cluster G to Cluster I. Figure 2 presented a cluster amount of three classes within different disciplines. The datasets were reduced to two components (i.e., X-axis and -axis in Figure 1) by using principal component analysis. Except that Cluster E and Cluster F only have a small fraction of overlap in social science, the results of clustering were acceptable in general. We calculated the cluster means (the mean value of z-score for each text feature) for different disciplines. Figure 3 showed those text features of different clusters in each discipline. In humanities, the most obvious characteristic of Cluster A was that the values of affect and cognitive words were higher than the rest of the clusters, and the score of self-reference was large as well. Cluster B had the highest score of cohesion and self-reference, and the lowest score of sentence length and big words. The scores of self-reference, tone, and cognitive processing of Cluster C were the lowest among all the clusters, whereas, the score of sentence length and big words were the largest, suggesting that those instructors whowere categorized into Cluster C prefer to use long sentence, complex words, and negative tone when they deliver a speech. In social science and science, both Cluster F and Cluster H have the highest sentence length and big words, and lowest tone, cohesion, and self-reference. In fact, they were similar to Cluster C. Cluster D, Cluster G, and Cluster B were similar as well, considering they all have the highest value of cohesion, selfreference, less big words and short sentence length. As for the Cluster E and Cluster I, the scores of the text features basically surrounded the mean values. In addition, the result of Pearson correlation analysis showed that both sentence length and big words have significant negative correlation with self-reference (r=-0.40, p< 0.001; r=-0.61, p < 0.001); tone and cohesion have significant positive correlations with self-reference (r = 0.33, p < 0.001; r = 0.32, p < 0.001); and emotional words and cognitive words were not correlated with self-reference (r = 0.06, p= 0.48; r = 0.17, p= 0.06). Naming for the Clusters In order to present the process of naming clusters intuitively, we selected two courses from Cluster G and Cluster C as the examples. The first course belongs to Cluster G. The second course belongs to Cluster C. Figure 4 presents the beginning of the two courses. It can be seen clearly that the self-reference of the first course (at the left hand side) was low, and there were three long sentences and many complex vocabularies at the beginning of the course; whereas the second course (at the right hand side) used many self-reference words, and the sentences in this course were easy to understand. The comments from the students of two courses were also consistent with data analysis results. The following presents some of these comments: Student A: \\\"Some instructors in this course are serious, some are cute, and the most impressive lesson is an instructor with curly hair took us to local movie studio. The majority of the time we just watched teachers read their slides.\\\" (Cluster C) Student B: \\\". . . the lecture of professor is really old-fashioned, but I think it is funny to some extents. . . \\\" (Cluster C) Student C: \\\"I feel this course may only suitable for small crowd of people, especially for the artistic youth. Although the content of course is really abundant, the lecture style of teachers is too monotonous, which makes us easy to fall asleep during watching those videos.\\\" (Cluster C) Student D: \\\"The instructor has a lovely English accent, and the curriculum is reasonable. It is friendly for the beginners.\\\" (Cluster G) Student E: \\\"The content is not boring, and it is easy to understand, the instructor is interesting as well. The length of the course video is suitable.\\\" (Cluster G) Student F: \\\"The instructor is approachable and humorous, I like him very much!\\\" (Cluster G) On the basis of cluster analysis, course video reviews, and student comments, we named the four clusters in the present study as follows: perfect (Cluster A), balanced (Cluster E and I), communicative (Cluster B, D, and G), and serious (Cluster C, F, and H). The Results of ANOVA An ANOVA was conducted with asynchronous discussion, notes taken, and overall course satisfaction as dependent variable, and the lecture styles within each discipline as independent variables and course popularity (i.e., the number of learners who followed the course) as the covariates. Table 1 presented the results of ANOVA and post-hoc test. The results of ANOVA showed that there were significant differences of asynchronous discussion (F = 11.32, p = 0.002, e2 = 0.28) and notes (F = 11.61, p = 0.000, e 2 = 0.30) for humanities among the three lecture styles. Only significant difference of notes (F = 22.13, p = 0.000, e2 = 0.20) between the different lecture styles was found in social science. As for the science, significant differences of notes (F = 5.42, p = 0.008, e2 = 0.20), discussion (F= 4.50, p= 0.016, e2 = 0.13), and course satisfaction (F = 3.59, p = 0.035, e2 = 0.13) between the three lecture styles were found. Furthermore, we conducted posthoc analysis by using TukeyHSD test, and the results of post-hoc test were presented in Table 2. DISCUSSION In summary, the present study extracted the semantic features of 129 MOOC transcripts and found four lecture styles (i.e., perfect, communicative, serious, and balanced). Specifically, \\\"perfect\\\" (Cluster A), \\\"communicative\\\" (Cluster B), and \\\"serious\\\" (Cluster C) lecture styles were found in humanities. As for the social science and science, three lecture styles emerged from these courses, namely, \\\"communicative\\\" (Cluster D and G), \\\"balanced\\\" (Cluster E and I), and \\\"serious\\\" (Cluster F and H). Then we collected student rating data from one of the largest MOOC learning communities inMainland China and attempted to figure out how these lecture styles influence the learning of students. The results of ANOVA and post-hoc analysis indicated that learning engagement and course satisfaction were significantly different between different lecture styles within each discipline. Different Lecture Styles in MOOCs The results of cluster analysis suggested that it is possible to portray MOOC instructors by using natural language processing, which answered research question 1 in the present study. There was almost no overlapping fraction in Figure 3 in any discipline, indicating the results of clustering were quite acceptable. The four types of lecture styles of MOOCs had distinctive characteristics. Similar to our results, recent studies have revealed that linguistic characteristics of texts vary across different genres and academic disciplines. The most significant difference between the \\\"perfect\\\" presentation style and other styles in our study were the number of emotional words and cognitive words used by instructors. The Z scores of emotional words and cognitive words of \\\"perfect\\\" style were larger than 1, whereas those of the other types were <0.5. The usage of more cognitive words represents more cognitive processing (including causality, comparison, certainty, and insight) in teaching, which may benefit student learning. For example, researchers found learners read text more quickly when two-clause sentences are connected with a causal word\\/phrase compared with text in which a connective is neutral. Atapattu and Falkner (2018) suggested that the causal connectives in the academic discourse might improve discourse processing of the learners. Previous studies also found a moderate correlation between the cognitive activation in classroom instruction and the learning achievement. Meanwhile, higher-order thinking and understanding are dependent on a high quality of cognitive learning activities in teaching. High use of emotional words in the \\\"perfect\\\" lecture style represents an emotional speech to some extent. Researchers have found that positive emotions favor the activation of cognitive resources, which fosters task-related learning processes  and metacognition. These evidences explain why we named this type of lecture as \\\"perfect.\\\" As for the other lecture styles, the instructors who have \\\"serious\\\" style rarely used self-reference words, probably because they only focused on the presentation of course materials and relatively ignored the existence of students when they delivered their speech in MOOCs. For example, they barely introduced themselves in their speech, and rarely used \\\"we\\\" to establish potential connection with students. Since we have found a positive correlation between use of first-person and cohesion, it is not surprising that the cohesion of \\\"serious\\\" style was almost the lowest. Two of the most notable features of \\\"serious\\\" courses were complex words and long sentence, and we found significant negative correlations between the two features and self-reference. Instructors who have \\\"serious\\\" lecture style probably prefer to use written language in their lecture (lowest cohesion, lowest selfreference, most big words, and many long sentences) rather than oral language. Also, the score of big words and sentence length were almost the largest in any discipline, suggesting that the effect of the number of terminologies in different disciplines on lecture style was not as important as we thought. Previous study has found that themost commonmistake instructors make is the lack of engagement during the teaching, which will make the lecture of the instructors become tedious and students will find it hard to concentrate on the lecture content. Instructors who have \\\"serious\\\" lecture styles may probably not be able to engage students during their teaching. For example, the use of \\\"we\\\" and \\\"us\\\" suggests social interaction, which helps the students sense that they are part of a class when engaging with the MOOC video. \\\"Serious\\\" instructors rarely use these self-reference words in their lecture video. However, the data in the present study could not verify this hypothesis directly, and further empirical studies are still needed to compare the engagement of instructors between different lecture styles. \\\"Communicative\\\" lecture style (i.e., Cluster B, D, and G) almost had the opposite semantic features when compared to \\\"serious\\\" style. Self-reference and cohesion of the \\\"communicative\\\" style were higher than other styles, whereas the scores of big words and the words in per sentence were relatively low, indicating that the speeches of these instructors may be quite colloquial. According to the comments of students, we found communicative instructors were often welcomed by students. Perhaps communicative lecture style conveys more enthusiasm. A study conducted by Guo et al. (2014) found learners engaged more with the course when instructor was speaking fast, which is similar to the communicative instructors in the present study. The researchers speculated that the fast-speaking instructors convey more energy and enthusiasm. \\\"Serious\\\" lecture style did not have obvious characteristics when compared with other styles. The course satisfaction of Cluster G (communicative) was significantly higher than that of Cluster I (balanced) and H (serious) in science. In general, all the semantic features of \\\"balanced\\\" lecture style (i.e., Cluster E and I) were located around the average level, which means that this lecture style probably does not have salient characteristics. Impact of Lecture Style on Course Satisfaction, Discussion, and Notes The differences of discussion and notes between the four different lecture styles were not significant in our initial study , because of the neglect of discipline. The present study addressed this issue and found that different lecture styles had distinct semantic features, and they also had significant effects on the overall course satisfaction. In both humanities and science, instructors with \\\"communicative\\\" styles were more satisfied than the others. These instructors had higher level of self-reference, cohesion, and tone, which makes them to be perceived as amiable teachers (according to the comments of students). However, the \\\"balanced\\\" lecture style was evaluated as more satisfactory than \\\"communicative\\\" and \\\"serious\\\" styles in social science. This is probably because of the lower level of affect words, cognitive words, and tone of the \\\"communicative\\\" lecture style in social science, whereas the \\\"balanced\\\" style had higher level of affect words, cognitive words, tone, and less complex words and long sentences. As for the learning engagement, Guokr MOOC community provides many learning tools, including a function for learners to take notes while taking a MOOC, as well as study groups and discussion boards for individual MOOCs. Many students who are not proficient in English prefer to participate in discussion in this community because they may obtain language support from the discussion forum. Therefore, the number of discussion posts and notes taken by students in each course were viewed as indices of learning engagement. Previous study has found that teacher-student interaction has a positive effect on student learning in terms of perceivedmotivational and cognitive learning quality of the student. Similar to their study, we found that the number of discussion posts for the \\\"communicative\\\" lecture style in humanities was significantly larger than \\\"serious\\\" lecture style. Since instructors with \\\"communicative\\\" style weremore likely to use oral language, they probably paidmore attention to teacher-student interaction, which triggered more discussion. The \\\"balanced\\\" lecture style in science yielded more discussion posts than the others, but there was no significant difference of discussion posts between the three lecture styles in social science. It seems like the \\\"balanced\\\" lecture style only works for science probably because the \\\"balanced\\\" lecture style in science was more likely to trigger the cognitive processing of students, considering it yielded more notes taken than the other styles. The major MOOC platforms did not provide note-taking function for MOOC learners, and many Chinese students would take notes directly on Guokr MOOC community. Notes taken reflected the cognitive processing of course content. Researchers have found that note-taking activities benefit students in exercising their self-regulated learning skills, which is an important cognitive activity in learning. Also, the benefits of note-taking activity include development of higher-order thinking skills , and improvement of the concentration of students. We found significant differences for the number of notes taken among different lecture styles. Specifically, the number of notes taken in Cluster A (perfect) was significantly higher than that of Cluster C (serious) style in humanities. Consistent with the result of cluster analysis, the \\\"perfect\\\" lecture style yielded a higher level of cognitive processing (i.e., notes taking), which may help students in their learning process and successfully increase their learning achievement. Interestingly, the notes of Cluster F (serious) was significantly more than Cluster E (balanced) and D (communicative) in social science. Even though the \\\"serious\\\" lecture style was perceived as tedious or verbose (according to the comments of students), it still yielded the higher level of cognitive processing than the other lecture styles in social science. Also, the notes of Cluster I (balanced) was significantly more than Cluster G (communicative) in science. Although the cohesion of the \\\"serious\\\" and the \\\"balanced\\\" lecture style was low, higher knowledge learners can benefit from low cohesion, because lower cohesion forces them to generate inferences to fill in the conceptual gaps. Furthermore, the \\\"serious\\\" lecture style MOOCs was not appreciated by the comments of students, but the other sides of the courses (e.g., reasonable curriculum design, abundant course materials, effective course assignments) might affect cognitive processing of the students as well. Limitations and Future Directions Some limitations of the current study should be noted. First, we did not acquire the permission to obtain the academic performance data, specific information about student profile, and other detailed data about learning engagement (e.g., the finegrained log data) for the 129 MOOCs. We can only use the public data from a third-party MOOC community to explore the influence of different lecture styles on course satisfaction, discussion, and notes taken. Thus, it is hard to draw the conclusions in regard to the impacts of lecture-styles. Second, as Coh-Metrix can only analyze a small text (i.e., <10,000 words), we had to slice the course transcript into several pieces and then aggregate the results of all slices. This process was timeconsuming, causing our sample size (i.e., 129 courses) to be relatively small. Third, we only selected seven semantic features from over 200 features to portray MOOC instructors according to previous studies and our own teaching experience. This proceduremay cause information loss; perhaps automatic feature selection is a good choice as well. Fourth, the student rating data was obtained from Chinese learners. Non-native English speakers may not have good enough language skills to evaluate English MOOCs. Future studies should acquire more detailed data about student learning in MOOCs, especially the fine-grained log data about learning progress of the students. It will allow researchers to explore the longitudinal effects of different lecture styles on the learning (e.g., engagement, affect, performance, and self-regulated learning) of the students. It is necessary to consider the effect of moderators (e.g., demographics and teaching experience) as well, especially on how teachers with different experiences moderate the effect of lecture styles on the learning of students. In addition, tracing the changes of semantic features of new teachers and providing feedback to their lecture might be helpful to improve their presentation skills. According to the current study, it seems like a good lecture should be emotional and rational. However, it may be difficult for instructors to give emotional lectures to the camera without immediate feedback from students. Moreover, simply encouraging instructors to use more emotional and cognitive words in the lecture may make instructors feel confused and have no operability. Thus, how to use the results of natural language processing to improve lectures of instructors in MOOCs is worth exploring in the future. Finally, the analysis of MOOC lecture style can also be extended to traditional classes. It might help to improve the quality of traditional courses by analyzing the presentation recording, course video, and standardized test. CONCLUSIONS The results of the current study have provided answers to our three research questions. First, the lecture styles of MOOC instructors can be well-identified by natural language processing. We found that four different lecture styles emerged from 129 MOOCs, which are as follows: \\\"perfect,\\\" \\\"communicative,\\\" \\\"balanced,\\\" and \\\"serious.\\\" Second, each lecture style in different disciplines has its unique semantic characteristics. Third, the lecture styles of MOOC instructors have significant effects on learning engagement and overall course satisfaction. However, it should be noted that it is not feasible to judge which lecture style is the best or the worst without considering the instructional contexts (e.g., discipline). And more importantly, the present study only provides initial evidence with certain drawbacks.\",\"1135745972\":\"INTRODUCTION Are there any observable behaviors or cues that can differentiate lying from truth-telling? Almost all researchers in the field of deception detection agree that there is no \\\"Pinocchio's nose\\\" that can serve as an easy indicator of deception. Nevertheless, many researchers are still trying to find cues to deception. The \\\"leakage theory\\\" asserts that high-stake lies (the rewards come with serious consequences or there can be severe punishments) can result in \\\"leakage\\\" of the deception into physiological changes or behaviors (especially microexpressions that last for 1\\/25 to 1\\/5 s;). Specifically, from the perspective of leakage theory (;,b), observable emotional facial expressions (microexpressions and macroexpressions) can, to some degree, determine who is lying and who is telling the truth (It is a probability problem. However, debate exists for this possibility. While some researchers  argued that emotional facial microexpression could be a cue to lies supported their claims by empirical evidence, Burgoon (2018) argued that detecting microexpressions is not the best way of catching liars. Furthermore, Vrij et al. (2019) even categorized microexpression into pseudoscience. Even if it can be difficult, or even impossible for human beings to detect liars based on microexpressions, there do exist some behavioral cues that can, to some degree, differentiate lying from truth-telling. Specially, pupil dilation and pitch are shown to be closely related to lying. Most of the deception researchers agree that lying involves processes or factors such as arousal and felt emotion. Therefore, emotional facial expressions can be valid behavioral cues to deception. Meanwhile, there are involuntary aspects of emotional expression. As noted by Darwin, some actions of facial muscles were the most difficult to be voluntarily controlled and were the hardest to be inhibited (the so-called Inhibition Hypothesis. When a strongly felt genuine emotion is present, the related facial expressions cannot be suppressed. Hurley and Frank (2011) provided evidence for Darwin's hypothesis and found that deceivers could not control some particular elements of their facial expression, such as eyebrow movements. The liars would feel fear, duping delight, disgust, or appear tense while lying, and would attempt to suppress these emotions by neutralizing, masking, or simulating. However, the liars could not inhibit them completely and the felt emotion would be \\\"leaked\\\" out in the form of microexpressions, especially under high-stake situations. The claim of emotional leakage is supported by some recent research. When liars fake an unfelt emotional facial expression, or neutralize a felt emotion, at least one inconsistent expression would leak and appear transiently. ten Brinke and Porter (2012) showed that liars would present unsuccessful emotional masking and certain leaked facial expressions (e.g., \\\"the presence of a smirk\\\"). In addition, they found that false remorse was associated with (involuntary and inconsistent) facial expressions of happiness and disgust. In addition to the support for emotional leakage, research also shows that leaked emotions can differentiate lies and truthtelling. Wright Whelan et al. (2014) considered a few cues that had successfully told liars and truth-tellers, including gaze aversion and head shakes. They combined the information from each cue to classify individual cases and achieved an accuracy rate as high as 78%. Meanwhile, Wright Whelan et al. (2015) found non-police and police observers could reach an accuracy of 68 and 72%, respectively, when required to detect deception in high-stake, real-life situation. Matsumoto and Hwang (2018) found that facial expressions of negative emotions that occurred for <0.40 and 0.50 s could differentiate truth-tellers and liars. These studies all suggested that leaked facial expressions could help human beings detect liars successfully. Besides human research, attempts have also been made to use machine learning to automatically detect deception by utilizing leaked emotions. A meta-analysis by Bond and DePaulo (2006) showed that human observers only achieved a slightlybetter-than-chance accuracy when detecting liars. Compared to humans, some previous works with machine learning used the so-called reliable facial expressions (or involuntary facial expressions) to automatically detect deceit and achieved an accuracy above 70% . Given that the subtle differences of emotional facial expressions may not be detected by naive human observers, computer vision may capture the different and subtle features between lying and truth-telling situations that cannot be perceived by a human being. Su and Levine (2016) found that emotional facial expressions (including microexpressions) could be effective cues for machine learning to detect high-stake lies, in which the accuracy was much higher than those reported in previous studies. They found some Action Units (AU, the contraction or relaxation of one or more muscles , such as AU1, AU2, AU4, AU, AU, and AU (blink), could be potential indicators for distinguishing liars from truth-tellers in high-stake situations. Bartlett et al. (2014) showed that computer vision could differentiate deceptive pain facial signals from genuine pain facial signals at 85% accuracy. Barathi (2016) developed a system that detected a liar based on facial microexpressions, body language, and speech analysis. They found that the efficiency of the facial microexpression detector was 82%. Similarly, the automated deception detection system developed by Wu et al. (2018) showed that predictions of microexpressions could be used as features for deception detection, and the system obtained an area under the precision-recall curve (AUC) of 0.877 while using various classifiers. The leakage theory of deception predicts that when lying, especially in high-stake situations, people would be afraid of their lies being detected and therefore result in fear emotions. These fear emotions could then leak and have the potential to be detected. Meanwhile, it is presumed that if the fear associated with deception is leaked, the duration of the leaked fear would be shorter due to the nature of leaking and repressing (which would be presented as fleeting fear microexpressions). Some may argue that the fear emotions may also appear in truthtelling. It can be true. Nevertheless, for a truth-teller, the fear of being wrongly treated as a liar would be less leaking, since a truth-teller does not need to try hard to repress the fear as liars do. As a result, the degree of repressing will be different between liars and truth-tellers. On average, the duration of fear (or AUs of fear) in lying situations would be shorter than that in truth-telling situations due to the harder repressing in the former ones. Stakes may play a vital role while using an emotional facial expression as a cue to detect deception. Participants experience fewer emotions or less cognitive load in laboratory studies. Almost all laboratory experiments are typical of low stakes and are not sufficiently motivating to trigger emotions giving rise to leakage (in the form ofmicroexpressions). Consequently, liars in laboratory experiments are not as nervous as in real-life high-stake situations, with no or little emotion leakage. As noted by Vrij (2004), some laboratory-based studies in which the stakes were manipulated showed that high-stake lies were easier to detect than low-stake ones. Frank and Ekman (1997) stated that \\\"the presence of high stakes is central to liars feeling strong emotion when lying.\\\" Therefore, lying in high-stake situations would be more detectable by using emotional facial expression cues, and leaked emotional facial expressions would mostly occur in a high-stake context. Hartwig and Bond (2014) had an opposite opinion and argued that even in high-stake situations, it could still be difficult to tell liars from truth-tellers. They claimed that the context of the high stake would influence both liars and truth-tellers, as liars and truth-tellers might experience similar psychological processes. In other words, high-stake situations would cause inconsistent emotional expressions, like fear, not only in liars, but also in truth-tellers. This claim is true to some degree , but high stakes do not necessarily eliminate all the differences between liars and truth-tellers. Even though highstake situations increase pressure on both liars and truth-tellers, it can be assumed that the degree of increment would be different, and liars would feel much higher pressure than truth-tellers under high stakes. In addition, fabricating a lie requires liars to think more and therefore would cause a higher emotional arousal in them than in truth-tellers. Consequently, for liars, the frequency or probability of leaking an inconsistent emotional expression (say, fear) would be higher and thus easier to detect. In theory, the higher the stakes are, the more likely cues associated with deception (e.g., fear) are leaked, and the easier the liars could be identified using these cues. Besides duration, other dynamic features  could also vary in genuine and fake facial expressions, such as symmetry. Ekman et al. (1981) manually analyzed the facial asymmetry by using the Facial Action Coding System (FACS) and showed that genuine smiles have more symmetry when compared to a deliberate smile. Similarly, the leaked emotional facial expressions of fear while lying and the less leaked ones when telling a truth may also show different degrees of symmetry. However, the approach Ekman et al. (1981) used could be time-consuming and subjective. Thus, in the current study, we proposed a method that used coherence (a measure of the correlation between two signals\\/variables) to measure the asymmetry. The more symmetrical the facial movements of the left and right face, the higher the coefficient of correlation between them. Consequently, the value of coherence (ranges from 0 to 1) can be a measurement of asymmetry or symmetry. Based on the leakage theory and previous evidence, we hypothesize that (1) emotional facial expressions of fear (fear of being caught) can differentiate lying from truth-telling in high-stake situations; (2) the duration of AUs of fear in lying would be shorter than that in truth-telling; (3) the symmetry of facial movements will be different, as facial movements in lying situations will be more asymmetrical (due to the nature of repressing and leaking). METHODS The Database The database we used were 32 video clips of 16 individuals telling lies in half of them and truth in the other half. All of the video clips were recorded in a high-stake game show. The reason we used the current design was that cues to detect deception could differ from person to person, and what spotted one liar was usually different from the signals that revealed the next liar. Consequently, cues may vary from sender to sender. The same person, however, would display almost the same facial expression pattern on different occasions. Therefore, the relatively ideal experimental materials should be composed by the same individual who tells both lies and truth to exclude or reduce the variation resulted from individual differences. The video clips recorded individuals' facial expressions in the game show \\\"the Moment of Truth.\\\" Prior to the show, the contestants took a polygraph exam when they answered 50 questions. During the show, 21 of the same questions were asked again and the contestants were required to answer them in front of the studio audience. The questions became progressively more personal as the contestants moved forward (an example of an extremely personal question is: Have you ever paid for sex?). If the contestant gave the same answer to a question as they did in the polygraph exam (which means they were telling the truth), they moved on to the next question; lying (as determined by the polygraph) or refusing to answer a question ends the game (see https:\\/\\/en.wikipedia.org\\/wiki\\/The_Moment_ of_Truth_(American_game_show) for details). During the game show, most of the people talked emotionally and showed natural emotional facial expressions because of the high-stake situations they were in. The ground truth was obtained by a pre-show polygraph test that determined whether an individual was lying or not in the game show. Meanwhile, the stakes in the game show can be high (the highest gain from the show can reach at 500,000 US dollars, and cues to deception will be more pronounced than when there was no such monetary incentive. Participants were eight males and eight females who ended the game with lying. That way, there was at least one lying video clip for each participant. The video clips consist of the moments when the individuals were answering the questions, that is, from the end of the questioning to the end of the answering. To simplify calculation, we merged all the truth-telling video clips for each participant into a single one, and we ended up having one video for each type, truth-telling and lying, for each person. The duration of the video clips ranges from 3 s to 280 s, with an average duration of 56.6 s. Because of the game show setting that lying ends the game, the truth-telling video clips were much longer than the lying ones (mean = 105.5 s for truth-telling videos, and mean = 7.8 s for lying videos). In total, there were 50,097 frames for truth-telling video clips and 3,689 frames for lying video clips. Themedian of frames is 199 for lying video clips and is 2,872.5 for truth-telling video clips, with a frame rate of 30 f\\/s. Using Computer Vision to Compare the Features in Video Clips While People Lying or Telling the Truth Asking people to find out the cues to deception is difficult. Furthermore, naive human observers may not be able to perceive the subtle differences of the emotional facial expressions between telling lies and telling the truth. Alternatively, computer vision may be more capable of doing so. We proposed a method aimed to use the AUs of fear to discern deceptive and honest individuals in high-stake situations. Emotional Facial Expressions of Fear We, first, imported the video clips into OpenFace  to conduct computer video analysis. This software automatically detects the face, localizes the facial landmark, outputs the coordination of the landmarks, and recognizes the facial AUs. OpenFace is able to identify 18 AUs. According to Frank and Ekman (1997), telling a consequential lie results in emotions such as fear and guilt. Therefore, we focused on the AUs of fear, i.e., AU1, AU2, AU4, AU5, AU, AU. For each frame of videos, we obtained presence (0 or 1) and intensity (any number from 0 to 5) for each AU from OpenFace. Once we obtained the AU information from OpenFace, we then used MATLAB to calculate AUs of the emotional facial expression of fear. It was done by multiplying the output values of presence (0, 1) and the value of the intensity (from 0 to 5) for each frame. We then analyzed the AUs with statistical analysis, and also made classification predictions with machine learning. For statistical analysis, we took the average of each AU across all frames in one condition per participant. We ended up with one AU value for each condition for each person. We then bootstrapped the data for statistical analysis. For machine learning, we resampled the data with SMOTE before building the model. SMOTE is an over-sampling technique that solves class imbalance problem by using interpolation to increase the number of instances in the minority class. Resampling was necessary because the data are unbalanced, with the video clips of truth much longer than those of deception, 50,097 frames vs. 3,689 frames. It was consistent with the real life that lying was not that frequent compared to truth-telling, but it could still affect the reliability and validity of the model. We then used WEKA , a machine learning software, to classify the videos into a truth group and a deception group. Three different classifiers were trained via a 10-fold crossvalidation procedure. The three classifiers were Random Forest, K-nearest neighbors, and Bagging. Random Forest operates by constructing a multitude of decision trees (which is also a better choice for unbalanced datasets. Knearest neighbors (lazy.IBK in WEKA) achieves classification by identifying the nearest neighbors to a query example and using those neighbors to determine the class of the query. Bagging is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The Duration of Fear We used MATLAB to count the duration of AUs of fear (the number of frames when the corresponding AU was present). Because the frame rates of all the videos were the same, the number of frames could represent the duration of AU. Then the precise duration was obtained by dividing the total number of frames by frame rate, i.e., 30. The Symmetry of Facial Movements Beh and Goh (2019) proposed a method to detect the changes in the Euclidean distances of facial landmarks to find out microexpressions. We used the distances of ld1 and rd1, which are distances between facial landmarks at the left\\/right eyebrow and left\\/right eye (index 20\\/25 and index 40\\/43, see Figure 1), to investigate the synchronization and symmetry between left and right facial movements. The MATLAB function of Wcohenrence (wavelet coherence, the values ranged from 0 to 1) was used for this purpose, as this function returns the magnitude-squared wavelet coherence, which is a measure of the correlation between two signals (herein ld1 and rd1) in the time-frequency domain. If the left and right facial movements have perfect synchronization and symmetry, the value of wavelet coherence would be 1. Summary of Data Processing All of the aforementioned steps of classifying the truth or deception in the video clips are demonstrated in Figure 2. First, OpenFace detected the face, localized the landmarks, output the presence and intensity of AUs. Following that, AUs of fear, as well as indicators used to calculate symmetry in each frame from both lying and truth video clips, were merged into a facial movement description vector (frame by frame). Finally, in the classification stage, classifiers of Random Forest, K-nearest neighbors, and Bagging were trained to discriminate deception and honesty. RESULTS Action Units of Fear Can Differentiate Liars From Truth-Tellers Machine Learning Classification Results The whole dataset was split into two subsets; we arbitrarily selected 12 out of our 16 participants' data to build the model, i.e., data collected from 12 participants (42,954 frames, with 2,999 frames of lying and the rest of truth-telling) were used for training the model; and the data collected from the remaining four participants (10,832 frames in total, with 690 frames of lying and the rest of truth-telling) were used to test how accurate were the model to make new predictions. Three classifiers were trained on a dataset of 12 participants to discriminate liars from truth-tellers using feature vectors of AUs of fear (i.e., AU, AU, AU, AU, AU, AU, and AU, for details of AUs of fear please, see https:\\/\\/imotions. com\\/blog\\/facial-action-coding-system\\/). All the three classifiers, Random Forest, K-nearest neighbors (IBK), and Bagging, were trained in WEKA via a 10-fold cross-validation procedure. In building the model, the 10-fold cross-validation procedure split all the data from the 12 participants into 10 subsets, and the algorithms were trained on 9 subsets and tested on the remaining 10th each time, repeating 10 times. When a classifier was deployed from 10-fold cross-validation, it was applied to the other four participants' data to calculate the accuracy of prediction. To highlight the relative importance of AUs of fear in classification accuracy, we eliminated all other indicators used by Beh and Goh (2019). Table 1 shows the performance of machine learning analysis, which was conducted on dataset of 12 participants and tested with the data of the remaining 4 participants. Table 1 reports the percentage of accuracy obtained on the testing dataset. In addition to accuracies, the table reports the weighted average of true-positive rate (TP rate, instances correctly classified as a given class), false-positive rate (FP rate, instances falsely classified as a given class), precision value (proportion of instances that are truly of a class divided by the total instances classified as that class), recall value (proportion of instances classified as a given class divided by the actual total in that class), F-measure (a combined measure for precision and recall), precision-recall curve (PRC) area value (a model performance metrics based on precision and recall), and kappa (which measures the agreement between predicted and observed categorizations). The details of these statistics can be seen in Witten et al. (2016). In addition, considering the size of the dataset is relatively small, we did leave-one-person-out cross-validation (LOOCV). LOOCV utilizes each individual person as a \\\"test\\\" set and the remaining dataset as the training set. It is recommended for smaller datasets. The Random Forest algorithm was applied. The results showed that the average accuracy is still above 90% (mean = 90.16%, range from 78.74 to 95.78%). The Differences of AUs of Fear Between Truth-Telling and Lying Video Clips This analysis was carried out by examining the statistical differences of AUs of fear between truth-telling and lying video clips through paired t-test. To avoid the multiple-testing problem, we applied Bonferroni correction and set p-value to 0.007. We also calculated Cohen's d to measure effect size. The results are presented in Table 2. When bootstrapping was used, the p-value of comparing AU in the two groups was 0.006 (for AU, the corresponding p-value is 0.008). This analysis revealed that liars and truth-teller had differences in the facial expressions of fear. There Were More Transient Durations of AU of Fear While Lying Ekman (2003) reported that many people could not inhibit the activity of the AU (stretching the lips horizontally) while examining videotapes of people lying and telling the truth. Our results reported in section The Differences of AUs of Fear Between Truth-Telling and Lying Video Clips also found significant differences between truth-telling and lying video clips in values of AU. Therefore, differences in the duration from onset to peak, from peak to offset, and total durations of AU between truth-telling video clips (in which the number of AU is 675) and lying video clips (in which the number of AU is 47) were analyzed with independent samples t-test, using bootstrapping with 1,000 iterations. The results showed that there were significant differences in the total duration and duration from peak to offset between truth-telling video clips and lying video clips (20.77 vs. 15.21 frames, p = 0.033, effect size = 0.276; 11.35 vs. 6.98 frames, p = 0.04, effect size = 0.347). The durations of AU in lying video clips were nearly four frames (133ms) shorter than those in truth-telling video clips on average because the facial movements (herein the AU20) disappeared more quickly in the lying condition. Figure 3 shows the distribution of total frames, frames from onset to apex, and frames from apex to offset of AU. The median is 12 in the truth-telling video clips and 8 in the lying video clips. For lying video clips, the 95% confidence interval is 10.32 to 20.11 frames for the mean of total duration, and 19.03 to 22.52 frames for truth-telling video clips. There were 16 (out of 47) AU20s whose durations were less than or equal to six frames (200ms, one of the commonly recognized thresholds differentiating microexpressions and macroexpressions) in the lying video clips, while there were 145 (out of 675) in the truthtelling video clips. There were 32 AU20s whose durations were <=15 frames (500ms, another microexpression\\/macroexpression boundary, more details in discussion) in the lying video clips, and the corresponding number is 407 in the truth-telling video clips. Asymmetries of the Facial Movements Were More Salient in Lying Than Truth-Telling We calculated ld1 and rd1, the distance between facial landmarks predicted at the left eyebrow and left eye and the distance between those predicted at the right eyebrow and right eye  in each frame. These two distances represented movements of the left and right eyebrows. Next, we used the MATLAB function Wcohenrence (wavelet coherence) to measure the correlation between ld1 and rd1 in each video. If the movements were exactly symmetrical (e.g., they have the exact same onset time, reach the apex at the same time, and disappear at the same time), the coherence between ld1 and rd1 would be 1. Any asynchrony would result in a coherence value of <1, with a smaller coherence value indicating more asymmetry. Figure 4 shows the wavelet coherence in truth-telling and lying video clips. The coherence outputs for each player (i.e., the average of coherence between ld1 and rd1) were then imported into the permutation test (see the following link for details: https:\\/\\/ github.com\\/lrkrol\\/permutationTest) to compare the asymmetry differences between the lying and truth-telling situation. Permutation tests provide elegant ways to control for the overall Type I error and are distribution-free. The results showed that lying and truth-telling situations caused different coherence in facial expressions (the means of coherence are 0.7083 and 0.8096, p= 0.003, effect size= 1.3144). DISCUSSION The current study supported the prediction of leakage theory that leaked fear could differentiate lying from truth-telling. The results of machine learning indicated that emotional facial expressions of fear could differentiate lying from truth-telling in the high-stake game show; the paired comparisons showed significant differences between lying and truth-telling in values of AU of fear (AU5 is marginally significant). The results also substantiated the other two hypotheses. The duration of AUs of fear in lying was shorter than that in truth-telling, with a shorter total duration and the duration from peak to offset of AU of fear when lying compared to telling truth. The third hypothesis predicted that the symmetry of facial movements would be different, and the findings indicated that the facial movements were more asymmetrical in lying situations than in truth-telling situations. In the current study, the use of machine learning classified deception and honesty. It made up the shortcomings of human coding and successfully detected the subtle differences between lying and truth-telling. Meanwhile, an objective measure of asymmetry was proposed. To our best knowledge, this is the first objectivemethod tomeasure the asymmetry of facial movements. By using these methods, we were able to find differences between lying and truth-telling, which is the prerequisite for looking for clues of deception. The machine learning approach could have some disadvantages. For example, the LOOCV is recommended for small datasets, like what we have in the current study. However, it yielded a higher variance than 10-fold cross-validation. The reason for this high variance might be that the training datasets in LOOCV have more overlap (each model was trained on an almost identical dataset), which made the outputs from different folds highly positively correlated with each other, and hence increases the overall variance (the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated (;, p185). In our data, the variance was represented as varying accuracy rates when different participants were left out in the training set; for example, 78.74% accuracy when participant 14 was left out compared to 95.78%when participant 11 was left out. Bengio and Grandvalet (2004) argued that when independent estimates in cross-validation were correlated, the correlation that is responsible for the overall increase of variance could increase with K in a K-fold cross-validation, with leave-one-out being an extreme case where K is equal to the number of data points. In our dataset, considering the similar procedure each individual generates the same facial expression, it is highly possible that the training sets are highly correlated. Future research with a larger sample size would reduce this variance. The leaked emotions can be cues to deception, but they are not deception per se. They are, however, closely linked with deception. As shown in the results, truth-tellers also experience fear. However, the dynamics of experienced fear of truth-tellers were very different from those of liars. Thus, the fear emotion could be considered as a \\\"hot spot\\\" of deceit. Looking for the non-verbal \\\"hot spots\\\" of individuals satisfies the demands of rapid evaluation. Some other approaches of deception detection, for example, brain activities, cannot provide real-time results. The results suggested that the \\\"hot spots\\\"--emotional expressions of fear-could distinguish between truthful and deceptive messages with a reasonable level of accuracy. Using machine learning, we achieved a higher accuracy (above 80%) than the average accuracy achieved by people (54%, see Bond and DePaulo, 2006). In addition, we carried out a human deception detection study , in which the video clips of the first and the last honest answering (both are from the end of the questioning to the end of the answering. We changed the durations of truthtelling video clips to keep the durations of lying and truth-telling video clips nearly same), the lying video clips are the same. Thirty college students took part in the study. The accuracy of detecting the lies was 0.34; for low-stake truth-telling video clips (the first honest answering), the accuracy of truth detection was 0.69, and for high-stake truth-telling video clips (the last honest answering), the accuracy of truth detection was 0.64, and the average of deception detection was 0.50. The results showed again that the accuracy for human deception detection was at chance level. Apart from accuracy, there was a large effect size for the AU of fear (AU20) while differentiating lies from truth. High-stake lies were used in some previous research. For example, Vrij and Mann (2001) used the videos from media where missing people's family members announced the missing of their family and asked for help. In these videos, some of the announcers were telling the truth, while the others were hiding the truth that the people claimed to be missing were murdered by the announcers themselves. One disadvantage of these type of materials is that researchers would not have access to the truth and therefore would not be able to tell for sure if one is lying or not. Our dataset consists of highstakes deception videos from a real game show, in which the veracity of the statements is supported by a polygraph test. That can help us achieve a relatively high ecological validity and internal validity. Considering the debate on the reliability of polygraph tests, future research could use materials where the truth is further affirmed. One example would be the game show Golden Balls, which utilizes a prisoner's dilemma setting and the truth becomes obvious after one makes a decision in the game. Were the facial expressions in lying video clips all microexpressions that last for <0.2 s? The current results of total duration showed that AU on average lasts for 20.77 frames, i.e., 692ms, in truth-telling video clips; and 15.21 frames, i.e., 507ms, in lying clips. The 95% confidence intervals of total duration were from 19.03 to 22.52 frames (634-751ms) while telling truth and were from 10.32 to 20.11 frames (344ms ~ 670ms) while lying. In the current study, the mean was affected by extreme values or outliers (see Figure 3). Thus, we used the median, which could be a more appropriate statistic for the duration. The median of duration in the truth-telling video clips was 12 (400ms) and in the lying video clips was 8 (267ms). Although the duration of (partial) fear was shorter in lying video clips than in truth-telling video clips, most of the durations in lying did not fit into the limits of traditional durations of microexpressions, i.e., <200ms. There were nearly 1\\/3 AU20s which durations were less than or equal to six frames (200ms) in the lying video clips, and only 1\\/5 of them in the truth-telling video clips were less than or equal to six frames. By using 500ms as the boundary between microexpressions and macroexpressions , there were almost 2\\/3 of the facial expressions that could be named after microexpressions. The results suggested that the leaked emotional facial expressions in real life were much longer (the duration of the apex of leaked emotional facial expressions would be <200ms). No matter what the duration is, or whether the facial expression is a microexpression or not, the durations of facial expressions were significantly shorter in the lying video clips than in the truth-telling video clips. Taken together, our findings suggested that deception is detectable by using emotional facial expressions of fear in highstake situations. Lying in high-stake situations will leak facial expressions of fear. The durations of fear were significantly different between lying and truth-telling conditions. Besides, the facial movements are more asymmetrical when one is lying than they are when one is telling the truth. Our findings prompted that attending to the dynamic features of fear (such as symmetry and duration) can improve the ability of the people to differentiate liars from truth-teller. Besides, the machine learning approach can be employed to detect real-world deceptive behaviors, especially those high-stake ones in the situations where strong emotions are generated, associated with attempts to neutralize, mask, and fake such emotions. Certainly, the number of participants (16) in the current dataset was relatively small, which could limit the generalization of the results. We consider the current work as a preliminary exploration. Pupil dilation and pitch of speech are found to be significantly related to deception by some studies of metaanalysis. These cues are closely related to leakage too. The findings of Bradley et al. (2008) indicated that the pupil's changes were larger when viewing emotionally arousing pictures which also were associated with increased sympathetic activity. Pitch of speech will be different between honest and deceptive interaction. Future studies should address all these leaked clues or the \\\"hot spots\\\" of the deception.\",\"1135746018\":\"INTRODUCTION Leadership research includes a large number of theories and models that have evolved from personality models such as the great man theory  or trait theory , behavioural models that focus on the characteristics and behaviours of a leader,  and broad models that acknowledge situational, contextual, communicational, and organisational factors such as the situational model  or the contingency model. A more recent theory from Bass (1985), the transformational and transactional leadership theory, recognised the existence of three leadership styles: transactional, transformational, and passive-avoidant. Transactional leaders present behaviours highly focused on the achievement of objectives and characteristically engage in unilateral decision-making without involving team members. The transactional leader distributes tasks, establishes the guidelines to be followed, and monitors these. If the tasks are executed correctly, positive reinforcement is applied, however, punishments for errors and deviations are also applied, which have negative effects on subordinates. In contrast, transformational leadership attaches greater importance to relationships, motivation, and communication. Such leaders present positive images of themselves and others and express concern toward their employees and their personal and workrelated problems. The ability of transformational leaders to express their opinions while respecting the rights of their subordinates also express their opinions is inferred. Transformational leaders will reinforce this behaviour by involving their teams in decision-making processes. Finally, passive-avoidant leadership is characterised by the total absence of leadership. According to previous studies  leadership style is closely related to organizational results, innovation, success and recognition of the company. Specifically, it has been seen that leaders who tend to adopt a more transformational leadership approach and avoid passive-avoidance attributes could improve organizational outcomes and work engagement of employees. Bearing in mind these data, it seems important to know before hiring the candidate his degree or level of connection with each of the leadership styles. In this way, companies could improve organizational outcomes by hiring leaders who are able to build trust in their followers, who inspire power and pride, and who become reference models for their followers, that is, leaders who present behaviours and attitudes typical of the transformational leadership style. The Multifactor Leadership Questionnaire (MLQ;) is a well-validated self-report questionnaire used to measure these three styles of leadership and their influence on subordinates. It consists of 45 items: 36 items refer to leadership styles, and nine questions refer to three organisational outcome variables (extra-effort, leader effectiveness, and employee satisfaction; see Table 1). Although self-reported questionnaires have traditionally been used for assessment in organisational leadership research, they present several limitations regarding ecological validity , as they are decontextualised from real situations and do not elicit real-life behavioural responses. Furthermore, self-report measures are determined by human perceptions, and therefore social desirability and acquiescence biases may affect the veracity of responses. Additionally, there is a growing concern in contemporary academia regarding the effectiveness of such instruments and scales. Some researchers have called for an analysis of existing leadership using other methodological evaluations to identify problems such as the halo effect, which fails to capture real behaviours , and threats to validity , which have also been a recent topic of interest. In order to overcome these limitations, advances in immersive virtual reality (VR) technologies, combined with implicit measures such as behavioural decision-making, eye-gaze patterns, and machine learning (ML) techniques, are enabling the creation of experiences similar to real-life and are therefore able to better identify implicit behaviours and recognise leadership styles. Virtual Reality and Human Behaviour Assessment Virtual reality can be viewed as a 3D synthetic environment able to simulate real-life experiences, where subjects can interact with their surroundings as if they were in the real world. The combination of various technological devices (visual, auditory, and haptic) and tracking systems that accurately reproduce stimuli creates a significant sense of presence. The user has the sensation of \\\"being there,\\\" and as a result, can forget that the situation is not real, and therefore behave (both cerebrally and physically) as if the VR experience were real life. These technologies allow information to be collected directly from the user in realtime (e.g., decision-making responses and times). Additionally, they also allow the integration and collection of other implicit measures, such as brain activity, skin conductance, cardiac variability, and eye-tracking. These measures provide valuable, indirect sources of information related to the implicit correlations of leadership competencies. This experience is complicated, or even impossible, to achieve in laboratory settings, as the use of multisensory laboratory stimulation does not present the complete, immersive, contextual experience that VR does. A recent review of social cognitive neuroscience and VR found that the use of this type of technology was effective with regard to affective induction, social psychology, and neuropsychological evaluation. Furthermore, VR environments can increase user participation through \\\"stealthy\\\" assessment design approaches. The design and development of virtual environments requires a methodology that enables the stratification and determination of knowledge layers while incorporating valid measurements that enable the evaluation of evidence-based competencies. Advances in VR technology have enabled the capture of implicit measures without the need for subjects to self-report on aspects related to their capabilities. For stealth assessment methods, technologies based on evidence-centred design (ECD) have been used as valid and reliable reference frameworks for test design. ECD was developed primarily in the education field to improve the validity and reliability of test measures for students. ECD considers evaluations as evidencebased arguments. That is, actions from which one can observe what students say or do at a particular time and then infer what the students know, can do or have achieved. The ECD framework defines several interconnected models, three of which form the core of the framework and are relevant to the present study: competency, structure, and task. * Competency model: this model describes the abilities or skills to be measured. * Evidence model: the objective of this model is to determine which observations are optimal by providing evidence of what the designer wants to measure. * Task model: the task model is responsible for defining the characteristics of the specific evaluation activities or tasks. In the leadership research field, VR has been primarily used for training skills , as its efficacy in differentiating leadership styles has been limited. Moreover, leadership VR training has traditionally used non-immersive 2D graphical stimuli, characterised by flat graphics that limits the transferability of learned skills to the real world. It has been shown that immersive learning using immersive 3D virtual environments for training skills is more effective than 2D, due to the higher sense of presence that 3D VR offers. Implicit Measures of Leadership Behaviour There are many organisational behavioural theories that assume users have conscious control over their attitudes and actions. These approaches are based on traditional theoretical perspectives that consider humans capable of verbalising and being conscious of the brain processes involved in attitudes, emotions, and behaviours. In contrast to the traditional approach of using explicit measures, many neuroscience researchers  have indicated that much of the processing related to behaviour, emotion, and attitude, within the context of work, occurs outside of consciousness, and therefore involves implicit processes that subjects themselves cannot verbalise due to their unawareness of them. Implicit processes can be defined as brain functions that occur automatically and outside of one's conscious control or awareness, whereas explicit processes occur through conscious executive control. The link between explicit and implicit measures could lend greater veracity and validity when measuring behaviours in complex contexts, such as day-to-day work. Implicit measures can involve both brain and physiological measures, such as electroencephalogram (EEG;), galvanic skin response , heart rate variability , and decisionmaking behaviours and eye-gaze patterns. By incorporating a balance of implicit and explicit measures in human resource management and organisational behaviour research, academics could develop more comprehensive and integrated theories of work phenomena. This study focuses on decision-making behaviours and eye-gaze patterns. Decision-Making Behaviours and Eye-Gaze Patterns as Implicit Virtual Reality Measures Rowe and Boulgarides (1983) decision style theory on leadership claims there is a relationship between a leader's decision-making style and whether their leadership style is transactional or transformational. According to the theory of decision-making, an individual's style of decision-making depends on how one understands and perceives a situation and how they respond to the contextual stimuli presented. Therefore, depending on their understanding of a situation, an individual may have a decision-making style that is focused on people-orientation and understanding the state of the team that surrounds them. However, an individual may also have a decision-making style that instead focuses on their decisions and on achieving objectives while the state of their team takes a back seat. Rowe and Boulgarides (1983) linked their typology of decision-making styles to individual needs for task or relationship orientation, a posture more suited to the standards of a transformational leader. Further, it has been postulated that managerial decisionmakers are primarily driven by their need for power, while behavioural decision-makers are concerned about the need for affiliation. Because those who make managerial decisions typically have little tolerance for ambiguity, they incidentally have a strong desire for structure, rules, and procedures. This is considered similar to the behaviour that a leader who is focused on achieving objectives would exhibit, focused more on a transactional style of decisionmaking and not taking into account the state of their team. This tendency leads such leaders to be inclined to make directive decisions, such as giving clear orders to subordinates and executing decisions themselves. Conversely, behavioural decision-makers are concerned with maintaining good relationships through offering psychological support and encouragement to their teams during complex situations, making collaboration and direct relationships with the team the basis of their leadership style, and thus, corresponds directly with the transformational style. An indication of this style of decision-making is that it involves consistently communicating with teams and seeking and using their comments in the final decision-making. Social gazing patterns refer to the implicit and automatic tendency of people to focus their attention on the behaviours of others and interpret the relevant social signals. According to social attention theory, visual attention allows people to recognise each other, communicate their mental states, and predict the behaviour of others. This has potential relevance when it comes to solving problems in group situations, such as selecting a leader. Evidence shows that people can predict leadership cues by watching silent voice clips. Similarly, people can perceive differences in visual patterns during presentations. The visual behaviour of audiences has been shown to be modified based on the charisma of the leader presenting. Specifically, a relationship has been found between influential leadership and direct gaze , from which the existence of different gaze patterns between the three leadership styles is inferred. Eye-tracking techniques provide two different indicators, the orientation of attention toward someone or something through the number of fixations and the maintenance of this attention throughout the duration of fixations. This means these techniques can monitor where attention is focused initially and automatically, to which stimuli, and in how this visual attention is maintained. Therefore, this type of measurement allows the analysis of the in-depth internal processes of an individual's visual attention in social situations and complex simulations. The ECD model for VR and implicit measures are promising tools and methods for the assessment of leadership styles, as they enable the collection of large amounts of real-time data relating to things such as eye-gazing, task execution, decision-making behaviours, and latency times. The analysis of this data can be complicated due to the amount and heterogeneity of the data. ML has emerged as an effective method to analyse large amounts of data. In the current study, ML methods were used to obtain predictive data regarding leadership styles. Machine Learning and Organisational Behaviour Machine learning is a scientific discipline within the artificial intelligence (AI) field that deals with the design and development of algorithms that allow computers to evolve behaviours based on empirical data, recognise hidden patterns, and use them to make predictions. Recently, an increasing number of researchers have noted how ML techniques applied to big data can be used to study individuals behaviours in workplaces. Some leading companies have started to use AI techniques, such as ML, to automate decision-making processes, improving the processes by increasing employee involvement and customers satisfaction. Other recent studies are proposing the use of ML as predictive models in organisational environments. For example, Na and Kim (2019) used a ML algorithm to predict the impact of disease on returning to work. Other studies have used ML to predict employee performance , employee turnover , and evaluate job candidates. Furthermore, in leadership studies, ML has been used to identify traits that define the leadership role  and measure personality traits. The use of ML techniques for implicit measures gathered within virtual environments has occurred primarily in clinical psychology (;,b) and less in organisational situations. The aim of this study was to recognise transformational, transactional, and passive-avoidance leadership styles while exposed to a 3D virtual environment that simulated workplace social interactions. Decision-making behaviour and eye-gaze tracking were used as implicit measures. Additionally, ML methods were used in the analysis of the implicit measures to explore if it is possible to discriminate between transformational, transactional, and passive-avoidance leadership styles and to identify the parameters that best discriminate between these styles. The main hypotheses were that participants' decisionmaking behaviours during the VR experience would be able to indicate their leadership styles and that participants' eyegaze patterns during the VR experience would also indicate leadership style. MATERIALS AND METHODS Participants The study sample consisted of 83 subjects, of which 32 were women and 51 were men (M = 42, SD = 3.44). The selection of the same was subject to a selection carried out through work criteria, in which they had to have a team under their charge for at least 1 year, considering that leading a team entails certain leadership skills. In the same way, a set of students was included, which had non-leadership criteria, based on the fact that they had not previously had teams under their charge. The fact of including a group of students in the sample, in addition to providing the lack of experience in leading teams, balanced the possibility of finding a very high level of specific leadership styles. In the same way, the MLQ was administered to them to determine the leadership style in the same way as in the rest of the sample, since in the absence of experience, leadership would appear as an inherent trait in the user's behaviour. The sample was completely Caucasian, all of Spanish nationality and Spanish speaking. The inclusion criteria for participation in the studies were that they were of legal age, had a team under their care for at least 1 year if in the sample of professionals, or had not had a team under their care, or prior work experience, if in the student sample. Individuals were excluded if they had any type of mental disorder or took medication that affected their cognitive and mental functions. This sample included leaders or professionals from a wide variety of industries, including pharmaceutical, banking, and consulting. The sample was counterbalanced at the leadership level through the results obtained in the MLQ questionnaire. Thus, a complete representation of each of the three leadership styles was obtained based on the responses to the questionnaire. The sample was counterbalanced in terms of gender and familiarity with the use of video games. Additionally, the level of leadership or human resource management an individual carried out in their workplace was considered. All participants submitted their written consent to participate in the study. The study was carried out in accordance with the Declaration of Helsinki (1964), and was approved by the ethics committee of the Polytechnic University of Valencia, Spain. Leadership Assessment The MLQ-Leadership form was used for each leader, while the MLQ-Subordinate form was used for a subordinate of each leader. This has become one of the most widely used instruments to measure leadership in the field of organisational psychology. The questionnaire describes the leadership style that the person perceives themselves to have or that the subordinate thinks the leader has. It consists of 45 items that were rated on a five-point Likert scale. For each leadership style, there are different dimensions measured. Transformational leadership has five dimensions: idealised influence (attributed), idealised influence (behaviour), inspirational motivation, intellectual stimulation, and individualised consideration. Transactional leadership has two dimensions: contingent reward and management by exception (active). Passive-avoidant leadership also has two dimensions: management by exception (passive) and Laissezfaire. In addition, the questionnaire analyses the effect of leadership on organisational outcomes across three factors: extraeffort, effectiveness, and satisfaction. Specifically, nine questions are related to these three organisational outcome variables, while 36 questions are related to the leadership styles, consisting of questions specific to each of the nine aforementioned dimensions that exist within the different leadership styles. Virtual Environment Modelling To create a valid measure to obtain reliable results from the VR experience, ECD guidelines were followed. Following these guidelines, a story narrative was designed, with different scenes set in different office environments. Specifically, it consisted of an office meeting room environment, where a series of dynamics are developed with other avatars, in which the participant must make decisions and carry out behaviours that determine the subsequent development of the scenes. The VR involved four adult virtual agents (two women and two men) that were designed with personality traits and competencies according to the transformational, transactional and passive-avoidant leadership styles (Figure 1). Specifically, one of the characters was defined as an organiser, another as emotional-interpersonal, another as logical, and the last as non-interventional: (1) The organiser virtual agent (woman) is characterised by planned, sequential and structured thinking (transactional leadership). Her role focuses on exposing the issues, and she is the one who decides what to do but does not get involved either positively or negatively. (2) The emotional-interpersonal or communicative virtual agent (woman) is characterised by presenting empathic traits, interpersonal warmth, fluid communication, and holistic thinking (transformational leadership). This agent talks about the topic to be discussed with confidence in herself and in the team and encourages everyone to be part of reaching a consensual decision. This character is approachable and respects the opinions of others. (3) The logical virtual agent (man) is characterised by presenting mathematical, technical, and analytical reasoning, with a tendency toward negativity (transactional leadership). This agent sets clear standards to follow and dishes out punishment for any mistakes made. (4) The last of the agents (man) is characterised by nonintervention. He avoids giving any kind of feedback regarding his opinion, leaves the decision in the hands of the team, and can be highly upset depending on the situation (passive-avoidant leadership). The VR experience consisted of four different situations, in the appendices we have put a functional diagram of each situation. At the beginning of each situation, a problem was presented by one of the virtual agents to the other agents and the participant. In each situation, there were two to three problems on the agenda to be resolved among those attending the meeting. To find a solution to each of the problems, the participant had to make various decisions freely by voice and by selecting the option that aligned with their opinion. Each decision made led the story narrative down a different path. Specifically, mini-games were designed for some decisions, but not every decision leads to a mini-game. The option to access the games depended on the decision style, being more proactive the cases where the user finally accessed the games. Each possible decision was developed according to a systematic method based on three decision-making behaviours, communication (a), control (b), and avoidance (c): (a) Communication refers to decision-making in which all team members are involved, where information regarded the opinions of the other team members is sought out and collected. It implies a desire to be open and accessible and to collect information of both a professional and personal nature. This style of decision-making is based on approach behaviours; new situations are seen as challenges and are faced with optimism. It is a decision-making style that is related to transformational leadership. (b) Control is a type of decision-making where an individual takes initiative without asking the opinion of the other team members and subsequently distributes and controls tasks and their development. It is a behavioural style associated with transactional leadership. (c) There is the possibility that the participant opts for the option of avoiding by doing nothing to solve the problems raised. The behaviour of this participant is related to a passive-avoidant leadership style. This type of leadership is characterised by delaying or avoiding decision-making and by delegating responsibilities to other team members. The four situations that were developed in VR were designed in accordance with the theoretical framework of reference (transformational, transactional and passiveavoidant leadership styles), the ECD model, and the MLQ instrument. Figures 2-4 display the competency models according to each leadership style and their relative indicators. For each style, a graphical model of the indicators (observable tasks, data collected from user performance, and unobservable, theoretical leadership constructs) is presented. Experimental Procedure To determine the leadership styles present within the sample, the participants completed the MLQ online. A short demographic questionnaire was also completed by each participant to collect data related to age, gender, and job position. Following the completion of these, participants visited the laboratory to complete the experimental phase of the study. The experimental phase consisted of a single 1.5-h session in which the participants experienced a simulation in an immersive VR environment. The first 2 min of the experience showed a brief tutorial explaining how to use the virtual environment. The room in which the experimentation took place was a neutral experimental room so that distractions could be avoided. At the beginning of the session, the eye-tracking application was started manually, and calibration was carried out once the head-mounted display (HMD) was placed on the participant. After these steps, the virtual environment simulation began and the first 2 min of the experience showed a brief tutorial explaining how to use the virtual environment. After that, the user is immersed in the first scene of the first situation, that is, the office. Once the first situation is overcome, the next situation begins, until completing the entire experience (four situations in total). The average execution time of the experience was 1.5 h. The longest execution time was approximately 1 h and 48 min, and the shortest was approximately 1 h and 10 min. The visual attention was measured using the HTC Vive Pro Eye HMD, with a combined resolution of 2,880 x 1,600 pixels (1,440 x 1,600 per eye), a 110* field of view, and a refresh rate of 90 Hz. The application of VR was carried out on the MSI GE Raider 9SF-1204XES, 17.3\\\" laptop (i7-9750H, RAM 32 GB, 1 TB NVMe PCIe Gen3x4 SSD, GeForce RTX 2070 GDDR6 8GB). The VR system was developed using Unity 5.5 1f1 software, applying C# pro programming language with the Visual Studio tool. Data Processing Three different data sources were available: behavioural data (i.e., decisions made by the participant in the VR experience), eyetracking data (i.e., sight fixations during the VR experience), and questionnaire answers that collected the psychological variables to study (e.g., MLQ scores). From the behavioural data and eyetracking data, several variables were processed, Table 2 presents a detailed description of the behavioural variables. A total of 63 variables were obtained from behavioural data. If a participant did not complete all VR game types or had missing values in the variables that measured performance and motive during the games, and there was more than one possible option to choose from, they were marked as \\\"not chosen.\\\" A total of 110 variables were extracted from eye-tracking data, as shown in Table 3. Statistical Analysis Three participants did not complete the MLQ-Leadership form, while 12 did not complete the MLQ-Subordinate form. As a result, their data were excluded from the analysis. A multivariate outlier analysis  was performed to detect and remove any participant whose scores in each questionnaire could be considered extreme. In order to detect these participants, the Mahalanobis distance between participants was calculated using the numeric score on each of the seven subscales of each questionnaire. The probability of this distance belonging to a Chi-square distribution was calculated. If this probability was below 0.01, the participant's scores were defined as outliers, and the participant was excluded from further analysis. One participant who completed the MLQ-Leadership form was considered an outlier, while two participants who completed the MLQ-Subordinate form were considered outliers. These participants were excluded only for the analysis of the variable in which their results were considered extreme. 77 participants' MLQ-Leadership scores and 68 participants' MLQSubordinate scores were analysed. The description of the target data (i.e., MLQ scales) was performed by obtaining the mean, median, minimum, maximum, standard deviation, and interquartile range for each scale. The normality of the scores was studied through a ShapiroWilk test. All 14 MLQ subscales were categorised into high or low scores depending on the median of each variable, as most of them were non-normally distributed (p < 0.05, Shapiro-Wilk test). This was a necessary step for building the ML models described in the following section. Machine Learning Multifactor leadership questionnaire recognition models were built using ML models and the data recorded during the VR experience. First, a feature selection was performed to reduce the dimensionality without defining any maximum limit of features. This feature selection was performed using a backward sequential wrapper. This method builds a model with all the available features using the selected ML algorithm and measures its performance. Then, at each subsequent step, a feature is removed, the model trained, and its performance measured. The feature where removal increased the performance measure most significantly (i.e., Cohen's kappa) was removed from the set of features that will be used in the next step. After several repeat steps where the performance metric does not vary by more than 0.01, the process stops. Multiple ML algorithms were used to obtain the best set of features, namely Random Forest, SVM, Naive Bayes, XGBoost, and kNN. No further hyperparameter tuning was done. The default hyperparameters defined in mlr package v2.14.0 were used. After obtaining the best set of features for each ML algorithm, the model was trained and validated, and its metrics were calculated (i.e., accuracy, Cohen's kappa, sensitivity true positive rate and specificity true negative rate). Cohen's kappa is used since it considers chance agreement as a baseline in a single metric, in contrast to accuracy, which can be highly affected in unbalanced models. A threshold of 0.4 is considered as a threshold to consider a model as \\\"good\\\" . Each step used repeated cross-validation (fivefold, four times), so the validation metrics correspond to the mean value across the 20 repetitions. Data from 10 participants were excluded from this model building process and only used as a test set. The test set was randomly chosen from all the participants who had scores from both MLQ scales available and was used to test the models for all the subscales. Both the statistical and ML analyses were performed in R (version 3.6.1). RESULTS Multifactor Leadership Questionnaire Scores Description Table 4 shows the description of scores on the different MLQLeadership and MLQ-Subordinate subscales. The distribution of all scores, except the MLQ-Leadership Transactional subscale scores, were non-Gaussian. Therefore, the median value was used to divide the scores into high or low scores. The balance between both categories varied from 50% in the MLQ-Subordinate Transformational subscale to 77% in the MLQ-Leadership Satisfaction subscale (i.e., 59 high-scoring participants vs 18 low-scoring participants). Multifactor Leadership Questionnaire Recognition Models Table 5 shows the metrics and characteristics of the best model achieved for each MLQ subscale following the validation process. For every subscale, except for the MLQ-Subordinate Effectiveness subscale, models with validation accuracy above 0.7 and kappa above 0.4 were achieved. These results were mostly maintained within the test set, with all but five models maintaining validation accuracy above 0.7 and kappa above 0.4. The best-modelled subscales in terms of validation and test results were the MLQLeadership Transactional, MLQ-Subordinate Transformational, MLQ-Subordinate Passive-Avoidant, and MLQ-Subordinate Satisfaction subscales, as they each achieved kappa values above 0.5 in both sets. The number of selected variables for the models varied between 14 and 43, with most of the variables coming from the eye-tracking data set (i.e., the percentage of behavioural data included in the models varied from 0 to 32%). DISCUSSION This study based on a multi-method approach, aims to offer a first approximation for the discrimination of different leadership styles through the joint use of VR and implicit measures, based on the results obtained in the MLQ questionnaire. Specifically, an immersive VR environment based on ECD was used in conjunction with eye-tracking measures. The use of these tools enabled behavioural decision-making in the virtual environment to be recorded, as well as the compartmental signals associated with eye-tracking for each of the three leadership styles. ML was used to build different models based on the two sources of information recorded during the VR environment experience. The main objective of this study was to replicate the MLQ classification based on implicit measures and in a VR environment. The simultaneous use of implicit measurements and VR allows an objective evaluation of leadership behaviour. This methodology improves the ecological validity compared to the self-report measures since it enables behaviours and decision-making to be captured in scenarios that simulate real management situations. The study includes an analysis of the frequency distribution (high vs low) of the different leadership styles to investigate the differences between the leadership styles, using a wide set of ML models that were based on decision-making behaviours and gaze tracking patterns. High and Low Transformational, Transactional, Passive-Avoidant, and Laissez-Faire Differences Between Measures The first objective was to identify the differences between the different leadership styles both in the traditional measure questionnaire and in the VR experience. The results of the traditional measure indicated that, for the leader's selfreport form, 53% of participants had a high score in the transformational subscale, 56% in transactional, 61% in passiveavoidant, and 66% in Laissez-faire. Regarding the questionnaire completed by subordinates, 50% of participants had a high score in transformational leadership, 51% in transactional leadership, 53% in passive-avoidant, and 57% in Laissez-faire. Regarding the organisational results of the self-report, 71% of the sample were classified in the \\\"high\\\" category for the Extra-effort variable, 73% for Effectiveness, and 84% for Satisfaction. Of the results obtained from the subordinate questionnaire, 57% of the sample were classified as \\\"high\\\" for Extra-effort, 53% for Effectiveness, and 53% for Satisfaction. These results indicate that the traditional evaluation measure can define and classify leadership styles. Furthermore, they indicated that there were a greater number of participants with high scores than low scores in all types of leadership and all the variables of the organisational results. Regarding the VR experience, the results indicated that the different styles could be differentiated by the eye-gaze patterns and behaviours carried out during immersive VR. However, according to the results, ML models selected more variables from eye-tracking than from behavioural data, as eye-tracking was represented between 64 and 100% in the selected variables for all models. This indicates that there was a more significant contribution of information from the eye-tracking metrics. As such, the results indicate that the eye-tracking pattern is a more relevant and distinctive aspect of the different leadership styles compared to decision-making or behaviours carried out in VR immersion. This could explain why previous studies have focused on the gaze pattern to identify the peculiarities among leaders and their impact on organisations , as the gaze reflects complex mental states such as intentions, thoughts, beliefs, emotions, desires, and characteristics of social interaction. Looking at the ML models metrics, it is inferred that the capacity of the virtual environment to provoke behaviours (eyegaze patterns and behavioural decision-making) enables the classification of participants according to their leadership style. It should be noted that this study achieved homogeneous results between validation and test, with the exception of the results in five models. The leadership recognition models for the different leadership styles and organisational outcomes achieved accuracies between 78 and 87%. In addition, ML models were balanced in terms of sensibility and specificity in all cases except for Leadership-Laissez and LeadershipSatisfaction, suggesting each group of styles was able to be precisely identified. In terms of test set, this balance is not achieved, but this can be due to it includes 10 samples. Further analysis need to be done increasing the sample size of the test set. Moreover, the backward sequential wrapper implemented allows to easily explore the importance of each feature, in contrast to other dimensionality reduction techniques such as principal component analysis. Theoretical Implications The use of a VR environment together with a non-intrusive method (eye-tracking) and ML for the evaluation of behavioural responses in complex situations can increase the knowledge about the attentional behavioural patterns and decision-making processes carried out by leaders with different leadership styles. Unlike most evaluations that use subjective self-report measures, this method combines neuroscience with VR, which, in turn, attributes greater objectivity and ecological validity to the results. With regard to implicit measures, previous studies have tried to identify leaders based on gaze-following behaviour among group members. In addition, ML models as a leadership classification tool have previously been used to carry out predictive analysis of eyetracking behaviour during social interactions in non-immersive environments. However, unlike previous studies, the present study did not require the participation of other team members for leadership evaluation. Only the leader's participation in the VR experience was required. This enabled the necessary data to be collected and analysed by ML to identify the participants according to their leadership styles. In addition, to evoke the typical behaviours associated with the leadership style of the participants, office spaces were recreated with high-pictorial realism and used hyper-realistic avatars. All these factors constitute an important contribution to Gerpott et al. (2018), which focused on the gaze pattern of leadership during social interactions to differentiate between leaders and non-leaders. Additionally, this study puts into practice the taxonomy proposed by Meibner and Oll (2019) and used the suggested ET measures, which are the number and duration of fixations required to capture the psychological and behavioural characteristics of the different leadership styles, as proposed by Parra et al. (2021). From this, the importance of nonverbal cues in the identification of leadership characteristics in organisations is inferred, as they cannot be evaluated through explicit measures. Practical Implications Expanding the knowledge about the neuropsychological aspects responsible for the behaviours of individuals can constitute the basis for the modification and training of effective leadership behaviours via interventions promoting them. Such changes will be motivated by training and consolidated due to neuroplasticity, enabling the learning of new ways of behaving and making decisions. Implicit measures can play an important role in the evaluation of behaviour and psychological leadership constructs. As such, they can be used to evaluate the results of the behaviour training within the context of effective leadership and employee satisfaction. Examples of the implicit measures by which these changes can be evaluated are fMRI, qEEG, and eye-tracking. If these measures are to be used together or with VR systems, the use of ML algorithms that enable the analysis of large data sets may be beneficial, as they can facilitate the evaluation and interpretation of the results obtained, thereby promoting the advancement of the neuroscientific study of leadership. LIMITATIONS AND FUTURE DIRECTIONS In this study, a small sample size (<70) was observed and therefore, the size of the test set for the ML models was also small. This could have affected the results due to variability and, therefore, compromised the generalisability of the theory. However, the objective was not to design a new tool to identify leadership at a general level but to ascertain if, through the use of VR and eye-tracking, it was possible to replicate the classification of leadership styles of an explicit measure within a specific population. This goal has been achieved through the use of ML, which provided a predictive classification model. Regarding future directions, this work can serve as the basis for the study of leadership using novel technology, such as VR and ML, and implicit measures. Furthermore, this methodology can be applied for the evaluation of other important aspects of leadership at the cognitive, behavioural, psychological, and social levels. CONCLUSION In this study, VR, implicit measures, and technological methods were used to evaluate three different leadership styles, transformational, transactional, and passive-avoidant. The combination of these methods, consisting of an immersive VR system, eye-tracking, and ML, offers a novel perspective on the study of leadership and the ability to replicate the results of the MLQ. Specifically, a VR environment was used to record the behaviour of each participant. Subsequently, ML was used for the analysis of the large dataset gained from the measurement of eyetracking and decision-making during the VR experience. From this dataset, it was possible to develop different models capable of categorising each participant according to their leadership style. Therefore, the main contribution of this study is that it offers a multi-method approach that enables the capture and analysis of behavioural leadership variables and is able to classify these variables into different leadership styles.\",\"1135746046\":\"1. INTRODUCTION Robots have an immense potential to help people in domains such as education, healthcare, manufacturing, and disaster response. For instance, researchers have designed robots that take steps toward helping children learn a second language , assisting nurses with triage , and participating as part of a search and rescue team. As such robots begin to collaborate with us, we should consider mediating interpersonal or social factors that can affect the outcome of the human-robot team. Methods for incorporating pro-social interpersonal factors like trust, friendliness, engagement, rapport, and comfort, when designed in a way that is appropriate across different contexts, can enable socially assistive robots to develop cooperative relations with their human partners. Trust, in particular, has been shown to facilitate more open communication and information sharing between people. Thus by establishing an appropriate sense of trust, robots may become more effective communicators and thereby increase their capacity to function as collaborative partners. When designing for such interactions, we need to answer how a robot can (1) behave such that humans develop trust toward the robot (i.e., the control signal). But to evaluate the effectiveness of such behavior, we first ask how a robot can (2) evaluate the degree to which an individual trusts the robot (i.e., feedback signal). Considering the development of trust from the perspective of control systems, a robot can continuously adapt its behavior to achieve a desired level of trust through a feedback signal that assesses a person's current level of trust toward the robot. This paper focuses on the development of a system that can infer the degree of trust a human has toward another social agent. For the purposes of the current investigation, we utilize a behavioral operationalization of trust as a willingness to cooperate for mutual gain even at a cost to individual asymmetric gain and even when such behavior leaves one vulnerable to asymmetric losses. In this paper, trusting behavior represents a person's willingness to cooperate with his partner and trustworthiness represents his partner's willingness to cooperate, which the person assesses before potentially engaging in trusting behavior. We present a computational model capable of predicting-above human accuracy--the subsequent trusting or distrusting behavior of an individual toward a novel partner, where these predictions are based on the nonverbal behaviors expressed during their social interaction. We predict trusting behaviors using a machine learning approach. Specifically, we employed supervised learning methods, which present a broad toolset that focuses on accurate prediction of the values of one or more output variables given the values of an input vector. There is certainly much overlap in supervised learning and statistical methods in psychology, with many of the same techniques going by different names, but supervised learning places more emphasis on the accuracy of the learned\\/fit model and permits a wider range of modeling techniques, both probabilistic\\/statistical and otherwise. Our work consists of three major parts and is organized into the following phases: * Trust-Related Nonverbal Cues (Phase 1): We include a summary of our prior work, in which we identify nonverbal cues that, when expressed by a social entity (i.e., humans or expressive robots), are perceived as signals of untrustworthy behavior. * Design of Prediction Model (Phase 2): We use the results from Phase 1 to inform the design of a computational model capable of predicting trust-related outcomes. We compare its prediction performance to a baseline model that is not informed by our Phase 1 results, a random model, an a priori model, and the predictions of human participants. * Temporal Dynamics of Trust (Phase 3): To improve the accu racy of the prediction model developed in Phase 2, we derive additional features that capture the temporal relationships between the trust-related cues. With the addition of these features, our computational model achieves significantly better performance than all the baseline models as well as human judgement. 2. BACKGROUND In situations where a person's past behaviors or reputation are unknown, we rely on other possible sources of information to infer a person's intentions and motivations. Nonverbal behaviors are a source of information about such underlying intentions, goals, and values and have often been explored as \\\"honest\\\" or \\\"leaky\\\" signals. These signals are primitive social signals that are thought to occur largely outside of people's conscious control. Nonverbal behaviors include body language, social touch, facial expressions, eye-gaze patterns, proxemics (i.e., interpersonal distancing), and vocal acoustics such as prosody and tone. Through these nonverbal expressions, we communicate mental states such as thoughts and feelings. Researchers working in domains such as human communication modeling and social signal processing have worked toward modeling and interpreting the meaning behind nonverbal behavioral cues. By observing the nonverbal communication in social interactions, researchers have predicted outcomes of interviews  and the success of negotiations. In other work, by observing head, body, and hand gestures along with auditory cues in speech, a hidden conditional random field model could differentiate whether a person was agreeing in a debate with accuracy above chance. In similar work, Kaliouby and Robinson (2004) used a dynamic Bayesian network model to infer a person's mental state of agreement, disagreement, concentration, interest, or confusion by observing only facial expressions and head movements. Other research has tried to model cognitive states, like frustration , and social relations like influence and dominance among groups of people. However, this article describes the first work toward computationally predicting the trusting behavior of an individual toward a social partner. To the best of our knowledge, trust-recognition systems currently exist only in the context of assessing trust and reputation information among buyers and sellers in online communities. By observing transaction histories, consumer ratings of sellers, and peer-to-peer recommendations, online services like Amazon and eBay utilize these computational models to decide whether an online service or product is trustworthy or reputable in the electronic marketplace. Research on detecting deception has also taken a behavioral approach to computational modeling, focusing on the atypical nonverbal behaviors produced by the cognitive effort of concealing the truth. Although related to the concept of trust, research on deception focuses narrowly on detecting purposeful deception and distinguishing lies from truths, whereas this article focuses more broadly on understanding how much an individual trusts another person in more natural social encounters. We suspect that the absence of work on predicting trust in face-to-face social interaction comes in part from the uncertainty about which nonverbal behaviors contain predictive information for trust-related outcomes. Thus, we began our investigation with a search for social signals that help predict the trustworthiness of an unfamiliar person in a social interaction. 3. TRUST-RELATED SOCIAL SIGNALS (PHASE 1) In this section, we summarize the key findings of our prior work , in which we identified a set of nonverbal cues that is indicative of untrustworthy behavior. We also demonstrated people's readiness to interpret those same cues to infer the trustworthiness of a social humanoid robot. After meeting someone for the time first, we often have a sense of how much we can trust this new person. In our prior work, we observed that when individuals have access to the nonverbal behaviors of their partner in a face-to-face conversation, they are more accurate in predicting their partner's trust-related behavior than when they only have access to verbal information in a webbased chat. Building from this result, we then investigated which specific nonverbal cues signal subsequent trusting or distrusting behavior. We hypothesized that the appearance of multiple nonverbal cues together, observed in the context of each other, would provide such predictive information as opposed to a single \\\"golden cue\\\" offering predictive ability. Rather than assuming a one-to-one correspondence between a specific behavior and its underlying meaning, we viewed the interpretation of nonverbal cues as highly context dependent. That is, by observing multiple cues in close temporal proximity, we gain more meaningful interpretations than by independently assessing the meaning behind a single cue. For example, an eye-roll in conjunction with a large grin can be more accurately interpreted as conveying humor, whereas observing an eye-roll in isolation could lead to the less accurate interpretation of contempt. The interpretation of the eye-roll is contingent upon the observation of the grin. We operationalized this contextual dependency through a mean value of occurrences across a set of nonverbal cues seen within an interaction (i.e., the mean frequency of cues). We identified four nonverbal cues--face touching, arms crossed, leaning backward, and hand touching--that as a set are predictive of lower levels of trust. We found that increased frequency in the joint expression of these cues was directly associated with less trusting behavior, whereas none of these cues offered significant predictive ability when examined in isolation. In our experiments, trusting behavior was measured as participants' exchange action with their partner during an economic game (the game details are in section 4.1.3). Thus, the more frequently an individual expressed these cues, the less trusting was their behavior toward their partner in this game. To confirm these findings, we validated the nonverbal cue set through a human-subjects experiment in which we manipulated the occurrence of nonverbal cues exhibited by a humanoid robot. By utilizing a social robotic platform, we took advantage of its programmable behavior to control exactly which cues were emitted to each participant. Participants engaged in a social conversation with a robot that either (a) expressed neutral conversational gestures throughout the interaction, or (b) replaced some of those gestures with each of the four target cues (shown in Figure 1). As predicted, the robot's expression of the target cues resulted in participants perceiving the robot as a less trustworthy partner, both in terms of participants' self reports as well as their trusting behavior (i.e., game exchange behavior) toward the robot. Thus, when individuals observed these cues from the robot, they trusted their robot partner less. From these human-subjects experiments conducted as part of our prior work , we extract three key findings that serve as guidelines to help inform the design of our computational trust model. 1. There exists a set of four nonverbal cues associated with lower levels of trust: face touching, arms crossed, leaning backward, and hand touching. 2. The joint appearance of cues, represented by their mean frequency, results in a stronger signal that is predictive of trusting FIGURE 1 | A participant engaging in a 10-min conversation with a teleoperated humanoid robot, Nexi, here expressing the low-trust cue of hand touching. outcomes, whereas the cues individually possess limited predictive power. 3. These nonverbal cues predict the trust-related behaviors of an individual that is either expressing or observing them. 4. DESIGN OF PREDICTION MODEL (PHASE 2) In this section, we incorporate the guidelines gained from our human-subjects experiments in Phase 1 into the design of a computational model capable of predicting the degree of trust a person has toward a novel partner, which we will refer to as the \\\"trust model\\\" for brevity. By utilizing this domain knowledge in the feature engineering process, we are able to design a prediction model that outperforms not only a baseline model built in naivete of our prior findings but also outperforms human accuracy. 4.1. MATERIALS (PHASE 2) First we describe the data collection material consisting of the human-subjects experiment, the operationalization of trust through an economic exchange game, and the video-coded annotations of the participants' nonverbal behavior. This data corpus is used to train and evaluate our trust model. We then describe our methods for model design, consisting of our strategies for feature engineering, the nested cross-validation method to estimate the model's true error, and the learning algorithm and model representation selected to create our prediction model. 4.1.1. Data collection material We leverage the pre-existing datasets from our human-subjects experiments in which the task scenario involved two participants interacting for 5-min and then asked to make trust judgements of one another. A total of 56 interaction pairs or 112 people participated in these studies. Some of this data (20 interaction pairs) originated from the human-human study described in Phase 1, and the remaining are from a separate study. The pool of participants was undergraduates attending Northeastern University in Boston, Massachusetts. 31% of the participants were male and 69% were female. The data collection materials included the raw videos of the human-subjects experiments, video-coded annotations of the participants' nonverbal behaviors, and trust measurements obtained through the Give-Some Game described in section 4.1.3. 4.1.2. Human-subjects experiment The experiments consisted of two parts. Participants first engaged in a 5-min \\\"get-to-know-you\\\" interaction with another random participant (whom they did not know prior to the experiment). This part of the study was held in a quiet room, where participants were seated at a table as shown in Figure 2. The participants were encouraged to discuss anything other than the experiment itself. To facilitate conversation, topic suggestions such as \\\"Where are you from?\\\" and \\\"What do you like about the city?\\\" were placed upon the table on slips of paper. Around the room, three time-synced cameras captured the frontal-view of each participant along with a side-view of the participants (the perspective shown in Figure 2). For the second half of the experiment, the interaction partners played the Give-Some Game, explained below. The participants were not told that they would play this cooperative game with their conversational partner until after the \\\"get-to-know-you\\\" period was over. 4.1.3. Operationalization of trust A participant's judgement of trust toward their novel partner was behaviorally measured through the Give-Some Game. The Give-Some Game is similar to a traditional Prisoner's Dilemma game in that it represents a choice between self-interested behavior and cooperative behavior. At the game's start, each player possesses four tokens. Each token is worth $1 to the player and $2 in the possession of their partner. Each player decides how many of the four tokens to give to their partner, keeping the remaining tokens for themself. For maximum individual payoff, a player must keep all four tokens. This strategy ensures that the player receives at least $4 (while giving nothing to their partner); anything they receive from their partner would further increase their payoff. For maximum communal benefit, both of the players would need to give away all four tokens to the other, resulting in each player earning $8. The participants are separated into different rooms to prevent the communication of strategies. To limit apprehension about having to face a partner to whom they were selfish, participants are also told that they will not see each other again. Although the game is played individually, the outcome (the money a player wins) depends on the decisions made by both the players in the game. In the game, players are asked to: 1. Decide how many tokens they want to give to their partner. 2. Predict how many tokens they believe their partner will offer them. In this article, we consider the number of tokens a participant gives to represent how much they trust their partner to play cooperatively. In addition, we consider the discrepancy between the predicted and the actual number of tokens received to represent how accurately people can judge the trustworthiness of a novel partner after a short interaction; this served as the human baseline which will be used in section 4.3.1. Rather than assessing purely economic decision making, interpersonal or social exchange games like the Give-Some Game have been shown by social psychologists to involve evaluations of a partner's personality, including his or her trustworthiness. For example, Fetchenhauer and Dunning (2012) had participants complete a monetary exchange game (where participants were given $5 and could gamble it for the chance to win $10). Half of the participants completed a social version of the game where they were told they were playing with another person, and their chances of interacting with a trustworthy person was 46%. The other half were told they were playing a lottery game (therefore, not involving any people) with the same probability of winning. Participants' behaviors were found to differ significantly in the social version of the game compared to the lottery version. That is, while participants made choices in the lottery that largely reflected the low probability of winning (46%), participants in the social version of the game largely ignored the given base rates (i.e., the likelihood one would encounter a trustworthy person) when making the same economic exchange decision and instead made far riskier choices. Thus, participants believed that people would behave differently than a purely probabilistic lottery, and notably, that people would be more trustworthy than stated. These findings suggest that behavior in social exchange games does not reflect purely economic decision making, but includes assessments or judgements of the other person involved, including their trustworthiness. Further supporting our operationalization of trust, data from the past validation experiment in Phase 1 demonstrated that a self-report measure of how much a participant trusted a robot partner was significantly positively correlated with how many tokens that participant decided to give the robot [r(75) = 0.26, p < 0.05]. 4.1.4. Video-coded annotations The nonverbal behaviors of the participants over the entire 5-min interaction were manually coded. The videos were coded independently by at least two coders who were blind to all hypotheses (average inter-rater reliability: r = 0.90). Given the high interrater agreement, data from only one coder is used for analysis. We coded for nonverbal behaviors that appeared frequently (by at least five participants) in the video-taped interactions. The start and stop times of the following behaviors were coded for each participant: smiling, laughing, not smiling, leaning forward, leaning backward, not leaning, eye contact, looking away, arms crossed, arms open, arms in lap, arms in conversational gesture, arms on table, hair touching, face touching, hand touching, body touching, no touching, head shaking, head nodding, and head still (see visualization and behavior categories in Figure 3). Coders were instructed to code the video for only one participant and only one nonverbal category at a time. 4.2. METHODS FOR MODEL DESIGN (PHASE 2) We employ two feature engineering strategies to find a subset of features that permits effective learning and prediction. After describing our feature engineering process, we detail the training and testing procedures used for model selection and model assessment. These procedures are chosen to assess the differences in the predictive power of a model that builds a subset of features using domain knowledge as opposed to another model that narrows its selection using a popular feature-selection algorithm. 4.2.1. Feature engineering Feature engineering encompasses both extracting features that are believed to be informative and selecting an effective subset from amongst the extracted features. Feature selection is the choosing of useful features that are not redundant or irrelevant to create a predictive model with high accuracy and low risk for overfitting (i.e., high generalizability). Domingos (2012) points out that \\\"feature engineering is more difficult because it's domainspecific, while [machine] learners can be largely general-purpose. . . the most useful learners are those that facilitate incorporating knowledge.\\\" We detail the initial full set of features that were extracted from our trust corpus, and we compare two strategies to narrow our selection of features to an effective subset. We first create a model that uses features chosen through a standard feature-selection technique called variable ranking. Leveraging our findings from Phase 1, we create another model that narrows and then extends the initial set of features by following the three guidelines listed in section 3. 4.2.1.1. Feature extraction. From the video-coded nonverbal annotations, we determined how many times a participant emitted a particular cue during the interaction (e.g., 25 smile instances) and how long a participant held the cue through the duration of the interaction (e.g., for 5% of the interaction, the participant was smiling). The duration of a gesture provides additional information and has been used as a feature in other work. For instance, if a participant crosses their arms throughout the entire interaction, then the frequency of that gesture would register as just one arms crossed. The duration measure--but not the frequency measure--reflects the prevalence of arms crossed in this scenario. The full set of features (42 in total) consists of the frequency and duration for each of the 21 nonverbal cues. 4.2.1.2. Feature selection. Variable ranking is a feature-selection method commonly used in practice; this method's popularity can be attributed in part to its simplicity and scalability. Variable ranking involves independent evaluation of each candidate feature. As a consequence, it may choose redundant features and does not consider the predictive power of complementary features. Nonetheless, this method has had many reports of success in prior work. In this research, variable ranking scores each of the 42 features by the absolute value of its Pearson's correlation coefficient with respect to the number of tokens given; we then incorporate only the most highly ranked features in the trust model. In addition to the variable-ranking model, we build a second predictive model using the domain knowledge gained from our prior work. The three guidelines derived from Phase 1 inform this model's feature selection. First, we considered the frequency and the duration of only the four trust-related cues--face touching, arms crossed, leaning backward, and hand touching--expressed by the participant (feature x1 to x8 in Table 1) and ignored other nonverbal behaviors. Secondly, since cues are more informative in their joint appearance, the model also uses as features the mean frequency and mean duration across the four trust-related cues (x9 and x10). Finally, the model draws features from the nonverbal cues not only of the participant but of the participant's partner as well. The model therefore includes the partner's gestural frequencies, durations, and mean cues (features x to x20); the model also includes the differences between the participant's and their partner's features to incorporate any interesting differences between their behaviors (features x to x30). These 30 features (listed in Table 1) represent the incorporation of domain-specific knowledge in the selection process. In absence of this knowledge, the alternative method of selection is to narrow the number of features using variable ranking. The model that uses this domain-knowledge selection method will be referred to as the domain-knowledge model, while its naive counterpart will be referred to as the standard-selection model. 4.2.2. Prediction model We aim to demonstrate the effect of incorporating domainknowledge in the feature-selection process on the performance of a prediction model. As such, rather than exploring and comparing the predictive accuracies of various machine learning algorithms, we focus on support vector machines (SVMs) as the primary tool for our feature-focused investigation. SVMs were chosen for their wide use and prior success in modeling human behavior. SVMs separate training examples into their classes using optimal hyperplanes with maximized margins; for a query, an SVM predicts based on which side of the class boundary the query example lies. To find effective separations, the training examples are transformed from their original finite-dimensional space into a higher dimensional feature space by a non-linear mapping function called a kernel. Each of the m = 112 training examples in our dataset (one per participant) contains a vector of n = 30 features,--x = (x1, x2, * * * , xn). Features were scaled to have values within [-1,+1], preventing an over-reliance of the SVM on large-ranged features. The class label for an example is the number of tokens the participant gave their partner in the Give-Some Game, y e {0, 1, 2, 3, 4}. With our dataset {(--x 1, y1), (--x 2, y2), * * * , (--x m, ym)}, we train and test our SVM model with a Gaussian kernel using the LIBSVM library. 4.2.3. Nested cross validation To estimate the true prediction error of a model selection process, we employ a nested cross-validation method. A challenge when modeling human behavior is collecting and annotating enough real-world data to partition into three substantial training, validation, and testing sets. The training set is used to fit the models. For model selection, the validation set is used in tuning the model parameters to yield the lowest prediction error. For model assessment, the chosen model's prediction error (also called generalization error or true error) is estimated using the previously unseen testing set. In cases such as ours, when the sample size is small (m = 112), the method of cross validation (CV) is often used to estimate prediction error by partitioning the dataset into subsets, and in multiple rounds, each subset acts as the validation or testing set (depending on the analysis) while the remaining is used as the training set. One benefit of using cross validation is that the model can be trained from almost the whole dataset. When using CV for both model selection and model assessment, one has to be careful that data involved in the model selection process is not reused in the final assessment of the classifier, which would occur in the case of first cross validating for model selection and then cross validating again with the same data for model assessment. When such reuse occurs, re-substitution error can falsely lower the estimate of true error. We avoid such misleading results by conducting nested CV to obtain an almost unbiased estimate of the true error expected on an independent dataset. We evaluate the trust models through leave-one-out nested CV and follow the nested implementation as described by Varma and Simon (2006). The leave-one-out nested CV method includes an inner loop for model selection and an outer loop for model assessment. In each iteration of the outer loop, a training example is removed. With the remaining examples, the best choices for hyper-parameters and features (via model selection) are determined and used to create a classifier. The resulting classifier then predicts the class of the \\\"left out\\\" example, resulting in some error. This process is repeated such that each training example is left out once. Prediction errors are then accumulated for a final mean prediction error (MPE) of the estimator. The MPE is calculated as the average absolute difference between the classifier's predictions and the true class labels. Of note, the nested CV process estimates the MPE of classifiers learned at every iteration of the outer loop. This provides a performance measure of an estimator (i.e., a learning algorithm) and not of a particular estimate (i.e., a single classifier). In general, the inner loop performs both feature selection and hyper-parameter tuning for model selection using the remaining data from the outer loop. But in our case, for the variableranking selection method (described in section 4.2.1.2), a subset of features is found before the inner loop, which then conducts CV for hyper-parameter tuning. More specifically, for our SVM models, the inner loop tunes the parameters by varying the values of the model's hyper-parameters C and g according to a grid search and chooses the values with the best prediction error, where the error is calculated by a leave-one-out CV. The cost parameter C balances the tradeoff between obtaining a low classification error on training examples and learning a large class-boundary margin, which reduces over-fitting to the training examples. In general, increasing the value of C reduces training error and risks over-fitting. The bandwidth parameter g controls the size of the Gaussian kernel's radius, which in effect determines the smoothness of the boundary contours. Large values of g create a less smooth boundary (i.e., higher variance), which can lead to over-fitting, whereas small values create smoother boundaries, which can lead to under-fitting. A pseudo-algorithm detailing the exact steps of our entire procedure is available in the Appendix. Although cross-validation methods are a standard alternative for estimating prediction error when sample sizes are small, they have some limitations. In particular, leave-one-out CV can result in high variance estimates of the prediction error since the testing partition contains only one example. But in utilizing almost the whole dataset for training, the method is also regarded in achieving low biases. In contrast to holdout methods (when a substantial independent test set is available), nested cross-validation does not provide an estimate of the true prediction error of a particular classifier (i.e., a single trust model) but instead reports on the average performance of classifiers built from different partitions of the data (in our case, the average performance of trust models are trained on data sets that each differ by one example). 4.3. RESULTS AND DISCUSSION (PHASE 2) Here we discuss the prediction performance of the two computational models: the domain-knowledge model (SVM-D) and the standard-selection model (SVM-S). We then compare these models to a random model, an a priori model (i.e., a model that always predicts the most common class), and a human baseline. 4.3.1. Results Through leave-one-out nested CV, the SVM-D model is estimated to have a MPE of 0.74. SVM-D's hyper-parameters have values of either [C: 8, g: 0.031] or [C: 2, g: 0.125] at different iterations of the outer loop (i.e., across the CV folds for model assessment). The SVM-S model is estimated to have a MPE of 1.00, and its hyper-parameters vary more than the SVM-D model (see Figure A1 in the Appendix for hyper-parameter plots). We statistically assess whether the prediction errors between classifiers are different through the Wilcoxon's Signed-Rank test. Since we compare the performance of the SVM-D model to that of the SVM-S model, random model, a priori model, and a human baseline--resulting in four statistical tests--we counteract the increased probability of Type I error (i.e., claiming a difference where there is none) by adjusting the significance level to a = 0.05 4 = 0.0125, as per the Bonferroni correction. According to this statistical test, our SVM-D model significantly outperforms the SVM-S model (see Table 2). In comparing to other baselines, we also found the SVM-D model to significantly outperform a random model, which uniformly guesses either 0, 1, 2, 3, or 4 tokens. In Table 2, the \\\"human\\\" category is not actually a model but rather the participants' predictions of how many tokens their partner will give them in the Give-Some Game (as mentioned previously in section 4.1.3). These predictions served as a human baseline for how accurately people can perceive the trustworthiness of a stranger after a 5-min interaction. The SVM-D model significantly outperforms the human predictions. An a priori classifier ignores the nonverbal information from the features but knows the class distribution (i.e., distribution of tokens given, as shown in Figure 4). The a priori model always predicts the class with the lowest mean error: two tokens given. The SVM-D model outperforms the a priori model but not with statistical significance. 4.3.2. Discussion By incorporating the guidelines derived from Phase 1 into the feature engineering process, we designed a prediction model (SVM-D) that outperformed not only the model built in naivete of those guidelines (SVM-S) but also outperformed human judgement. Of note, participants gave an average of 2.58 tokens yet predicted that they would receive an average of 2.28 tokens. We believe this bias toward predicting a less generous return contributed to the error in the human predictions. Thus the SVM-D, SVM-S, and the a priori model all have an added advantage of being biased toward the majority class. Our SVM-D model outperformed the a priori model, which ignores nonverbal behavior data. However, the difference was not significant, and so we cannot yet say with confidence that the nonverbal data improved the prediction performance of our modeling algorithm. By identifying where our SVM-D model is making prediction errors, we can aim to find additional features that can help discriminate between the examples the model is most confused about. According to SVM-D's confusion matrix (Table 3), the model has difficulty distinguishing when an individual has a higher degree of trust toward their partner. For people who gave four tokens, the model generally predicts that two tokens will be given, contributing 55% of the total prediction error. We therefore seek to improve upon the SVM-D model in Phase 3 by deriving new features that can help differentiate individuals with greater levels of trust toward their partners, ultimately resulting in significantly more accurate predictions than the a priori model. 5. TEMPORAL DYNAMICS OF TRUST (PHASE 3) In this final phase, we improve upon the accuracy of the prediction model developed in Phase 2 by deriving additional sequencebased temporal features. In the last part of our investigation, the model built with features chosen through domain knowledge (SVM-D) outperformed a model unaware of this knowledge (SVM-S). Additionally, SVM-D was more accurate than the a priori model, but not significantly so. To improve the performance of our SVM-D model, we again turn to domain knowledge to guide our search for new features. As mentioned in Phase 1, we hypothesized that the appearance of multiple nonverbal cues together, observed in the context of each other, would provide reliable information about trusting outcomes. We operationalized this contextual joint appearance of the trust-related nonverbal cues through their mean frequency and mean duration as features for our trust model (described in section 4.2.1.2). These mean values attempt to roughly capture the temporal proximity of the trust-related cues occurring within a social interaction. We extend our hypothesis to another form of operationalizing \\\"context\\\" through the sequence of emitted cues. We anticipate that the contextual information given in the sequence of trust-related nonverbal cues contains predictive information not captured by the cues' mean frequencies or durations. Furthermore, rather than only observing the sequence of the four nonverbal cues associated with lower levels of trust (low-trust cues), we aim to observe their interplay with nonverbal cues associated with higher levels of trust (high-trust cues), since we anticipate more discriminating patterns to emerge from the dynamic of both high-trust and low-trust cues. In Phase 3, we first describe the redesign of our experiment, from which we identify a set of high-trust nonverbal cues. We then present the construction of hidden Markov models (HMMs) to find temporal relationships among the new high-trust cues and the previously identified low-trust cues. From this temporal model, we derive additional features that improve upon the performance of SVM-D from Phase 2. 5.1. MATERIALS (PHASE 3) In this subsection, we describe the additional human-subjects experiment and analysis performed to identify a set of high-trust nonverbal cues. 5.1.1. Human-subjects experiment redesign We redesigned our previous human-subjects experiment (detailed in section 4.1) with one key difference: after the social interaction and before the game, the participants were told that the average number of tokens given is one. However, participants were also told that the average may not be indicative of what their particular partner would do and were encouraged to let their social interaction guide their predictions. In the experiments that produced our original data (section 4.1), participants decided how many tokens to give their partner without any prior knowledge of the expected or \\\"normal\\\" giving behavior. These participants may have varied in their belief of how many tokens are thought to be unfairly low, fair, and overgenerously high. By introducing this manipulated information about the average tokens given, we aim to shift the expectation of token-giving behavior such that individuals expect the mean level of giving to be lower (i.e., closer to one token on average compared to 2.28 tokens on average from the previous experiment). This manipulation had two intended outcomes. First, we aim to lessen participants' variation in their beliefs of normal giving behaviors--a potential source of noise in our original data-and thereby increase the likelihood of a goodness-of-fit in how well certain nonverbal cues can predict token-giving outcomes. Second, by biasing participants toward a lower norm of one token, we aim to shift \\\"norm-followers\\\" away from higher levels of giving. Norm-followers are those without preference in wanting to give more or less to their partner and therefore give according to the behavioral norm. We speculate that participants that then give more than one token are most likely those trusting their partners to play cooperatively and therefore willing to deviate from the established norm. Having shifted the norm-followers away from higher levels of giving, we then expect to have more homogeneity in the variance among the nonverbal behaviors that predict higher levels of trust. Through this manipulation, we anticipate to find a set of nonverbal cues that are significant predictors of trusting behavior in a positive direction, which we were unable to identify with the original dataset from Phase 2. A total of 16 interaction pairs (i.e., 32 participants), again undergraduate students at Northeastern University, participated in this redesigned study (41% male and 59% female). The two independent coders of the videos for this new study were also blind to all hypotheses, including any knowledge of our previous findings. Each coder coded half of the videos from the study. To establish inter-rater reliability, each coder also coded a subset of the videos originally coded by the other independent coder (r = 0.93). 5.1.2. Identifying high-trust nonverbal cues To identify a set of nonverbal cues that are indicative of higher levels of trust, we employ the same procedure from our prior work. We briefly outline the procedure here. We begin by examining the zero-order correlation between the frequency of a nonverbal cue emitted and the amount of tokens given. We identify which of the 22 nonverbal cues (i.e., those listed in section 4.1.4 with the addition of hand gesturing) positively correlate with the number of tokens given. None of the correlations were both positive and significant individually, so we again considered sets of cues. We chose candidate sets through an ad hoc examination of correlation coefficients and p-values, and we tested their joint predictive ability through a multilevel regression analysis that controls for dyadic dependencies. As we hypothesized, we were able to find a set of cues--leaning forward, smiling, arms in lap, and arms open--that positively and significantly predicted token-giving outcomes in the Give-Some Game ( = 0.11, p < 0.04). Concluding that this cue set is indeed predictive of higher levels of trust would require experimental manipulation (as in the robotic experiment described in Phase 1) to confirm that this relationship is not merely the result of spurious correlation. But our primary interest is deriving new features that capture temporal relationships between particular cues. We therefore continue to refer the set of cues--leaning forward, smiling, arms in lap, and arms open--as indicative of higher levels of trust. 5.2. METHODS FOR MODEL DESIGN (PHASE 3) Suspecting that the sequence of nonverbal cues contains predictive information not captured by the cues' mean frequencies or durations, we built a temporal model, capable of modeling processes over time. More specifically, we constructed HMMs to capture the temporal relationship between the newly identified high-trust cues and the previously identified low-trust cues using the original dataset from Phase 2 (section 4.1). Of note, the data from the redesigned experiment (section 5.1.1) is only used for the identification of the high-trust cues. HMMs are common representations for temporal pattern recognition. However, HMMs are commonly viewed as having low interpretability; the model's internal complexity hinders any qualitative understanding of the relationship between the input and predicted outcomes. Although often treated as a black box technique, HMMs are capable of finding structures that reveal interesting patterns in the data. Our technique described below demonstrates one method for leveraging an HMM's learned structure to derive new features. 5.2.1. Temporal model In applications that have a temporal progression, HMMs represent a sequence of observations in term of a network of hidden states, from which observations are probabilistically created. State st at time t is drawn probabilistically from a distribution conditioned on the previous state st-1, and an observation ot probabilistically conditioned on the current state st. Thus, to model a temporal process by an HMM, three probability distributions must be learned: over initial states, P(s0); over state transitions, P(st |st-1); and over observations, P(ot |st). In this work, the parameters for these distributions are iteratively adjusted by expectation maximization  to maximize the likelihood of the observation sequences in the data given the parameterized model. Possible observations were the eight highand low-trust cues: smiling, leaning forward, leaning backward, hand touching, face touching, arms open, arms crossed, and arms in lap. Based on cues' coded start times, we extracted the sequence of only these eight gestures for each participant during their 5-min interaction. The sequence length per participant varied (min = 9, max = 87), since some individuals gesticulated these cues more often than others. Once the initial state, state transition, and state observation probabilities are selected, a trained HMM can generate a sample sequence of observations. The simulation first generates a sample state path (i.e., Markov chain) of a given length by selecting an initial state drawn from the initial state probability distribution and selecting subsequent states based on the transition probabilities. Given a sample state path, an observation is drawn based on each of the states' observation probabilities to then form a sample sequence of observations. When a model has many states, transition paths, and possible observations per state, deciphering the meaning of a state and its role in the state network is especially difficult. However, by simulating a trained HMM, we can qualitatively examine the generated observation sequence for informative patterns. To find discriminative patterns, we trained one HMMlow from participants that gave two tokens away and another HMMhigh from participants that gave four tokens away and then searched for differences in their samplesequencesofemittednonverbalcues.Twotokenswaschosen for HMMlow because few participants gave 0 or 1 token (leading to insufficient data for training). By comparing a sample sequence of observations from HMMlow and HMMhigh, we can discover any informative distinctions between their simulated outputs. For example, we could observe certain patterns of nonverbal cues that appear in succession in HMMlow's output that do not appear in HMMhigh's. We can then use these unique and differentiating patterns to construct new features. 5.2.2. Leave-one-out cross validation We determined the best model parameters for HMMlow and HMMhigh via leave-one-out CV. A nested method was unnecessary, since we aimed only to draw insight from the trained model and not to estimate the true prediction error. From our original dataset from section 4.1, we have 26 training examples for HMMhigh and 46 training examples for HMMlow. We ran 6000 simulations using the Bayes Net Toolbox. At every run we randomly initialized (drawn from a uniform distribution) the number of states and the initial state, transition, and observation probabilities for both of the HMMs. To determine the prediction performance of the models' parameters, we use a leave-one-out CV method, where we leave the data from one participant out and train on the remaining 71 participants for the two HMMs. The omitted example is classified as either high or low trust, determined by which of HMMlow or HMMhigh has a higher log-likelihood of creating the observations in the example. 5.3. RESULTS AND DISCUSSION (PHASE 3) Below we interpret the resulting learned structure of our HMMs and discuss the ability of the newly derived temporal features to improve on the prediction accuracy of the SVM-D model from Phase 2. 5.3.1. Results Through leave-one-out CV, our best model training result, with three states for HMMlow and five states for HMMhigh, has a recognition accuracy of 71% (51 hits and 21 misses) compared to 64% at chance due to the uneven distributions. Our goal, however, is not discriminative accuracy of these generative HMM models but rather to identify what interesting temporal patterns they capture. By simulating the trained HMMhigh, we get an output observation of: smiling - smiling - face touch- smiling - smiling hand touch- arms in lap- arms crossed- arms in lap * * * And by simulating HMMlow, we get an output observation of: smiling - arms crossed- face touch- hand touchsmiling - face touch- arms crossed- smiling - smiling * * * To make the pattern easier to decipher, we denote (+) as hightrust and (-) as low-trust cues to form: HMMhigh = ++-++-+-+ * * * HMMlow = +---+--++ * * * Both models alternate between high-trust and low-trust cues. HMMlow's sequence contains frequent consecutive low-trust cues, whereas HMMhigh's sequence contains more consecutive hightrust cues. As posited previously, these observation sequences suggest that the order of nonverbal cues may provide further information for predicting trust-related outcomes. We use these findings to derive new features for our prediction model. Since SVMs do not naturally capture temporal dynamics, we represent the ordering of low- and high-trust gestures emitted using encoding templates. That is, by stepping through the sequence, we count the number of times (i.e., the frequency) in which we observe the following templates: * low-trust { , , , } * high-trust { , , , } With a sliding window of three cues, these templates in essence profile the neighboring cues (ones right before and right after a particular cue). We considered adding the following new feature-types for our trust model: 1) high-trust templates 2) high-trust cues 3) lowtrust templates. When adding the frequencies of the high-trust templates as features to our model, the MPE increased to 0.80 as compared to the previous SVM-D's MPE of 0.74 in Phase 2. When instead adding the features (that are analogous to the ones listed in Table 1) of the four high-trust nonverbal cues-leaning forward, smiling, arms in lap, and arms open--the model again increased in error with an MPE of 0.83. We therefore did not include the high-trust templates nor the four high-trust nonverbal cues into the final selection of features for our trust model. We created 12 new features, consisting of the frequencies in which the low-trust templates are emitted by the participant, their partner, and the difference in frequency between them (shown in Table 4). Through the inclusion of the low-trust template features toward the training of a final model, our new trust model achieves an overall MPE of 0.71, which now significantly outperforms the a priori model (see Table 5). Our final trust model consists of 42 features listed in Tables 1, 4. To better understand the contribution of different components of trust signals, we performed an additional analysis to study the effects of removing particular categories of features on the trust model's performance. As shown in Figure 5, the MPE increases when excluding certain categories of features. When removing duration-type features (features x5 * * * x8, x * * * x, and x * * * x listed in Table 1) from the full set of 42 features, the trust model's performance is most heavily effected. This suggests that the duration, or prevalence, of a gesture provides important information for the trust model. Interestingly, removing information about the partner's nonverbal behaviors has greater effects than removing the behavioral information of the individual whose trusting behavior we are trying to predict. This may suggest that when predicting the trusting behaviors of an individual, rather than directly observing their nonverbal behavior for \\\"honest\\\" or \\\"leaky\\\" signals, it is more informative to observe their partner whose behaviors greatly influence the individual's decision to trust. 5.3.2. Discussion The inability of the high-trust cues and templates to enhance the predictive power of our model is not surprising in light of the differences between the study from which the high-trust cues were identified and the original studies whose data is used to train and test our model. The experiments that provided our original data collection material (in section 4.1) were conducted in a friendly and prosocial context; as shown in Figure 4, the number of tokens participants tended to give away fell on the high or trusting end of the distribution. When the default expectation is cooperation (i.e., most participants give away high numbers of tokens), then those that deviate from this expectation are most likely the participants that did not trust their partner to play cooperatively; this scenario is the direct opposite of the one described in section 5.1.1. Thus, it is not surprising that the nonverbal cues we identified as being most predictive in these experiments were negative predictors related to lower levels of giving. In our context where the behavioral norm is for people to be more cooperative or trusting, the high-trust cues and templates lose predictive power. In line with what we observed, when adding the high-trust cues and templates as potential features, the trust model's predictive performance decreased. 5.4. GENERAL DISCUSSION Our research sought to answer how a robot can (1) behave such that humans develop trust toward the robot (i.e., the control signal) and (2) evaluate the degree to which an individual trusts the robot (i.e., feedback signal). Our prior work in Phase 1 demonstrated the capacity of a social humanoid robot to influence a person's trust toward the robot through either the presence or the absence of specific nonverbal gestures. Our current work in Phases 2 and 3 demonstrates the capacity of a computational trust model to evaluate the degree to which a person trusts another social agent. An important next step is to answer how a robot can dynamically adapt its behavior to achieve or maintain a desired level of trust based on its continual reading and interpretation of a person's current level of trust toward the robot. However, before the computational trust model will readily work for a social robot in a wide range of real-world interactions, the model's current limitations will need to be addressed. We discuss three limiting dependencies of the model below. In its current implementation, the model relies on hand-coded annotations of the nonverbal behaviors. For a robot to determine how much an individual trusts the robot, it will need to recognize these gestures autonomously. To model these gestures, 3D motion capture technology (like the Microsoft Kinect) can track the body movements of people, and gesture recognition algorithms can detect when particular nonverbal cues are being expressed. This low-level gesture recognition system can then feed into a highlevel trust recognition system, which will be primarily driven by the trust model [see Lee (2011) for an initial framework]. Secondly, the trust model relies on the behavioral operationalization of trust through the Give-Some Game. A difficult but important question to consider is the game's ability to measure real-world interpersonal trust. In section 4.1.3, we provided support that behavioral trust games like the Give-Some Game do not seem to purely assess economic decision making but instead involve social evaluations of other players. We also found that subjective measures of trust (via self report) were significantly positively correlated with participants' monetary decisions in the Give-Some Game. This suggests that the Give-Some Game is capturing behavior that is related to trust behaviors in the real world, and thus we expect that the current model can generalize to predict other measures of trust or trusting behavior, particularly in situations similar to those in our experiments. However, future studies exploring how cue selection will be altered by changes in context (e.g., in situations where the default expectation is for others to be untrustworthy) will be necessary to expand the predictive ability of the current model to new contexts. Lastly, the model relies on the contextual constraints that are implicit to the laboratory setting. The data gathered in these experiments was based on undergraduate students around the age of 18-22 attending Northeastern University in Boston, MA. The participants met unfamiliar partners (a fellow student affiliated with the same school), in a lab space (not a natural social setting), and for a short 5-min conversation. Given that the interpretation of nonverbal cues is highly context dependent, factors such as age, culture, group membership, and social environment, which are largely specified in the lab setting used in our experiments, can influence how an individual interprets trustrelated social signals. The trust model is context dependent in that it has no information about these factors in its representation. Therefore, the model performs accurately when making trust judgements in the setting in which its training data originated. If we were to use this model to determine how much an interviewer trusted an interviewee, we can anticipate a drop in performance. However a model that incorporates contextual knowledge as variables can generalize to a greater variety of situations. Similarly, by incorporating other communication modalities such as facial expression, prosody, and verbal semantics, the model's predictive accuracy will most likely improve. 6. CONCLUSION We developed a computational model capable of predicting the degree of trust a person has toward a novel partner--as operationalized by the number of tokens given in the Give-Some Game--by observing the trust-related nonverbal cues expressed in their social interaction. Our work began with a study to demonstrate that when people have access to the nonverbal cues of their partner, they are more accurate in their assessment of their partner's trustworthiness. Confident that there is trust-related information conveyed through nonverbal cues, we furthered our investigation to identify and confirm a set of four nonverbal cues indicative of lower levels of trust and demonstrated people's readiness to interpret these same cues to infer the trustworthiness of a social humanoid robot. Through these studies, we drew three important guidelines that informed the design of our computational trust model. We demonstrated that by utilizing this domain knowledge in the feature engineering process we could design a prediction model that outperforms a baseline model built in naivete of this knowledge. We highlighted the importance of representing an individual's nonverbal dynamics, i.e., the relationship and influence of behaviors expressed sequentially by an individual. We represent the context within which an individual's nonverbal behaviors appear in two ways. The temporal dependency was first operationalized through the mean value of occurrence of the trust-related nonverbal cues expressed within an interaction. Then in finer temporal granularity, we investigated the sequential interplay of low-trust and high-trust nonverbal cues through the construction and simulation of hidden Markov models. Through the inclusion of new sequence-based temporal features, our computational trust model achieves a prediction performance that is significantly better than our baseline models and more accurate than human judgement. Our multi-step research process combined the strength of experimental manipulation and machine learning to not only design a computational trust model but also to deepen our understanding about the dynamics of interpersonal trust. Through experimental design and hypothesis testing, we were able to narrow the wide field of variables to consider for our prediction model to the empirically found trust-related nonverbal cues. And by constructing a machine learning model capable of revealing temporal patterns, we discovered that the sequence of nonverbal cues a person emits provides further indications of their trust orientation toward their partner. This intersection of methodologies from social psychology and artificial intelligence research provides evidence of the usefulness of interdisciplinary techniques that push and pull each other to advance our scientific understanding of interpersonal trust.\",\"1135746073\":\"INTRODUCTION Social Capital and Neighborhood Physical Environment Social capital in neighborhoods and local communities have been gaining attention of researchers from various fields since studies have suggested effects of social capital on various aspects of residents' daily life over the last couple of decades. Research on public health is one such field, which focused on positive effects of social capital on people in local communities from the start and has since been increasing based on quantitative data. However, associations between social capital and man-made infrastructure or 'built environment,' such as local facilities, walkways, and greenery, have been investigated by a limited number of studies , though neighborhood physical environment seems to be an important factor in discussions on social capital. Previous studies suggested significant associations between elements of social capital and some aspects of neighborhood physical environment. A systematic review by Mazumdar et al. (2018) on articles published until 2015, indicated significant positive relations between social cohesion and walkability or access to local destinations such as libraries. As for greenery in the neighborhood, some recent studies argued for its positive relations with social cohesion and safety. Hong et al. (2018) analyzed survey data of the elderly living in Seattle-King County and Baltimore-Washington DC regions in the US. They showed significant positive associations between subjectively reported natural sights, social cohesion and interaction, and similarly between pedestrian safety and social capital, controlling for demographic variables. Moreover, people felt safe, comfortable, and friendly amidst natural greenery, which contrasted with the atmosphere from streetscapes, such as maintained parks or buildings. Heinze et al. (2018) demonstrated the intervention effect of maintaining neighborhood greenery by local people on violent crimes in their neighborhood through a five-year longitudinal survey conducted in Flint in the US. Their analysis statistically controlled for demographic data and the number of violent crimes in the previous year, and results indicated that vacant neighborhood lots maintained by community members in the local government's program had less violent crimes than abandoned lots. Although this case study focused on collective interventions in the neighborhood's physical environment, only a limited number of such studies discussed interrelations between social capital and physical environment. Furthermore, previous studies mainly targeted urban or suburban areas and were conducted mostly in the US or Europe. Investigation on Physical Environment Using Google Street View Since mid-2010, automated image recognition through Google Street View (GSV;) has been applied for various purposes, allowing the processing of a huge number of images, resulting in strong consistency in results. On the GSV application first released in the US in 2007, users can see interactive and sequential 360-degree panorama views from roads, which are integrated with map data. These images were usually taken in daylight at intervals of several meters using a camera mounted on top of cars, which was approximately two meters from the ground. However, GSV has limitations in that the update frequency and density of images varies depending on the region or district. In terms of research practice, several studies examined the validity of virtual auditing objects on GSV, which is sometimes termed as virtual systematic social observation. Queralt et al. (2021) compared virtual audit on GSV with physical observation in cities of five different countries. They showed that levels of concordance for streetscape elements seemed to be high enough between those observations, though physical disorders (e.g., trash) and aesthetics (e.g., trees) tended to be at relatively lower levels of concordance, compared with buildings and roads. Additionally, audit on GSV also seems to have enough validity compared to observations by local residents, especially for aesthetics such as roadside trees, plantings, and other features of landscape on the roads.1 In summary, GSV provides the opportunity to virtually audit streetscapes in any area where GSV imagery data is available, instead of in-person surveillance which was the conventional observational method for streetscapes. Automated image recognition through GSV has not only been employed to develop alternatives of existing observation methods, such as assessment of infrastructure condition  or mapping certain species of trees or crops , but also used to capture features of certain areas from elements in their streetscapes. Especially, the greenery and walkability of neighborhoods have been the focus of several studies for the purpose of exploring influences of these physical environments on residents' health. Those studies suggested that aesthetics and well-kept streets might encourage people to walk for health benefits, which seemed to be intuitive. Observations of greenery and walkability by automated methods through GSV could also reflect social inequality between areas , and be positively related with the vitality of neighborhood organizations (e.g., sports clubs) and strength of social networks in local communities. While those findings were based on hypothesis-driven methodologies, associations between greenery or walkability and social conditions of neighborhood (including social capital) can be also uncovered by data-driven methodologies, which could be feasible through GSV and machine learning. Zhang et al. (2018) conducted an online survey where 81,630 participants chose one image from a pair of GSV images that they thought had more features of each of six perceptual indicators (i.e., safe, lively, beautiful, wealthy, depressing, and boring), for a total of 1,169,078 times across all participants. They trained a deep learning model using the results of the above choices, and highlighted objects or features on GSV imagery that contributed to each of the perceptual indicators. Specifically, greenery and walkability had positive effects on human perceptions of safety, beauty, and wealth on the streetscapes. Furthermore, Wijnands et al. (2019) trained a deep learning model with the results of a computer-aided telephone interview survey by the government as annotations of GSV imagery on each targeted area in the Greater Melbourne area, Australia. The model could modify visual features of GSV images based on parameters from the survey including health, happiness, and social capital (i.e., conversation with others). For instance, GSV images of the targeted areas with high social capital were converted by the model to images with features of areas with low social capital, and the difference between images indicated visual features of streetscapes related to social capital. With this method, they suggested that high social capital in the neighborhood would be associated with more grass and smaller trees. In other words, these studies indicated that certain streetscape elements affect people's intuitive impression of a certain area. In sum, GSV could be an efficient tool through which novel perspectives might be gained by examining the associations between social capital and physical environment within a neighborhood. Specifically, positive relations between social networks and greenery or walkability have been demonstrated by several studies that targeted various cities in different countries using GSV. Importantly, while greenery and walkability are features of the physical environment influenced mainly by urban planning, the direct influence of local residents on their physical environment tends to be overlooked in those discussions. The latter factor, which affects the atmosphere of local communities, would be crucial to deepen and integrate discussions on interrelations between social capital and neighborhood physical environment, especially from a practical standpoint, as shown by Heinze et al. (2018). Community-Level Socioecological Factors and Contextual Effects on Psychological Tendencies To explore associations between social capital and physical environment within local communities, social environment and contextual effects at the community level need to be considered. Social ecology comprising the social and physical environment  is an important factor that could influence individuals' behavior and psychological tendencies and is reflected in the streetscapes of local communities. Especially, the type of primary industry or socio-economical\\/ecological factors in a community could explain the difference in collective psychological tendencies of the local residents. Previous studies indicated that primary industries (e.g., farming, rice\\/wheat agriculture) that required people to cooperate would be more associated with collectivistic tendencies compared to primary industries (e.g., fishing) that required people to make decisions independently. Additionally, such cultural tendencies would be shared among people differently, depending on the type of community. Uchida et al. (2019) demonstrated through largescale surveys in Japan that interdependent tendencies were shared by farmers and the other residents in farming communities, while independent tendencies were dominant among fishermen, but not the other residents in fishing communities. They explained that these differences in psychological tendencies might reflect people's adaptation to their primary industries and related social activities. Furthermore, cultural tendencies connected to social ecology would explain people's attitude toward their neighbors. Liu et al. (2019) compared participants' behavior in a competitive situation between those who were originally from regions in China where either rice or wheat agriculture was dominant. They also compared responses by participants from China and the US, and concluded that people in rice agricultural regions tended to be more vigilant and paid more attention to ingroup members' vicious intentions. This suggests that trust among community members as a component of social capital could be influenced by socioecological factors of the community. Present Study As discussed above, previous studies using GSV tended to target relatively similar urban or suburban areas, mainly because of the density of available GSV images. However, virtual audit using GSV can be applied not only to conventional targets such as urban or suburban areas, but also to different types of rural areas (e.g., farming and fishing communities). This enables us to explore associations between social capital and neighborhood physical environment in various types of communities, which could be influenced by socioecological factors. In the present study, to explore such interrelations while taking into account community-level factors, we used a large-scale mail survey conducted in several types of communities and compared them to GSV imagery. The mail survey was originally conducted in another study  to investigate associations between happiness, social capital, and types of community, namely, farming, fishing, and urban communities. The scales in the survey included multiple aspects of social capital such as trust, reciprocity, and social networks as defined by Putnam (2000), and items about relationships with neighbors, happiness, health, and economic status. We extracted available samples from the survey, including farming and fishing communities to consider the difference in interrelations between social capital and their physical environment. GSV images were obtained by referring to the geographical area of each target community, and assessed by machine learning models and human raters. We focused on plantings at houses observed from the public road as the target element of the physical environment that might reflect residents' behavior, because of three reasons. First, the maintenance of outdoor plantings would influence the streetscape by beautifying it ; second, residents may find it easier to modify outdoor plantings decorating their houses than the exterior of houses, such as outer walls; and third, plantings need residents' continuous effort for maintenance in most cases. Regarding the first reason, indoor plantings as private interior decorations are usually hidden from public roads, while outdoor plantings could be part of the streetscape. In addition, regarding the second and third reasons, roadside trees or greenery at public places (e.g., parks and squares) are usually under the control of the local government, while plantings within houses are private property that might reflect local people's intentional interventions of the physical environment. Finally, we examined correlations in an exploratory manner between several scales in the survey and the frequency of plantings by the target community. Following the discussions in previous studies , we predicted that the frequency of plantings at houses, as part of the aesthetics of streetscapes, would be positively correlated with social capital, happiness, and health, reflecting participants' relationships with neighbors in positive ways. METHOD Survey Data To analyze the psychological aspects and social relationships of people living in different communities, we utilized prior data from a large-scale survey conducted in the Kinki and Shikoku regions of Japan. The survey was conducted in two parts, and the focal data of the current study was collected in the second part during January\\/February 2016 to compare different types of communities, including farming, fishing, and urban communities. 301 target communities were selected based on 'the first survey' conducted in the Western regions of Japan. In the first survey, 412 communities were sampled from 60,807 eligible communities within the Kinki, Shikoku, and Chugoku regions in January\\/February 2013. The samples were stratified by two dimensions: geographical region and type of community. Target regions were categorized into seven regional blocks based on climate division, and communities were categorized into five types, namely farming, fishing, urban, mixed, and other. Based on the census data by the Statistics Bureau, Ministry of Internal Affairs and Communications of Japan (2010), they defined farming community and fishing community as communities with relatively high percentage of farmers or fishers respectively (>= 25%), and urban community as those with a high population density (>= 4,000 persons\\/km2). First, all 20 'mixed communities' consisting of any combination of the above types of communities were included in the samples, because of the limited number of those types of communities. Subsequently, 30 farming, five to six urban, and five 'other' communities were sampled from each of the seven regional blocks. As the number of fishing communities were less than farming communities within some target regions, a range from 6 to 32 fishing communities were selected from each block. Consequently, the first survey was mailed to all households in the sample communities, and 7,364 individual responses were received from 408 communities. From those samples, the following survey ('the second survey') focused on the selected communities in the Kinki region, and the leftover fishing communities in the Kinki and Shikoku regions. However, fishing communities in Mie prefecture within the Kinki region were not included, because of the limited number. As a result, 156 communities were targeted, which amounted to approximately half of the total sampled communities in the second survey. As with the first survey, the second survey was mailed to all households in those communities, and 1,916 individual responses were received from 152 communities. There were 56 farming, 83 fishing, 19 urban, and nine 'other' communities, including 17 'mixed communities' (the categorization of two communities were unknown, because the census data was unavailable). Response rate at the individual-level differed across communities, ranging from 2% to 58% (M = 18%, SD = 11%). The second survey was approved by the Institutional Review Board at Kyoto University. A statement of consent was included in the second survey. All information offered by participants was anonymized, except for the zip code of each target community, which had been printed on the questionnaires before mailing. No data from the first survey was used for analysis in the current study. Measures The survey aimed to explore the structure of individuals' happiness and social relationships moderated by type of community. For that purpose, it consisted of items that measured participants' and their neighbors' health and happiness, social relationships, psychological tendencies, daily feelings, participation in community activities, and demographic data. To analyze target communities' psychological aspects and social relationships, the present study focused on items which were related to health and happiness, social capital, impression of their community, individual- and community-level wealth. The survey contained 12 items regarding health and happiness measured using a 11-point Likert scale (0 to 10) that asked about participants' and their neighbors' health and happiness, other daily feelings (e.g., life satisfaction) and financial status. There were 30 items regarding social capital and impression of community measured using a 5-point Likert scale that were related to participants' trust (e.g., \\\"I trust the people who live in my neighborhood.\\\"), reciprocity (e.g., \\\"If people in the neighborhood need help, I will help them.\\\"), community's norms (e.g., \\\"There are many rules in the neighborhood that we must obey.\\\"), social relationships (e.g., \\\"As a member of my neighborhood, I think it is important to maintain the harmony in the neighborhood.\\\"), and attachment to the community's values (e.g., \\\"I feel attached to my neighborhood.\\\"). In addition, we also included an item that measured psychological distance between participants and their neighbors (\\\"To what extent do you and people in the neighborhood overlap as represented by two circles [in the figure].\\\"), and an item that asked participants about their economic status within the community. The item measuring household income consisted of nine options from \\\"under 2 million yen\\\" to \\\"16 million yen and above\\\" in intervals of 2 million yen. Google Street View Imagery Collection For the purpose of the present study, while psychological aspects of the target communities were captured using the survey data, aspects of the physical environment had to be obtained by observing landscapes around residences of the target communities. Therefore, we focused on the visual data around buildings, including houses, in each of the target communities. To obtain imagery of the target communities located in various places in the Kinki and Shikoku regions, we extracted GSV image data through Google's application programming interface (API). The API needed location coordinates to display the GSV imagery nearby. If the purpose of collecting GSV images is to capture comprehensive streetscapes of target regions regardless of the existence of houses, referring to coordinates spotted at a certain interval on the API would be a common procedure. On the other hand, if the purpose of image collection is to observe streetscapes around certain buildings or facilities, using their addresses or coordinates would be a more reasonable procedure. Thus, we defined the geographical boundaries of the target communities by referencing locations from the census data, and subsequently acquired the coordinates of buildings within each area by referring to another governmental open data which will be elaborated below. First, because the API allowed users to download only the latest version of GSV imagery at the point of access, coordinates of the target communities needed to be defined based on the most updated boundary on the map. Therefore, we referred to the latest census data from 2015 instead of the earlier version conducted in 2010, from which the target communities were selected. Second, to obtain coordinates of houses in the target communities for the API, we used polygon data of buildings on the geographical information system (GIS), which was distributed as open data by the geographical department of the Japanese government. In this process, the QGIS 3.4.15  was used. The polygon data did not contain the same information as the census data regarding the local communities' boundaries. Thus, polygons of the buildings within each target communities' area were extracted by overlaying the boundaries as vector data with the polygon data. As a result, 'center of gravity' points of the selected polygons were obtained and used as their coordinates. Groups of those coordinates would correspond to small areas where local people resided within each of the target communities, though those would also include abandoned buildings. Through this process, 39 communities were excluded from the target communities, because they did not match with the boundaries in the open data due to differences in versions of the census data as mentioned above. GSV imagery of 115 target communities was extracted through the following process. The size of images was set as 640 x 640 pixels, which was the largest available size the free plan of the API allowed users to download. We chose the largest size because the larger the target images were, the more information we could utilize in image classification (described later). The location of images was limited to the outdoors to exclude indoor images. The vertical angle of images was set as horizontal. The horizontal angles were set as 0, 90, 180, and 270 degrees for every single coordinate. This means that four images were obtained from each of the coordinates, which approximated a panorama view from a certain point on GSV (Fig. 1 shows an example). Following the parameters mentioned above, we downloaded images referring to the buildings' coordinates of each target community as closely as possible using the Street View Static API from March 11 to 12, 2020. The extracted images were the latest version of each coordinate on GSV at the point of extraction, but there might exist several years gaps between them, sometimes even in the same community. Because the API did not offer metadata, including the date of capture, directly with the images, the exact range of dates of image capture in each community could not be calculated. Therefore, those images were expected to consist of landscapes of the neighborhood of each target community in recent years. However, the same images could be extracted from different coordinates whose nearest points with available GSV images overlapped. Also, blank images could be obtained from coordinates whose nearest available street view image was too far. Hence, after excluding those duplicates and blank images, 39,187 images from 104 target communities remained for the following analyzes. The number of usable images extracted from GSV varied across communities, ranging from 2 to 2,852 (M = 377, SD = 451). Additionally, we obtained another dataset to train the machine learning model (described later) by the same process as above. In this collection of images, 12 communities were randomly selected from the 115 target communities, comprising 10% of the total, and up to two adjacent communities were chosen from each of those communities. If a selected community had more than two adjacent communities, the nearest two communities according to their region codes on the census were chosen. On the other hand, if a selected community had none or only one adjacent community (e.g., island regions), then less than two adjacent communities were chosen. As a result, 19 communities were listed, and 4,812 GSV images were extracted from 17 selected communities, ranging from 40 to 580 (M = 283, SD = 155). Image Classification Each of the extracted image data was classified by machine learning models or human raters to exclude mistakenly downloaded images and calculate the appearance ratio of a certain object in the images of a target community. Because 88 of 104 target communities of which we downloaded GSV images were farming communities, fishing communities, or both, they tended to be located in rural areas, and often included images without any observable buildings (e.g., only forest and road appeared in the frame of an image). Such images were removed from the dataset, as no landscapes of the neighborhood of the target communities were apparent. After this process, we classified the rest of the images into those who had plants alongside the buildings or not. Through those two image classification processes, we built two machine learning models to identify houses and plants in the extracted GSV images. While image segmentation has been employed to automatically classify objects from a massive number of GSV imagery by many previous studies , a deep learning method termed as \\\"chopped picture\\\"  was applied instead in those models. The \\\"chopped picture\\\" method was originally developed to identify amorphous objects such as groups of various shapes of plants, unlike distinctive objects (e.g., human faces, dogs, and automobiles), which previous machine learning methods had focused on. Importantly, this method would need less effort and images for its training process than previous object detection or semantic segmentation. Therefore, this method was expected to perform well in identifying houses and plants in 39,187 GSV images. However, this machine learning method could not identify objects by referring to contextual information, namely, other objects around the target objects. In the current process, machine learning models built by the \\\"chopped picture\\\" method could identify plants with similar texture (e.g., green leaves) nearly as well as human raters, but could not further distinguish between plants nearby houses or any buildings from them. Besides, another limitation of the current method was that the 640 x 640 pixels resolution of GSV images would not be enough to classify types of plants which appeared as groups in the images. This means that the models developed by this method would hardly distinguish maintained plants from wild weeds around houses. Thus, we applied this deep learning method for houses and plants (not maintained plants), and subsequently got human raters to identify plants grown alongside buildings. In the process of developing two deep learning models, 531 and 564 clipped images were used for the models to identify houses or plants respectively (details are presented in supplementary materials). We used the house model to identify houses in each of all the extracted images (Fig. 2 shows an example of object identification by the model) to select images in which houses were seen nearby. Two human raters also independently classified 392 randomly selected images, which made up 1% of the total number of extracted images, into those who had any house within its frame or not. Cohen's kappa between their judgements was sufficiently high (.87). Unmatched judgements were discussed and resolved by the raters, and 245 images were concluded as 'building images.' Afterwards, we defined the threshold for classifying 'building images' via the percentage of parts identified as 'house' in any image by the model. This threshold needed to be set, in order for the model's classification to replicate human raters' judgement of sampled target images well enough, with lower bias for false positives and negatives. As a result, the threshold rate of 8% for parts containing houses seemed to be feasible enough, because the kappa value with human raters (.66) was higher than the other percentages among threshold rates of 6% to 10% at 0.5-point intervals, and the number of false positives and negatives were relatively balanced (27 and 36 respectively). Using this threshold, 15,171 images classified as not 'building images' were excluded from the extracted images, and two communities were removed from the target communities as they had no 'building images.' As with classification of 'building images, ' two human raters independently classified 245 sampled target images into ones in which maintained plants were seen or not (k = .67). Unmatched judgements were discussed, and 158 images were agreed as 'plant images.' The plants model was used to identify plants in each of the 24,019 extracted images classified as 'building image' (Fig. 3 shows an example of object identification by the model). Unlike classification of 'building images,' the threshold(s) of the percentage of plant parts within an image was set to exclude images in which human raters would hardly identify any plants, such as house walls or bamboo forest taking up most of the frame. 'Planting image' classified by human raters from the sampled images ranged from 0% to 39% of plant parts identified in those images by the model, so the threshold was defined as 39%. As a result, 553 images were excluded from all the remaining extracted images. Lastly, two human raters classified these 23,466 extracted images into 'planting images' or not. 941 images making up approximately 4% of the extracted images were independently judged by both raters. Their classifications matched highly enough (k = .84), and these were randomly chosen as the outcome of each judgement. The rest of the extracted images were classified by either rater. Consequently, 3,224 images were classified as 'planting images.' Preprocess of the Analysis Before analyzing the data, we excluded several samples that did not have enough data for the following statistical analysis. At the community level, samples who had less than 10 'building images' were removed from the target communities, leaving 100 communities and 1,481 individual samples. These remaining target communities consisted of 27 farming, 62 fishing, 10 urban, and nine 'other' communities, including nine 'mixed communities' (one community's type was unknown, because of unavailability of the census data). As for the individual samples, participants consisted of 808 males and 575 females (98 individuals' gender was unknown), and the mode of their age in an ordinal scale was 65 to 69 (the median was the same). Additionally, data of individuals who consecutively chose the same option throughout a series of multiple-choice questions (even when the items covered diverse questions) were not included for those items, because they would not have properly paid attention to the questionnaires. Lastly, if the number of valid responses for an item was less than five by a target community, that item was treated as a missing value for that target community. RESULTS Summary of the Data The survey included 30 items related to participants' trust, reciprocity, community norms, social relationships, and attachment to the community's values. To categorize those items into discrete measures using individual level data, we assessed internal consistency through a principal component analysis (PCA). Items that were sufficiently consistent with each other were averaged to form a single measure. Three items were averaged as a measure of 'trust' as a component of social capital , as they showed acceptable Cronbach's coefficient alpha (.69) and McDonald's coefficient omega (.83). An example of an item asking about participants' trust toward others would be: \\\"I trust the people who live in my neighborhood.\\\" Six items captured reciprocity as another component of social capital, associated with social support between participants and their neighbors (e.g., \\\"If people in the neighborhood need help, I help them.\\\"). The items showed sufficiently high Cronbach's coefficient alpha (.84) and McDonald's coefficient omega (.89). Four items captured participants' attachment to their community's values (e.g., \\\"I feel attached to my neighborhood.\\\") and showed sufficiently high Cronbach's coefficient alpha (.88) and McDonald's coefficient omega (.92). As for community norms, two of five items that asked about the strictness of the norms in participants' community had low factor loadings. Hence, only the remaining three items (e.g., \\\"There are many rules in the neighborhood that we must obey.\\\") were averaged as a measure of community norms, which had acceptable Cronbach's coefficient alpha (.66) and McDonald's coefficient omega (.82). Two items regarding cooperativeness in a community, (\\\"I\\/People in my neighborhood think it is important to maintain the harmony in the neighborhood.\\\") were positively correlated (r = .51, p < .001), and thus integrated as a measure of cooperativeness. Items excluded in the above process were used as single item scales in the analysis. The summary of all the measures and items at the individual level is shown in Table 1 (refer to the Appendix for the community level). The extracted GSV images and classifications by target community are summarized in Table 2. 'Building image rate' was the percentage of 'building images' over 'collected GSV images, ' and 'planting images rate' was the percentage of 'planting images' over 'building images.' Correlation Analysis To explore relationships between participants' psychological tendency and physical characteristics in the target communities, we calculated the Spearman's rank correlation coefficient between planting images rate and the other measures from the survey. The rank correlation test was applied instead of the Pearson's product-moment correlation test due to violation of normality in the distribution of planting image rate as detected by the Lilliefors test (p = .013; its histogram is shown in Fig. 4). The results of the rank correlation test are shown in Table 3. As shown in Table 3, the rate of planting images for all target communities was weakly positively correlated with four single item scales: ideal happiness, concern about neighbors, wealth of the neighborhood, and household income. It also had significant weak negative correlations with three measures: trust, cooperativeness, and attachment to the community's values. However, differentiating between farming and fishing communities produced different patterns of correlations. For farming communities, planting images rate was moderately or strongly positively correlated with four single item scales, in addition to subjective happiness and health, stress and anger in daily life. Furthermore, planting images rate was also moderately negatively correlated with a single-item scale regarding openness toward migrants and the composite measure for trust. Lastly, fishing communities showed less similar patterns of correlations with the overall sample than farming communities. The rate of planting images had a significant moderate positive correlation with the happiness of young neighbors, and significant weak negative correlations with attachment to the community's values and three single item scales: connection with neighbors, neighbors' acceptance toward opinions, and closeness with neighbors. Additionally, the rate of planting images was weakly positively correlated with household income. DISCUSSION The present study explored interrelations between social capital and neighborhood physical environment as a reflection of individuals' behavior, by examining correlations between several measures from a mail survey and the frequency rate of plantings at houses observed through the GSV. Additionally, we also investigated differences in correlational patterns depending on types of community, namely farming and fishing communities. Associations Between Plantings and Social Relations in the Neighborhood First, results for the overall sample showed opposite patterns of correlations from expectations based on previous studies. In the current research, the frequency rate of plantings had no positive correlations with scales of social capital (i.e., trust and reciprocity), and even had negative correlations with trust, cooperativeness, and attachment to the community's values. On the other hand, the rate of plantings was positively correlated with subjective wealth of the community, household income, concern for reputation, and ideal happiness. These findings suggest that plantings at houses could reflect residents' financial leeway, and serve as signals of wealth to other residents, creating competition in the neighborhood. This interpretation could perhaps explain the former negative correlations for plantings, that is, the more plantings the houses had, the more conspicuous the financial competition became in the neighborhood, which contrasts with having cooperative relationships between neighbors. Second, farming communities showed relatively similar correlation patterns with all communities other than fishing communities, though the rate of farming communities was lower than fishing communities among all the samples. Correlations between plantings, and cooperativeness, attachment to community's values, or household income were not significant, but the directionality of the relationships were similar to all other communities. Besides, hospitality toward migrants8 was negatively correlated with plantings, which could also be explained through the earlier mentioned financially competitive relationships signaled by outdoor gardening, because new residents would be potential competitors of old residents. By contrast, subjective happiness and health had positive correlations with plantings, which is consistent with findings by previous studies. However, stress and anger in daily life9 also had moderate positive correlations with plantings, which was unlike previous studies showing that exposure to greenery on the street might have positive influences on people's mental health. Rather, our findings suggest that relatively happy or healthy residents might tend to grow plants around their houses, while feeling stress and anger in their daily lives, which would not necessarily contradict with having financially competitive relationships. Third, unlike farming communities, fishing communities showed different correlational patterns from all other communities, even though fishing communities accounted for more than half of all samples. One of the notable results for fishing communities was the lack of association between subjective wealth of the community and plantings. This contrasts with the positive correlations found for all other communities and farming communities, which was similarly found between household income and plantings. Furthermore, subjective happiness and health had no correlations with plantings in contrast to farming communities. Unique correlation patterns for fishing communities emerged for two scales about psychological connections with neighbors, which were negatively correlated with plantings. In addition, openness toward each other's opinion was also negatively correlated with plantings. These correlation patterns could be understood in the context of independent tendencies in fishing communities in Japan. In other words, independent tendencies might motivate the residents to grow plants around their houses as segregations between private spaces and streets in fishing communities. However, independent tendencies are unable to account for the moderate positive correlation between happiness of minors in the neighborhood and plantings, and the current data does not provide sufficient evidence for interpretation. Through the above discussion, we suggest the following hypothesis on interrelations between social capital and neighborhood physical environment. Plantings around houses could reflect residents' financial leeway, and this might make the financially competitive relationships more conspicuous in the neighborhood. In such situations, neighborhood cooperative relationships including social capital might be undermined, and plantings around houses could arouse residents' negative feelings, such as stress or anger. Additionally, that tendency could be more obvious in communities in which interdependence is dominant, such as farming communities. This interpretation of the results corresponds to the discussion by Liu et al. (2019) that people from collectivistic cultures would tend to be more vigilant toward other members in their community under competitive situations than individualistic cultures. Furthermore, we expect that this phenomenon could not occur in individualistic communities, such as fishing communities, where independent tendencies are dominant. Rather, weak psychological connections among neighbors could be reflected in the plantings around houses which acts as additional walls between private and public spaces. The above difference between farming and fishing communities in using plantings could support the discussion of cultural differences associated with socioecological factors in Uchida et al. (2019) from the perspective of observable physical environment. Consequently, while the previous studies indicated positive associations between social capital and overall greenery on the streets , our findings suggest that individuals' direct intervention to the streetscapes, a specific type of greenery in the form of personal landscaping\\/gardening, could negatively influence social capital in the neighborhood, depending on the type of community. Therefore, the influence of greenery on social capital in local communities would need to be discussed while considering categories and contextual effects of different communities. In terms of contextual effects, while Uchida et al. (2019) pointed out that collective activities (e.g., maintenance of community infrastructure) in a local community could play an important role, our findings suggest that elements (e.g., plantings managed by individuals) observed in streetscapes also could be a mediator of such effects. In other words, the present study extended the theory suggested in the previous study by combining the self-reported survey on psychological aspects and image data of the shared physical environment, which was a more objective measure. This is an important approach to investigate contextual effects and for community building on the ground. Furthermore, compared with the previous studies in Western countries , the present study focusing on Japanese rural areas showed a country-level cultural difference in the position of plantings around houses within the streetscape. While plantings observed around houses in Japan might be privately owned (see Fig. 3) and function as a boundary for each, outdoor gardening in the Western countries might be parts of the streetscape of public roads. In other words, the former could be more likely controlled by house residents who care about their reputation within the community, while the latter could be more likely influenced by residents who care about the public spaces of the community. Therefore, the present study also suggests that 'publicness' of streetscapes around houses could differ by types of communities, regions, or nations, which could be quantitatively investigated as practiced in the present study. Limitations and Future Studies The samples used by the present study came from a mail survey that originally focused on farming and fishing communities in certain regions, and thereby had several limitations for statistical analyses and generalization of the results. Specifically, the samples were from the Kinki and Shikoku regions, which may not be representative of all local communities across Japan. Moreover, because of low population in rural areas, the number of valid responses by target community were very low for some items of the survey, which limited available statistical analysis including multivariate analysis. Ideally, the differences of the correlation patterns between the farming and fishing communities were to be tested through a multiple regression analysis using community type as a moderator. However, the relatively small sample size would make it difficult to detect those differences in that analysis, even if they were fairly large. Therefore, our suggestion from the results needs to be considered with this limitation in mind. Regarding GSV imagery collection, given that the samples were mainly from rural areas, the probability of obtaining GSV imagery from a target community would also be low, because GSV imagery would be relatively sparse in the countryside, compared to urban areas. This shortage of GSV images in some target communities also contributed to the limited availability of sample size for statistical analysis. Another limitation of GSV was that the dates of the latest images were not controlled, and sometimes ranged across several years even within communities. Additionally, the low resolution of GSV imagery hindered the classification of types of plants by machine learning models. The permissions of use of GSV imagery may be another limitation of the present method, as well as above issues about the image data (see footnote 2). Referring to our discussion in the methods section, users are left to interpret Google's restrictions by themselves, which might allow studies utilizing GSV imagery to be published continuously as reviewed in the present study. Thus, researchers also need to consider alternative resources (e.g., Mapillary). Lastly, though we focused on the smallest unit of area based on the census, mainly because of the availability of the dataset, the actual geographical range of the 'local community' or neighborhood could be smaller than that. The limitations in size and geographical variety of the samples in the present study could be addressed by conducting larger scale mail or online surveys in future studies. Furthermore, different data and perspectives that were not utilized in the present study can be used to verify the current findings. First, with a satisfactory sample size, the differences in the correlation patterns between the community types can be tested by a multiple regression analysis. For demographic and socioecological factors, governmental statistics (e.g., census data) that objectively show socioeconomic status and governmental policies for certain areas such as urban planning would offer more information to describe actual situations in the target communities. For the physical environment, classifying specific types of plants that might correspond to the type of community could be conducted by using higher resolution GSV images sold by Google. Elements of streetscapes reflecting individuals' behavior apart from plantings at houses and longitudinal data of GSV, would also be important to further uncover the dynamics of social capital and neighborhood physical environment. Moreover, the influence of neighborhood facilities (e.g., parks, stations, or schools) on associations between social capital and streetscapes also need to be considered especially in urban areas where the density of buildings is much higher than rural areas. Finally, to explore people's motivation to care for the streetscape, including plants, future studies can tap into the advantage of using GSV to not only compare communities within a country, but also between countries. In conclusion, the present study shed light on interrelations between social capital and neighborhood physical environment by combining a conventional mail survey and a newly developed virtual systematic social observation. Despite the limitations of the samples, the results suggested that neighborhood socioecological environment could potentially reflect the dynamics of social capital in a community. We encourage future studies to verify the current hypothesis by utilizing geographically larger and more diverse imagery data, and aggregating various statistics about the target areas. Additionally, from a broader psychological perspective, conventional latent values or factors (e.g., social capital) embedded in interactions between individuals and the 'built environment' around them could be discussed more comprehensively by utilizing the present method. This is an important step to extend the psychological discussions from data from self-reports and laboratories to behaviors observed in real situations.\",\"1135746295\":\"1. Introduction Over the last few decades, workplace spirituality has been one of the most debated themes of business literature. The significance of the construct and its relationship with the organizational, leadership, and management outcomes has been greatly emphasized in various settings: academic circles, mass media, websites, social groups, and even newsletters. The seminal work of Krishnakumar and Neck (2002) managed to outline a direction and structure to this emerging field and demonstrated the importance of understanding the role of religion and workplace spirituality in business. As a result, workplace spirituality has gained a considerable amount of attention of scholars from diverse disciplines including philosophy, psychology, theology, sociology, sociology of religion, and business management. The same passion and commitment have also been displayed by the corporate sector. As demonstrated by business organizations increasingly arranging activities (such as symposiums, workshops, trainings, cultural change, and organizational transformation projects, etc.) to harness the soul and minds of employees to achieve competitive advantage. Despite the increasing interest in workplace spirituality, there still seems a lack of agreement on the meaning of workplace spirituality. Certainly, for systematic and scientific inquiries of any construct one requires a clear definition and a generally agreeable means to measure it. Hence, it is deemed that the current literature still faces a significant challenge to not only define workplace spirituality but also to conceptualize the same. Interestingly, in the absence of a valid, reliable, and widely acceptable scale, it is quite challenging and difficult to empirically examine the theoretical propositions presented in previous studies. Equally, the notion of generalizability also becomes a key concern for several investigators when it comes to examining the relationship between workplace spirituality and other outcome variables. Nevertheless, the efforts made so far in extending the existing body of literature on workplace spirituality are indeed commendable. With the constantly changing business environment, organizations are searching for greater meaning and purpose. This realization as discussed earlier has led several business leaders and scholars to recognize the significance of bringing religiosity and faith back to the business realm. Over the same course of time, several investigations have been conducted in assessing workplace spirituality from different religious convictions like Christianity, Confucianism, Hinduism, Jainism, Judaism, Shinto, Sikhism, and so forth. Despite Islam having the second-largest religious following in the world, there is a lack of empirical studies that focus on Islamic workplace spirituality and religiosity. Islam is a comprehensive religion and a complete code of life. As of 2010, there were nearly 1.6 billion Muslims worldwide making Islam the second-largest religion: \\\"the major share subsists in Indonesia (13%), followed by India (11%), Pakistan (11%), Bangladesh (8%), Nigeria (5%), Egypt (5%), Iran (5%), Turkey (5%), Algeria (2%) and Morocco (2%)\\\" (;, p. 10). The Muslim world along with the western sphere are also experiencing the challenges of global capitalism. Predominately Islamic countries that spread from Morocco to Indonesia, over the last few years have observed a \\\"return\\\" to the Islamic belief, values, and traditions as a way of affirming their uniqueness and identity. The return of Islamic values aims at managing the increasing materialism, stresses, and burdens of the capitalist and globalized 21st Century. So much so that the Muslim world has intensified their efforts to reinstitute and establish their educational, public, business, and commercial organizations in compliance with the tenets of Islam and their local culture. Indeed, this purpose has reinvigorated several Muslim Organizations to inculcate Islamic religiosity and spirituality in the workplace. In Islam spirituality holds great significance. It is essential to first understand Islamic Piety (Taqwa) to comprehend and acknowledge spirituality. Islamic Piety, defined as consciousness of God, is one of the most significant attributes of Islam. Islamic Piety and its derivatives have been mentioned almost 258 times in the Quran. The magnificence of Islamic Piety is that it not only comprises of Islamic spirituality but also Islamic social responsibility. Recently, the Islamic Piety construct has been incorporated in the field of business and management. The multi-dimensional construct has likewise been tested with few output variables. However, as any other construct, the scale established for measuring Islamic Piety still requires further assessment and analysis. In effect, further examination is required to confirm its soundness and generalizability amongst the Muslim world. Hence the present study aims to investigate a hybrid model of Islamic piety, by merging exploratory factor analysis (EFA) and an artificial neural network (ANN). 2. Background 2.1. Islamic piety The intent and meaning of life for a Muslim believer is found in Islamic Piety. Living a life of Islamic Piety fundamentally means being a true friend (servant) to Allah (SWT) and possessing a heart that can understand and recognize the innermost depth of Devine blessings. Truly, Islamic piety means to abide by Allah's (SWT) commands and remain away from all that has been disallowed in order to seek eternal blessing and avoid punishment in the life hereafter. M. Kamil et al. (2010, p. 166) posited that \\\"Islamic Piety is an essential element for the wellbeing of a believer in this life and the Hereafter. It encompasses one's perception of obligations and accountability towards Allah (SWT) that includes tasks and duties in all walkways of life, may it be family or work. And that when people possess Islamic piety, there is a high tendency that they will be honorable in their actions, both at dwelling and at the workplace\\\". Islamic piety is as well referred to as God-consciousness. Al-Ghazali (1993) asserted that Islamic piety is often seen as a state of absolute maturity that fully synchronizes the belief, spirit, and the body of the believer. Ergo, anyone whether private or public, who follows and observes the laws of Allah (SWT) is a person of greater piety. The higher the degree of piety a believer has, the closer that individual will be to Allah (SWT). Islamic piety is manifested in all aspects of human conduct that includes one's soul and body. In the Quran, Allah (SWT) defines Islamic piety by describing the characteristics of people who possess piety as Allah (SWT) states: \\\"This is the Book; in it is guidance sure, without doubt, to those who fear Allah. Who believe in the Unseen, are steadfast in prayer, and spend out of what We have provided for them. And who believe in the Revelation sent to thee, and sent before thy time, and (in their hearts) have the assurance of the Hereafter\\\" (Quran, 2: 1-4). Evidently, love and fear of Allah (SWT), humbleness, and thankfulness are key facets of Islamic piety. Nevertheless, offering prayers, performing Hajj, fasting, and giving charity makes a believer more disciplined, ethical, self-satisfied, patient, and spiritually motivated ;). In Islamic standpoint, one can achieve true peace, tranquility of mind and body by obedience and submission to Allah (SWT) . An individual's relationship with The Creator (Allah SWT) should, by right, regulate the mode of association with corresponding servants of God. This theoretical basis allows Muslims to avoid competing interests and leads to a society in which everyone has the unanimity of his\\/her own purpose in life. This also contributes to a harmonious culture whereby everyone endeavors to collaborate rather than compete with one another. Additionally, channeled through the association with Allah (SWT), the person's everyday activities, commercial transactions and interactions would be encouraged by the principles of firmness, truthfulness, respecting the law, fairness, sympathy, forbearance, ostentation, tolerance and uprightness, instead of deceit, greediness, haughtiness, jealousy, backbiting, class consciousness, revenge, possessiveness, envy, and self-aggrandizement. Nevertheless, in an organizational context it is generally deemed that Islamic piety inspires a believer towards improving work quality, excellence, brilliance, self-discipline, and organizational development. When adequately focused, it could be easily realized that Islamic piety comprises of two fundamental components namely. \\\"Islamic Spirituality\\\" and \\\"Islamic Social Responsibility\\\" . 2.1.1. Islamic spirituality (IS) \\\"O you who believe! Fear God, and be with those who are true in word and deeds\\\" (Quran 9:119); \\\"Say: He is Allah the One and Only; Allah, the Eternal, Absolute; He begetteth not, nor is He begotten; and there is none like unto Him\\\" (Quran 112: 1-4). The spiritual dimension of human nature is essential to mankind's wellbeing, as it is through spirituality that human beings can develop and grow. Nottingham (1993) advocates that one of the most important ways to study human behavior is through spirituality, as it balances the overall individual's personality. And that individual behavior is composed of three factors: spiritual, physical, and psychological. These factors continuously interact with one another and if one of these factors becomes unstable, one may view problems in general human nature. \\\"In Arabic the word spirituality is described as \\\"rawahaniyah\\\". It is stemmed from the phrase \\\"ruh\\\" which implies \\\"spirit\\\" . Islamic Spirituality alludes to belief, deeds and actions that are in accordance with the overall Islamic principles and values. Spirituality is rooted in the faith of the believer and is displayed in all their activities for the achievement of glory, compassion, support and forgiveness of Allah (SWT). A believer who submits him\\/herself to Allah (SWT) is in a state of absolute worship, hence strengthening him\\/her spiritually. Mohsen (2007, p. 55) maintains that \\\"spirituality is a critical part of human nature\\\" for living a steady, balanced, and a satisfied life. The beauty of Islamic spirituality is that it brings believers close to Allah (SWT) and also contributes to making them valuable members of society. Spirituality, moreover, adds strength, conviction, and self-confidence to a believer, hence, encouraging them to showcase the best of their abilities, attitude, and behavior. Spiritually strong individuals are likely, to be honest, loyal, hardworking, principled and more efficient for the reason that they take work as a form of worship. Also, they are likely to be more devoted, keen, and committed to achieving excellence. Islam ascribes great significance to spirituality as it can offer some meaningful answers to a number of organizational inquiries. A stated earlier, Islamic spirituality has been incorporated in the field of business and management. The multi-dimensional construct has likewise been tested with few organizational output variables like; organizational effectiveness, efficiency, sustainability, economic growth\\/performance and so forth. Table 1 further elaborates on a variety of spirituality studies detailing as how Islamic spirituality encourages employee attitudes and behaviors that contribute to organizational success and concomitant financial benefits. 2.1.2. Islamic social responsibility (ISR) Mohsen (2007) affirms that Islamic social responsibility is a primary responsibility of a believer towards oneself, people and nature as a whole. \\\"Islamic social responsibility is defined as actions and behaviors that a believer espouses in daily life which results in respect, harmony, justice, integrity and development of people and society with the realization of achieving the forgiveness and pleasure of Allah (SWT)\\\" (; p. 96). Topbas (2009) explains that Allah (SWT) has set precise guidelines and responsibilities for believers that he\\/she must perform with oneself and others. In Quran Allah (SWT) states: \\\" ... .And fulfil the covenant; verily the covenant shall be asked about\\\" (Quran 17:34); \\\"Fulfil the covenant of Allah when ye have entered into it, and break not your oaths after ye have confirmed them; indeed ye have made Allah your surety; for Allah know eth all that ye do\\\" (Quran 16:91). Allah (SWT) also says: \\\"Woe to those that deal in fraud. Those who, when they have to receive by measure from men, exact full measure, but when they have to give by measure or weight to men, give less than due. Do they not think that they will be called to account? On a Mighty Day, A Day when (all) mankind will stand before the Lord of the Worlds?\\\" (Quran 83:1-6). In the organizational context, Islamic social responsibility exhibit respect for others, personal and societal duties that can play the social hopes and potentials of an organization's role. Islamic social responsibility stresses on minority interest, employees, societal justice and is also concerned with problems related to the progress of the humanity. Employee well-being is increased by following social values of tolerance and uprightness, truthfulness, forbearance, firmness, fairness and equality, respect for the law, kindness, instead of deceit, haughtiness, ostentation, class consciousness, insubordination, backbiting, envy, jealousy and self-aggrandizement. Individual with high social responsibility has a great deal of potential for success and high achievement , social responsibility refers to code of human behavior that guide actions in various situations, defining right and wrong acts. Islamic guidance, with its commitment to justice, integrity, truthfulness, patience, helping others carry an equilibrium between the rights of every individual and duties as well as responsibilities towards others. This delivers a robust and self-propelling drive for just and good conduct, without negating the natural dispositions for personal gain  and hence benefits the organization at large and the individual employees. 3. Data collection and research method The present research was divided into three phases. The first phase applied the Delphi method to identify the dimensions of Islamic piety. In the second, the dimensions established using the Delphi method were used in exploratory factor analysis (EFA) to uncover the underlying structure of the Islamic Piety construct. In the last phase, an artificial neural network (ANN) was used to rank the factors discovered to establish their significance. The scale for measuring Islamic Piety was adapted from Mohsen (2007) which comprises 53 items which include 18 items (Cronbach's Alpha 0.89) measuring Islamic Spirituality, whereas 35 items measured (Cronbach's Alpha 0.88) Islamic Social Responsibility. To estimate the necessary sample size, the researchers followed Hair et al.'s (2014) advice that affirms that factor analysis minimum sample size must be at least five times the number of reflections to be examined. Hence, for the present research \\\"the acceptable sample size had a 5:1 ratio\\\" that is a sample size of 53 x 5 = 265. Notably, data was collected through questionnaires using a convenient sampling technique. The target population for this research was the managerial level employees working in the Banking sector in Turkey. This setting was chosen as it furnishes a strong concentration of literate workers (especially English literate) who can fill out the questionnaires with ease. The population setting was performed by cross verifying the list of \\\"Bank Association of Turkey\\\". To get the maximum responses the researchers distributed 1100 questionnaires. The questionnaires distributed were in English Language, hence, a total of 736 questionnaires were received. Further, based on the detailed evaluation all illegal, illogical, inconsistent and incomplete questionnaires were removed as advised by Hair et al. (2014). In total only 500 responses were useable for analysis. Questionnaires were screened manually for missing values or inappropriate values before initiation of data analysis. The distribution of the frequency of the demographic variables of the respondent in the study is exhibited in Table 2. The sample has a clear distinction in terms of the gender of the respondents, with a majority of respondents being male, 70.8%, compared to 29.2% being female respondents. The age spectrum of the sample varied with 44% of respondents aged between the 25-29, 26% between the ages of 30-34, 12.6% between the ages of 35-39, 8.8% between the ages of 20-24 and 8.6% aged 40 and above. 51.2% of the respondents were married and 48.8% unmarried. The sample was well educated as expected with 59.8% of respondents having a master's degree, 24.2% holding a bachelor's degree, and 14.6% having a degree with professional certification while 1.4% have intermediate certificates. The sample's job category breakdown consists of 54.8% line managers, 33.2% support staff, 11% middle managers, and 1% top managers. The breakdown of the number of years of experience in the present position consists of 60% between 1 to 5 years, 30.4% less than 1 year, 7.8% between 6 to 9 years, and 1.8% equal to or greater than 10 years. The breakdown for the total work experience in general consists of 45.8% between 2 to 5 years, 40% equal to or greater than 6 years, 14.2% less than 1 year. The breakdown of the number of employees in the company (bank branch) consists of 48.4% between 11 to 20 employees in the company, 30.6% equal to or greater than 20 employees in the company, 21% between 1 to 10 employees in the company. Notably, all the respondents were Muslims in this study. Table 2 below presents the distribution of respondents per sample characteristics. 4. Results 4.1. Delphi method In the initial stage, the Delphi method has been applied. This is a widely accepted method of consensus building among experts regarding a special theme or topic. The Delphi method has been established by the RAND Corporation in the 1950s and 1960s, with vital efforts placed by Dalkey (1967, 1969) and Dalkey and Helmer (1963). This particular survey tool is employed in organizing and structuring group views, opinions, and discussions. The Delphi method initiates a discourse on a specific subject and then compiles the perspectives of a devoted group. Based on the following, decisions are made in the wake of the views at the group level. In general, this specific method entails two or more phases in which respondents can give their opinions through a questionnaire. In simple terms, this framework can allow a group to come together on an agreement with a \\\"predefined criterion\\\" . As this is one of the initial studies to be acquitted in Turkey, in the first round 30 academic and religious scholars were approached to explain and rationalize Islamic Piety. The respondents were asked to justify Islamic Piety and what sub-variables constitute Islamic Piety. It is to be noted that the answers were presented based on the literature, Quran, and the Sunnah. In the second round, a seven-point scale was employed to measure the Islamic Piety and its sub-dimensions. Following this round, the questionnaire was analyzed; the less significant sub-dimensions of Islamic Piety (with average scores of less than 3) were deleted. The third-round questionnaire followed the design of the second round, which included a 7-point scale. In this round, two main dimensions i.e., Islamic Spirituality and Islamic Social responsibility, as well as fourteen sub-dimensions of Islamic Piety were endorsed. 4.2. Exploratory factor analysis (EFA) The present study uses EFA which is a multivariate interdependence technique and is mainly used in survey-based studies. It has two primary objectives; firstly, to identify the least number of factors that comprise the possible amount of information included in the variables, along with the maximum possible reliability. The second goal is to assess how indicators are organized in factors that are not explicitly observed, signifying the dimensions of the phenomenon being analyzed or investigated. While continuing with the EFA, the two essential tests are \\\"Battlett's Test of Sphericity\\\" and \\\"Kaiser-Meyer-Olkin\\\" (KMO), these tests measure the sampling adequacy. Battlet's test \\\"supplies the statistical significance that the correlation matrix has significant correlations between at least some of the variables\\\"  and becomes more sensitive as the size of the sample increases. A statistically significant Bartlett test (p<0:05) shows that there are adequate correlations between the variables to continue with the process of analysis. While the number of cases decreases , this test is more reliable and robust than the KMO test. Table 3. depicts the KMO score for the current study and that is 0.938, which is an acceptable determination of sampling. Bartlett's Test of Sphericity was significant (p<0:001) and supported conducting an EFA. Table 4 shows the commonalities extraction. Principal component analysis works on the initial assumption that all variance is common; hence before extraction, the communalities are all 1. The commonalities reflect the common variance in the data structure. In Table 4 commonalities were examined and it was found that no item had commonalities near 0, indicating that all items contributed to the EFA. The factor structure was examined, 2 Factors were extracted from IS and 3 Factors were extracted from ISR shown in Table 5, 13 variables of Islamic spirituality and 14 variables of ISR were finalized. Discussing the EFA outputs, overall, five factors were run for EFA indices. One EFA factor was generated for each subfactor which means that factor one is truly representing the overall variance of each variable. Moreover, 2 EFA factors were generated for a major factor of Islamic spirituality and 3 EFA factors were generated for a major factor of Islamic social responsibility. 4.3. ANN modelling for variable ranking A neural network is a complete analytical data modeling\\/mining tool used in technical applications that involve predictions and classifications. Artificial neural networks are primarily used due to its power, flexibility, and ease of use. Artificial neural networks are analytical models appropriate for problems that are too complex to be modeled and solved by traditional mathematics and usual procedures. Neural networks are capable of modeling input-output functional connections. An ANN comprises of input variables, which shall be classified by their characteristics. In the study, we employed an ANN using SPSS software for ranking the dimensions of Islamic piety. The ANN modeling for variable ranking was performed with 2 inputs representing rituals and belief as sub-factors of Islamic spirituality and 3 inputs representing integrity, love of family, and justice of Islamic social responsibility was added as an independent variable (see Figure 1). While Islamic piety was treated as output (dependent) variable using an ANN model. In this study, a neural network is established with the Multilayer Perceptron (MLP) algorithm using SPSS. The case processing summary as showcased in Table 6 affirms that 355 cases were assigned to the training sample, 145 to the testing sample. No cases were rejected\\/excluded from the analysis Table 7 displays the network information. The input layer is the covariates such as rituals, belief, integrity, love of family, and justice. The number of units excluding the bias unit is 5. In rescale methods for covariates are standardized. The number of hidden layers is 1 and the number of units in the hidden layers is 3. The activation function is based on a hyperbolic tangent with a dependent variable: Islamic Piety. The output layer is with 1 unit based on a standardized method of rescaling scale dependents. The activation function is identity and the error function is the sum of squares. In the present case, SPSS has produced a network diagram exemplified in Figure 2. At this point, \\\"synaptic weight\\\" implies the strength or amplitude of a connection between two nodes. The diagram indicates the 5 input nodes, the 3 hidden nodes, and the 1 output node symbolizing Islamic Piety. Table 8 highlights the overall information regarding the results of training and applying the MLP network to the holdout sample. The sum of squares error is shown as the output layer and has scale dependent variables. This in particular is the error function that the network intends to reduce and curtail during training. One successive step with no reduction in error was applied as a stopping rule. The relative error of the scale-dependent variable is the ratio of the sum of squares error for the \\\"null\\\" model, in which the mean value of the dependent variable is used as the predicted value for each case. The average overall relative errors are constant across the training (0.01) and the testing (0.01) sample, which presents us with the confidence that the error in future cases, scored by the network will be close to the error indicated the table presented. Table 9 identifies the significance of the independent variable based on the covariates with the dependent variable. In the present study, the Islamic Piety is a measure based on 2 sub-constructs of Islamic spirituality and 3 sub-constructs of Islamic social responsibility. The most dominating variable as per the results is rituals (100%), followed by integrity (97%), belief (57%), love of family (54.7) and the last influencing variable is justice (42.1%). Figure 3 displays the normalized importance chart of the variables that is based on the independent variable's importance. 5. Discussion and conclusion This study investigates a hybrid model of Islamic piety, by merging exploratory factor analysis (EFA) and an artificial neural network (ANN). Past studies in this area have certainly not employed this hybrid approach. The dimensions for Islamic piety were extracted using the Delphi method and tested in a large sample (500 employees). The results of EFA confirm that the dimensions of Islamic piety can be explained by the model. Indeed, it can be established that the EFA results offer a new model of Islamic Piety in the given context. It is also acknowledged that Islamic piety comprises five factors: rituals, integrity, justice, belief, and love of family. These five factors showcase a strong relationship and constitute the main construct that is Islamic piety. It can be further argued that the findings of this study give extra weight to these factors to fully grasp the construct of Islamic Piety from a Turkish perspective. An ANN model was formed and enhanced using the determination of model variables. The results revealed that the main variables in the Islamic piety are rituals, integrity, belief, love of family, and justice. The EFA results confirmed that the Islamic piety is explained by 2 variables falling under the head of Islamic spirituality and 3 variables of Islamic social responsibility. The results of ANN modeling for variables imply that following Islamic ritual is one of the most significant sub- constructs of Islamic piety. It may be argued that in the Turkish context the primary focus or emphasis of employees\\/believers is on practicing rituals by which one feels nearer to the Creator. It is deemed that Rituals cleanse a person's soul and by worshiping Allah (SWT) a person proves himself as a valuable entity for the society or the organization, through his\\/her kind dealings with others. This leads to a reduction in stress, fear, anger, jealousy, and guilt while engendering happiness and peace. Notably, rituals play an important role in Islam and in the life of a Muslim. An identity of a Muslim believer is primarily constructed, preserved, enforced, and maintained through observing rituals such; prayers, fasting, pilgrimage and charity. It is to be realized that \\\"Islamic rituals have become an effective means of constructing Muslim identity through its highly embodying practices of religious values and teachings into embodied individual Muslims\\\" (;, p. 16). Moreover, the rituals when practiced properly help in strengthening loyalty, sensitivity, and identity among Muslim believers. Hence, \\\"rituals can strengthen the link or connection between oneself and Allah (SWT) which brings strong support for enjoining good and forbidding all immoralities\\\" (;, p. 111). In the Quran, Allah (SWT) says: \\\"O you who believe, fear Allah, and be in the company of the honest\\\" (Quran, 9:119). Integrity is the second most important factor identified by this study. Mohsen (2007) advocates that one of the key traits of a true believer is his\\/her honesty: \\\"A believer's honesty would be reflected in their devotion towards Allah (SWT), as well as when he\\/ she speaks or deals with others. Moreover, devoted Muslims would not incline in any kind of cheating, misrepresenting of facts or falsifying information from others\\\". Particularly, maintaining integrity has societal benefits, for individuals that have integrity, can build trust in professional as well as nonprofessional relations with others. Popular and well-liked authentic individuals also benefit from societal support and thus enjoy close relationships with others. In Islamic Management, integrity can be attributed directly, and in parallel, to the attributes of honesty, trust, faith, and stout belief, influential and noble personality. Personal and organizational integrity lead to good and smooth relationships framed with morality and ethics. \\\"Belief\\\" in the current study ranks third. Belief (Iman in Arabic) provides a believer with an aspiration to make their self-analysis and understand their actual role in life. It is this role in life that would carry on even after their death through their righteous deeds and good children. In the Quran, Allah says, \\\"Say: He is Allah, the One! Allah, the eternally Besought of all! He begetteth not nor was begotten. And there is none comparable unto Him\\\" (Quran, 112: 1-4). N. M. Kamil (2012, p. 110) asserts that \\\"belief in Allah (SWT) implies a profound understanding of the unity of direction (Tawhid), clarity of goals, prevention of misconduct, and equality between people\\\". Belief works as protection that restrains one from all those deeds and actions that are forbidden and lights one's mind to recognize virtue, good values, and morality. In short, belief is to follow the right path that has been set by Allah (SWT) and avoid all that has been forbidden. Lastly, it is to be noted that Islam teaches the value of both belief and practice and Muslims are required to put their beliefs into practice by performing the rituals devotedly. It can therefore be inferred as to why Rituals is ranked first on the basis of the results of the ANN followed by honesty and belief. Based on the overall results of the ANN love of family is ranked fourth. It could be argued that the love of family is the core basis of Islamic culture and the same can be observed in the Turkish context: \\\"the harmony and confidence presented by a stable family are highly respected, valued, and seen signs for the divine and spiritual growth of all members in the society\\\" (;, p. 108). It is relatively common in a Muslim community to find extended families who live together and providing each other with comfort and support. Islam places a premium on respect for parents. Especially, mother and father are highly honored and regarded. Justice is the fifth most significant subconstruct that forms Islamic Piety. In the Quran it is written that: \\\"Allah commands justice, the doing of good, and liberty to kith and kin, and He forbids all shameful deeds, and injustice and rebellion: He instructs you, that ye may receive admonition\\\" (Quran, 16:90). M. H. Kamil (2011) affirms that Muslims are repeatedly reminded in the Quran of their responsibility when carrying out trade to be honest and just, as well as being equitable and fair in the distribution of wealth in the society. It is apparent that in Islam the relationship between the employer and the worker must be developed based on justice and mutual consent to appreciate employee honor and dignity. Notably, Islam promotes compassion, love, and kindness towards one another, along with the element of civility and esteem in the conversation and discussion. Moreover, Islam inspires honesty, constructing of communal trust and confidence along with salutation and thanking people while communicating. In brief, this is one of the pioneering studies in the field of Islamic management and the first to employ ANN. The results of this research affirm that Islamic Piety is one of the key fundamentals of the Islamic faith. It includes two key dimensions that are Islamic spirituality and Islamic social responsibility. The findings of this study also make a new contribution to the theory, that two subconstructs of Islamic spirituality are \\\"rituals and belief\\\", and three subconstructs of Islamic social responsibility namely; integrity, love of family and justice defines Islamic piety in the given Turkish context. Further, by focusing on these dimensions, one may achieve greater ethical behaviour among employees in a workplace that can regulate much undesirable behaviour of employees. Additionally, this model of Islamic piety may accomplish and engender a more productive and progressive workplace.\",\"1135746334\":\"The global COVID-19 pandemic dramatically reduced the amount of in-person social interaction that adolescents could engage in. The Chinese government, in particular, followed a \\\"zero-COVID\\\" policy and implemented lengthy periods of social distancing measures and lockdowns, which included online schooling and remote work. As such, the internet became essential for adolescents' lives, and was the primary tool for both socializing and learning. Unfortunately, the increased use of digital technology, as well as the disruptions of the pandemic, likely placed adolescents at risk of cyberbullying and cybervictimization (;, b). Indeed, researchers have consistently found that cyberbullying behaviours significantly increased during the pandemic. Although previous research has suggested that various factors may influence adolescents' cyberbullying behaviour , little is known about which variables have the greatest impact on cyberbullying. Furthermore, the majority of research in this field has been conducted in Western cultures. To date, very little is known about cyberbullying among adolescents from Mainland China, and even less is known about cyberbullying when they were hardest hit by lockdown measures. Accordingly, the goal of the present study was to identify predictive factors (e.g., cybervictimization, socializing online, problematic smartphone use, parental trust and alienation, gender, and media habits) that are related to cyberbullying by using supervised machine learning in a sample of Chinese adolescents during the COVID-19 pandemic. Cyberbullying in China During the Pandemic Cyberbullying refers to aggression that occurs via digital technologies including smartphones, computers, and\\/or tablets. Chinese adolescents' typical cyberbullying behaviour includes sending hurtful messages, neglecting someone's posting, and harassing them on social media. Research has shown that being cyberbullied can lead to maladaptive outcomes, and has increasingly shown to be a significant problem experienced by children and adolescents (;, b). More specifically, studies have linked cyberbullying and cybervictimization with a host of internalizing and externalizing problems such as hyperactivity, conduct problems, substance use, gambling, and peer problems (;, b). As mentioned, face-to-face social activities were severely restricted during the COVID-19 pandemic in China. Several studies have shown that as technology-us increased for adolescents during the pandemic so did the likelihood of engaging in cyberbullying behaviour. By way of explanation for this, it has been theorized that spending time online increases the likelihood of being exposed to violent content, including cyberbullying, and that adolescents who are constantly exposed to this type of behaviour may come to accept, and even replicate, it. Indeed, researchers found that during the pandemic cyberbullying prevalence was very high in China. For example, a recent study showed that 64.32% of Chinese students (from 15 to 25 years old) reported that they had been exposed to cyberbullying, and 25.98% reported bullying others online (e.g., express hostility on social media and online games;). Similarly, Zhu et al. (2021a, b) demonstrated that, among several countries, China ranked fourth (44.5%) with respect to the prevalence of cyberbullying victimization, and these authors attribute the increased rates of cyberbullying to the pandemic. Risk Factors of Cyberbullying In recent years, researchers have increasingly focused on identifying the risk factors associated with cyberbullying. Much of this work has focused on demographic variables, as well as variables related to internet usage and access. Recently, other psychopathology-related variables (e.g., aggression, mental health problems) have also revealed important roles in explaining cyberbullying. Cyberbullyvictims and Routine Activities Theory In traditional bullying research, extant work has shown that most people who engage in bullying are either a victim or a perpetrator, with a much smaller group of individuals who are engaged in both (so called bullyvictims;). This group has received a lot of research attention because they tend to fare the most poorly in terms of both predictor and outcomes associated with bullying. In contrast to this, for cyberbullying and cybervictimization, researchers have found that it is much more common for adolescents to simultaneously be engaging in cyberbullying and be the victim of cyberbullying. For example, Brewer and Kerslake (2015) reported a high correlation between being cybervictimization and cyberbullying (r = .80). Further, a recent meta-analysis also demonstrated that the effect size of the correlation between being a cyberbully and cybervictim was moderate-high at r = .44 . We propose the Routine Activities Theory to explain why this is unique for aggression that is happening online. According to the Routine Activities Theory , three elements are important when considering the association between crime and bullying: motivation (e.g., previous bullying experience, desire to be online to socialize with friends), suitable targets (e.g., in online spaces anyone can be a target, regardless of social or physical power differences), and inadequate guardianship (e.g., online spaces are not typically monitored by parents or other adults). According to Routine Activities Theory, if any of these three elements are missing, the bullying behaviour is less likely to happen. Based on the Routine Activities Theory, then, adolescents who have experienced cybervictimization may experience a range of negative emotions such as shame, anger, and helplessness, and these negative emotions might lead them to engage in cyberbullying behaviour themselves. This might occur from simply being in virtual contact with potential offenders. IndeedQuintanaOrts and Rey (2018a, b) have reported that experiencing cybervictimization is one of the most important factors for predicting cyberbullying engagement. Risk variables that we are exploring in this study relevant to a Routine Activity Theory include: cybervictimization, digital technology use, and parental attachment. Digital Technology Use Digital technology use is also a known factor that is related to adolescents' cyberbullying behaviour. According to Routine Activities Theory, when adolescents are placed in a high-risk situation that is in proximity to potential offenders, the risk of cyberbullying increases. Indeed, work has shown that adolescents who spend more time socializing online are more likely to engage in cyberbullying behaviour. Zhou et al. (2013) showed specifically that time spent on social media was an important factor in predicting cyberbullying. Similarly, Demir and Kutlu (2018) found that more frequent and intense social media use can expose adolescents to aggressive behaviour, including cyberbullying. Finally, Craig et al. (2020) conducted a crossnational study in 42 countries, and found that engagement in social media was related to increased reporting of cyberbullying behaviours. To explain the positive correlation between social media use and engagement in cyberbullying, it has been argued that adolescents become especially vulnerable to engaging in cyberbullying behaviours when they spend an increasing amount time on social media because of the increased exposure to these behaviours. More specifically, the exposure to cyberbullying behaviours can serves to normalize and model them, and eventually make them seem acceptable. It also makes sense that these behaviours become further reinforced if they are successfully used to gain power and influence. Other factors that have been identified as risks for cyberbullying engagement include media habits and problematic internet use, which reflect different drivers for why individuals spend time online. Media habits refers to an automatically initiated behavioural response to social media. An example of this is when someone's phone buzzes and everyone in the vicinity looks at their own phone. In contrast, people tend to be much more consciously aware of problematic internet use, which involves spending an excessive amount of time online such that one's daily activities are impacted. Individuals who experience this also tend to have a cognitive preoccupation with the Internet, and view the world outside of the Internet as unattractive. It has been hypothesized that unconscious media habits if unchecked might contribute to problematic internet use (e.g., excessive internet usage and internet addiction;), which means we need to understand both mechanisms for technology engagement. Researchers have claimed that individuals engaging in problematic Internet use are more likely to exhibit increased irritability, which can lead to aggressive behaviour online (e.g., insulting other users;). From this perspective, according to Compensatory Internet Use Theory (CIUT;), which argues that people use the Internet to compensate for problems they are having in real life, if adolescents are overusing the Internet as a way to deal with negative emotions, a likely consequence of this is that they are more likely to engage in cyberbullying behaviours. This might be especially true for those who are struggling with social relationships in real life and are turning to the Internet to compensate for this. Indeed, previous studies have consistently found that both problematic internet use and media habits were associated with the increased risk of cyberbullying. For example, Liu et al. (2020) found that problematic internet use predicted adolescents' cyberbullying behaviour in China. For the current study, we similarly predicted that digital technology use (e.g., time spent online, media habits and problematic internet use) would be positively associated with cyberbullying among Chinese adolescents during the COVID-19 pandemic. Parental Attachment Another important factor in Routine Activities Theory that has been found to contribute to adolescents' cyberbullying behaviour is parental attachment. In the present study, we focused on two types of parental attachment: parental trust and alienation. Parental trust refers to the mutual understanding and respect between parents and their children. Previous work has suggested that positive parent-child relationships (e.g., parental trust) could effectively reduce cyberbullying involvement. For example, Zhu et al. (2019) found that positive parent-child relation was associated with less cyberbullying behaviour among Chinese adolescents. Adolescents who trusted their parents were more likely to develop positive understandings and skills that could facilitate positive interpersonal interactions, including interactions via the internet. The authors argued that this reduced the risk of cyberbullying involvement. By contrast, adolescents who feel alienated from their parents tend to develop problematic social skills and engage in more aggressive behaviour, including cyberbullying. Indeed, Shapka and Law (2013) found that parental trust was associated with less cyberbullying behaviour, especially among East Asian adolescents. Therefore, we expected that parental trust would be negatively related to adolescents' cyberbullying behaviour. In contrast, parental alienation would be positively related to adolescents' cyberbullying behaviour. Gender Previous studies have found mixed results around gender and cyberbullying\\/cybervictimization , however, when gender has emerged as a significant factor, it is boys who tend to report higher levels of engagement in cyberbullying behaviours and girls who tend to report higher levels of cybervictimization. Studies conducted with Chinese participants have found that boys tend to report higher levels of cyberbullying. For example, Chen and Chen (2020) discovered that boys in Taiwan, Hong Kong, and Mainland China reported higher levels of cyberbullying perpetration than girls. Similarly, Zhou et al. (2013) discovered that in China, boys were more likely than girls to be both perpetrators and victims of cyberbullying. According to the Gender Role Stereotype Theory, males are traditionally perceived as more dominant and females as more passive. Thus, it has been suggested that boys are more likely to engage in aggressive and\\/or cyberbullying behaviour because of the gender-stereotype. Therefore, we expected that compared to girls, boys might be more likely to engage in cyberbullying behaviour in this study. Machine Learning The main goal of machine learning is to create systems that can learn from data and generalize those learnings to new data. In comparison to statistical methods that place a strong emphasis on inference, machine learning focuses on prediction by employing general-purpose learning algorithms to discover patterns in large data sets. Machine learning has several advantages when studying psychological phenomenon in adolescents. For example, machine learning has a higher accuracy in detecting predictors of adolescents' emotional and behavioral problems in large data sets. Furthermore, unlike statistical modeling, which requires assumptions be met (e.g., normal distributions, linearity), machine learning does not and is therefore a more flexible analytic approach. Finally, dividing the data set into training and test data allows us to understand the adolescents' emotional and behavioral problems while also affording us the ability to predict future outcomes, which increases the confidence in our findings. Researchers do acknowledge some limitations of machine learning, including the need for a relatively large sample size and the difficulty in interpreting the results. Nonetheless, machine learning is becoming more prevalent in psychological research, with applications such as predicting cyberbullying. This advancement is resulting in enhanced accuracy in modeling. For example, Balakrishnan et al. (2020) developed an automated system to detect cyberbullying on Twitter, which utilized machine learning techniques to analyze psychological features of users such as personality traits, emotions, and sentiment. Similarly, Islam et al. (2020) combined natural language processing with machine learning to develop a successful method for identifying abusive and bullying messages online. Thus, in the current study, we used machine learning to investigate the predictors of cyberbullying behaviour. The Present Study The current study sought to investigate the most prominent statistical predictors of cyberbullying among Chinese adolescents during the COVID-19 pandemic by using supervised machine learning. The hypotheses are as followed: Hypothesis 1 we hypothesized that cybervictimization, online socialisation, problematic smartphone use, media habits, and parental alienation would be positively associated with cyberbullying. Hypothesis 2 parental trust would be negatively associated with cyberbullying. Hypothesis 3 boys would be more engaged in cyberbullying behaviour as compared to girls. Method Participants The study involved a total of 2053 participants (Mage = 16.36 years, SD = 1.14 years; 44.6% male) who were students in grades 5-12 from four public schools in Fujian province, People's Republic of China. All participants in the sample (2053 individuals) completed the survey questionnaire without any missing data, as it was mandatory for them to answer all the questions. Almost all participants were of Han nationality, which is the dominant nationality in China. The majority of participants (85.2%) had their own cell phone. These demographic variables had no significant impact on the variables of interest in this study. Measures Cyberbullying. The Cyberbullying Scale  is an 11-item measure of cyberbullying (e.g., \\\"posted or re-posted something embarrassing or mean about another person online?\\\", \\\"sent or forwarded a hurtful message electronically to someone (by email, text, on Facebook, etc.?\\\") that uses a 5-point Likert type scale (1 = \\\"has never happened\\\", 2 = \\\"has happened rarely\\\", 3 = \\\"happens every month\\\", 4 = \\\"happens every week\\\", and 5 = \\\"happens every day\\\"). This measure has been successfully used in previous work in Western culture  and has been shown to have sound psychometric properties among Chinese participants. Internal consistency in the present sample was 0.96. Cybervictimization. The Cybervictimization Scale  is an 11-item measure of cybervictimization (e.g., \\\"had something embarrassing or mean posted or re-posted about you online?\\\") that uses a 5-point Likert type scale (1 = \\\"has never happened\\\", 2 = \\\"has happened rarely\\\", 3 = \\\"happens every month\\\" 4 = \\\"happens every week\\\", and 5 = \\\"happens every day\\\"). This measure has been successfully used in previous work in Canada  and has been shown to have sound psychometric properties in China. Internal consistency in the present sample was 0.94. Socializing online. To assess whether the participants were typically going online for social purposes, four items asked how much time was spent online doing social things (e.g., \\\"socializing with friends on social network sites\\\"). Responses were based on a 5-point Likert-type scale ranging from 1 = (none of my time) to 5 = (all of my time). This measure has been successfully used in previous work in Canada. Internal consistency in the present sample was 0.65. Problematic internet use. Problematic smartphone use was measured using the Problematic Internet Use Questionnaire (PIUQ;). The measure is comprised of 18 items constructed to establish a valid and reliable instrument to identify the components of internet addiction and to measure the problems associated with excessive internet use. All questions asked respondents to provide a self-rating on a 5-point scale (0= \\\"never\\\", to 4= \\\"always\\\"). Sample items include \\\"I prefer social interaction rather than face-to-face interaction.\\\" and \\\"I have used the internet to talk with others when I was feeling lonely.\\\" A composite variable was created by calculating the average score across all items. This measure has been successfully used in previous work in China. Internal consistency in the present sample was 0.89. Parent attachment. In order to assess participants' attachment to parents, a shortened version of the Inventory of Parent and Peer Attachment was used (IPPA-SV;). Of particular interest, parent trust (e.g., \\\"My parents respect my feelings\\\", \\\"When I am angry about something my parents try to be understanding\\\"; a = 0.77), and alienation (e.g., \\\"I don't get much attention at home\\\", \\\"I feel alone or apart when I am with my parents\\\"; a = 0.62) subscales were used. Items are rated on a 4-point Likert scale (1 = \\\"Never true for me\\\", to 4 = \\\"Always true for me\\\"). This measure has been shown to have sound psychometric properties in China. Media habits. Habit strength was assessed using the Self-Report Habit Index (SRHI;). Participants were presented with the stem \\\"Going online (via a device or computer)\\\" followed by 12 items (e.g., \\\"I do frequently\\\", \\\"I do automatically\\\", and \\\"that's typically \\\"me\\\") using a 5-point Likert type scale (1 = \\\"has never happened\\\", 2 = \\\"has happened rarely\\\", 3 = \\\"happens every month\\\", 4 = \\\"happens every week\\\", and 5 = \\\"happens every day\\\"). This measure has been shown to have sound psychometric properties in China. Internal consistency in the present sample was 0.84. Procedures The present study obtained ethical approval from the institutional review board of [BLINDED FOR SUBSMISSION]. Written consent was obtained from all participants and passive consent was obtained from parents through the schools. All self-report questionnaires were completed online through Wechat Applet (this is the most popular social App in China; almost every adolescent has an account) when students were at home on the weekend. In terms of a response rate, 95% of adolescents completed the survey. Statistical Analytic Strategy SPSS 22.0 and R software, version 3.6.1 , were used to conduct preliminary analysis and machine learning (caret R package). The effective sample's participant rows were first shuffled using a fixed random seed (for subsequent, consistent replication). We also compared results by using a different random seed and found that the results were highly consistent (please see supplementary tables). Next, 70% (n = 1452) of the effective dataset were randomly selected as our training sample, and the remaining 30% (n = 621) as our test sample. The training sample included 640 males (44.1%, Mage = 16.35 years, SD = 1.13 years), most of the major study variables did not exceed reference skew values (i.e., > 1.96), indicating that they were normally distributed (range: - 0.40-0.99). Similarly, the test sample included 285 males (45.9%, Mage = 16.38 years, SD = 1.16 years), most of the major study variables did not exceed reference skew values (i.e., > 1.96), indicating that they were normally distributed (range: - 0.42-1.07). However, cyberbullying and cybervictimization were not normally distributed in both training sample (cyberbullying:.27; cybervictimization:.04) and test sample (cyberbullying:.76; cybervictimization:.66). Predictor and dependent variables were preprocessed after assigning training and test samples (to keep the test sample's data uncontaminated by the training sample). Next, regression-based symptom forecasting methods with supervised machine learning were conducted with cyberbullying as the continuous dependent variable. Different from traditional statistics, machine learning can create algorithms by using training samples and then applying algorithms to test samples (called \\\"supervised learning\\\";). The predictor variables included gender, socializing online, problem use, attachment with parents, cybervictimization and media habits. Five separate machine learning algorithms were compared (e.g., \\\"shrinkage\\\" methods - ridge, lasso, and elastic net regression algorithms, extreme gradient boosting, and random forest). When compared to linear regression, \\\"shrinkage\\\" methods produce better regression models because they reduce the likelihood of overfitting the data by including a penalty term in the Residual Sum of Squares. Specifically, lasso regression promotes sparsity by forcing some coefficients to become exactly zero, effectively performing feature selection. For ridge and elastic net regression, some regularization terms were added to the objective function, which penalizes the squared magnitude of the coefficient estimates. Furthermore, both extreme gradient boosting and random forest are tree-based algorithms that divide the dependent variable into increasingly homogeneous subgroups based on predictors. Leaveone-out\\\" cross-validation (LOOCV) was used to simulate validation data for each analysis. In LOOCV, the dataset is divided into subsets, where each subset consists of all the samples except one. The model is trained on the remaining samples and tested on the one sample that was left out. This process is repeated for each sample in the dataset, with each sample taking turns as the \\\"left out\\\" sample. The aggregated final predictive model was then applied separately to the external test sample to validate its performance further. All algorithms were utilized with their default parameters. See Fig. 1 for the proposed machine learning-based analysis method. Algorithms were compared using root mean squared error (RMSE), mean absolute error (MAE), and R-squared values. The higher R2 (close to 1), lower RMSE, and lower MAE (close to 0) indicate a better model. According to previous studies , R2 values above 0.5 are generally deemed acceptable. Additionally, Singh et al. (2004) suggest that RMSE and MAE values lower than half the standard deviation of the measured data can be considered low, and is acceptable for evaluating the model. It should be noted that this criteria may not be applicable to all situations, and the interpretation of these metrics should be done with caution. Finally, to assess the differences in performance between the algorithms, the Friedman tests were employed. Results Preliminary Analysis Means and standard deviations for all study variables are presented in Table 1. An exploratory t-test was conducted to examine the overall effects of gender on all variables. The results indicated that compared to girls, boys had lower scores on socializing online and problematic smartphone use (ps < 0.001) and higher scores on cyberbullying and cybervictimization (p < .001) (see Table 2). Correlations among study variables are presented in Table 1. Overall, the pattern of associations was consistent with theoretical expectations. Adolescents' cyberbullying was significantly and positively associated with cybervictimization, socializing online, problem internet use, and parent alienation. Machine Learning Next, the different algorithms that were employed were compared. As shown in Table 3, based on our criteria, R-squared values all 0.5. MAE values lower than half the standard deviation of the measured data for both training sample (standard deviation range from 0.36 to 0.75) and test sample (standard deviation range from 0.40 to 0.72). RMSE was slightly higher than half the standard deviation of the measured data for training sample. Moreover, the shrinkage algorithms performed slightly better than other methods. Specifically, the shrinkage algorithms have smallest RMSE (ranging from 0.203 to 0.208 in training sample, 0.193 to 0.195 in test sample), and largest R-squared values(ranging from 0.67 to 0.68 in training sample, 0.73 in test sample). Ridge regression was the best performing algorithm, while elastic net and lasso performed best when applied to the test sample. However, the performance (e.g., RMSE, MAE, and R-squared) among the algorithms are not significant different. To illustrate in terms of explained variance, the ridge and lasso algorithms led to 68% in the training sample and about 73% in the test sample (barely exceeded by elastic net in the test sample, also approximately 67%). Based on the superior performance of the ridge, lasso, and elastic net algorithms, these techniques were employed to analyze the regression coefficients of predictors in the prediction of cyberbullying. The findings revealed that cybervictimization had the most substantial impact on predicting cyberbullying, followed by gender, parent alienation, and problem internet use. For detailed importance indices, which were standardized as z-scores and can be interpreted as standardized regression coefficients, please refer to Table 4. Discussion The COVID-19 pandemic has reduced adolescent faceto-face interaction and increased the need for technology, which has augmented the risk of engaging in cyberbullying behaviour. In this study, a machine learning approach was used to identify the most significant statistical predictors of cyberbullying among Chinese adolescents during the COVID-19 pandemic. A regression model was fit to a training sample, and validated well when simulated against an external test sample. The results were comparable across the various machine learning statistical algorithms used, but shrinkage algorithms outperformed the others. Cybervictimization, in particular, was found to be the most predictive of cyberbullying. Cybervictimization Findings from this work indicate that, when compared to other risk factors, cybervictimization had a relatively large contribution in predicting cyberbullying among Chinese adolescents. Removing cybervictimization as a predictor resulted in a 52-60% reduction in variance when using the shrinkage algorithms. The findings are consistent with previous studies, which have demonstrated that cybervictimization is strongly associated with cyberbullying among adolescents (;, b). Researchers have argued that adolescents can play a dual cyberbullying role by simultaneously engaging and experiencing bullying and victimization. We do know from previous research that individuals who experience strong negative feelings may engage in aggressive behaviour in the future as a way to manage their anger and fear. As a result, victimization, as a negative stimulus, may lead some adolescents to employ poor coping strategies, such as engaging in more aggressive behaviour, in order to relieve the pressure and negative emotions associated with being victimized. Given that this is occurring in an online environment, where there is a diffusion of responsibility and\\/or a protection from being behind the screen , the relationship between cybervictimization and cyberbullying makes sense. This finding has important implications for cyberbullying prevention and intervention for adolescents. For example, the findings suggest that work to identify and support adolescents who have been victims of cyberbullying and programs to help them manage their negative emotions appropriately. One way of doing this would be for schools to provide young people with more access to counseling services. Another possible intervention would be to create peer-lead intervention and education programs. Given that adolescents are deemed the technological experts, with adults relatively speaking being the novices , it is recognized that programs that are for, by, and about peers are more impactful. Digital Technology use In terms of technology engagement, results found that problematic internet use, time spent online, and media habits played a relatively less important role in determining cyberbullying behaviour during COVID-19. Previous studies suggested that problematic internet use was associated with cyberbullying behaviour among adolescents. However, our findings indicate that, while digital technology use may pose a risk for cyberbullying behaviour, it is not the primary risk factor during the pandemic. Due to school closures and limited physical interaction, using technology became essential in adolescents' lives. Fortunately, while it may have provided additional opportunities for having negative interactions with peers, it did not appear to contribute to cyberbullying behaviour. These findings should go a long way to alleviating concerns about the consequences of adolescents' increased technology use during the pandemic. Parental Attachment Compared to parental trust, this study found that parental alienation was more strongly related to cyberbullying behaviour among Chinese adolescents. According to the Social Control Theory, parents play an important role in reducing bullying among adolescents  and previous literature supports this. For example, Patchen and Hinduja (2011) found that adolescents who experienced family problems were more likely to engage in cyberbullying than others. Similarly, Shapka and Law (2013) found that East Asian adolescents' parents with a higher level of parental trust were less likely to engage in cyberbullying behaviour. Finally, Buelga et al. (2017) found that father alienation significantly predicted adolescents' cyberbullying behaviour. Our results imply that, rather than a positive psychological attachment, parents' alienation and their role in restricting online communication of adolescents, plays an important role in predicting cyberbullying behaviours. Future work needs to explore the efficacy of intervention and educational programs that target parent\\/adolescent interactions around screen-time and device use. Gender Our results are consistent with the findings of several Chinese studies, which demonstrated that compared to girls, boys engaged in more cyberbullying behaviour. Males, as previously stated, are perceived to be more dominant, whereas females are perceived to be more passive. Indeed, gender stereotypes, where boys are taught to be active, brave, and self-sufficient, and girls are taught to be gentle, polite, and kind are even more prevalent and accepted in Chinese culture. As a result, it is not surprising that our results found that girls engage in less cyberbullying behaviours than boys. The findings suggested that when addressing the issue of cyberbullying and developing relevant prevention efforts for Chinese adolescents, gender differences need to be considered. Similarly, future interventions to reduce cyberbullying might be more fruitful if they specifically target boys in Chinese culture. Limitation and Future Direction The results from the current study add to the field of research on cyberbullying. Specifically, the present study contributes to prior literature on relationships between several factors (e.g., internet use, parent-child relations, and cybervictimization) and cyberbullying, and emphasize the important role of cybervictimization in predicting cyberbullying. Machine learning was used in this study to model cyberbullying, which, as noted, has the potential to offer advantages over traditional statistics. Specifically, using machine learning with pattern detection and training on a subset of data before applying it to another subset of data can produce higher prediction accuracy than traditional approaches. Moreover, it allows us to understand the current situation while also allowing us to predict future outcomes, thereby increasing our confidence in our findings. In the current sample, across the training sample and the test sample, our results consistently demonstrated that cybervictimization plays the most important role in contributing to adolescents' cyberbullying behaviour. This has important implications for the development of intervention and education programs, as well as in thinking about how we provide support for victims of cyberbullying. Despite the contributions of this research to the extant literature, some limitations should be noted when interpreting the findings. First, the measures used in this study were based solely on self-report, which makes social desirability bias a concern, especially given that we were measuring an anti-social behaviour (e.g., cyberbullying). As such, future researchers might benefit from acquiring convergent information from alternative sources of cyberbullying\\/ victimization behaviours (e.g., peer ratings, observations). Furthermore, the sample size in this study may not be sufficiently large to adequately capture the intricate nature of the relationship between predictors and cyberbullying, particularly when examining interactions among these predictors. It is recommended that future studies address this limitation by employing a larger sample size to further investigate these dynamics. In addition, it would be beneficial for future studies to categorize participants as either engaged in cyberbullying or not. This would allow for a comparison between the model's predictions and the actual outcomes, utilizing Kappa statistics as a measure of agreement. Our study was also cross-sectional which limited our ability to investigate the direction of effects between the constructs. For example, previous studies found that cyberbullying could predict problematic internet use and poor parent-child relations. Therefore, future studies should adopt a longitudinal design to investigate those effects over time. In addition, it's important to note that the majority of the students who participated in this study were from middle schools, which means that the prevalence rate of cyberbullying among Chinese high school students remains unknown. As a result, future studies should look into cyberbullying and its predictors in people of different ages (e.g., high school students). Additionally, it is important to note that our findings are derived from a specific sample of participants, and it remains unclear whether these findings can be generalized to other populations or contexts. To address this limitation, future studies should aim to replicate these findings in diverse populations and different contextual settings. Finally, the presence of other risk factors, such as personality traits  or peer relations  that could also predict cyberbullying behaviour were not examined in this work. van Geel et al. (2017) found that agreeableness and sadism were associated with adolescents' cyberbullying, suggesting that future studies should further investigate other predictors. In addition, it is important to note that our study only compared five specific machine learning algorithms. In future research, it would be valuable to include a comparison with other algorithms, such as K-nearest neighbours or naive Bayesian algorithms. Despite these limitations, the findings of this study contribute to the existing body of knowledge about the risk factors associated with cyberbullying among Chinese adolescents. Specifically, findings from the current study suggest that cybervictimization plays the most important role in contributing to adolescents' cyberbullying behaviour. As a result, our findings have significant implications for intervention and prevention efforts aimed at adolescent cyberbullying in China. As discussed above, these efforts should specifically aim to identify and target adolescents who experience cybervictimization. Furthermore, assisting adolescents and their parents in developing positive parentchild relationships, particularly focused on reducing parental alienation, may be beneficial in lowering their risk of engaging in cyberbullying behaviour. The datasets generated during and\\/or analysed during the current study are available from the corresponding author on reasonable request. Declarations of Interest The authors have no relevant financial or nonfinancial interests to disclose. Ethics Approval Approval was obtained from the ethics committee of East China Normal University. The procedures used in this study adhere to the tenets of the Declaration of Helsinki. Consent to Participate Informed consent was obtained from all individual participants included in the study.\",\"1135746357\":\"INTRODUCTION The family planning policy is the basic state policy of China, and its strict implementation has driven a dramatic shift in China's population growth pattern. Between the 1970s and the midto late-1990s, China's total fertility rate (TFR) dropped sharply, from 5.81 children per family to approximately 1.80. By 2015, the National Bureau of Statistics released data on the 1% population sample survey, which indicated that China had entered a stage of severe under-population. Once the TFR falls below 1.5, the society falls into a \\\"low-fertility trap,\\\" and it is difficult to return to normal levels of generational replacement over a longer period of time. When China entered an aging society in 2000, there were about 130 million people over the age of 60, accounting for 10.2% of the total population;, the number of people over the age of 60 had risen to nearly 254 million, representing 18.1% of the total population. The alarming trend of couples having fewer children, combined with an aging population, has led to the disappearance of the demographic dividend, posing a significant challenge to China's economic and social development. To address the challenges of continued low fertility and an aging population, in January 2016 the Chinese government endorsed the birth of two children per couple. There have been mixed views among scholars on this policy. Proponents argue that the policy will lead to a great increase in TFR and slow down the process of population aging. However, other researchers take the opposite view, citing the high cost of raising children and sense of gender equality. Today it has become a social consensus that the two-child policy does not work well in China because the TFR did not increase as expected. With the change in fertility policy, the fertility incentives faced by families are bound to change. Therefore, it is important to clarify the factors influencing couples' willingness to have a second child so that we can promote an effective intervention policy that will optimize the demographic structure. At the same time, the study on the fertility intentions of second child can help to design the future progressive liberalization of population policy (three-child policy or even full liberalization). Considering the above realities of China's socioeconomic situation, this paper seeks to examine the following core questions: (1) What factors have important impacts on couples' willingness to have a second child in China? (2) What are the differences between men and women in terms of their willingness to have a second child? Specifically, this paper uses data from the Chinese General Social Survey (CGSS) to analyze the relationships between the fertility intentions of different cohorts and the variables impacting those intentions. The paper uses three machine learning methods--artificial neural network (ANN), random forest (RF), and XG-boost--to systematically identify the important factors; next, a logistic regression model is used to investigate the mechanisms of the factors that influence the willingness to have a second child. LITERATURE REVIEW Scholars have conducted many studies investigating the factors that influence fertility outcomes, and that body of research can generally be categorized into micro, meso, and macro perspectives. At the micro level, these can be further divided into psychological decision-making dimensions, such as the desire to have children ; partnership dimensions, such as marital relationships and the division of household labor ; individual socioeconomic dimensions, such as income and human capital, including education and employment ; and the influence of underlying biological and genetic traits. The meso level can be divided into the social-interaction dimension, including personal networks and social learning ; the place-of-residence dimension, which focuses on the heterogeneity of regions ; and the social capital dimension, which includes goods as well as information, money, work capacity, influence, power, and positive help. At the macro level, the dichotomy between \\\"economy and culture\\\"  dominates and considers economic and employment trends  along with values and culture. Family policies and welfare systems can strongly influence fertility outcomes. As the birthplace of the modern welfare system, European countries have implemented complementary family policies such as paid leave entitlement, childcare services, and financial transfer to avoid a continuous decline in population by extending state involvement to the family-reproduction sphere. This state involvement in the family domain has prevented a steady decline in the birth rate in recent years. Although many studies have attempted to capture the causality between patterns of demographic transitions and types of family policies and welfare regimes , encouraging fertility is a very complex, systemic ambition, and there are significant differences in the values, programs, and support of fertility policies depending on the welfare regimes, resulting in very different fertility outcomes across countries. In terms of the statistical methods used to study fertility, there are currently two focuses of research. The first is the analysis of causality and endogeneity, such as the link between the field of education and age at first birth at the micro level , the problem of correctly identifying socialinteraction effects at the meso level , and the difficulty of distinguishing the effects of policies from other factors--observable or unobservable--at the macro level. The second focus is on the prediction of fertility outcomes by machine learning, which has been very poorly covered in the literature. Studies from the United States, India, Indonesia, and other regions  have used different machine learning algorithms like regression, decision trees, k-nearest neighbors, and ANN, as well as other ensemble methods like bagging and boosting, to determine the population. This relied on the availability of historical demographic data like population, fertility, mortality, and life expectancy. However, the current studies have not focused on estimating population and TFR in a data-constrained environment and with a lack of international comparison. Since the population policy is unique to China and has been implemented since 2016, scholars have conducted extensive research on second-child fertility. Chen et al. (2019) conduct a comprehensive analysis using stepwise regression and found that household economic and health risks significantly influence the intention to have a second child. Zhou and Guo (2020) use multilevel regression to find that men, the younger and wealthier, ethnic minority, and rural populations are more likely to have a second child. Scholars have also attempted to focus their research on one perspective. Economic factors are the focus of current research, with Lan (2021) demonstrating that women with better socioeconomic status and those who were born into better-off families show a relatively strong desire to have children, and Shen and Jiang (2020) investigating highly educated women and finding that their fertility choices are the result of the intersection of state policy interventions and career choices. Urban-rural differences have captured the attention of many scholars as well. Using the Blinder-Oaxaca decomposition technique, Zhou and Guo (2021) find that both education and son preference play a prominent role in explaining the willingness to have a second child in rural areas, and Li et al. (2022) note that women face the dilemma of having children or seeking employment stability, and that this effect is stronger for urban than rural women. In China today, small families are preferred, and women also have more of a voice in family issues and decision-making , so scholars have also studied gender equality and fertility intentions. Bao et al. (2017) and Li and Jiang (2019) find that the more equal gender role attitudes are, the more women with more economic and family\\/social resources are more likely to have a second child, while women who work in the non-agricultural sector and have higher decision-making power in the household are less likely to have a second child. The influence of siblings has also intrigued scholars, and the number of siblings has been shown a significant predictor of women's fertility intentions. The loneliness experienced by only children during childhood and adulthood leads most of them to believe that having siblings is better than being single. However, Zhang L. et al. (2021) argue that sibship size may also have negative direct effect presumably due to sibling competition for intergenerational support. Scholars have sought to further analyze it also through qualitative analysis methods such as interviews. Using qualitative data from 53 urban parents in China, Peng (2020) suggests that the fertility decision to have a second child is an ongoing bargaining process rooted in the life course rather than an isolated family event. In addition, attitudes and behaviors toward fertility in China are rooted in Confucian philosophies and traditions of ancestor worship and may also be influenced by religious notions of fertility. Therefore, the status of fertility studies in China needs to be improved in three aspects. First, the data need to be updated. Due to the delay in data release, the existing research mainly comprised survey data through 2015. However, the data from 2017 to 2018 represent important improvements in the content of the questionnaire, with more informative content, a more reasonable design, and higher data quality. Second, the methodology needs to be improved. Most of the existing studies used a single model for empirical evidence, such as probit or logistic regression. If the relationship between the independent and dependent variables does not conform to the regression form, it will not be extracted by the model, and the important variables can be easily missed. This results in one-sided empirical results, which cannot achieve the desired comprehensive, systematic, and integrated test. Third, the generalizability of research findings needs to be improved. The existing studies focused on the local population and cannot grasp the relationship between the influencing factors and fertility from the overall population. In summary, inspired by the micro-meso-macro classification and applying it to the specific national situation of China and the characteristics of CGSS data (mainly on micro and meso levels), this study focuses on three characteristics and proposes four hypotheses. Hypothesis 1: Individual characteristics (e.g., gender, age, ethnicity, health, and education level) will affect fertility intentions. Hypothesis 2: Family characteristics (e.g., family income and number of siblings) will affect fertility intentions. Hypothesis 3: Social Characteristics (e.g., location, religion, health insurance, and hukou) will affect fertility intentions. Hypothesis 4: The above three groups of characteristics will have different effects on the fertility intentions of males and females. The subsequent portion of this paper is structured as follows: The third section introduces the machine learning models that will be used in this paper and briefly describes the data sources and classifications; the fourth section introduces the data and methodology; the fifth section describes an empirical analysis in which the importance of different factors is analyzed by machine learning and quantified and explained using logistic regression. Finally, a discussion and summary are presented for the benefit of future research. MATERIALS AND METHODS Machine Learning Methods The connection of the variable \\\"willingness toward fertility\\\" to different independent variables is always complex and shifting, and the relationship between variables and willingness toward fertility is non-linear. Because machine learning methods can fit non-linear information as well as linear data, this paper adopts three common machine learning methods to identify the influencing factors: ANN, RF, and XG-boost, and it adopts a logistic regression model to study the mechanism of those influencing factors of willingness toward fertility. Because this paper adopts machine learning, data mining, and data-driven research methods and ideas, and starts from data and objective reality rather than a priori assumptions, the relevant variables are selected to reflect the respondent's basic personal information, work information, and health information. This method makes the profile more detailed, the behavior measurement more accurate, and the empirical conclusion more comprehensive, systematic, and effective. At the same time, these three machine learning methods can support each other to obtain more scientific conclusion. First, from the perspective of optimization methods, the ANN is a local search-optimization method, which may fall into local extremes instead of global optima, thus affecting the overall training effect. The RF technique is a random method to build a forest, which adopts the principle of \\\"minority follows majority\\\" to perform integrated discrimination and ensure the overall optimum. The XG-boost is a method to grow a forest by continuously splitting feature variables to ensure the overall optimum. XG-boost grows the tree by splitting the feature variables continuously and relearns every tree generated to continuously improve the learning quality and dynamically approach the overall optimum. Second, from the viewpoint of applicability, RF has the tendency of overfitting when the data is noisy; XG-boost can effectively prevent the overfitting problem by introducing penalty terms. The ANN has strong fault tolerance and can work normally even after local damage. Finally, in terms of computational efficiency, XG-boost pre-sorts the nodes features before iteration and iterates through them to select the optimal partition point, which is a greedy algorithm that takes a longer amount of time when the data volume is large. Artificial Neural Network Model The ANN is an abstract computational paradigm modeled after the human brain that consists of interconnected neurons, i.e., processing units, which simulate the human brain's thinking for computational modeling. Figure 1 shows the common two-layer and three-layer ANN. The B-P ANN model used in this paper is a kind of feed-forward ANN, which has many advantages. First, it has a strong non-linear mapping capability. As many of the independent and dependent variables in the underlying data are non-linear, and the strength and form of the relationships are unknown, the ANN can solve this problem well. Second, the ANN has outstanding self-learning and self-adaptive capabilities. During the training process, the model can automatically extract the data rules between the input data and the output data and write down the learned \\\"rules\\\" through the weights. This allows the ANN to spontaneously learn the connections between the data and objectively reflect these connections. Third, it is highly fault-tolerant, ensuring that the global training results do not deviate significantly when local or partial neurons are damaged and that the system can still function properly even after local damage. Random Forest Model The RF model is an RF technique that contains several decision trees with high prediction accuracy that are weakly correlated or even uncorrelated. Based on the classification results of these decision trees, the principle of \\\"majority rule\\\" is applied to make a comprehensive, integrated discrimination. Random forest is a combined prediction model that contains multiple decision trees. As a common method of machine learning, RF has wide applicability due to its simple and easy-to-understand modeling principle, and it offers several advantages. First, RF can handle many input variables, i.e., it can handle high-dimensional data. Second, the RF uses error estimation for errors, which makes the model generalize well. For unbalanced data, the RF can balance the errors and maintain the accuracy of the model even when the features are missing. Finally, since the RF is composed of decision trees, and these decision trees are independent of each other, each tree can be processed in parallel, which makes the model training fast. Through the idea of integration, the integrated discriminative classification is performed based on the classification results of all trees. XG-Boost Classification The basic purpose of the XG-boost algorithm is to keep generating the tree and to grow the tree by continuously splitting the feature variables. Each time a tree is generated, a new function is relearned to fit the residuals of the last prediction, which improves the learning quality and approximates the actual value. Compared with the traditional gradient boosted decision tree (GBDT) algorithm, XG-boost has significant advantages. First, the traditional GBDT algorithm model only uses the information of first-order derivatives in the optimization process, while XGboost performs the second-order Taylor expansion of the penalty function and uses the information of first-order derivatives and second-order derivatives for optimization, which makes XGboost's optimization faster. Second, XG-boost can automatically learn the splitting direction when there are missing values in the sample, and after each iteration, the model assigns the learning rate to the leaf nodes, reduces the weight of each tree, reduces the impact of each tree, and provides better learning space for subsequent training. Finally, XG-boost also supports parallelism but unlike RF, this parallelism is at feature granularity rather than tree granularity. In sum, the XG-boost model can quickly and accurately complete the classification and prediction of data, and the addition of penalty terms can effectively prevent overfitting based on high accuracy. Evaluation Indicators of Results All the machine learning model results are analyzed based on the confusion matrix, as shown in Table 1. True positives (TP) indicate the number of samples with positive actual results and positive predicted results; false positives (FP) indicate the number of samples with negative actual results but positive predicted results; false negatives (FN) indicate the number of samples with positive actual results but negative predicted results; true negatives (TN) indicate the number of samples with negative actual results and negative predicted results. There are three main associated indexes. (1) Precision rate: Precision = TP\\/(TP + FP), which indicates the proportion of the actual positive samples among the predicted positive samples. (2) Recall rate: Recall = TP\\/(TP + FN), which indicates the percentage of predicted positive cases in the sample of actual positive cases. (3) Area under curve (AUC), which indicates the size of the area under the ROC curve. This study also uses machine learning to rank the variables according to their importance from largest to smallest according to the F-score. Logistic Regression Logistic regression modeling is a multiple regression analysis method used to study the relationship between dichotomous dependent variables and their influencing variables, i.e., it assesses whether an event occurs and what the probability of occurrence is when the influencing variables take on different values. Assume a vector X = (x1, x2, x3,. . ., xn) with n independent variables, and assume that the conditional probability P (y = 1| x) = p is the probability of occurrence of the event in the dependent variable when the independent variables take values. The logistic model can be expressed as: P(y = 1 |x ) = p(X) = 1 1+ e-g(x) (1) f (x) = 1 1+e-g(x) is called logistic function, g(x) = o1 + o1x1+. . . + onxn, then the probability of not occurring under condition x is: P(y = 0 |x ) = 1- P(y = 1| x) = 11 1+ e-g(x) = 1 1+ e g(x) (2) Therefore, the ratio of the probability of an event occurring to the probability of it not occurring is: P(y = 1 ||x) P(y = 0 ||x) = P 1- P e g(x) (3) Logistic regression models can be solved iteratively by using the gradient ascent algorithm or by using the Newton-Raphson iteration, and the values of the dependent variable are (0, 1), and after modeling, the probability values of the dependent variable represent the probability of the event. The logistic regression model can predict not only in-sample but also out-of-sample data, and it can compare and test the prediction results. DATA Data Source The data came from the CGSS questionnaire (resident questionnaire) for two consecutive years--2017 and 2018. The survey is the first nationwide, comprehensive, and continuous large-scale social survey project in China, and it includes 125 counties (districts), 500 streets (townships), 1,000 neighborhood (village) committees, and 10,000 individuals in households. A total of 25,369 samples were obtained from the survey data in 2017 and 2018, and according to the characteristics of the research subjects of this paper, the sample of people of childbearing age between 20 and 50 was selected. The sample was selected according to the characteristics of the research population, and after determining the relevant variables that conform to the research content of this paper, some \\\"missing,\\\" \\\"don't know,\\\" \\\"indifferent,\\\" \\\"unable to answer,\\\" and \\\"not applicable\\\" were excluded. Ultimately, 15,909 valid samples were obtained. Variable Description In keeping with the purpose of this study, the dependent variable is whether or not respondents are willing to have a second child. Specifically, the questionnaire asks, \\\"How many children would you like to have if there are no policy restrictions?\\\" If the answer is fewer than two (in other words, the respondent does not want to give birth to more than one child), we assign the value \\\"0\\\"; for those who are willing to have two or more children, we assign the value \\\"1.\\\" The independent variables include basic individual characteristics, family characteristics, and social characteristics. Individual characteristics include age, ethnicity, health, and education level. Family characteristics include the respondent's marital status, family economic level, and the number of people living together in the family. Social characteristics include the respondent's geographic location, hukou, religious beliefs, and decision about participating in medical treatment insurance. The specific variable descriptions are shown in Table 2. EMPIRICAL ANALYSIS Identification of Factors Influencing Fertility Intention From the descriptive statistical analysis, the influence of willingness to have two children on different independent variables is complex and variable. That relationship is non-linear, and machine learning methods can fit the non-linear information well. Therefore, this paper adopts the three commonly used machine learning methods previously mentioned in attempting to identify the influencing factors. Artificial Neural Network Modeling A total of 15,916 samples are included in the base data of this study. The categorical independent variables (such as gender, region, and hukou) are transformed into dummy variables with horizontal signs, and the ordinal independent variables (such as education, health status, and income level) are transformed into factors with numerical signs. This may cause the model to learn the characteristics of those who are willing to have a second child, but not those who are not willing to have a second child, resulting in poor classification results. Therefore, in this paper, we use a down-sampling method to randomly select 3,366 samples of people who are willing to have a second child and obtain a total of 6,732 samples for modeling. The main purpose of this paper is to separate the data into two categories: those who are willing to have a second child and those who are unwilling to have a second child. Before training, the data are randomly split into a test set and a training set in the ratio of 3:7. In determining the topology of the ANN, a trial-and-error method is used to build a four-layer ANN after trying different ANN structures. The prediction results on the test set are obtained based on the training set model, and the confusion matrix with the actual categories in the test set is shown in Table 3. It is apparent that the category \\\"willing to have a second child\\\" is applicable to 1,078 people, of whom 597 are judged to be \\\"willing to have a second child,\\\" and 481 are judged to be \\\"unwilling to have a second child.\\\" For the category \\\"unwilling to have a second child,\\\" 731 people are judged to be \\\"unwilling to have a second child,\\\" and 348 are judged to be \\\"unwilling to fertility.\\\" For the \\\"willing to have a second child\\\" category, the precision rate of the model is 597\\/(597 + 481) = 55.4%, and the recall rate is 597\\/(597 + 348) = 63.2%. This means that about two-thirds of those who are willing to fertility are judged to be correct, while the model misclassified two-fifths of the sample as \\\"willing to have a second child.\\\" The result of classification of the ANN is acceptable. The independent variables are more effective in distinguishing those who are willing to have a second child, but the model cannot give a numerical measure of which variables are more important for classification. In practice, there are significant differences in the values of \\\"willing to have a second child\\\" and \\\"unwilling to have a second child\\\" in terms of gender, age, health status, education, number of siblings, and region. In conclusion, although the ANN can fit the non-linear relationship between variables well, it cannot measure the importance of each variable, and to remedy this deficiency, this paper uses RF to investigate further. Random Forest Modeling The same extended 6,732 samples are used to build the RF model. To determine the number of decision trees in the RF, the relationship between the out-of-bag error rate and the number of decision trees is drawn, as shown in Figure 2. When the number of decision trees reaches 286, the error rate reaches its lowest point and stabilizes; therefore, an RF model with 286 decision trees is established. The model obtained in the training set is applied to the test set, and the prediction results are compared with the actual values of the sample. The classification confusion matrix is shown in Table 4; of the 1,068 people who are willing to have a second child in the test set, 647 are judged as \\\"willing to have a second child\\\" and 421 are judged as \\\"unwilling to have a second child.\\\" The actual number of those \\\"unwilling to have a second child\\\" in the test set is 1,089, among whom 659 are judged as \\\"unwilling to have a second child\\\" and 430 are wrongly judged as \\\"willing to have a second child.\\\" For the \\\"willingness to have a second child\\\" category, the precision rate of the model is 647\\/(647 + 430) = 60.1%, and the recall rate is 647\\/(647 + 421) = 60.6%. The RF has lower precision than the ANN, but a higher recall rate. The RF classification model can play a vital role in the classification and can prevent the omission of important variables. The results of the importance assessment of the variables are shown in Table 5. Here, the top five most important variables are age, number of siblings, health, education, and district. Age is the most important factor--neither income nor health. The second-most important variable is the number of siblings, which reveals the importance of the family of origin. XG-Boost Modeling The values of the XG-boost model at each sample point are obtained with the AUC indicator due to the optimization target and the maximum depth of the number of five. To classify the samples into two categories based on the values of the sample points, an optimal threshold is determined based on the ROC curve (subject operating characteristic curve), which is used as a threshold to segment the samples to achieve the highest accuracy of the classification results. The ROC curves in Figure 3 reflect the classification accuracy obtained with different values of the classification threshold. As demonstrated, the classification results are better when the threshold value is around 0.40. The best classification result is obtained when the threshold value is 0.40, and the AUC is the largest. The samples less than 0.40 are classified as \\\"unwilling to have a second child,\\\" and those greater than or equal to 0.40 are classified as \\\"willing to have a second child.\\\" The confusion matrix of classification results and actual categories is shown in Table 6. Among the 1,068 people who are actually \\\"willing to have a second child\\\" in the test set, 620 people are judged by the model as \\\"willing to have a second child,\\\" and 448 people are judged by the model as \\\"unwilling to have a second child.\\\" Among the 1,089 people in the test set who are actually \\\"unwilling to have a second child,\\\" 697 are judged by the model as \\\"unwilling to have a second child,\\\" and the remaining 392 are wrongly judged as \\\"willing to have a second child.\\\" The precision rate of the model is 620\\/(620 + 392) = 61.3%, and the recall rate is 620\\/(620 + 448) = 58.1% for the category of \\\"willing to have a second child.\\\" The precision rate of the XGboost model is in between those of the ANN and RF, but the recall rate is the lowest. The XG-boost model also examines the importance of variables, and the importance histograms are shown in Figure 4. The XG-boost modeling process plays an important role in age, number of siblings, education, health, and district. Although some of the other variables changed (e.g., gender and marriage), they are consistent with the variables identified in the RF model, and the results of the two models can be corroborated by each other. Comparative Analysis of Empirical Machine Learning Results To more objectively compare the results of each model, the modeling method of fivefold cross-validation is used, and the data on the average precision rate and average recall rate obtained are shown in Table 7. All of the average precision and average recall rates of the three models are above 60% under the fivefold cross-validation. The ANN has the highest mean precision rate (64.3%), and the RF has the highest recall rate (60.6%) for the category \\\"willing to have two children.\\\" The important variables obtained from the RF and XG-boost models--age, number of siblings, health status, income, and beliefs--are basically the same, although there are very small differences in ranks. Notably, economic level, although important but ranked sixth, is not as important as the previous five items. Therefore, we can validate hypotheses 1, 2, and 3: Individual characteristics, family characteristics, and social characteristics all have an important impact on the intention to have a second child in China. Logistic Regression If we use the machine learning classification model, it can only be determined that these variables are significantly related to \\\"willingness to have two children.\\\" The exact quantitative relationship and significance would need to be determined by regression models. Since the dependent variable of the sample is a dichotomous variable, a logistic regression model is used for further empirical study. Logistic Regression of Full Sample The correspondence parameters and variables after using the logistic regression model are shown in Table 8. Overall, there are more significant coefficients in family and social characteristics than in individual characteristics. For family characteristics, age has a strong effect on the willingness to have a second child, with each additional year of age increasing the willingness to have a child by 0.5%. High school and bachelor's education, on the other hand, decrease the willingness to have a second child. Gender, health, ethnicity, and other education levels do not have a significant effect on second-child fertility intentions. Individuals in first-time marriages have a very strong desire to have children, while remarriage does not have a significant effect on secondchild birth intentions. Income and number of siblings have a substantial effect on fertility intentions: The higher the economic level of a family, the more it can afford the cost of raising children (education, medical care, etc.); the more siblings in the family of origin, the greater an individual's willingness to have a second child. It could be also explained by the traditional Chinese idea of \\\"more children, more happiness.\\\" The coefficients of social characteristics, except for health insurance, are all highly significant. Compared to western regions, individuals in the eastern and northeastern regions are more reluctant to have a second child. Those with religious beliefs are more likely to want to have a second child, probably because many religions have fertility concepts that encourage childbearing. For example, the Islamic faith opposes birth control and abortion, and Buddhism promotes the flourishing of incense. Non-agricultural hukou are only 0.968 times as willing to have subsequent children as agricultural hukou, possibly because traditional agriculture is more labor-intensive, and rural patriarchal attitudes favor sons over daughters. Logistic Regression by Gender The logistic regressions by gender are shown in Table 9. Although most of the regression results are similar to those of the entire sample, they differ in some ways. For example, among the male group, members of the Han ethnic group are more willing than other ethnic groups to have a second child. Females with a high school education are less likely to have a second child, but males are not. Also, among the first-marriage group, males prefer to have a second child. Females with religious beliefs and males with non-agricultural hukou demonstrate a stronger desire to have second children. Therefore, hypothesis 4 is verified--the factors have different effects on men's and women's fertility intentions. CONCLUSION Compared with other studies, this paper is the first to systematically identify the influence of second-child fertility intentions in China through machine learning and logistic regression methods. Based on the 2017 and 2018 CGSS data, this paper systematically examines the factors influencing intentions for a second child from three dimensions: individual characteristics, family characteristics, and social characteristics. Three machine learning methods--ANN, RF, and XG-boost--are used to systematically screen and cross-validate the influencing factors, and a logistic regression model is used to empirically analyze the influence strength of the factors. This paper is innovative in finding that age is the most important factor influencing the intention to have a second child, and the intention becomes stronger with age. This result contradicts previous findings that suggest that second-child policies are more likely to increase fertility intentions among younger cohorts. Economic factors have been the focus of research , and this study demonstrates that higher family income increases secondchild fertility intentions, the results of the logistic regression are statistically significant. However, the research shows that family income is only the sixth most important in both RF and XG-boost models of all variables. In other words, economic factors are not decisive as expected. Another breakthrough result in this study is that the larger the number of siblings, the stronger the intention to have a second child. Although the role of siblings has been discussed previously in the literature , the results of the machine-learning model show that it ranks second and is higher than health, education, and family income. At the same time, as the current literature has been lacking a regional comparison, this paper further confirms that district has an impact on second-child fertility intentions. Like the study by Chen et al. (2019), we paper find the same result those who are healthier or have lower health risks will have higher fertility intentions for a second child. This paper considers gender differences as well. The relationship between education and fertility has been discussed in the existing literature  and the results of the model show that education is in the top five of all factors. This paper uses logistic regression to further reveal that women with high school and university degrees are less willing to have a second child. The current literature is less designed for men, but this paper finds that men who are married and have a non-agricultural hukou are with higher intention to have a second child. In addition, religious beliefs may increase women's fertility intentions. In short, this study suggests that future fertility intentions should be explained more through the interaction of individuals with their family of original and their geographical areas, rather than concentrating too much on economic factors. It also provides implications for future governmental demographic stimulus policies: traditional family policies (e.g., tax deduction, cash transfer) may not have the desired effect, and policymakers should focus more on family and socio-cultural orientations, and take into account regional and gender differences. Although this paper utilizes the latest CGSS data and adopts a machine learning approach, it still has some shortcomings, and there is room for improvement in the future. First, the feature variables found in this paper (e.g., family policy and childcare service) may still be insufficient because many factors affect the intention to have a second child. Second, the time span is not long enough. The implementation of the second-child policy started in 2016, so the relevant data are not abundant, and most of them are cross-sectional data. Third, models of machine learning are frequently updated, and this paper only considers mainstream learning approaches. Therefore, a potential improvement in future research can be applied from these three perspectives.\",\"1135746392\":\"1. INTRODUCTION The COVID-19 pandemic has resulted in a devastating loss of human life worldwide and extraordinary challenges to public health. Besides directly affecting patients' well-being, indirect impacts have caused disruptions to many aspects of people's lives globally. Since March 2020, state and local authorities have taken steps to limit large gatherings and close non-essential businesses, triggering a near-total shutdown in many U.S. states. University students have faced particular challenges during the COVID-19 pandemic. Many universities have suspended in-person activities and adopted virtual or hybrid teaching modalities. Study-abroad programs have been shortened or suspended. Given the uncertainty of when normalcy will return, students are uneasy about the course of their academic careers. As a result, many students are facing unprecedented challenges, which can lead to a high level of psychological distress and changes in behavioral patterns. In addition, gender differences have been shown to exist after individuals are exposed to traumatic events. To respond to potentially traumatic events or life stressors, females are more vulnerable to developing mental health symptoms or physical problems thanmales. Thus, in the study, we aimed to explore gender differences in psychological, cognitive, and behavioral patterns among undergraduate and graduate students during the COVID-19 pandemic. Association rule mining (ARM) is a machine learning technique for discovering non-trivial, implicit, and useful relationships between the covariates of a dataset. ARM has emerged as one of the most popular techniques for pattern recognition in various application domains , including COVID-19 related studies. The goal of this study was to leverage a large dataset our team collected after the onset of the pandemic and apply machine learning techniques to discover psychological, cognitive, and behavioral patterns among U.S. university students when facing unprecedented challenges. In particular, we utilized association rule mining  to extract implication rules among the male and female participants, respectively. Based on those findings, we examined university students' gender differences in response to the COVID-19 pandemic. In our discussion, we compare our findings to related studies and discuss potential clinical implications of our study. The contribution of this study is twofold. First, to the best of our knowledge, there is no existing work that applies ARM to studying gender differences in students' emotional and behavioral while coping with COVID-19. Our study aims to fill this gap by mining similar and distinct patterns among male and female university students at the peak of the pandemic. Second, although there have been studies examining individuals' psychological distress and changes in cognition and behaviors during COVID-19 , there is a lack of research examining how females and males differ in their psychological, cognitive, and behavioral patterns in response to the pandemic. Our study focuses on gender differences in the aforementioned domains among university students who face unprecedented challenges. Our findings can help professional institutions and community agencies develop multi-tiered (university-, school-, program-, and community-level) policies to improve students' emotional stability, self-control, and overall well-being during the pandemic. 2. RELATED WORK 2.1. Psychological Distress Government intervention to reduce the spread of the COVID virus, including school closures, bans of social gathering and public events, and lockdowns, can have a non-negligible impact on the mental health of the general public. Previous studies have shown that individuals may experience various mental health disorders including post-traumatic stress disorder (PTSD), depression, generalized anxiety disorder, panic disorder, and substance abuse due to isolation in managing infectious diseases. In particular, suspension of social events and requirements of social distancing reduce opportunities for inperson interaction, which is an essential source of emotional support. The substantial changes in working conditions (e.g., increased workload for health professionals, telecommuting from home) can cause significant pressure and challenges in maintaining productivity. School closures have forced working parents into additional roles of full-time care givers for young children. Consequently, many individuals have experienced confined physical activities and increased screen time. The correlation between psychological distress and the COVID-19 pandemic has been examined in numerous studies. 2.2. Cognitive Patterns According to the health belief model (HBM), individuals react to a health disease and decide whether to take action to prevent or control possible risks through their perceptions of both the susceptibility and severity of the condition and the benefits and barriers of behavioral changes. The HBM is derived from psychological and behavior theory, suggesting that health-related behaviors are determined by individuals' desires to avoid risks and their beliefs about whether specific actions will prevent or reduce the risk of serious disease. Perceived susceptibility refers to subjective perceptions of the risk of acquiring a disease, which is related to personal feelings about vulnerability to the risk. Perceived severity refers to subjective perceptions of the seriousness of contracting a disease and often involves considerations of possible medical consequences (e.g., disability, death) and social consequences (e.g., family functions, interpersonal relationships). If people view themselves as susceptible to a disease that could potentially lead to serious consequences, they are more likely to experience emotional reactions and take behavioral action to reduce their risks. Perceived benefits refer to perceptions of the effectiveness of available actions in diminishing the disease risk. Perceived barriers refer to perceptions of the obstacles to performing these actions. People's evaluations of the benefits and barriers vary greatly, leading to a cost\\/benefit analysis, which plays an important role in their reactions to a health condition. People are more likely to accept recommended health-related behaviors that are perceived as beneficial and that have fewer barriers. The HBM has been widely applied to understanding people's adherence to disease prevention and treatment as well as developing interventions for improving adherence. Many recent studies focusing on the COVID-19 health crisis have also applied HBM to predict the likelihood that people will adopt certain preventive behaviors, including wearingmasks, maintaining social distance, and receiving vaccines. 2.3. Behavioral Patterns 2.3.1. Preventative Behaviors Suggested by theHBM, people's preventive behaviors are typically determined by their perceived susceptibility and severity and perceived benefits and barriers. In response to authority-imposed measures, individuals exhibit different levels of compliance governed by their cognitive patterns under specific conditions. Recent studies examining individuals' behavioral response to the COVID-19 have also indicated the variance in people's preventative behaviors. 2.3.2. Communication Additionally, people tend to demonstrate different patterns of communication regarding the COVID-19 pandemic, which have exposed people to an array of challenges, including psychological distress, anxiety, fatigue, fear, and social isolation. Effective communication has become a critical barrier in the workplace, communities, and families. Communication is defined as sending or exchanging messages through the modalities of speaking, writing, or other types of media. During peaks in the pandemic, social media, the internet, and social networking have generated a deluge of information about COVID-19. Although individuals have largely relied on recommendations by local governments or central administration agencies such as the Centers for Disease Control (CDC) to regulate their behavior and expectations, fear, distrust, and resistance have also been common reactions to different types of response measures suggested by authoritative sources. In addition to public information, credible information obtained and confirmed through communication with friends and family members has been critical for motivating people to take action. 2.4. Gender Differences The effects of increasing psychological distress and changes in cognitive and behavioral patterns in response to COVID-19 can vary among individuals due to different demographic factors. Gender is one important factor for such variance. In research by Yan et al. (2021), a survey of 1,749 females and 1,339 males in China revealed that females and males differed significantly in their adjustment to living\\/working conditions, responses to having a fever, and the need for psychological support during the COVID-19 outbreak. Similarly, Stefanut et al. (2021) reported that levels of depression, anxiety, and stress were significantly higher in females than in males in response to the COVID pandemic. These empirical findings indicate the importance of examining gender differences in emotional, cognitive, and behavioral responses to the COVID-19 outbreak in the United States. Many recent studies examined the gender differences among the general population. However, undergraduate and graduate students are going through a unique stage of life experiences, such as leaving their families to develop more independence and living in a dormitory environment that is quite different from a home environment. Thus, the gender differences among undergraduate and graduate students warrant further examination. 2.5. The Impact of COVID-19 on Undergraduate and Graduate Population School closures reduce social contact among students, faculty, and staff, and therefore interrupt transmission of disease. A 2018 metanalysis of 25 studies examining school closures found that implementing school closures before or after an epidemic reached its peak reduced overall influenza spread. Earlier school closures in history reduced and delayed epidemic peaks, and longer closures resulted in delayed epidemic peaks. By the end of spring 2020, over 4,234 colleges and universities across the United States had been impacted by COVID-19 . Like the rest of the population, university students experienced the psychological effects of social distancing. Social distancing precautionary measures resulted in people experiencing separation from loved ones, loss of freedom, boredom, and hyper-uncertainty over disease status. A 2020 metanalysis examining quarantining during previous epidemics worldwide (e.g., North America, Asia, Africa, Australia) highlighted the negative effects of social isolation, including post-traumatic stress symptoms, confusion, and anger. Stressors during quarantine include duration of quarantine, fears of infection, frustration, boredom, and inadequate supplies and information. In short, it is critical to examine undergraduate and graduate students in the areas of psychological, cognitive, and behavioral characteristics in response to the impact of COVID-19. 3. MATERIALS AND METHODS 3.1. Participants and Procedures This research was approved by the Institute Review Board of Fordham University. We recruited the participants through email broadcasting, posts on social media such as Facebook or Instagram, and announcements through some student organizations. All participants were provided informed consent, which noted the confidentiality rules and their voluntary participation in this study. There were some eligibility criteria, including all participants must be at least 18 years old and physically reside in the United States when they completed the questionnaires. Furthermore, all participants should enroll in an undergraduate or graduate program. The questionnaire took about 10-15 min to complete, and an incentive was offered to participants for completing the questionnaires (e.g., special postcards or gift cards through raffles). All data were collected from May 2020 to July 2020, a peak period of COVID-19. During this period, most public school systems in the United States were mandated to conduct virtual schooling, and many universities in the United States also resorted to fully virtual classrooms. Within the educational systems, large social gatherings, athletic events, and even inperson graduation commencement were temporarily suspended. In addition, the United States placed air-travel restrictions on locations where COVID-19 outbreaks had occurred. International travelers were asked to self-quarantine for 14 days. A total of 366 US university students were recruited and administered a set of self-report questionnaires through Qualtrics. Data quality control was performed by manually examining all survey reports to ensure participant eligibility and no excessive missing answers in each report. We excluded 37 participants who reported being outside the US at the survey time. Furthermore, we recognized that there were more than two gender categories. However, for this study, we excluded seven survey reports rendered by non-binary gender participants due to their limited sample size. The final data for our analysis consisted of 322 male and female university students. Table 1 presents the demographic characteristics of our dataset. Approximately 28.6% of the 322 participants reported residency within the Metropolitan New York area, a COVID epicenter during the early stage of the pandemic. In addition, the dataset consisted of an equal number of undergraduate and graduate students (161, respectively) and an approximate male to female ratio of 1:7. It is worth noting that using a convenient sample through authors' professional networks could have caused the disproportion between the two gender groups. Specifically, students in psychology and social science fields were largely recruited, leading to more female participants. Nevertheless, according to our experimental results, we were able to obtain a sufficient number of meaningful patterns for the male group (Table 4) and conduct a valid comparison to the findings of the female group. 3.2. Measures 3.2.1. Perceived Susceptibility and Perceived Severity The questionnaires used to assess perceived susceptibility and perceived severity of COVID-19 were developed based on the health belief model. There were four items for each variable and all items (e.g., \\\"my chances of getting COVID-19 are high\\\") used a 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). Higher scores indicated a higher level of perceived susceptibility\\/severity in relation to COVID-19. The questionnaires demonstrated good reliability and validity in the present study [perceived susceptibility: Cronbach's a = 0.76, composite reliability (CR) = 0.85, average variance extracted (AVE)= 0.60; perceived severity: Cronbach's a = 0.77, CR= 0.85, AVE= 0.60]. 3.2.2. Perceived Benefits and Perceived Barriers The questionnaires used to assess perceived benefits and perceived barriers of staying at home during the peaks of COVID-19 were developed based on the measure that was used to assess outcomes in implementing SARS-preventive behaviors. The items specifically in relation to benefits and barriers of preventive behaviors were extracted and adapted to fit the context of COVID-19. There were four items for each variable, and all items (e.g., \\\"staying at home prevents me from getting COVID-19\\\") used a 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). Higher scores indicated a higher level of perceived susceptibility\\/severity in relation to COVID-19. The questionnaires demonstrated good reliability and validity in the present study (perceived benefits: Cronbach's a = 0.79, CR= 0.86, AVE= 0.62; perceived barriers: Cronbach's a = 0.90, CR= 0.93, AVE= 0.71). 3.2.3. Psychological Distress In the domain of psychological distress, 10 questions were developed based on Gross and John (2003). The questions consisted of items such as, \\\"During the peak time of COVID-19, I felt depressed.\\\" All items used a 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). Higher scores indicated higher levels of endorsement of psychological distress. The questionnaires demonstrated good reliability and validity in the present study (Cronbach's a = 0.91, CR= 0.92, AVE= 0.55). 3.2.4. Communication In the domain of communication, 13 questions were developed based on Morton and Duck (2001). The items mainly included three aspects of communication, including communication through interpersonal relationships (e.g., with peers, families, healthcare professionals, colleagues) and public media (e.g., television, newspaper, magazine, social media such as Facebook, official and unofficial websites), and how much attention was directed to such communication. There were five questions regarding communication through interpersonal relationships that used a 5-point Likert scale, ranging from 1 (not at all) to 5 (a great deal). There were seven questions regarding communication through public media that used a 5-point Likert scale, ranging from 1 (never) to 5 (always). One question regarding whether the participant paid attention to news and information regarding COVID-19 in the media used a 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). The questions consisted of items such as \\\"How much have you discussed COVID-19 with the following people during the peak time of COVID-19?\\\" The questionnaires demonstrated good reliability and validity in the present study (Cronbach's a = 0.80, CR= 0.85, AVE= 0.31). 3.2.5. Preventative Behavior The questionnaire used to assess participants' preventative behaviors was originally designed by the team at Beijing Normal University. Given the unique nature of COVID-19, the government imposed a series of prevention behaviors, including staying at home, wearing masks, washing hands, cleaning surfaces, and maintaining social distance, which was the main focus of this questionnaire. Some sample questions included, \\\"How frequently did you go outside during the peak time of COVID-19?\\\", \\\"How frequently did you wash your hands during the peak time of COVID-19?\\\", and \\\"How frequently did you maintain social distance during the peak time of COVID-19?\\\" A 7-point Likert scale was used for this questionnaire ranging from 1 to 7. The questions were initially developed based on previous health crisis-related research with modifications to fit the situations applicable to COVID-19. 3.3. Methods 3.3.1. Association Rule Mining Association rule mining (ARM) is widely used in market basket analysis, which provides retailers with non-trivial, implicit, and previously unknown information to understand customers' purchase patterns. The uncovered relationships are represented in the form of \\\"association rules\\\" extracted from a retail store's transaction database, such as {Diaper} -- {Beer} . This rule suggests that a strong relationship exists between the sale of diapers and beer in one transaction (i.e., many customers who bought diapers also bought beer). Consequently, retailers can benefit from this type of rule to identify new opportunities such as cross-selling or product shelf arrangements. Formally, let I = {i1, i2, . . . id} be the set of all distinct items in a market basket data and T = {t1, t2, . . . tN} be the set all customer transactions. Each transaction ti (i = 1, 2, . . .N) contains a subset of items from I. We define an itemset U to be a collection of items, i.e., A = {a1, a2, . . . ak} where ai I i [1, k] and |A| <= d. An association implication takes the form X -- Y where both X and Y are itemsets. Given a set of transactions T, the overarching goal is to find all association rules with nonnegligible coverage and strong implications. The prevalence is measured by support which is defined as the fraction of transactions that contain both X and Y among the total |T| transactions. The implication strength is defined by confidence whichmeasures how often items in Y appears in transactions that contain X i.e., confidence(r) = s(X,Y) s(X) where s denotes the support. In practice, two thresholds minsup and minconf are employed to identify strong association rules with support and confidence above the minsup and minconf , respectively. In addition to market basket analysis, ARM is employed in other application areas including web usage mining , medical diagnosis , intrusion detection , customer relationship management , and bioinformatics. 3.3.2. Lift ARM algorithms rely on the minimum support and confidence thresholds to select interesting patterns. One limitation of this framework is that an identified association rule can be misleading even if its support and confidence are above the minsup and minconf thresholds. For example, Table 2 summarizes a hypothetical dataset of 1000 students to study the impact of a mentor's feedback type (positive or negative) on a student's creativity in research. Any rule with a support above 10% and a confidence above 70% is arguably a strong association rule. Thus, using minsup = 10% and minconf = 70% as model parameters, an ARM algorithm will extract r1 : {Negative Feedback} - {Creative} as one of the behavioral patterns due to its high support (15%) and confidence (75%) levels. Nevertheless, we observe that the expected percentage of creative students in the entire study cohort is 90%, while only 75% of students are creative among those who received negative feedback. Thus, negative feedback indeed has a negative association with being creative. One approach to address the above limitation is to further filter the generated strong association rules using lift. Given an association rule r: X -- Y , the lift of r is defined as: lift(r) = c(X -- Y) s(Y) = s(X,Y) s(X)s(Y) where s and c denote support and confidence, respectively. Consequently, lift(r) = 1 if X and Y are independent > 1 if X and Y are positively correlated < 1 if X and Y are negatively correlated and we are only interested in rules with lift > 1. In the above example, although r1 is a strong association rule, it will not be retained because lift ({Negative Feedback} -- {Creative}) = 0.15\\/(0.2*0.9) = 0.8333, indicating a negative correlation. In our study, we require lift > 1.2 to ensure the validity of the extracted behavioral patterns. 3.4. Encoding Q&As as Market Basket Items We employed association rule mining techniques to our proprietary survey data to study gender dissimilarities in the various psychological domains. To formulate our task under the conventional market basket analysis framework, we modeled each unique question-answer pair in the survey questionnaire as a market basket item. We viewed participants as customers, and their survey reports were consequently analogous to customer transactions on a collection of items. Table 3 illustrates our encoding method to transform question-answer pairs into market basket items. Since all questions in our survey have less than ten answer choices, we represented each unique question-answer pair by concatenating a question's number and a corresponding answer index as one item. That is, the last digit of an item code specifies the answer index and the number proceeding \\\"-\\\" indicates the question number. As an example, Question 153 states \\\"During the peak time of COVID-19, I felt worthless.\\\", and there are five choices for this question. Consequently, five unique items (\\\"153-1\\\", \\\"153-2\\\", \\\"153-3\\\", \\\"153-4\\\", and \\\"153-5\\\") were generated from this question, each of which the number 153 specifies the question index and the last digit (1-5) indicates the five answers, respectively. Following this approach, each question contributed k unique items to the market basket where k < 10 is the number of answer choices for the question. 3.5. Frequent Pattern Generation Figure 1 presents the pipeline of our frequent pattern generation process. First, all unique items were generated (left component) using the above Q&A encoding system. A sample itemset could take the form of \\\"2-0\\\", \\\"37-2\\\", \\\"41-4\\\", which indicates the individual selected \\\"2\\\" for Question 37, \\\"0\\\" for Question 2, and \\\"4\\\" for Question 41. Next, survey reports were modeled as itemsets purchased by customers (i.e., participants), with the question-answer pairs mapped to the encoded items. As a result, the entire dataset (middle component) is equivalent to a set of transactions on market basket items. Finally, we applied association rule mining techniques to extract all strong frequent patterns (right component). Tomine the strong implication rules in our data, we employed the frequent pattern growth FP-growth algorithm , which compresses the input data into a tree structure and recursively extracts the frequent patterns. FP-growth was selected due to its efficiency and scalability compared to other ARM algorithms, such as the Apriori algorithm. We implemented the algorithm using the Python programming language  and mined our strong association rules using 20 and 70% as the minimum support and confidence thresholds, respectively. The discovered rules were further filtered by requiring the interest factor (i.e., lift) above 1.20. For example, a sample association rule produced by our algorithm was as follows: {'2-0', '37-2', '41-4'} -- {'38-2'}, support = 0.2, confidence = 0.75, lift = 1.4 where \\\"2-0\\\": I am a male \\\"37-2\\\": I am more likely than average person to get COVID 19. Answer: disagree \\\"41-4\\\": If I got COVID-19; my life would change. Answer: agree \\\"38-2\\\": COVID-19 is a hopeless disease. Answer: disagree The rule indicated that, with 20% support, 75% confidence, and lift = 1.40, males acknowledged that COVID-19 would result in severe consequences or change their lives. However, they were optimistic about not being infected and did not regard COVID-19 as hopeless. Our findings suggested that significant differences existed for the two gender groups in participants' psychological distress levels and coping strategies. In addition, the two groups exhibited minor differences in cognitive patterns and consistent preventive behaviors. Details are provided in the Results and Discussion sections. 3.6. Quantitative Evaluation In addition to examining the individual association rules, comparing the total number of rules extracted for each gender group in each psychological category can also provide insightful information. In particular, a larger number of discovered rules indicates more consistent behaviors in a category. Furthermore, the quantitative difference of rule numbers between the two gender groups for a given category signifies their degree of behavioral dissimilarities. Therein, we computed both the difference and the ratio of the total number of rules for each psychological category between the two groups. The difference was calculated as the number of rules for the male group minus that for the female group. The ratio was define as: ratio = max(M, F) min (M, F) where M and F denote the numbers of rules for the male and female group, respectively. 4. RESULTS In this section, we present our main results in studying the behavioral differences between male and female university students' responses to the COVID-19 pandemic. We first examine the quantitative differences in the number of rules discovered for the two gender groups in each category. We then investigate specific different and similar behavioral patterns. 4.1. Quantitative Comparison of Implication Rules Table 4 presents the total number of strong association rules discovered in the male and female groups across the five psychological categories in our survey. Columns \\\"Diff\\\" and \\\"Ratio\\\" are the two evaluation metrics defined in Section 3.6. We observe that the two gender groups demonstrated significant differences in Categories 1 and 2, with a difference ratio of 26 and 14.31, respectively. In Categories 3 and 4, males and females exhibited noticeable differences with a difference ratio of 5.33 and 2.49, respectively. For the \\\"Preventive Behaviors\\\" category, the difference was less salient (i.e., ratio = 1.57). Thus, we expected the most interesting findings to occur in the first two categories. 4.2. Psychological Patterns The two gender groups displayed significant differences in psychological distress level and coping strategies. Table 4 shows that the male group outnumbered the female group in the total number of rules (186 vs. 13) with a ratio of 14.31. Thus, males exhibited more consistent behavior than females in this category. Furthermore, we observed that males were more optimistic and resilient when facing extraordinary challenges evidenced by the following findings, which were not present in the female groups: * Males agreed that they felt down, but they disagreed that \\\"nothing could calm me down.\\\" * Males agreed that everything took effort, but they disagreed that they felt so restless they could not sit still. * Males agreed that they felt nervous, but they disagreed that they felt hopeless, uncontrollable, or worthless. 4.3. Cognitive Patterns Both cohorts showed very few extracted patterns (Female:; Male:) in perceived susceptibility and severity regarding the pandemic. Among the 13 male rules, we found that males acknowledged their worries about COVID-19 but were optimistic. Furthermore, males acknowledged that COVID could result in severe consequences or change their lives. However, they were optimistic about not being infected and did not consider the pandemic as hopeless. Our findings also suggested that males perceived that they were safe staying at home but that doing so affected their schedule and daily activities. This pattern was not observed among the female participants. 4.4. Behavioral Patterns Males and females showed consistent preventive behaviors in coping with the pandemic, that is, those who seldom went outside during the peak time of COVID-19 always followed the preventive behaviors advised by authorities. Furthermore, females exhibited more consistent behavior than males in establishing their communication networks, evidenced by the number of rules (i.e., female:; male:) and their ratio of 26. A closer examination of individual implication rules revealed that females tended to reach out to a greater variety of resources compared to males. For example, males who visited official university websites for updates were also likely to visit social media websites. For females, in addition to visiting social media websites, also shared information by talking to family members and peers. These interpersonal communication patterns were not present in the male cohort. 5. DISCUSSION 5.1. Psychological Pattern The findings suggest that there were noticeable differences in psychological distress level and coping strategies between male and female university students in response to the COVID-19 pandemic. Overall, males were more optimistic and resilient when facing extraordinary challenges evidenced in a number of areas. Our findings concur with previous research, suggesting that females tend to be more vulnerable to developing mental health problems or physical problems in response to stressful life circumstances or traumatic events. Similar findings have been reported regarding psychological responses in relation to COVID-19, with more female medical workers reporting more negative responses associated with pandemic-related challenges  and a higher prevalence of anxiety disorders being reported by women than by men. This suggests that males and females differ considerably in adapting to working\\/living conditions, symptoms of illness, and the need for psychological services. Our findings further support the need to provide mental health service to individuals with consideration of gender differences. 5.2. Cognitive Pattern People's perceptions of susceptibility to and severity of health conditions play a key role in deciding whether to engage in behaviors that reduce their risks. However, certain selfreinforcing biases that may be related to gender differences can prevent accurate knowledge of susceptibility, resulting in some people feeling invulnerable and some people feeling hopeless in the face of a serious health crisis. Our findings support this argument in that, compared to females, males acknowledged their worries about COVID-19 but were more positive about it. Galasso et al. (2020) examined the perceived threats and fear of COVID-19 based on a sample of 21,649 participants, concluding that females were more likely to perceive the COVID-19 pandemic as a severe issue, which was confirmed by Nino et al. (2021) based on a sample of 7,441 U.S. adults. Our findings reinforce previous research by showing that males recognized that COVID-19 could result in serious consequences or change their lives, but they were optimistic about not considering COVID-19 as hopeless. Additionally, males demonstrated a pattern of perceiving both high benefits and high barriers for following preventive behaviors at the same time, which was not found in females. One explanation for this discrepancy is that males of all ages are more physically active than females , resulting in more barriers for males to stay at home. 5.3. Behavioral Pattern In the area of communication, females in the present study exhibited more consistent behavior than males, and females explored more interpersonal communication approaches (e.g., with family members) than males. During the COVID-19 pandemic, people might be stereotyped, discriminated against, or suffer due to perceived contact with COVID-19. Dayto-day behaviors (e.g., travel bans, quarantine requirements) might revolve around communication related to the pandemic. Effective communication about the pandemic might benefit the public, such as information about the physical and psychological well-being of themselves and their family members. Given that women tend to presentmore severe symptoms of anxiety, depression, and acute stress during the pandemic , it is not surprising that women also present different communication patterns, which were revealed in the present study. It is plausible that males and females rely on different sources of communication to confirm their interpretation of the circumstances associated with COVID-19 and, thus, they differ in the sources of information they rely on and the need for interpersonal communication. In recent studies that examined preventative behaviors during the COVID-19 pandemic, females were found to exhibit higher levels of precautionary behaviors guided by public policy from the government, such as washing hands, social distancing, nonutilization of public transport, and stay-at-home restrictions. However, our findings indicated that both males and females showed consistent preventive behavior patterns in coping with the pandemic. It may be the nature of human behavior that both men and women have similar patterns of vigilance and enforcement of necessary self-prevention in the face of a significant risky situation. 5.4. The Impact of COVID-19 on Undergraduate and Graduate Students A survey of 2,086 students by Active Minds Inc., a national nonprofit supporting mental health awareness and education for students, found that 80% surveyed reported that the pandemic negatively affected their mental health, with one in five students stating that during the COVID-19 pandemic, their mental health significantly worsened. Additional challenges faced by university students during the pandemic related to (lack of) independence, being back home for an extended and unexpected period of time, social isolation, and high conflicts and tension. Our findings suggest that significant differences exist between the two gender groups among undergraduate and graduate students in psychological distress and coping strategies. The two groups exhibit minor differences in cognitive patterns and consistent preventive behaviors. The findings suggest that mental health counseling services and workshops focusing on stress management might be particularly important to high education students. Gender differences manifested through psychological, cognitive, and behavioral aspects need to be addressed to provide customized prevention and treatments. 5.5. Limitations and Future Research Although our study demonstrates interesting gender differences and the effectiveness of the ARM algorithm, our dataset was limited to university students who were self-motivated to participate in the survey. Thus, our findings could be affected by a selection bias. We foresee one potential future study is to apply the same machine learning technique to validate our findings in a more general population. Another limitation of this study involved the metrics used to assess participants' responses. It is worth noting that given COVID-19's unprecedented challenges, its unique nature, and the rigorous preventive rules imposed by the public health officials, there were no existing questionnaires that fit our research purpose. Thus, we modified the questionnaires in previous health crises-related research with additional selfdesigned questions. The quantitative measures employed in the questionnaires have not been established in classical psychometric publications. Nevertheless, we believe our metrics were proper and sufficient, evidenced by the consistency between our findings and existing psychological theories. In addition, our findings indicated that males were more resilient than females during the pandemic, and females tended to report more adverse effects from the stress. In reality, men tend to be more self-reliant and internalize their distress, leading to fewer externalized behavioral symptoms. Some males might not realize that they were experiencing emotional distress and might not actively report such symptoms. Thus, the level of effectiveness for the machine learning approach to differentiate the gender difference warrants further examination. Finally, this study mostly recruited individuals attending undergraduate or graduate schools, and the participants represented those who have received a high level of education in the United States. Therefore, the generalization of the findings might be limited. Future studies would need to recruit participants from more diverse educational and socio-economic backgrounds. 5.6. Clinical Implications Our study is a novel approach that applies the ARM technique to explore gender differences through the perspectives of psychological, cognitive, and behavioral patterns in response to the COVID-19 pandemic. Our findings underscore the importance of paying attention to themental health of the general public during the pandemic, especially those with higher risk of psychological distress (e.g., women, older individuals). The identified gender differences among participating students could help professional institutions to facilitate customized advising or counseling for males and females in periods of unprecedented challenges. In addition, a machine learning approach could be used in the blended approach (i.e., combining mental health counseling and computer science) and could be used as an assistive tool for screening and tracking distress.\",\"1135746588\":\"Introduction The aim of this study was to test whether, and how, language in organizational descriptions reflects gender segregation in the organizations, by the use of different computational methods to investigate the relationship between organizational descriptions and the employee gender ratios of the examined organizations. Previous research in this area has mainly examined language in job requirement descriptions and job adverts. However, little is known about the language used in more general texts describing organizations. Organizational descriptions often accompany job adverts or stand alone to let people assess organizations they may be interested in working for. On the Internet, these texts are often found under the heading \\\"About us.\\\" In contrast to job adverts, which usually focus on information about the specific position, descriptions of organizations are typically more general and comprehensive, thereby including information relevant to all employees in the organization, regardless of their position. Organizational descriptions tend to focus on attributes of the organization and may include the line of work the organization is involved with; origins and background of the organization; government of the organization; vision, objectives, goals, and previous accomplishments of the organization; geographical and demographic information related to the organization; various policies of the organization (e.g., career development and diversity); qualifications and competencies within the organization. Language in job adverts has been shown to reflect the gender distribution in a profession. Adverts for women-dominated jobs include more words related to stereotypical feminine attributes, i.e., related to communion (e.g., caring, understanding, and compassionate), whereas adverts for men-dominated jobs include more words related to stereotypical masculine attributes, i.e., related to agency (e.g., confident, determined, and ambitious;). Such differences affect whether women and men perceive a fit between themselves and the job position. The present study investigates if these findings can be extended from job adverts to organizational descriptions, and thereby examines whether organizational descriptions reflect the gender distribution in the organization. The focus is organizational descriptions on LinkedIn, an online platform used for professional networking where employers post job openings and job applicants can post their CVs. Gender segregation on the job market is common. For example, women dominate the health care sector and men the technical and engineering professions. Even though Sweden is one of the most gender equal countries in the world , with a nearly equal employment rate for women (84.5%) and men (89.4%), the job market is still gender segregated, based on legal gender. Gender segregation appears both between different industries and within specific organizations. The professions of women are often lower in status, lower in salary, and there are fewer women in senior positions as compared with men. According to social role theory , gender stereotypes and a gender segregated job market are deeply intertwined. More specifically, observing women and men working in different industries and positions, influence perceptions of personality traits corresponding to the competencies needed in those professions. Thus, when women perform the role of homemaker or in professions related to the domestic role in jobs, they are presumed to be communal, whereas, when men perform tasks in the job market associated with higher status, they are presumed to be more agentic. In this way, a better balanced gender equality between women and men in the job market would theoretically decrease gender stereotypes of women and men. To counteract gender segregation on the job market, both legal and affirmative actions are needed. For example, gender discrimination in job adverts is prohibited in Sweden according to Swedish law, yet research suggests that recruitment situations still contribute to maintaining gender segregation. Through wordings regarding characteristics mainly associated with either femininity or masculinity (e.g., related to communion or agency), job adverts and organizational descriptions may still implicitly address a particular category of applicants. Language implicitly conveys norms , and specific choices of words can reflect stereotypes relating to gender. Because language affects individuals' perception of themselves and the world, as well as how they convey perceptions of reality , language can contribute to maintaining a segregated job market. Some job titles and wordings are explicitly gendered, for example chairman, fireman, and policeman, which may lead to men perceiving a better fit with the job. Masculine generics have also been used in job adverts , meaning that a potential employee was referred to as he. This is no longer the case. However, in grammatically and linguistically gendered languages, job titles are still more often described with the generic masculine form instead of a gender-neutral equivalent. Masculine generics influence perceptions that a man better matches the position than a woman applicant. Other wordings are more implicit. For example, job adverts for women-dominated professions and positions have been found to use a more communal wording (e.g., supportive and kind), whereas adverts for men-dominated professions and positions use a more agentic wording (e.g., competitive, dominant;). Traits described and requested in job ads, such as understanding - communion, and assertive - agency, can activate gender related stereotypes regarding who is best suited for the job. Language can therefore contribute to reinforcing, as well as counteracting, existing gender segregation, by affecting both who is seen as a fit with, and who feels motivated to apply for the job. According to the lack of fit model , a match or a mismatch between gender stereotype characteristics and perceived professional role requirements influence judgments in recruitment settings, for both possible applicants and recruiters. A match enhances expectations of role performance success, while a mismatch brings expectations of performance failure. For applicants, a lack of fit might decrease motivation to apply and belief in having the right competences; and, for recruiters, a lack of fit might lead to beliefs that the applicant lacks the capability needed to do well in this position and therefore make them refrain from hiring that person. Perceived fit in recruitment contexts can be applied both to the individual's fit with a specific job, person-job fit, as well as to the individual's fit with an organization as a whole, personorganization fit. From an applicant's perspective, perceived person-organization fit between oneself and organization can predict the appeal of the organization and of a new job, as well as intentions of pursuing it. Person-organization fit occurs when employer and\\/or employee \\\"provides what the other needs, when they share similar fundamental characteristics, or both\\\" (;, pp. 4-5). Hence, it is important that values and principles of the organization align with those of the potential employee, and that employee and job\\/organization do not experience a mismatch. A challenge for organizations is, thus, to communicate organizational values without excluding qualified presumptive candidates. The organizational description allows organizations to brand themselves and thereby let presumptive employees assess their person-organization fit. Furthermore, the organizational description is a way for organizations to broaden their workforce. Organizations might need to change their word choices to attract employees that, due to gender stereotypes, are not typical to the organization. Recruitment gives real life opportunities of increasing gender equality in professions and organizations. It is therefore important to examine whether language in job adverts and organizational descriptions reinforce and perpetuate gender segregation in the job market. Before, organizations used print advertisements and agencies to reach job candidates. Today, the Internet and social media are the principal way for both employers and prospective employees to market themselves. This new way of recruitment allows organizations to target not just active job candidates, i.e., currently unemployed individuals actively seeking jobs, but also passive job candidates, i.e., currently employed individuals not actively seeking new jobs, but who would consider changing jobs if they perceive a better match between themselves and the organizations. In the present study, organizational descriptions from large Swedish organizations published on LinkedIn were used for the analyses. LinkedIn is an online networking service that has over 700 million users, and 9 million corporate accounts in more than 200 countries worldwide. As of October 2018, LinkedIn had 3.5 million Swedish users, i.e., 35% of the population. An advantage of using LinkedIn, is that the organizations present themselves toward potential applicants also when they do not have ongoing recruitment processes. Using observational data, the present study thus examines the relationship between different language and gender distribution of work organizations. The aim of this study was to test whether, and how, language in organizational descriptions is associated with the ratio of women and men in the organizations, using computational text analysis. In light of previous research, there is a need for studies that illuminate the language organizations use to describe themselves, and that specify in which ways language might differ between organizations with different gender ratios. We used legal gender as measurement of gender distribution within the organizations, therefore we could not include other genders than women\\/men. Computerized text analyses were used to examine the association between texts, i.e., organizational descriptions downloaded from LinkedIn, and a numerical value, i.e., employee gender ratios of the examined organizations. The text analyses were completed by Latent Semantic Analysis  and Bidirectional Encoder Representations from Transformers (BERT;), which are two completely data-driven methods. Pearson correlations between linguistic measures and statistical measures of gender ratio were computed with the aim of answering the following questions: 1. Is the language in organizational descriptions associated with the gender ratios in organizations? 2. How do content and word choice differ between organizational descriptions for organizations with a majority of women and a majority of men employees? Materials and methods Sample and coding Inclusion and exclusion criteria All private and public Swedish organizations (N = 564) with more than 1,000 employees in Sweden were selected for the study, as these medium to large sized organizations were probable to market themselves on LinkedIn. Private organizations (n = 256, 45%) and public organizations (n = 308, 55%) were selected based on statistics from Statistics Sweden. The analyzed texts were downloaded from LinkedIn under the template headline \\\"About us.\\\" Sixty-six (12%) of the organizations had no LinkedIn-page or no organizational description, and were therefore excluded from the analyses. Also, 89 (16%) organizations that only had English organizational descriptions were excluded from the analyses. The final sample thus consisted of 409 Swedish organizational descriptions. Of these, 278 (68%) were public and 131 (32%) were private. The total number of words in the sample, after excluding organizations that were not included, was 54,383. The mean number of words in the texts were N = 133 (SD = 65) and the mean sentence length was N = 9.2 words (SD = 2.8). Neither the number of words in the texts, nor the sentence length, correlated with the gender ratios. Figures 1, 2 show an overview of the included and excluded organizations. Basis for semantic analyses To make sure that the analysis is not dependent on specific model implementations, we analyzed our data using two distinctly different models: Latent Semantic Analysis, LSA  and the Bidirectional Encoder Representations from Transformers, BERT. LSA is a bag-of-word model that creates semantic representations of words, but cannot capture the grammatical structure of the data. BERT is a deep neural network that uses transformers to quantify the meaning of texts. This model can capture grammatical structure of text data. The results of the LSA model are presented in the main texts, whereas the results of the BERT model are found in the Appendix. The overall aim was to study gender differences in organizational descriptions. For methodological details, and how the models can be used to predict a numerical value and optimization, we refer to other articles  and the Appendix. The data analysis was conducted in SemanticExcel, which is an online statistical software application that analyzes texts that allows calling the LSA model and the BERT model that is described elsewhere. Finally, the source code for the LSA analysis is available on Github. Results Semantic analyses To answer the first research question regarding whether organizational descriptions are associated with the gender ratio (i.e., women\\/men) in the organizations, we used the linear regression method described in detail in Kjell et al. (2018) and the Appendix. In this, the relationship between texts (i.e., from LinkedIn) and a numerical variable (i.e., gender ratio of women\\/ men) was analyzed using the semantic representations of the texts as generated from LSA or BERT as input. Table 1 shows examples of LinkedIn texts and their associated observed gender ratios and ratios predicted from the regression model (based on legal gender; percentage women). The Pearson correlations between observed and predicted values were computed to indicate how well the LinkedIn texts predict the observed gender ratio based on legal gender. If the organizational texts predict the proportion of women exactly, the correlation is 1. The Pearson correlation between the observed and predicted binary employee gender ratio of the organizations was significantly larger than zero for the LSA model (r = 0.65, p < 0.0001; r2 = 42%, Mean Absolute Error = 12.3%, n = 409, see Appendix for the results of the BERT model). The explained variance in the text set indicated a large effect. Thus, LinkedIn texts are associated with the gender ratio of employees in an organization. A possibility is that our results simply could be explained by whether the organization was a private company, or a part of the Swedish public sector. To deal with this issue, we added a binary variable coding for private\\/public organization as a covariate to the multiple linear regression model based on the LSA representations. The results showed that the gender ratio still could be predicted with a reasonably high Pearson correlation (r = 0.45, p < 0.001, r2 = 20%, Mean Absolute Error = 12.6%, see Appendix for the results of the BERT model), although the Pearson correlation was somewhat lower compared to when the analysis without this covariate. Content analyses of organizational descriptions To answer the second research question, i.e., how organizational descriptions for organizations with a majority of women or men employees differ from each other, the words from the LinkedIn texts were further analyzed semantically and illustrated using word clouds. The closeness of two semantic representations (i.e., words or whole texts) is a measure of their semantic similarity. This can be calculated by the cosine of the angle between the two semantic representations in the semantic space. A high semantic similarity score signifies a high semantic similarity between two semantic representations, and a low score indicates that the two semantic representations are unrelated in meaning. For example, the semantic similarity between organization and employee is higher than that between organization and balloon. In this way, similarities and differences between texts or words can be measured with numbers. This, in turn, enables standard statistical methods, such as t-tests to test how words or text sets differ. The word clouds in Figure 3 represent which words were most indicative of the LinkedIn text set using the semantic t-tests by comparing the semantic representations of the LinkedIn texts (text set 1) with text based on the Swedish version of Google n-gram set (text set 2;). The clouds are based on z-values of the semantic t-test. The more semantically typical for the LinkedIn data set a word is, the higher the z-value. All plotted words were significant following Bonferroni correction for multiple comparisons: The font sizes in all word clouds represent the frequency of the words in the data set - the bigger the word, the more frequent it is. Colors represent z-values of the significance testing of the words (specified in the legend in the upper right corner of Figures 3-5). Translation of the Swedish organizational descriptions was made in Semantic Excel based on Google Translate. As can be seen, the pronoun we stood out as the most commonly used word. To examine differences in word choice between organizations with a majority of women and men employees, word clouds comparing the two were created. Figures 4, 5 show the words that significantly discriminate between the LinkedIn texts of organizations with high percentages of women and men employees. In Figure 4, significance testing was made by chi-square tests based on the relative word frequency. That is, Figure 4 shows the significant difference in relative frequency (e.g., occurrences per million) of words between data with a majority of women and men employees. Significant words, following Bonferroni correction for multiple comparisons, are shown in these word clouds: The two clouds contrasted each other, showing different themes. The clouds depicting organizations with a majority of women employees were focused on local and regional services aimed at the citizens, whereas organizations with a majority of men employees were focused on money, competition, customers and both Sweden and other countries. Figure 5 shows words that significantly discriminate between the ratio of women employees in the organizations in the LinkedIn. Figure 5 was created by the multiple linear regression model described in the Appendix. That is, the regression models were trained using the summarized semantic representations of the LinkedIn texts as input, to predict gender ratio. The words shown in the plots were significant following Bonferroni correction for multiple comparisons. The word clouds that discriminated between organizations with a majority of women and a majority of men employees were analyzed in terms of agency and communion , and by dictionaries used in Linguistic Inquiry and Word Count (LIWC;). In the cloud depicting texts from organizations with a higher percentage of women (to the left), we found words related with communion (e.g., care, society, and welcome), work (e.g., employee, job, working). In the cloud depicting a higher percentage of men (to the right) we also found words related to communion (e.g., services and people), as well as numbers\\/quantity (e.g., billion, many, and various), and space (e.g., world and around). In both clouds we found agency related words (e.g., ability, strives for women dominated industries, and proud, actively for men-dominated industries). All in all, the cloud with a higher percentage of women employees included words reflecting the cooperation and the employer\\/employee relationship, whereas the cloud with a higher percentage of men employees included words relating to customer relations and numbers. Discussion This study investigated the relation between language in organizational descriptions and the gender ratios of those organizations, based on legal gender (i.e., women\\/men). How organizations choose to present themselves might reflect whether women or men are the typical employees. The recruitment process is crucial in preventing gender segregation. Because organizational descriptions are likely to influence who identifies with the workplace, and thereby who wants to apply for a job there, a greater understanding of such descriptions and the way they are presented, may help in preventing gender segregation. The current research examined a Swedish context, observing Swedish organizational descriptions on LinkedIn. In contrast to much earlier research that has focused on job adverts, we here argue that a broader focus on organizational descriptions reveals something about how the organization is constituted. We found a strong relationship between language in organizational descriptions and the employee gender ratio of women\\/men in the organization. This finding is in line with research on job adverts. Pietraszkiewicz et al. (2018) found that job descriptions reflect gender ratio in the profession as measured by percentages of agency and communion words. Gaucher et al. (2011) found that job adverts for men-dominated areas, as compared with women-dominated areas, used words associated with masculine stereotypes. While certain jobs may require communal skills and thus are difficult to entirely change, organizational descriptions could include more balanced language in terms of both agency and communion, thereby increasing a perceived fit with the organization for both women and men. Our results indicate that this may be a neglected area in gender equality work. The present study further extends prior research on job adverts by using a completely data-driven method examining how organizational descriptions differ between organizations with different gender distributions. Previous research used dictionaries representing feminine\\/communal and masculine\\/agentic stereotypical characteristics and word choice. Gaucher et al. (2011) used manual coding, and Pietraszkiewicz et al. (2018) used coding based on the methods of LIWC. The present study was data-driven and thereby inductively examined what words differentiate between texts in organizations with a majority of women and men employees. Descriptions of organizations with a higher percentage of women were characterized by communal words, and also words related with the workplace. In contrast, descriptions of organizations with a higher percentage of men were characterized by an equal balance of agency and communion, but also with more words related with time, space and numbers. The organizations with a higher percentage of women or a higher percentage of men employees represent different sectors, i.e., women dominate the public sectors (e.g., healthcare and education) and men dominate the private sectors (e.g., industry, transport, and agriculture). Hence, the organizational descriptions could differ due to differences between industries. To further examine this, an additional analysis was conducted. In it, coding for private or public organization was added as a covariate to the multiple linear regression model. The gender ratio was still predicted with a reasonably high Pearson correlation. This indicates that differences in the organizational descriptions to a certain degree do reflect the gender ratios of the organizations, rather than just the different sectors that the organizations belong to. Social role theory  posits that women are assumed to possess communal traits, such as being caring while men are assumed to possess agentic traits such as leading because they are observed in roles that require the corresponding traits. This is also what we found in our analyses. Words reflecting communality and the workplace were found in the organizational descriptions of organizations with a majority of women employees, and words reflecting numbers, space and quantities could be found among the organizational descriptions of organizations with a majority of men employees. While not all words that differed between the women and men dominated organizations completely aligned with the adjectives often presented in social role theoretical research, it is important to keep in mind that such research is focused on descriptions of individuals and therefore are qualitatively different from descriptions of organizations. Still, the important words that were uniquely associated with the women\\/men-dominated organizations do reflect more relationship-building words among the women-dominated organizations and more goal-oriented words among the men-dominated organizations. Examples of words from the women-dominated organizations were nature, culture, children, care, health, county and quality of life. Such words clearly imply a sort of \\\"connectedness.\\\" Examples of words from the men-dominated organizations were builds, solution, development, actively and revenue. These words are rather focused on goalachievement. Nonetheless, these words reflect the two overarching dimensions often seen as central in person perception , where they are often referred to as morality and ability , or in terms of stereotypes, the terms warmth and competence can also be applied. We used two different language models for predicting the gender ratio in the LinkedIn texts. BERT is a modern language based on a transformer deep learning architecture, and we therefore predicted that it would outperform the older and simpler bag-ofword model, LSA. This was also the case, where the BERT model showed somewhat higher accuracy in prediction of the gender ratio compared to the LSA as measured by the Pearson correlation between estimated and empirical measured gender ratios. The gender ratio of women averaged over all organizations is 64% in our dataset, indicating that there is oversampling of organizations with a higher ratio of women compared to men. It would have been optimal to use a dataset with equal gender ratios. However, to our knowledge the unequal gender ratio does not influence our finding that the gender ratios can be predicted by the text describing the organizations. Practical implications The organizational description can affect potential applicants' perceptions of the organization as a whole, whereas the job description only affects perceptions of the job. This, in turn, might influence perceived person-organization fit and person-job fit, respectively. The extent to which an applicant is attracted to an organization is the first step in the recruitment process, and here, different messaging strategies are implemented to generate the largest possible pool of qualified applicants. In a meta-analysis , attraction toward a new job and job pursuit intentions were predicted by perceived person-organization fit, whereas intentions to accept a job offer were predicted by perceived person-job fit. That is, in the early stages of recruitment, personorganization fit predicts critical attitudes of the prospective employee. Recruitments are a central event in maintaining or reducing gender segregation. Organizational descriptions could influence who identifies with the work place and who wants to apply for a job there. The results of the present research indicate different ways of addressing the assumed receiver of the texts, i.e., the presumptive employee, depending on the gender distribution of the organization. If women-dominated and men-dominated industries are described in very dissimilar ways, this could contribute to maintenance of a segregated job market. It is therefore important for organizations to be aware of this in order to prevent qualified candidates from rejecting places of work at an early stage, simply because they do not experience a personorganization fit. In the long run, such practices contribute to uphold gender stereotypes since women will continue to be observed in jobs that require communal skills and men will continue to be observed in jobs that require agentic skills. In addition, studies indicate advantages in having organization with a greater gender balance such as less sexual harassment , economic growth and innovation , and better health. In accordance with Swedish law (;:567), employers should promote an equal gender distribution in different types of work and employee categories, and when the distribution is not more or less equal in a certain type of work or employee category, the employer is to make a special effort when recruiting new employees to attract applicants of the under-represented gender. If organizations want to attract an under-represented category, the lack of fit model  might be of use in helping organizations understand how to do it. The model is made up of two constituents: gendered job requirements and gender stereotype characteristics. Minimizing either of these, will also minimize expectations of performance failure and biased recruitments. One way for organizations to target the first component is for them to change the perception of their field and of themselves. To achieve this, practices promoting and sustaining views of the organization as gendered must be addressed. Organizational texts must describe the core activity (e.g., medicine or engineering); however, organizational descriptions with a gendered language may help preserve the idea that these organizations require exclusively stereotypically feminine or masculine gendered behavior (e.g., by emphasizing cooperation or competition). Gendered language in job descriptions has been shown to negatively influence both inclination to apply  as well as prospect of being chosen  when there is perceived gender incongruence between job and job seeker. When changing the language in organizational or job descriptions, organizations should preferably use a balanced language, associated with both women and men stereotypes, or a more gender-neutral language, so that an intervention does not increase a mismatch among majority members of the organization  although majority members may pay less attention to smaller shifts in how the organization is framed. Limitations and future research The present study had a number of strengths, but was not without limitations. First, as already discussed, the examined organizations were made up of widely different industries, for example commerce, education, public authorities and production\\/ extraction. Certain differences of language in the organizational descriptions were therefore to be expected. This was to a certain degree accounted for using an additional analysis, in which coding for private or public organization was added as a covariate to the multiple linear regression model. The gender ratio could still be predicted with a reasonably high Pearson correlation. However, future research may want to examine more organizations with varying gender distribution within the same industry, for example commerce or finance. This requires information about exact employment demographics from a large number of organizations within the same industry. Second, only observational data was employed, with no experimental manipulations. Future studies should examine potential applicants' experiences of the organizational descriptions: their attraction toward the organizations and the likelihood of applying, by using core words from organizational descriptions of organizations with a majority of men employees in descriptions of organizations with a majority of women employees. This would give the opportunity to examine if and how an organization describes itself influences potential candidates. It would also be of interest for future studies to investigate the overlap between an employee's description of their work place and the official organizational description, as these are promotional texts not necessarily consistent with reality. The LSA model used in the LinkedIn texts is a generic model of the Swedish language, whereas the BERT model is a multi-lingual model applicable to several languages. Thus, neither of these models have been fined tuned to the specific type of language used in the Swedish LinkedIn texts. Future research may analyze organizational texts with a language model that is fine tuned to the to-be-analyzed text data set. To what extent this will further improve the accuracy in the gender ratio predictions is a topic for future research. Another avenue for future research would be to test if the gender of the author affects how an organizational description is framed. For example, an organization with a higher female ratio is perhaps more likely to have a woman describe the organization. Thus, the gender of those in charge of making the text could influence how they understand and describe the organization. This would therefore be an interesting variable to look at. However, unfortunately, we do not have access to the gender of the authors. A possibility here is to apply an NLP model that the predicts gender of authors on the LinkedIn texts. However, such an analysis is further complicated by the fact that the texts are presumably written and edited by several people in company with a mix of men and women. Due these complications, we have avoided to make such analysis, although it would be an interesting topic for future research. Additionally, it would be interesting to examine the relation between language in organizational descriptions and that in job adverts. Previous research has shown that language in job adverts reflect the gender distribution in a profession and vary in word use depending on whether it is a women-or men-dominated job. However, on LinkedIn the organizational descriptions are independent from potential job adverts, as it is a way for the organization as a whole to introduce themselves. Hence, the examined organizations and organizational descriptions in the current study were not linked to any specific job adverts. Lastly, because the gender distribution of the employees was determined based on legal gender, only the binary genders of women\\/men were included in the analysis. Given that gender is not a binary category , these analyses did hence not include the variation in gender identities. Future research could expand the definition of gender segregation, by also include non-binary identities (i.e., individuals not identifying as women or men). However, this requires other measures of gender distributions, such as self-reported gender identity among the employees. Conclusion In summary, this study showed that organizational descriptions reflect the gender distribution in organizations, based on the legal gender of women\\/men, with moderate to strong Pearson correlations between observed proportions of women\\/men in the organizations, and predictions based on training from LinkedIn texts. This suggests that the gender ratio of an organization is in some way linked to the way that organization chooses to describe themselves. The organizational descriptions could thereby communicate subtle signals in regards to what potential candidate is most sought after, and risk not attracting those who are underrepresented in the organization.\",\"1135746594\":\"Introduction America and China are the world's two largest economies, and they are currently locked in a tense rivalry. In a democratic system, public opinion shapes and constrains political action. How the American public views China thus affects relations between the two countries. Because few Americans have personally visited China, most Americans form their opinions of China and other foreign lands from media depictions. Our paper aims to explain how Americans form their attitudes on China with a case study of how The New York Times may shape public opinion. Our analysis is not causal, but it is informed by a causal understanding of how public opinion may flow from the media to the citizenry. Scholars have adopted a number of wide-ranging and even contradictory approaches to explain the relationships between media and the American mind. One school of thought stresses that media exposure shapes public opinion. Another set of approaches focuses on how the public might lead the media by analyzing how consumer demand shapes reporting. Newspapers may attract readers by biasing coverage of polarizing issues towards the ideological proclivities of their readership , and with the advent of social media platforms such as Facebook and Twitter, traditional media are now more responsive to audience demand than ever before. On the other side of this equation, news consumers generally tend to seek out news sources with which they agree , and politically active individuals do so more proactively than the average person. Two other approaches address factors outside the media-public binary. The first, stresses the role of elites in opinion formation. While some, famously including Noam Chomsky, argue that news media are unwitting at best and at worst complicit \\\"shills\\\" of the American political establishment, political elites may affect public opinion directly by communicating with the public. Foreign elites may also influence American opinion because American reporters sometimes circumvent domestic sources and ask trusted foreign experts and officials for opinions. The second stresses how the macrolevel phenomenon of public sentiment is shaped by micro-level and meso-level processes. An adult's opinions on various topics emerge from their personal values, many of which are set during and around adolescence from factors outside of the realm of individual control. Social networks may also affect attitude formation. In light of these contradictory interpretations, it is difficult to be sure whether the media shape the attitudes of consumers or, on the other hand, whether consumers shape media. Moreover, most of the theories summarized above are tested on relatively small slices of data. In order to offer an alternative, \\\"big data\\\"-based contribution to this ongoing debate, this study compares how the public views China and how the news media report on China with large-scale data. Our data set, which straddles 50 years of newspaper reporting and survey data, is uniquely large and includes more than a quarter-million articles from The New York Times. Most extant survey data indicate that Americans do not seem to like China very much. Many Americans are reported to harbor doubts about China's record on human rights  and are anxious about China's burgeoning economic, military, and strategic power. They also think that the Chinese political system fails to serve the needs of the Chinese people. Most Americans, however, recognize a difference between the Chinese state, the Chinese people, and Chinese culture, and they view the latter two more favorably. In Fiske's Stereotype Content Model , which expresses common stereotypes as a combination of \\\"competence\\\" and \\\"warmth\\\", Asians belong to a set of \\\"high-status, competitive out-groups\\\" and rank high in competence but low in warmth. The New York Times, which calls itself the \\\"Newspaper of Record\\\", is the most influential newspaper in the USA and possibly even in the Anglophonic world. It boasts 7.5 million subscribers , and while the paper's reach may be impressive, it is yet more significant that the readership of The New York Times represents an elite subset of the American public. Print subscribers to The New York Times have a median household income of $191,000, three times the median income of US households writ large. Despite the paper's haughty and sometimes condescending reporting, it \\\"has had and still has immense social, political, and economic influence on American and the world\\\" (;, p. 81). The New York Times may be a paper for America's elite, and it may be biased to reflect the tastes of its elite audience, but the paper's ideological slant does not affect our analyses as long as the its relevant biases are consistent over the time period covered by our analyses. Our analyses support the intuition of qualitative work on The Times  and show that these biases remain more or less constant for the decades in our sample. These analyses also illuminate some of the paper's more notable biases, including the paper's particular predilection for globalization. The impact of social media on traditional media is not straightforward. While new media have certainly changed old media, neither has replaced the other. It is more accurate to say that old media have been integrated into new media and, in some ways, become a form of new media themselves. Twitter has accelerated the 2000s-era trends of information access that made it possible for news readers to find their own news and also enabled readers to interact with journalists , and the The New York Times seems to have made a significant commitment to the Twitter ecosystem. A quick glance at the follower count of The Times' official Twitter account shows that it is one of the most influential accounts on the site, with almost 50 million followers. For comparison, both current president Joe Biden and vice president Kamala Harris have around 10 million followers. Most New York Times reporters additionally have \\\"verified\\\" accounts on the platform, which means that individual reporters may be incentivized to maintain public-facing profiles more now than in the past. The media consumption patterns that made new media possible have changed the way The New York Times interacts with its audience and how it extracts revenue. The New York Times boasts a grand total of 7.5 million subscribers, but only 800,000 of them subscribe to the print edition. The Times' digital subscription base has boomed since the election of Donald J. Trump, growing almost sixfold from a paltry 1.3 million in 2015 to a staggering 6.7 million in 2020 . The Times increasingly relies more on digital subscriptions and less on print subscriptions and ad sales for revenue. Ad revenue for most papers has been in sharp decline since the early 2000s , and this trend has only continued into the present. The New York Times now operates almost like a direct-to-consumer, subscription tech startup. New media have not replaced but have certainly changed old media. The full impact of these changes is beyond the scope of this paper, and we suggest it as an area for further research. A small body of prior work has studied the The New York Times and how The New York Times reports on China. Blood and Phillips use autoregression methods on time series data to predict public opinion. Wu et al. use a similar autoregression technique and find that public sentiment regarding the economy predicts economic performance and that people pay more attention to economic news during recessions. Peng finds that coverage of China in the paper has been consistently negative but increasingly frequent as China became an economic powerhouse. There is very little other scholarship that applies language processing methods to large corpora of articles from The New York Times or other leading papers. Atalay et al. is an exception that uses statistical techniques for parsing natural languages to analyze a corpus of newspaper articles from The New York Times, the Wall Street Journal, and other leading papers in order to investigate the increasing use of information technologies in newspaper classifieds. We explore the impact of The New York Times on its readers by examining the general relationship between The Times and public opinion. Though some might contend that only elites read NYT, we have adopted this research strategy for two reasons. If the views of NYT only impacted the nation's elite, the paper's views would still propagate to the general public through the elites themselves because elites can affect public opinion outside of media channels. Additionally, it is a widely held belief that NYT serves as a general barometer of an agenda-setting agent for American culture. Because of these two reasons, we interpolate the relationship between NYT and public opinion from the relationship between NYT and its readers, and we extrapolate that the views of NYT are broadly representative of American media. Our paper aims to advance understanding of how Americans form their attitudes on China with a case study of how The New York Times may shape public opinion. We hypothesize that media coverage of foreign nations affects how Americans view the rest of the world. This reduced-form model deliberately simplifies the interactions between audience and media and sidesteps many active debates in political psychology and political communication. Analyzing a corpus of 267,907 articles on China from The New York Times, we quantify media sentiment with BERT, a state-ofthe-art natural language processing model with deep neural networks, and segment sentiment into eight domain topics. We then use conventional statistical methods to link media sentiment to a longitudinal data set constructed from 101 cross-sectional surveys of the American public's views on China. We find strong correlations between how The New York Times reports on China in one year and the views of the public on China in the next. The correlations agree with our hypothesis and imply a strong connection between media sentiment and public opinion. Methods We quantify media sentiment with a natural language model on a large-scale corpus of 267,907 articles on China from The New York Times published between 1970 and 2019. To explore sentiment from this corpus in greater detail, we map every article to a sentiment category (positive, negative, or neutral) in eight topics: ideology, government and administration, democracy, economic development, marketization, welfare and well-being, globalization, and culture. We do this with a three-stage modeling procedure. First, two human coders annotate 873 randomly selected articles with a total of 18,598 paragraphs expressing either positive, negative, or neutral sentiment in each topic. We treat irrelevant articles as neutral sentiments. Secondly, we fine-tune a natural language processing model Bidirectional Encoder Representations from Transformers (BERT)  with the human-coded labels. The model uses a deep neural network with 12 layers. It accepts paragraphs (i.e., word sequences of no more than 128 words) as input and outputs a probability for each category. We end up with two binary classifiers for each topic for a grand total of 16 classifiers: an assignment classifier that determines whether a paragraph expresses sentiment in a given topic domain and a sentiment classifier that then distinguishes positive and negative sentiments in a paragraph classified as belonging to a given topic domain. Thirdly, we run the 16 trained classifiers on each paragraph in our corpus and assign category probabilities to every paragraph. We then use the probabilities of all the paragraphs in an article to determine the article's overall sentiment category (i.e., positive, negative, or neutral) in every topic. As demonstrated in Table 1, the two classifiers are accurate at both the paragraph and article levels. The assignment classifier and the sentiment classifier reach classification accuracy of 89-96% and 73-90%, respectively, on paragraphs. The combined outcome of the classifiers, namely article sentiment, is accurate to 62-91% across the eight topics. For comparison, a random guess would reach an accuracy of 50% on each task (see Supplementary Information for details). American public opinion towards China is a composite measure drawn from national surveys that ask respondents for their opinions on China. We collect 101 cross-sectional surveys from 1974 to 2019 that asked relevant questions about attitudes toward China and incorporate a probabilistic model to harmonize different survey series with different scales (e.g., 4 levels, 10 levels) into a single time series, capitalizing on \\\"seaming\\\" years in which different survey series overlapped. For every year, there is a single real value representing American sentiment on China relative to the level in 1974. Put another way, we use sentiment in 1974 as a baseline measure to normalize the rest of the time series. A positive value shows a more favorable attitude than that in 1974, and a negative value represents a less favorable attitude than that in 1974. Because of this, the trends in sentiment changes year-over-year are of interest, but the absolute values of sentiment in a given year are not. As shown in Fig. 1, public opinion towards China has varied greatly from 1974 to 2019. Table 1 Accurately quantifying media sentiment. Article sentiment (%) Paragraph assignment (%) Paragraph sentiment (%) It steadily climbed from a low of -24% in 1976 to a high of 73% in 1987, and has fluctuated between 10% and 48% in the intervening 30 years. Results We begin with a demonstration of how the reporting of The New York Times on China changes over time, and we follow this with an analysis of how coverage of China might influence public opinion toward China. Trend of media sentiment. The New York Times has maintained a steady interest in China over the years and has published at least 3,000 articles on China in every year of our corpus. Figure 2 displays the yearly volume of China-related articles from The New York Times on each of the eight topics since 1970. Articles on China increased sharply after 2000 and eventually reached a peak around 2010, almost doubling their volume from the 1970s. As the number of articles on China increased, the amount of attention paid to each of the eight topics diverged. Articles on government, democracy, globalization, and culture were consistently common while articles on ideology were consistently rare. In contrast, articles on China's economy, marketization, and welfare were rare before 1990 but became increasingly common after 2000. The timing of this uptick coincided neatly with worldwide recognition of China's precipitous economic ascent and specifically the beginnings of China's talks to join the World Trade Organization. While the proportion of articles in each given topic change over time, the sentiment of articles in each topic is remarkably consistent. Ignoring neutral articles, Figure 3 illustrates the yearly fractions of positive and negative articles about each of the eight topics. We find four topics (economics, globalization, culture, and marketization) are almost always covered positively while reporting on the other four topics (ideology, government & administration, democracy, and welfare & well-being) is overwhelmingly negative. The NYT views China's globalization in a very positive light. Almost 100% of the articles mentioning this topic are positive for all of the years in our sample. This reveals that The New York Times welcomes China's openness to the world and, more broadly, may be particularly partial to globalization in general. Similarly, economics, marketization, and culture are covered most commonly in positive tones that have only grown more glowing over time. Positive articles on these topics began in the 1970s with China-US Ping-Pong diplomacy, and eventually comprise 1\\/4 to 1\\/2 of articles on these three topics, the remainder of which are mostly neutral articles. This agrees with the intuition that most Americans like Chinese culture. The New York Times has been deeply enamored with Chinese cultural products ranging from Chinese art to Chinese food since the very beginning of our sample. Following China's economic reforms, the number of positive articles and the proportion of positive articles relative to negative articles increases for both economics and marketization. In contrast, welfare and well-being are covered in an almost exclusively negative light. About 1\\/4 of the articles on this topic are negative, and almost no articles on this topic are positive. Topics regarding politics are covered very negatively. Negative articles on ideology, government and administration, and democracy outnumber positive articles on these topics for all of the years in our sample. Though small fluctuations that coincided with ebbs in US-China relations are observed for those three topics, coverage has only grown more negative over time. Government and administration is the only negatively covered topic that does feature some positive articles. This reflects the qualitative understanding that The New York Times thinks that the Chinese state is an unpleasant but capable actor. Despite the remarkable diversity of sentiment toward China across the eight topics, sentiment within each of the topics is startlingly consistent over time. This consistency attests to the incredible stability of American stereotypes towards China. If there is any trend to be found here, it is that the main direction of sentiment in each topic, positive or negative, has grown more prevalent since the 1970s. This is to say that reporting on China has become more polarized, which is reflective of broader trends of media polarization. Media sentiment affects public opinion. To reveal the connection between media sentiment and public opinion, we run a linear regression model (Eq. (1)) to fit public opinion with media sentiment from current and preceding years. mt 1\\/4 1<= k<= 8 j2 1\\/2t;t1;t2; s2fpositive;negativeg bkjsFkjs; d1Th where mt denotes public opinion in year t with possible values ranging from -1 to 1. Fkjs is the fraction of positive (s= positive) or negative (s= negative) articles on topic k in year j. Coefficient bkjs quantifies the importance of Fkjs in predicting mt. There is inertia to public opinion. A broadly held opinion is hard to change in the short term, and it may require a while for media sentiment to affect how the public views a given issue. For this reason, j is allowed to take [t, t- 1, t- 2, ...] anywhere from zero to a couple of years ahead of t. In other words, we inspect lagged values of media sentiment as candidate predictors for public attitudes towards China. We seek an optimal solution of media sentiment predictors to explain the largest fraction of variance (r2) of public opinion. To reduce the risk of overfitting, we first constrain the coefficients to be non-negative after reverse-coding negative sentiment variables, which means we assume that positive articles have either no impact or positive impact and that negative articles have either zero or negative impact on public opinion. Secondly, we require that the solution be sparse and contain no more than one non-zero coefficient in each topic: maximize b r2dm; b; FTh subject to bkjs >= 0; 8k; j; s kbk;k0 <= 1; 8k where r2(m, b, F) is the explained variance of m fitted with (b, F). The l0-norm bk,,0 gives the number of non-zero coefficients of topic k predictors. The solution varies with the number of topics included in the fitting model. As shown in Table 2, if we allow fitting with only one topic, we find that sentiment on Chinese culture has the most explanatory power, accounting for 31.2% of the variance in public opinion. We run a greedy strategy to add additional topics that yield the greatest increase in explanatory power, resulting in eight nested models (Table 2). The explanatory power of our models increases monotonically with the number of allowed topics but reaches a saturation point at which the marginal increase in variance explained per topics decreases after only two topics are introduced (see Table 2). To strike a balance between simplicity and explanatory power, we use the top two predictors, which are the positive sentiment of culture and the negative sentiment of democracy in the previous year, to build a linear predictor of public opinion that can be written as mt 1\\/4 0:791th 3:112Fculture;t1;positive th 1:452Fdemocracy;t1;negative; d2Th where Fculture,t-1,positive is the yearly fraction of positive articles on Chinese culture in year t- 1 and Fdemocracy,t-1,negative is the yearly fraction of negative articles on Chinese democracy in year t- 1. This formula explains 53.9% of the variance of public opinion in the time series. For example, in 1993 53.9% of the articles on culture had a positive sentiment, and 46.9% of the articles on democracy had negative sentiment (Fculture,positive= 0.539, Fdemocracy,negative=-0.469). Substituting those numbers into Eq. (2) predicts public opinion in the next year (1994) to be 0.208, very close to the actual level of public opinion (0.218) (Fig. 4). Discussion By analyzing a corpus of 267,907 articles from The New York Times with BERT, a state-of-the-art natural language processing model, we identify major shifts in media sentiment towards China across eight topic domains over 50 years and find that media sentiment leads public opinion. Our results show that the reporting of The New York Times on culture and democracy in one year explains 53.9% of the variation in public opinion on China in the next. The conclusion that we draw from our results is that media sentiment on China predicts public opinion on China. Our analysis is neither conclusive nor causal, but it is suggestive. Our results are best interpreted as a \\\"reduced-form\\\" description of the overall relationship between media sentiment and public opinion towards China. While there are a number of potential factors that may complicate our conclusions, none would change the overall thrust of our results. We do not consider how the micro-level or meso-level intermediary processes through which opinion from elite media percolates to the masses below may affect our results. We also do not consider the potential ramifications of elites communing directly with the public, of major events in US-China relations causing short-term shifts in reporting, or of social media creating new channels for the diffusion of opinion. Finally, The New York Times might have a particular bias to how it covers China. In addition to those specified above, a number of possible extensions of our work remain ripe targets for further research. Though a fully causal model of our text analysis pipeline may prove elusive , future work may use randomized vignettes to further our understanding of the causal effects of media exposure on attitudes towards China. Secondly, our modeling framework is deliberately simplified. The state affects news coverage before the news ever makes its way to the citizenry. It is plausible that multiple state-level actors may bypass the media and alter public opinion directly and to different ends. For example, the actions and opinions of individual high-profile US politicians may attenuate or exaggerate the impact of state-level tension on public sentiment toward China. There are presumably a whole host of intermediary processes through which opinion from elite media affects the sentiment of the masses. Thirdly, the relationship between the sentiment of The New York Times and public opinion may be very different for hot-button social issues of first-line importance in the American culture wars. In our corpus, The New York Times has covered globalization almost entirely positively, but the 2016 election of President Donald J. Trump suggests that many Americans do not share the zeal of The Times for international commerce. We also plan to extend our measure of media sentiment to include text from other newspapers. The Guardian, a similarly elite, Anglophonic, and left-leaning paper, will make for a useful comparison case. Finally, our analysis was launched in the midst of heightened tensions between the US and China and concluded right before the outbreak of a global pandemic. Many things have changed since COVID-19. Returning to our analysis with an additional year or two of data will almost certainly provide new results of additional interest. Future work will address some of these additional paths, but none of these elements affects the basic conclusion of this work. We find that reporting on China in one year predicts public opinion in the next. This is true for more than fifty years in our sample, and while knowledge of, for example, the opinion diffusion process on social media may add detail to this relationship, the basic flow of opinion from media to the public will not change. Regarding the putative biases of The New York Times, its ideological slant does not affect our explanation of trends in public opinion of China as long as the paper's relevant biases are relatively consistent over the time period covered by our analyses.\",\"1135746671\":\"Introduction Is there a systematic pattern in human mating? This question has been the source of introspection for centuries among philosophers and psychologists. Evidence from genetic and evolutionary\\/social psychology widely demonstrated that human mating strategies strongly depend on the similarity. The similarity refers to stable partner characteristics, such as ethnicity, religion, attitudes, values, or personality, which can promote long-term relationship satisfaction by setting the stage for similar emotional experiences. At a social level, couple similarity predicts marital satisfaction since similar spouses can create more homogeneous rearing environments that may well have important consequences for their offspring development. Again, individuals tend to choose a partner based on gene similarity, which stabilizes relations and leads to greater reproduction. Among similarity factors, personality has ever attracted much attention, since being similar to a partner can give more positive effects on relationship functioning. From an evolutionary point of view, several authors theorized that adaptation has created systematic patterns in the human mating based on the trust and stability, which confers several biological advantages (;Buss and Schmitt 1993;). The popular culture calls this mate preference as \\\"type\\\" when it comes to their partners' personalities , whereas psychological science have tried to statistically define it in terms of \\\"similarity\\\" . The idea was to understand and demonstrate why people end up with particular partners who are similar to them in terms of personality and which kind of similarity characterized this relationship. The key theoretical principle that underlies the concept of the ideal partner preference is that personality traits might influence mate selection. This is because personality relates to the likelihood of experiencing certain sets of emotions and their convergence (or less) might influence people to mate following a stability and similarity pathway. At a psychological level, stability refers to the emotional states required for consolidating life-long mate. As claimed by Caspi and Herbener 1990\\\"stability is possible in part because individuals create the environments in which they live\\\". Researches demonstrated that emotional stability in partnering patterns is preferred either considering longitudinal evaluations (9 years: Park and MacDonald 2019; 1 year: Gonzaga et al. 2007) or even in the descriptions of an ideal partner's personality. Emotional stability is a personality trait (as defined by the five-factor model of personality) (FFM)  showing the most robust effects on relationship satisfaction. Obviously, its opposite, neuroticism, conceptualized as proneness to feel negative emotions (i.e. anxiety\\/depression), is associated with poor health condition and increased risk for marital distress. Other broad personality constructs have been proposed to predict better relationship functioning and mate preference. For instance, extraversion and openness to experience are other personality traits widely investigated in mating literature. Extraversion and openness to experience reflect a high attraction to environmental rewards and exploration, respectively, and a high level of extraversion is associated with happiness and fewer rates in marital divorce. All evidence provided so far would seem to suggest that people tend to choose marriage partners who have similar personality traits (i.e. neurotic husband tends to mate neurotic wife). When researchers tried to directly demonstrate this hypothesis, weak evidence was reported only for the impact of neuroticism and extraversion. These findings suggest that people end up with romantic partners who are similar to them in terms of their attitudes and values but not in terms of personality, although a profound heterogeneity in the literature has been reported. In fact, there are a lot of factors that may contribute to contradictory findings: a) the small sample size; b) the employment of different personality taxonomies; c) the employment of different similarity indices aimed at quantifying human attraction in a statistically meaningful way, and d) the fact that previous data are collected from couples who already have been married for a substantial length of time. The Present Study In this study, we sought to provide new evidence in this field of study. To demonstrate if there is (or not) a systematic multi (or single)-trait similarity pattern in the human selection of long-life mate we conducted, for the first time, a data analysis approach characterized by the employment of two combined predictive models: Partial Least Squares and Random Forest. In particular, the random forest is a method of machine learning designed to answer questions about the prediction showing more advantages over conventional regression models. This algorithm is sensitive to nonlinear relationships, including complex interactions among predictors, such as the similarity (or less) between personality traits. Material and Methods Participants The sample consisted of 235 married couples identified from pre-marriage courses in the Christian Churches of Italy. Couples who met the inclusion criteria for the study then were sent a letter inviting them to participate. Inclusion criteria were: 1) married for few days; 2) age over 18 years; 3) couples at the first marriage. We did not find significant differences between couples' demographical data. Indeed, the husbands' average age and educational level were 32.4 +- 4.9 years and 14.3 +- 3.1 years, respectively. While the wives' average age and educational level were 29.5 +- 4.6 years and 15 +- 2.9 years, respectively. The totality of the sample was Caucasian. Concerning occupation, 42% of husbands were professionals, 34% were laborers, 4% were managers, and 20% of them did not have a job. While 30% of wives were professionals, 34% were laborers, 2% were managers, and 34% of them did not have a job. Procedure Similar to a previously validated procedure , our research staff members contacted and invited (by phone, e-mail or personally) potential participants to give their consent to participate in the study and informed them that we were conducting a research project on personality profiles. The staff contacted potential participants in their marriage's Church. If individuals expressed interest in the study, they received a one-page written description of the research explaining the research history and meaning and the procedure for filling out the personality questionnaire. Having obtained informed oral consent, we organized weekly meetings with subjects who accepted to participate. During this meeting, the staff members followed the same protocol explaining that the research purpose was to evaluate the personality profile of a large group of people and that data of every single subject are pooled together and used for statistical purposes. Finally, individuals were invited to anonymously complete the personality questionnaire sitting quietly at a separate desk. Moreover, we ensured the participants that there were no right or wrong answers and that their responses were confidential. Again, an additional brief demographic questionnaire was assessed which yielded information about, age, educational level and occupation. We did not offer monetary compensation for participation. One psychologist was always available to respond to requests for clarification or other difficulties. Personality scores from every single questionnaire were extracted by a different staff group blinded to the group membership. Participants were treated following the ethical standards expressed in the Declaration of Helsinki. Personality Assessment The FFM of personality was assessed with the Italian version  of the Revised NEO-PersonalityInventory (NEO-PI-R), which measures 30 facets, six for each of the five major dimensions of personality. The FFM of personality is a conceptualization of personality comprising behavioral, emotional and cognitive patterns. The 240 items are answered on a five-point Likert scale, from strongly disagree to strongly agree. In this sample, the NEO-PI-R showed good psychometric properties: internal consistency reliabilities (Cronbach's alpha) for the five factors ranged from 0.8 to 0.89. Raw scores were converted to T scores (M= 50, SD= 10) using combined-sex norms. The Statistical Analysis The Big Five personality traits of the 235 married couples were used for the statistical analysis. For the sake of clarity, let us denote by X the 235 x 5 matrix whose columns contain the Big-Five traits of wives' personality (N, E, O, A, C) and similarly let us denote by Y the 235 x 5 matrix corresponding to the husbands' personality, the i-rows of matrices X and Y representing the i-th couple. In the following sections, we describe the methods used to perform data analysis. Hypothesis Test As a preliminary analysis, we performed five hypothesis tests to check for each of the five personality traits if there was a difference between men and women. In particular, without using the information about who is coupled to whom, we compared the two groups in each individual personality trait by Mann-Whitney U test, i.e. we compared column X*j and column Y*j for each j = 1, . . . , 5. The Mann-Whitney test is a univariate non-parametric test, which is equivalent to the rank sum test, and it tests the null hypothesis that data in the two groups are samples from distributions with equal medians, against the alternative that they are not. We employed nonparametric test because some columns (or personality traits) are found not sampled from a Gaussian population by Shapiro-Wilk test. Univariate Correlation Analysis As the first step, to investigate the relationship between the five traits of the personality of wives and those of husbands, we performed 25 univariate correlation analyses between each trait of wife and each trait of husband. In particular, we evaluated the Spearman correlation which considers general relationships other than linear. Moreover, Spearman correlation is based on the variables rank and this makes this choice consistent with that made for the hypothesis test. Because Spearman's rank correlation is computationally identical to Pearson product-moment coefficients, we used GPower tool (http:\\/\\/www.gpower.hhu.de\\/) to make power analysis. It revealed that our sample size (N= 235) was sufficient to reach a statistical power of 88% (12% risk of type II error) at a significance level of a = 0.05 (5% risk of type I error) for testing zero correlation with effect size 0.2 (correlation under the alternative). Multivariate Regression Analysis From a statistical point of view, the problem of learning a mathematical relation between the variables X and Y is a multivariate regression problem; so that, as the second step of our analysis, we performed a Partial Least Squares (PLS) regression. PLS regression is a recent technique that generalizes and combines features from Principal Component Analysis (PCA) and multivariate regression. While PCA decomposes X in order to obtain new components which best explain X (and then with these new components perform regression on Y), PLS regression finds components from X that best predict Y. In particular, PLS decomposes both X and Y as a product of a common set of orthogonal factors and a set of specific loading, so thatX = TP t and Y = TBCt, where Y is the best estimate you can get of Yusing the best latent vectors of X. By analogy with PCA, T is called the score matrix (T tT = I) and its columns are the latent vectors, while P is called the loading matrix. However, in contrast to PCA, these latent vectors are obtained performing a simultaneous decomposition of X and Y with the constraint that these components explain as much as possible of the covariance between X and Y, instead of the covariance of X. The matrix Y^ is the multivariate regression prediction of Y, in fact it can be expressed as Y = XbP LS with bP LS = (P t)+BCt. When such a decomposition is performed the first column of matrix T, i.e. the first latent direction of X, corresponds to the direction along which X has maximum variability and which is most correlated to the first latent direction of Y, i.e. the first column of matrix Y^. In other words, the first latent directions of X and Y represent the best linear combinations of columns of matrix X and of columns of matrix Y, such that the covariance between these two latent vectors is maximum. It is essential to note that the relative proportion of X variance explained by the latent directions of X cumulates to one, since all the latent directions explain all the variance of X in different proportion, the first explain more and so on. While, instead, the relative proportion of Yvariance explained by the latent directions of Y does not cumulate to one, since the latent directions explain the variance of Y^ and not of Y. How much the latent directions of Yexplain the variance of Y is then an important measure of the prediction power of the partial least square regression model. Random Forest Many machine learning techniques can be used to study the predictive power of one variable over the other, i.e. to investigate whether, even without an explicit mathematical formula, it is possible to predict Y given X. One of the more powerful methods to investigate such a question is the Random Forest. This method is based on a collection of de-correlated trees, which are averaged to get a prediction, therefore the Random Forest is sensitive to nonlinear relationships and can capture the complex interaction among predictors. To construct a Random Forest model, it is necessary to fix three parameters: the number of predictors to select at random for each decision split, the minimum size of terminal nodes and the number of trees to be averaged. As suggested by classical machine learning literature, we fix the first two parameters equals to p (with p the number of predictors) and 5, respectively; while we tuned the number of trees to be averaged by using the Out Of Bag (OOB) error plot. Once the three parameters are fixed, it is possible to measure the power of the Random Forest prediction of X on Y by the following procedure: 1. Randomly split the data into two groups of size 200 (Xtr, Ytr) and 35 (Xts, Yts) respectively, the first being the training data set, the second being the test data set. 2. For each column of training matrix Ytr, train a Random Forest algorithm using the training matrix Xtr 3. Using that trained models, predict each column of test matrix Yts by using the test matrix Xts. Hence, estimate the predictive accuracy by calculating the correlation between the actual and predicted column of matrix Yts. Results Hypothesis Test: Results In Table 1 we show results obtained comparing each of the Big Five personality traits of males with that of females. In particular, we reported variables distribution as median and interquartileranges (IQR) and p-values obtained by MannWhitney U test. Similarly to previous evidence , we found significant sex differences in trait N, as well as in the trait O (both larger in females), while in the other three traits E, A, C, there is no significant difference between the two groups. Univariate Correlation Analysis: Results First, we evaluated the personality traits of wives comparing them with those of their husbands. We obtained a correlation matrix of 5 x 5 = 25 elements. Of this matrix, we report only the six statistically significant elements, see Table 2. More importantly, the correlation between the wife E and the husband N traits is the only surviving a statistical threshold Bonferroni corrected (p-value is < 0.05\\/25 = 0.0020). In Fig. 1, we report the results of correlation analysis between the most significant relationship between couples' traits. In the figure, each point represents a couple, while the red line is the least square regression of the wife E trait on the husband N trait. Regression analysis revealed a coefficient of determination R2 = 0.07, so that only 7% of the variability of the wife E trait is explained by the N trait of its husband. This kind of analysis shows us how a linear mathematical model between any wife's personality trait and any husband's personality trait has no predictive effectiveness, although it gives us some interesting interpretations. In particular, from Table 2, we can infer that a person (female or male) tend to choose a partner which is similar to him\\/herself along with the traits of personality E, O, A, C, while a neurotic husband is usually paired with a lesser extrovert and open wife. Multivariate Regression Analysis: Results The second step of our statistical approach is aimed at investigating if there exists a mathematical model that could better explain the relationship between female and male traits. In particular, we perform a PLS regression of Y on X, obtaining the following relative proportion of Y variance explained by the latent directions of Y: 2:32% 1:61% 0:95% 0:50% 0:10% These proportions are very low, for example, the first latent component of Y only explains 2.32% of the entire variability of Y. Moreover, only 5.48% of the total variance of Y is explained by X. Hence, although we have used one of the most advanced multivariate regression methods we do not obtain a reliable mathematical model to predict Y by X. To enforce such assertion, in Fig. 2 we plotted the first latent component of X versus the first latent component of Y, revealing a weak association between the two latent components. Random Forest: Results In the last part of our analysis, we have applied a machine learning technique to understand whether there is a complex relationship between X and Y, which cannot be expressed by a mathematical formula. In particular, we used the Random Forest method to explore the predictive power of X on Y according to the steps 1-2-3 explained before. First, we tuned the number of trees of the Random Forest algorithm by looking at the OOB error plot. In Fig. 3, we reported the OOB error for all the Y traits of personality regressed using the X five traits of personality. It is noteworthy that 100 trees are enough to stabilize the OOB error and that N and E traits have a smaller OOB mean error confirming that these two traits have less pronounced stochasticity than the others. To conclude our analysis, in Table 3 we report the mean and SD of the correlations and their respective p-value obtained performing steps 1-2-3 100 times. As clearly shown, the prediction power of the random forest is very weak. Indeed, Table 3 demonstrated that there is no significant relationship between the predicted and the actual husband traits. For the sake of completeness, we also report the same table when the role of X and Y are reversed, i.e. exploring the predictive power ofX on Y, see Table 4. Being Tables 3 and 4 very different, we note that the relationship between X and Y is not symmetric, confirming non-uniformities in the partner choice between females and males based on personality traits. Since correlations, in both Tables 3 and 4 are weak and not statistically significant, we can conclude that we cannot predict Y given X, or vice versa, with any meaningful level of accuracy. Discussion Our study provides two fundamental advances in the mating literature. First, we demonstrated that the relationship between personality traits and mate selection is a complex process rather than trait-specific. Indeed, previous literature reported a single trait-specific relationship (that is, similarity) between personality and mate selection (i.e., neurotic wife tends to choose neurotic husband) . We extend the empirical support to the century-old adage \\\"opposites attract\\\" hypothesis, suggesting that the assortment process for personality may be a multi-trait complementarity process. In fact, using simple univariate statistical methods, we reveal a significant relationship between neuroticism, extraversion and openness traits, demonstrating that a neurotic husband is usually paired with a lesser extrovert and open wife. Second, for the first time, we employed a combined PLS and machine learning data analysis approach to build a predictive model of mate selection. Both statistical models showed that linear (or non-linear) relationships between couple's personalities are unable to predict human mating. The first statistical model ends up with only 5.48% of the total variance of husbands' traits explained by wives' traits. The same weak relationship in personality prediction was also confirmed by the random forest analysis. Indeed, the maximum correlation between the predicted and the actual values of Y (the husband's personality) is 0.247 (Table 3), as well as the maximum correlation between the predicted and actual values of X (the wife's personality) is 0.282 (Table 4). It is worth observing that, Joel et al. (2017) used only the random forest algorithm to test whether it is possible to predict romantic desire using behavioral and psychological measures (including personality) in a student population. The accuracy of prediction varied from 4% to 18%. As for newlywed couples, the authors reported negative results on the ability of machine learning approach to extract reliable \\\"markers of love\\\" useful to predict human behavior. Similar results were obtained by other two independent studies , where the couple similarity was evaluated in a large sample of newlywed by using a multiple linear regression approach. With this in mind, all previous evidence together with our findings, suggest that a mathematical formula linking Yand X cannot be built probably because this relation is more complex or there are others different factors\\/ predictors not linked to the Big Five traits of personality, which may guide the partner choice. The role of personality in mating behavior has recently been revived by Park and MacDonald (2019), who shed new light by performing a 9-years longitudinal study examining the similarity between an individual's ex- and current romantic partner's personality. They identified a pattern of consistency across partner personalities where there is a tendency of dating with a particular type of personwith possible individual differences. In particular, they found that individuals high in extraversion and openness to experience were dating a partner who was more distinctively similar to themselves, whereas neuroticism was negatively linked with self-partner similarity, demonstrating that more neurotic individuals were dating a partner less similar to themselves. Our data confirm the presence of a multi-trait complementarity relationship in newlywed couples (Table 2), differently to couple similarity described in previous studies (the so-called positive assortative mating) (Caspi and Herbener (1990), Thiessen et al. (1997), Klohnen and Mendelsohn (1998), Glicksohn and Golan (2001)). Watson et al. (2004) also confirmed our observation, revealing the presence of negative assortment in personality traits investigating 291 married couples. They found that the wife's neuroticism correlated more strongly with the husband's agreeableness and conscientiousness than with his Neuroticism. The presence of complementary in personality traits reported in all these studies could be explained following the same evolutionary hypothesis proposed by Kenrick et al. (1993). If human adaptation has created a favorite similarity pattern for people with a low level of neuroticism to promote emotional stability in the family , what does happen for people with high level? Probably the tendency of a neurotic husband to mate with an introvert and less open wife could be interpreted as a compensatory mate preference useful for consolidating a life-long marital satisfaction and relationship functioning. Limitations Some aspects might have affected our analysis. Firstly, as already done by Watson et al. (2004) and by Luo and Klohnen (2005) we did not evaluate the influence of length in the couples' premarital phase. Indeed, Lavner et al. (2018) demonstrated that after the first years of marriage, either in husbands or wives, some personality traits (i.e. extraversion) changed differently on average. For wives, there was a decrease in openness and neuroticism, whereas, for husbands, extraversion declined and conscientiousness increased. Second, we focused our investigation only on personality profile, excluding attitudes and values, which characterize the couples' life. Several studies confirm that similarities in these factors are the most important for mate selection , although none has applied mathematical predictive models including these variables. Conclusions Concerning the definite claim of Eysenck (1990) who considered mating as essentially random for personality differences, our study demonstrates that there is a sort of complementarity between traits in newlywed couples but that this pattern is not enough to build a predictive model. We extend recent evidence  demonstrating that prediction of mate selection behaviors may not be achievable through mathematical models using measures collected either before the couple meets or during the onset of marriage. However, recent data provided by Park and MacDonald (2019), together with our findings on the multidimensional construct (high level of neuroticism related to lower extraversion and openness) of couples' profile could suggest that the century-old adage \\\"opposites attract\\\" needs a new mathematical reviewing aimed at better figuring out the complexity of human mate behavior.\",\"1135746697\":\"One only has to be a casual reader of social psychology to know about the minimal group paradigm and the dogma that merely separating people into arbitrary groups creates a variety of intergroup biases. However, are the group distinctions used in this research really arbitrary? It is no coincidence that Tajfel, Billig, Bundy, and Flament (1971) used rather contrived group distinctions, overestimators versus underestimators and preference for paintings by Klee versus Kandinsky, in their landmark article. They were aware that 2 years prior, Rabbie and Horwitz (1969) reported that when group distinctions were completely arbitrary then impressions of novel ingroup and outgroup members did not differ. In reference to Rabbie and Horwitz's research, Tajfel et al. (1971) demurred that expecting purely random groupings to produce intergroup bias would be as nonsensical as expecting people to show biases based on \\\"sitting on the same and opposite benches in a compartment of a train\\\" (p. 152). Although Billig and Tajfel (1973) later reported that overt random assignment could lead to biases in resource allocations, the effect of this random grouping on intergroup bias was much weaker than was the case with their original contrived group distinctions. Much of the minimal group research that followed used paradigms that implied similarity of novel group members  or common fate implied by a competitive context  to create entitative groups. The qualifier minimal in the name of the paradigm reflects the fact that there is typically not a complete absence of differences between groups. Among researchers, the belief has always been that the category labels might differ but these differences do not carry meaning in the intergroup context beyond demarcating ingroup and outgroup. Tajfel (1970) set the stage for this long-held assumption by calling the category labels \\\"artificial and insignificant\\\" (p. 97) and \\\"flimsy and unimportant criteria\\\" (p. 101). This belief is so entrenched that many researchers do not even report whether they carefully counterbalanced the category labels or provide statistics showing that category labels do not matter. However, it is merely conjecture that the typical minimal group labels are stripped of their inferential qualities. Do people really see overestimators and underestimators as the same? What about Klee and Kandinsky fans? Is it possible that people read into the meaning of novel category labels and infer underlying attributes in ways that influence intergroup responses? The conventional wisdom is that labels do not matter, but if they do, then this unacknowledged truth has been hiding in plain sight since Tajfel's foundational research. Why Category Labels Might Matter In a minimal group situation, participants are confronted with categories that are plausible but novel to them. The novelty of the categories is a celebrated feature of the paradigm because it is thought to strip away the complexity that makes established group distinctions, such as race and gender, so difficult to study. For instance, when investigating the dynamics that contribute to race bias, it is often challenging to know whether effects are due to status or power differences between the groups , stereotypes circulating in the culture , personal antipathy , direct experience with the groups , own group preference , or other confounded variables. Conventional wisdom in social psychology is that minimal groups sidestep this problem because novel categories by virtue of their novelty signal whether a target shares one's group membership or not, but do not convey much else. However, just because this is what experimenters want and assume to be true does not mean that participants will allow it to be so. From the perspective of a participant in a minimal group situation, they are faced with a task (e.g., making judgments of faces, allocating resources, evaluating people), but the experimenter has made this task difficult by putting them in an explanatory vacuum. How should they go about solving the task at hand? With established groups such as race, perceivers are usually able to draw on their knowledge of stereotypes and life experiences with members of the groups to guide their judgments and behaviors. Without this concrete knowledge, it is assumed by researchers that participants will default to a heuristic that ingroups should be preferred. For example, social identity theory suggests that this occurs to maintain self-esteem. Evolutionary perspectives argue that ingroups should be preferred because ingroup members are the conspecifics who through phylogenetic history have provided coalitional support to ward off threats and other support that allowed individuals to thrive. These perspectives see the category label as a mechanism to signal who is an ingroup member and who is an outgroup member and once it has served this purpose it is discarded like a fuel tank that has launched a rocket into orbit. However, it is notable that in most cases the category label distinction (e.g., overestimator versus underestimator) is explicit and the ingroup versus outgroup distinction is implicit. This overt emphasis on the category labels gives participants several reasons to not conform to the experimenter's desires and instead try to make sense of minimal group labels to guide their task decisions. First, if one looks at the minimal group situation as a communicative context between the experimenter and perceiver, several of Grice's Maxims are applicable to understand the pragmatics of this situation. Grice's Maxim of Quality says that people tend to make statements that are truthful and supported by facts, so the participants should default to believing the minimal group distinctions that the experimenter asserts, even if they seem convoluted. Furthermore, Grice's Maxim of Relation suggests that people only share relevant information, so participants might assume that there must be a deeper meaning to the category labels, otherwise the experimenter would not take the time to study them. However, even if participants trust the claims about the category labels and assume that the distinctions must be important, how would participants go about inferring meaning from category labels that are designed to be meaning poor? Social perception research finds that when perceivers find themselves contemplating puzzling situations, such as when a target person is described as having conflicting trait attributes or a target person performs behaviors that are incongruent with an existing schema, perceivers engage in reasoning to make sense of what confuses them. This type of processing is consistent with a long tradition in psychology of characterizing people as meaning-makers, including Bruner's observation that people frequently go beyond the information given , work on epistemic motives suggesting that people have a need to understand the world around them , neuropsychology research on the tendency for people to confabulate to make sense of confusing circumstances , early social cognition work suggesting that people go about telling more than we can know , and socialcognitive models of transference that show that when novel targets superficially resemble a significant other, trait information about the significant other is applied to make sense of the novel person. In the puzzling explanatory vacuum that is the minimal group situation, the category labels might provide information to latch onto that can fill in inferential gaps. Although people might not have experiences with the category labels, they might come up with different associations that give them meaning. For instance, consider the popular overestimator\\/underestimator minimal group paradigm. The label overestimator is objectively defined as someone who assumes there is more of a quantity than is actually the case. Who is likely to be an overestimator? Maybe people who are optimistic or confident or people who are arrogant. Underestimators, on the other hand, might be more cautious and timid. These inferences can lead to assumptions about who is more dominant and who you should be more likely to trust to not take advantage of you. Maybe the halo of being optimistic or dominant leads people to value overestimators more than underestimators. Thus, perceivers can very quickly go beyond the information given. To be clear, we are not arguing that inferences about the meaning of category labels supplant intergroup bias, but it is possible that they have an unrecognized effect on responding. Some Category Labels Might Provide More to Latch Onto Than Others In the minimal group literature, all versions of the minimal group paradigm are typically viewed as different means to the same end. That is, they have their own unique ways of manipulating novel group memberships, but they are interchangeable and whether one version or another is used is often left to the preferences of the researchers. However, if you take the possibility seriously that perceivers might be motivated to read into category labels, then this raises the question of whether some minimal group operationalizations have more inductive potential than others. For instance, as explained above, it is rather clear to see how overestimators and underestimators might be viewed differently. However, what about people who prefer paintings by Klee versus Kandinsky? It is easy to see how based on associations between ethnicity and surname that Klee might be assumed to be Western European and Kandinsky might be assumed to be Eastern European. All the stereotypes associated with these groups could then become accessible. However, it does not logically follow that people who prefer one abstract artist or the other would share their preferred artist's ethnicity. Thus, unlike is the case with overestimators and underestimators, it is hard to overtly reason how people who prefer one abstract artist versus another differ from each other. For this reason, a close look at the overestimator\\/underestimator and Klee\\/Kandinsky paradigms illustrates how category labels across minimal group paradigms have different inductive potential. This is not different from the reality that some groups in the real world (even when these groups are otherwise novel) have names and other attributes that allow characteristics about group members to be inferred more easily than do names of other groups. Some Tasks Might Provide More Reasons to Latch Onto Category Labels Than Others Over the years, minimal group effects have been shown in a myriad of domains, ranging from resource allocations , to explicit attitudes and trait inferences , to memory for person information , to implicit attitudes , to face representation , and face perception. Across all of these domains, a rather consistent pattern of ingroup preference is observed. Yet, given the differing task demands that are necessary for these various types of processing, it might be the case that category labels have more of an influence on some of these processing modalities than others. Take for instance, a recent demonstration by Ratner, Dotsch, Wigboldus, van Knippenberg, and Amodio (2014) that people visualize minimal ingroup faces differently than minimal outgroup faces. The task demands of visualization are particularly onerous. It is difficult to visualize an abstract distinction, such as ingroup versus outgroup. The reason for this is that visualization is most vivid when concrete details are available, which at a minimum occurs at the basic level of categorization. This is where the category labels could become meaningful. As mentioned earlier, several successful instantiations of the minimal group paradigm use labels that allow deeper qualities of the people who fit into the group to be inferred, including the overestimator and underestimator distinction used by Ratner et al. (2014). Thus, instead of relying on an abstract distinction like ingroup and outgroup to infer qualities of a face, it might be easier for participants to imagine what an overestimator or underestimator should look like and this more concrete representation could then provide a basis from which ingroup bias is demonstrated. Compare this with the task demands of making resource allocation decisions--the main dependent variable in Tajfel et al.'s (1971) classic studies. Deciding whether to allocate resources to another individual does not require a detailed visualization of their face. Thus, if one's goal is to allocate resources without any information besides a novel category label, then it might be cognitively efficient to not worry about the meaning of the minimal group label and use the heuristic that ingroups should be favored to guide one's decisions. For these reasons, the extent to which category labels might have an effect in a minimal group situation could depend on the processing goals of the perceiver (e.g., visualizing faces vs. allocating resources). Overview of the Studies The current set of studies were designed to examine two primary objectives. The first was to determine if people imbue classic minimal group labels with any meaning whatsoever. The second was to examine whether the inductive potential of the labels and the processing goals of the perceiver are critical levers on whether category labels have an influence on responses in an intergroup context. To address this first aim we turned to reverse correlation image classification, which is a technique that has been used widely in social psychology to investigate how people represent social categories. As mentioned above, Ratner et al. (2014) investigated how minimal ingroup and outgroup faces are represented. They used reverse correlation to show that ingroup and outgroup faces are represented differently, but their analyses did not address whether category labels were also represented differently. Study 1a uses a preregistered, highly powered replication of Ratner et al.'s (2014) Study 1 to first establish that their intergroup bias findings were not simply false positives. It goes beyond Ratner et al. (2014), however, by also examining representational differences between overestimator and underestimator at the group-level. This latter analysis provides insight into whether the overestimator versus underestimator distinction was represented, which would be consistent with participants inferring meaning from the category labels. Study 1b uses representational similarity analysis  to examine whether the overestimator\\/underestimator distinction or the ingroup\\/outgroup distinction equally contribute to the face representations or if one distinction is weighted to a greater degree than the other. Study 1c uses a machine learning approach to examine the representational differences between overestimator and underestimator at the participant-level to provide convergent support for the results identified in Study 1a. Study 2 examines the generalizability of these phenomena by replicating the findings from Study 1 with the Klee versus Kandinsky minimal group paradigm, a version of the minimal group paradigm that uses labels that less clearly imply traits of the group members than does the overestimator versus underestimator paradigm. Given that the gold standard for consequential behavioral effects of minimal group paradigms is resource allocations, Studies 3 and 4 examine whether any representational effect of overestimator and underestimator and Klee and Kandinsky have meaningful behavioral implications on a resource allocation task. Overt trait ascriptions and evaluations were also measured. By using minimal group paradigms that vary to the extent that attributes can be logically inferred and also tasks that vary in how functional it might be to use category label information, these studies collectively test the assumption that Tajfel and many researchers who followed him made about minimal group labels--that they are flimsy and uninformative. Study 1 There were multiple goals of Study 1. First, we attempted to replicate the findings of Ratner et al. (2014) by demonstrating that people show ingroup positivity in face representations. Second, we tested for differences in people's face representations of overestimators and underestimators regardless of whether they are ingroup or outgroup. Third, after establishing that minimal group labels might have meaningful distinctions in face representations, we sought to understand how the ingroup\\/outgroup distinction and the overestimator\\/underestimator distinction differentially contribute to face representations. Lastly, we examined whether the representational differences between overestimator and underestimator exist at the participant level using a novel method of analyzing reverse correlation images with machine learning. To clearly demarcate between different approaches to analyzing our data, we break down Study 1 into three parts (a, b, and c). Study 1a Study 1a was conducted in two phases, in which we (a) created visual renderings of participants' mental representations of minimally defined groups, and (b) collected trait ratings of these images from a separate group of participants naive to the face generation stage. In Phase 1, participants were randomly assigned to minimal groups and then categorized faces as belonging to either of two minimal groups. We used the reverse correlation image classification technique to create visual representations of faces. The reverse correlation method examines response biases to different stimuli to infer patterns in the stimuli that may have caused the responses. These patterns then are visualized and provide an approximation of the mental representation upon which participants based their responses. In Phase 2, we assessed whether these visual renderings could reveal differences in face representations of different groups by asking an independent sample to rate classification images of different minimal group faces. Following Ratner et al. (2014), the faces were rated on 13 trait dimensions that Oosterhof and Todorov (2008) used to assess trait impressions of faces. This study was designed to determine whether people have different mental representations of faces of different minimal groups. On the one hand, overestimator and underestimator distinctions might not be represented by the perceiver--these minimal groups are designed to be arbitrary and novel to the participants and thus, there is no intended basis for a difference other than whether a target shared the same group membership (ingroup) or not (outgroup). On the other hand, people are motivated to make sense of the world around them  as evident from cases of confabulation in patients  and nonpatients , and social-cognitive transference. Thus, they could imbue novel category labels with meaning, infer different traits from them, and generate different face representations as a result. We predicted that people would show ingroup favoritism as indicated by more positive trait ratings of ingroup faces than outgroup faces. We remained unsure a priori about whether the minimal group labels would have an influence on face representations. Method Phase 1: Generating visual renderings of face representations. Participants. We recruited 362 University of California, Santa Barbara students (Mage 18.92, SD 1.61; 245 female, 109 male, and eight unidentified) to participate in a study about categorizing faces in exchange for course credit. We sought to maximize our power by (a) preregistering our sample size on the Open Science Framework (https:\\/\\/osf.io\\/s9243\\/?view_only 92afae84a38548e8a9412e8353f30905) and (b) more than doubling the sample size of a similar study that utilized the same procedure and methods (n 174 of Study 1 from Ratner et al., 2014). Our sample was obtained from the UCSB Psychological and Brain Sciences subject pool, which consisted of people from diverse backgrounds including but not limited to different genders, racial and ethnic backgrounds, religions, national origins, and political beliefs. The racial and ethnic breakdown of our sample was 110 White, 106 Latinx, 90 Asian, 22 multiracial, two Pacific Islander\\/ Hawaiian, 15 other, and eight unidentified. Up to six participants were run simultaneously. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. As with Ratner et al. (2014), participants were first told that they would perform several tasks on a computer. Next, the Numerical Estimation Style Test (NEST) version of the classic \\\"dot estimation\\\" procedure (Experiment 1 from Tajfel et al., 1971) was used to assign participants to novel, but believable, groups. Then participants completed a face categorization task optimized for a reverse correlation analysis. Numerical Estimation Style Test (NEST). In this task, we told our participants that people vary in numerical estimation style, which was defined as the tendency to overestimate or underestimate the number of objects they encounter. We also told the participants that approximately half the population are overestimators and half are underestimators, and that there is no relationship between numerical estimation style and any other cognitive tendencies or personality traits. We then told our participants that they would categorize photographs of students from a previous quarter whose numerical estimation style had been determined with a well-established task called the Numerical Estimation Style Test (NEST). We also told them that people can reliably detect numerical estimation style from faces and that the purpose of the current study was to test whether people can determine numerical estimation style when faces appear blurry. Next, participants completed the NEST themselves. In this task, they attempted to estimate the number of dots in 10 rapidly presented dot patterns, which each appeared for 3,000 ms. At the end of the test, the computer program provided predetermined feedback (counterbalanced across participants), indicating that each participant was either an overestimator or underestimator. We did not actually take participants' NEST responses into account; the NEST was used to provide a rationale for the group assignment. We used additional procedures to make the novel groups (i.e., overestimator and underestimator) as salient as possible in participants' minds throughout the remainder of the study. First, participants reported their numerical estimation style to the experimenter, providing a public commitment to their ingroup. The experimenter then wrote each participant's identification number and numerical estimation style on a sticky note and attached it to the bottom center of the computer monitor (in the participants' line of sight) to constantly remind them of their group membership during the face categorization task. Participants also typed their numerical estimation style into the computer, as another act of commitment to the ingroup. Face categorization. After the group assignment, participants completed a forced-choice face categorization task for 450 trials. On each trial, participants selected either an overestimator or underestimator face out of two adjacent grayscale face images. Half of the participants were asked on every trial to choose which of the two faces was an overestimator and the other half of the participants were asked on every trial to choose underestimator faces. If the targets shared the same numerical estimation style (i.e., overestimator or underestimator) with the participant, then the participant was selecting ingroup faces, whereas if the targets did not share the same numerical estimation style with the participant, then the participant was selecting outgroup faces. We used the grayscale neutral male average face of the Averaged Karolinska Directed Emotional Faces Database  as the base image to generate 450 pairs of face stimuli used in the face categorization task. Different noise patterns, which consisted of 4,092 superimposed truncated sinusoid patches, were added to the same base image, generating 450 different face pairs. A noise pattern was applied to the base image, and the inverse of that noise pattern was added to the base image, creating a pair of images. We presented inverse noise faces equally on the left and right sides of the screen in a random order. We used the same pairs of faces for all participants. Face representation data processing. Following the logic of reverse correlation analysis, we generated visual renderings of different groups by averaging noise patterns of selected faces. We argue that the reverse correlation analysis is suitable for capturing the difference between overestimator and underestimator face representations because if participants selected faces based solely on their group membership, overestimator and underestimator faces should look the same. If participants imbued meaning into the category labels, then systematic patterns would reveal the difference in mental representations of overestimator and underestimator faces. Thus, using the reverse correlation method allowed us to examine not only biases in favor of the ingroup, but also differences between overestimator and underestimator face representations. Participant-level classification images. The R package, rcicr , was used to conduct the reverse correlation analysis. We first averaged noise patterns of the chosen 450 faces from the face categorization task for each participant and superimposed the normalized average noise pattern back onto the original base image to create participant-level classification images. The images reflected participants' mental representations of what an overestimator or underestimator face should look like. A classification image was ingroup if the target's group membership was shared with that of the participant, whereas the image was outgroup if the target's group membership was different from that of the participant. Group-level classification images. After creating participantlevel classification images, we created eight group-level classification images. First, to test whether our findings replicate the ingroup positivity effect found in Study 1 of Ratner et al. (2014), we created ingroup (n 180) and outgroup (n 182) classification images by averaging the appropriate noise patterns from the participant-level. That is, we averaged noise patterns of participant-level classification images of ingroup faces and superimposed the normalized average noise pattern back onto the base image to create the group-level classification image for the ingroup face. We did the same for the outgroup face (see Figure 1). Second, to examine the difference in trait impressions elicited by the category labels, we also created overestimator (n 181) and underestimator (n 181) classification images by following the same procedure for the ingroup and outgroup group-level classification images (see Figure 2). Finally, we examined the interaction between group membership and the category labels by creating four classification images by crossing the two variables: ingroup-overestimator (n 91), ingroup-underestimator (n 89), outgroup-overestimator (n 90), and outgroup-underestimator (n 92). All four classification images can be seen in Figure 3. Phase 2: Assessing impressions of face representations. In Phase 2, we objectively assessed the differences in these face representations, specifically in how they elicited different trait impressions. To do this, we had independent samples of participants who were not aware of the face categorization stage from Phase 1 rate the eight group-level classification images from Phase 1. To assess relative differences between ingroup and outgroup (Group), overestimator and underestimator (NEST), and Group NEST images, we obtained ratings from three different samples of participants. That is, participants only rated ingroup and outgroup images, overestimator and underestimator images, or Group NEST images. Participants. We recruited a total of 301 participants (Mage 35.98, SD 11.44; 145 female, 156 male) through the TurkPrime website (www.turkprime.com) to complete an online survey administered through Qualtrics (www.qualtrics.com). Ninety-nine participants rated ingroup and outgroup classification images, 102 participants rated overestimator and underestimator classification images, and 100 participants rated ingroup-overestimator, outgroupoverestimator, ingroup-underestimator, and outgroup-underestimator classification images. We recruited a comparable number of Mechan ical Turk (MTurk) raters as Ratner et al. (2014). The racial and ethnic breakdown of our sample of raters was 226 White, 28 Asian, 22 Black, and 11 multiracial participants. The samples in this portion of the study were collected from MTurk, which are comparable with typical undergraduate student samples, if not more diverse. Participants were expected to complete the study in 10 min. All participants did not know about the face categorization stage of the study (i.e., Phase 1). They were compensated with $1 for their participation. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. Participants rated the classification images on 13 trait dimensions (i.e., To what extent is this face. . . trustworthy, attractive, dominant, caring, sociable, confident, emotionally stable, responsible, intelligent, aggressive, mean, weird, and unhappy?;). Each face was presented by itself in a random order. Ratings were made on scales from 1 (not at all) to 7 (extremely). The order of each trait presentation was also random. Results For each sample of raters, we conducted a repeated-measures multivariate analysis of variance (rMANOVA) followed by a univariate analysis of variance for each trait. We show the results below separated by sample. Group membership (Group). A rMANOVA comparing the trait ratings of ingroup and outgroup classification images was significant, Pillai's Trace.85, F 36.26, df (13, 86), p.001, indicating some difference in trait ratings between ingroup and outgroup classification images. The univariate F tests showed that all trait ratings of ingroup and outgroup images were significantly different from each other at the.001 significance level. The means, F values, p values, and effect sizes for each comparison are presented in Table 1. The ingroup face was rated significantly more trustworthy, attractive, caring, emotionally stable, responsible, intelligent, and sociable; the outgroup face was rated significantly more dominant, aggressive, mean, weird, and unhappy. Numerical estimation style (NEST). A rMANOVA comparing the trait ratings of overestimator and underestimator classification images was significant, Pillai's Trace.68, F 14.57, df (13, 89), p.001, indicating some difference in trait ratings between overestimator and underestimator classification images. The univariate F tests showed that the majority of trait ratings of overestimator and underestimator images were significantly different from each other at the.001 significance level. The means, F values, p values, and effect sizes for each comparison are presented in Table 2. The overestimator face was rated significantly more dominant, confident, emotionally stable, aggressive, mean, and sociable; the underestimator face was rated significantly more trustworthy, caring, and unhappy. Attractive, responsible, intelligent, and weird ratings were not significantly different between overestimator and underestimator images. Group NEST. We used rMANOVA to test the effects of Group, NEST, and the interaction between the two on trait ratings. Significant multivariate effects were found for all variables: Group (Pillai's Trace.58, F 30.57, df (13, 285), p.001), NEST (Pillai's Trace.48, F 20.49, df (13, 285), p.001), and Group NEST (Pillai's Trace.10, F 2.53, df (13, 285), p.003). Similar to the Group results reported earlier, ingroup faces were rated more trustworthy, attractive, caring, confident, emotionally stable, responsible, intelligent, and sociable, whereas outgroup faces were rated more dominant, aggressive, mean, weird, and unhappy for both overestimators and underestimators. Interaction effects were found for some traits including attractive, caring, emotionally stable, aggressive, mean, unhappy, and sociable. The univariate F test results including the means, standard deviations, F values, p values, and effect sizes (comparing ingroup and outgroup within overestimator and underestimator) for each trait are presented in Table 3. Discussion In Study 1a, we investigated the face representations of minimally defined groups. First, our results replicated the findings of Ratner et al. (2014): Ingroup faces elicited overall more positive trait impressions compared with outgroup faces. The current study was preregistered (https:\\/\\/osf.io\\/s9243\\/?view_only92afae84a 48e8a9412e8353f30905) and highly powered--twice the sample size of the original study by Ratner et al. (2014), providing strong evidence that their demonstration of ingroup positivity in face representations is a replicable effect. More interestingly, however, we also found that participants generated different face representations of overestimators and underestimators. The overestimator and underestimator faces differed on various traits dimensions that do not necessarily signal favoritism toward one group over the other. Most notably, the underestimator face image was rated as both more trustworthy and more unhappy than the overestimator face image. This finding is contrary to the general assumption in the literature that the differences between minimal group labels are arbitrary. Instead, findings from the current study showed that people might utilize novel category labels when visualizing faces of ingroup and outgroup members, and infer different traits from those labels. We also found several interaction effects for the Group NEST trait rating data indicating that the magnitude of differences between ingroup and outgroup faces were different for overestimator and underestimator (i.e., larger ingroup positivity for underestimator than overestimator for many traits), providing additional evidence that face representations of overestimator and underestimator are different and can influence intergroup bias. For instance, overestimators were generally rated as more attractive, emotionally stable, sociable, aggressive, and mean than underestimators, and perhaps this constrained variability on these trait dimensions for overestimators, which resulted in stronger ingroup and outgroup differences on these variables for underestimators. Interestingly, for the caring dimension, there was no NEST main effect, but there was an interaction effect indicating that the ingroup versus outgroup caring effect was larger for underestimators. Additionally, underestimators were generally rated as more unhappy than overestimators, but the Group difference was still larger for underestimator on this variable. Together, there seems to be evidence on multiple trait dimensions that the degree to which ingroup and outgroup differences emerge is influenced by the meaning derived from the overestimator versus underestimator distinction. Study 1b In Study 1b, we used multiple regression representational similarity analysis (RSA;) to more directly examine how much participants were weighting the Group (ingroup or outgroup) versus NEST category labels (overestimator or underestimator) when representing faces of different groups. Specifically, this technique allowed us to explore relationships between trait ratings of Group NEST group-level classification images (i.e., ingroupoverestimator, outgroup-overestimator, ingroup-underestimator, and outgroup-underestimator) and the linear combinations of trait ratings of Group and NEST group-level classification images from Study 1a. Our theoretical premise was that participants completed the face categorization task from Study 1a with two pieces of information: (a) the category label (overestimator or underestima tor) of the targets (NEST); and (b) whether the targets shared their group membership or not (ingroup or outgroup--Group). The RSA technique uses similarity matrices to examine the relationship between different representational spaces (e.g., the relationship between how ingroup faces are generally rated and how overestimator faces are generally rated). Each cell in each similarity matrix is a pairwise similarity (e.g., correlation) between two traits (e.g., trustworthy and attractive). Quantitatively, if participants had only those two pieces of information (Group and NEST) at hand during the face categorization task and indeed used them, trait representational space of Group NEST images should reflect linear combinations of trait representations of Group images and those of NEST images. Thus, by using multiple regression RSA, we attempted to tease apart unique contributions of category labels (NEST) and whether the target shared the same group membership with the participant or not (Group) in how people chose faces who belonged to one of four Group NEST groups (ingroupoverestimator, outgroup-overestimator, ingroup-underestimator, and outgroup-underestimator) during the face categorization task. Method Participants. The data from the same 301 participants recruited in Phase 2 of Study 1a were reanalyzed here. As stated in Phase 2 of Study 1a, 99 participants rated ingroup and outgroup classification images, and 102 participants rated overestimator and underestimator classification images. Additionally, 100 participants rated ingroup-overestimator, outgroup-overestimator, ingroupunderestimator, and outgroup-underestimator classification images. See the Participants section of Phase 2 of Study 1a for a more detailed description. Procedure. To quantitatively examine contributions of Group and NEST in the face categorization task, we first computed pairwise correlations of trait rating data from Study 1a (e.g., the correlation between trustworthy and attractive ratings), generating a correlation matrix for each group-level classification image. We then vectorized unique pairwise correlation matrices (i.e., excluding duplicate correlation coefficients). Finally, we used multiple regression RSA to predict correlation vectors of Group NEST trait rating data with linear combinations of correlation vectors of appropriate Group and NEST trait rating data. For example, we predicted trait representations (i.e., a vector of unique pairwise correlation coefficients) of the ingroup-overestimator group-level classification image using the linear combination of trait representations of the ingroup group-level classification image and the overestimator group-level classification image (see Figure 4). By using multiple regression RSA, we tested unique contributions of group membership (Group) and minimal group labels (NEST) to Group NEST images while controlling for each other. If participants used one type of information more than the other, it should yield a higher slope value. For example, if participants used the ingroup\\/outgroup distinction more than the underestimator label when choosing ingroup underestimator faces during the face categorization task, the trait representation of ingroup should have a higher beta value than the trait representation of underestimator. Results Ingroup overestimator. We used ordinary least squares multiple regression to predict the pairwise correlation vector of the trait rating data of the ingroup overestimator face image with the linear combination of the correlation vectors of the ingroup face trait rating data and the overestimator face trait rating data. We found that both the ingroup ratings ( .206, SE.118, t(77) 2.279, p.026) and overestimator ratings ( .752, SE.099, t(77) 8.303, p.001) were significant predictors of the ingroup overestimator ratings. We also conducted linear hypothesis testing to test whether ingroup ratings and overestimator ratings were significantly different from each other and found that overestimator ratings predicted ingroup overestimator ratings significantly better than ingroup ratings, F(1, 75) 6.811, p.011. Outgroup overestimator. We followed the same procedures described above for the ingroup overestimator to predict the outgroup overestimator trait rating data with the linear combination of the correlation vectors of outgroup face trait rating data and overestimator face trait rating data. We found that both the outgroup ratings ( .172, SE.081, t(77) 3.475, p.001) and overestimator ratings ( .824, SE.055, t(77) 16.632, p.001) were significant predictors of outgroup overestimator ratings. Linear hypothesis testing showed that overestimator ratings predicted outgroup overestimator ratings significantly better than outgroup ratings, F(1, 75) 23.696, p.001. Ingroup underestimator. We used multiple regression to predict ingroup underestimator trait rating data with the linear combination of ingroup face trait rating data and underestimator face trait rating data. We found that both the ingroup ratings ( .534, SE.058, t(77) 10.131, p.001) and underestimator ratings ( .497, SE.054, t(77) 9.418, p.001) were significant predictors of ingroup underestimator ratings. The linear hypothesis testing showed that ingroup ratings and underestimator ratings did not significantly differ in predicting ingroup underestimator ratings, F(1, 75) .574, p.451. Outgroup underestimator. We used multiple regression to predict outgroup underestimator trait rating data with the linear combination of outgroup face trait rating data and underestimator face trait rating data. We found that both the outgroup ratings ( .496, SE.078, t(77) 7.084, p.001) and underestimator ratings ( .482, SE.073, t(77) 6.887, p.001) were significant predictors of outgroup underestimator ratings. Linear hypothesis testing showed that outgroup ratings and underestimator ratings did not significantly differ in predicting outgroup underestimator ratings, F(1, 75) .137, p.712. Discussion Study 1b examined how people generated face representations of different minimal groups using multiple regression RSA on trait rating data of group-level classification images. We found that for both ingroup overestimator and outgroup overestimator face representations, participants seemed to have used the overestimator label more than the ingroup\\/outgroup distinction, as indicated by larger trait representational similarities between Group NEST face images (i.e., ingroup-overestimator and outgroup-overestimator) and the overestimator face image than the ingroup or outgroup face image (i.e., larger values for overestimator trait representations than ingroup or outgroup trait representations). On the other hand, for ingroup underestimator and outgroup underestimator face representations, participants seemed to have used group membership (ingroup or outgroup) and the underestimator label equally, as indicated by equally similar trait representations between Group NEST face images (i.e., ingroup-underestimator and outgroup-underestimator) and the underestimator face image and ingroup or outgroup face image. The larger role that the overestimator label played during the face categorization task can also be interpreted, both conceptually and mathematically, as indicating that trait representations of ingroup overestimator and outgroup overestimator faces were similar. In other words, ingroup overestimator and outgroup overestimator face representations elicited overall similar trait impressions from an independent sample of participants. In contrast, ingroup and outgroup underestimator face representations did not show as much correspondence in their trait representations with each other. This may suggest that in the aggregate, people have more consistent representations of overestimator faces (i.e., consensus across participants) compared with underestimator faces. Together these findings showed that minimal group labels may indeed be meaningful when visualizing faces, but different labels may have different levels of influence. It is important to note that our interpretations of the multiple regression RSA results are drawn from trait ratings of group-level classification images. Although past research suggests that trait impressions and behaviors elicited by group-level classification images resemble those elicited by participant-level classification images , we examined whether the difference we found in mental representations of overestimator and underestimator faces holds at the participant level in Study 1c. Study 1c Study 1a and 1b provided evidence that face representations of minimally defined groups can vary and lead to trait impressions that differ on various dimensions of social perception and that different minimal group labels have different degrees of influence on people's mental representation of ingroup and outgroup faces. One potential limitation to these findings is that we assessed trait impressions of group-level classification images, which are the summary representation (i.e., average) of many participant-level classification images. Although this summary representation of what the face of a given group member (e.g., overestimator) might very well represent most of the cases that make up the average, using summary representations does not necessarily indicate that the individual participants were actually visualizing ingroup and outgroup members differently as a function of the specific group labels. In Study 1c we tested whether representational differences found in trait impressions of different minimal groups in Study 1a also exist at the participant level by examining the representational differences in participant-level classification images of ingroup and outgroup as well as overestimator and underestimator faces. To do so, we used a machine learning analytic approach that examines the relationship between pixel intensity data of each image and its category labels, thus circumventing biases that might arise from subjective trait ratings. This approach has not been used previously to examine biases in reverse correlation classification images and is vastly different from the trait impression analytic approach used in Study 1a and 1b. Finding similar representational differences between different categories using this approach would therefore provide strong convergent evidence that the previous Study 1 effects we report are robust. Method Stimuli. In Study 1c, we used 362 participant-level classification images from Phase 1 of Study 1a. Each image had three dimensions: (a) Group (ingroup or outgroup); (b) NEST (overestimator or underestimator); and (c) Group NEST (ingroupoverestimator, outgroup-overestimator, ingroup-underestimator, or outgroup-underestimator). Procedure. We used the R package e  to conduct the machine learning analyses following three steps: (a) vectorizing and down sampling pixel intensity data of each image (see Figure 5); (b) standard scaling (i.e., standardization); and (c) classification using support vector machines (SVM) with a radial basis function (RBF) kernel. We performed cross-validation for each analysis to ensure that every image was in (not at the same time) both training and testing data sets. We down sampled participant-level classification images by simply resizing them from 512 512 pixels to 64 64 pixels. We conducted the same true label analysis using 512 512, 256 256, 128 128, and 64 64 image sizes, and found no detrimental effect of down sampling on the classification accuracies. Thus, we downsampled the images due to the computationally intensive nature of machine learning analyses needed for the 1,000 permutation tests. Standard scaling was done by mean-centering pixel intensity data and dividing the values by their standard deviation. Finally, we used the SVM to classify each image to the appropriate category, using a radial basis function with default cost and gamma hyperparameters (cost 1 and 1\\/n features 64 64). We used this same procedure to classify between ingroup and outgroup faces, overestimator and underestimator faces, and Group NEST faces. To minimize overfitting and maximize our chance of detecting real differences in classification images, we used 10-fold crossvalidation with our SVM model. Each fold yielded a training set (90% of data 325 to 326 cases) and a testing set (10% of data 36 to 37 cases), both of which were evenly divided between classes (e.g., approximately equal numbers of ingroup and outgroup images). The SVM algorithm then learned the relationships between features (64 64 vectorized pixel intensities of each image) and class labels (e.g., ingroup and outgroup) from the training set, and classified images from the testing set consisting of images that were not part of the training set for a given fold. We repeated this step 10 times until every instance of data was in both training and testing sets at some point. We then computed accuracy scores by averaging accuracies from these 10 folds. By using this method of cross-validation, we ensured that class labels were balanced for both training and testing sets, and no image was included in both training and testing sets at the same time for any given fold. Next, we used permutation tests to determine whether accuracies of our SVM classifications differed significantly from chance. For each permutation, class labels (e.g., ingroup or outgroup) were randomly permuted for every image, followed by the classification steps described above. We repeated the same procedure 1,000 times, creating our own null distribution against which we could compare the accuracies of our classification results with true labels. We then estimated the p value from the proportion of permutation accuracies that exceeded the accuracy with true labels (i.e., percentage of permutation tests that had higher accuracy than the accuracy with true labels). We also compared the accuracies of our model's classifications of Group and NEST labels using the 5 2-fold cross-validation paired samples t test. That is, we performed five replications of twofold cross-validation (splitting data into equal number of training and testing data), resulting in 10 accuracy scores for each classification. We then used a simple paired samples t test on those accuracy scores to test whether the model performed significantly better classifying group or NEST labels. We did not use the paired samples t test on accuracy scores from the 10-fold cross-validation because it violates a key assumption of the t test. Specifically, for 10-fold cross-validation, an instance of data is used in the training set nine times, and therefore accuracy scores are not independent from each other. This in turn leads to inflation of Type I error. With the 5 2-fold cross-validation, each instance of data appears only in the training or testing set for any given fold, ensuring independence between accuracy scores, thus reducing the likelihood of Type I error. Results We were able to classify between ingroup and outgroup images from pixel intensity data significantly better than chance (accuracy 59.20%, p.001). The same was true for overestimator and underestimator images (accuracy 66.01%, p.001). For both classifications, all permutation tests yielded lower accuracy scores than the accuracy scores with true labels (see Figure 6). Next, we compared classification accuracies for Group and NEST using the 5 2-fold cross-validation paired samples t test. This resulted in slightly different accuracy scores for each classification from 10-fold cross-validation accuracy scores (Group 55.75% and NEST 62.88%). The t test result showed that our model performed significantly better classifying NEST labels than Group labels, t(9) 4.65, p.001, two-tailed, Cohen's d 1.47, 95% CI [3.66, 10.60]. Finally, multiclass SVM results showed that our model performed significantly better than chance (accuracy 37.83%, p.001). Unlike the previous two cases, the chance accuracy for the current classification was 25% (one out of four). Upon examining the confusion matrix of results using the true labels, we found that our model misclassified within NEST labels (99\\/225) more than within Group labels (72\\/225), such as classifying ingroup overestimator face images as outgroup overestimator rather than ingroup underestimator. Discussion In Study 1c, we investigated whether the difference between face representations of overestimators and underestimators exists not just in the aggregate trait-rating data but also at an individual level in the pixel intensity data. We examined the differences between Group (ingroup and outgroup), NEST (overestimator and underestimator), and Group NEST participant-level classification images using a novel approach for analyzing reverse correlation images, specifically a machine learning algorithm called sup port vector machine. We found that using this method we could classify Group, NEST, and Group NEST participant-level classification images better than chance, suggesting that the differences between face representations of all category types exist at the participant level. We also found that the SVM classified between overestimator and underestimator face images significantly better than ingroup and outgroup face images, providing a piece of evidence that NEST labels were used more than the ingroup\\/outgroup distinction during the face categorization task, resulting in more consistent face representations of overestimator and underestimator than those of ingroup and outgroup across different participants. One explanation of this effect is that our face categorization task created an explicit task goal of choosing overestimator or underestimator faces, whereas group membership was implicit-whether the participant shares the same group membership with the targets or not. Thus, this might have contributed to more consistent face representations of overestimator and underestimator than ingroup and outgroup. However, the results of Study 1b may partly address this possibility. Specifically, the more \\\"consistent\\\" face representation of overestimator and underestimator versus ingroup and outgroup found in Study 1c was true mostly for overestimator faces but not necessarily for underestimator faces in Study 1b. Thus, we argue that minimal group labels can be meaningful, albeit to different extents for different labels. In short, we showed that the representational differences between ingroup and outgroup as well as overestimator and underestimator exist not only in the summary representations (i.e., average of many participant-level classification images), but also in individuals' face representations of different groups. We also did not use subjective trait ratings to arrive at this conclusion, thus providing stronger evidence that ingroup and outgroup faces as well as overestimator and underestimator faces are objectively different from each other. We were also able to show the same findings as Study 1a despite the fact that we used very different methods (i.e., trait ratings vs. image classification using pixel intensity data), suggesting that representational biases that arise with this minimal group paradigm are robust. The finding of more misclassifications within NEST labels than within Group labels of Group NEST participant-level classification images provided another piece of evidence that people might have used NEST labels more than group membership when visualizing faces during the face categorization task. Although these findings are descriptive, the minimal group labels seemed to have played a greater role in the face categorization task than whether the targets shared the same group membership with the participant or not. Study 2 So far, we showed that people have different mental representations of different minimal groups, and that this difference may be driven more by people's mental representations of what an overestimator should look like rather than what an underestimator should look like. Thus, people seem to imbue meaning to minimal group labels but to different extents for different minimal groups. One critical limitation is that we used only one version of the minimal group paradigm (i.e., the overestimator vs. underestimator distinction), therefore we have not shown whether people imbue meaning to other minimal group labels (e.g., Klee vs. Kandinsky). Additionally, although we showed that one type of minimal group paradigm can be meaningful to some people, it is still unclear what the implications of that are for research using minimal group paradigms to investigate other forms of intergroup bias. The study to follow investigated the generalizability of our findings from Study 1 with a different type of minimal group paradigm, the Klee versus Kandinsky distinction (Experiment 2 from Tajfel et al., 1971). As discussed previously, the Klee and Kandinsky paradigm differs from the overestimator and underestimator paradigm in that the former on its face seems to have labels with less inductive potential than does the latter. Study 2 used the same set of methods from Study 1 to empirically examine whether people represent faces of people who like Klee paintings differently from faces of those who like Kandinsky paintings. Study 2a Following the procedure of Study 1a, Study 2a was also conducted in two phases. In Phase 1, participants were randomly assigned to minimal groups (Klee vs. Kandinsky groups) and then categorized faces as belonging to either of these two minimal groups. We used the reverse correlation image classification technique to create visual representations of Klee and Kandinsky fans as well as ingroup and outgroup faces. In Phase 2, we assessed whether images of these different minimal group faces would be rated differently by independent samples of participants on the 13 trait dimensions used in Study 1. Although we found some differences in trait impressions between different minimal groups (overestimator vs. underestimator) from Study 1, we chose to remain agnostic about whether the Klee and Kandinsky group labels would result in different face representations because it is possible that these labels have less inductive potential. However, given that this version of the minimal group paradigm has revealed ingroup favoritism in past research , we still predicted that people would show ingroup positivity as indicated by more positive trait ratings of ingroup faces than outgroup faces. Method Phase 1: Generating visual renderings of face representations. Participants. We recruited 200 University of California, Santa Barbara students (Mage 18.82, SD 1.07; 149 female, 47 male, and four unidentified) to participate in a study about categorizing faces in exchange for course credit. We preregistered our sample size on the Open Science Framework (https:\\/\\/osf.io\\/s9243\\/ ?view_only92afae84a38548e8a9412e8353f30905). Our sample was from the UCSB Psychological and Brain Sciences subject pool, which consisted of people from diverse backgrounds, including but not limited to different genders, racial and ethnic backgrounds, religions, national origins, and political beliefs. The racial and ethnic breakdown of our sample was 65 Asian, 65 White, 35 Latinx, 24 multiracial, five other, and six unidentified participants. Up to four participants were run simultaneously. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. The current study followed the same procedure as Study 1a except for the version of the minimal group paradigm used to assign participants to different groups. As with Study 1a, participants were first told that they would perform several tasks on a computer. Next, we used a classic aesthetic preference procedure (Experiment 2 from Tajfel et al., 1971) to assign participants to novel, but believable, groups. Then they conducted a face categorization task optimized for a reverse correlation analysis. Artistic Preference Test (ART). In this task, we told our participants that people can reliably figure out another person's artistic preference simply by looking at their face. We then told our participants that they would categorize photographs of students from a previous quarter whose artistic preference had been determined. We also told them that the purpose of the current study was to test whether people can determine artistic preference when faces appear blurry. Next, participants completed the artistic preference test themselves. In this task, they viewed 12 pairs of paintings (a pair per trial) by modern European artists, Paul Klee and Wassily Kandinsky, and chose whichever painting they liked better on a given trial. On each trial, one of the paintings was by Kandinsky and the other one was by Klee. The location of each painting (whether on the left or right of the screen) did not correspond to the painter, and the signature of the painter was hidden from each painting to prevent participants from choosing on the basis of the painter's name. At the end of the test, the computer program provided predetermined feedback (counterbalanced across participants), indicating that each participant had a preference for paintings by either Kandinsky or Klee. We did not actually take participants responses into account; the ART was used to provide a rationale for the group assignment. We used additional procedures to make the novel groups (i.e., Klee and Kandinsky) as salient as possible in participants' minds throughout the remainder of the study. First, participants reported their artistic preference to the experimenter, and the experimenter then wrote each participant's identification number and artistic preference on a sticky note and attached it to the bottom center of the computer monitor (in the participants' line of sight) to constantly remind them of their group membership during the face categorization task. Participants also typed their artistic preference into the computer. Face categorization. After the group assignment, participants completed a forced-choice face categorization task for 450 trials. On each trial, participants selected a face of someone who prefers paintings by either Kandinsky or Klee out of two adjacent gray scale face images. Half of the participants were asked on every trial to choose which of the two faces belonged to a person who preferred Kandinsky and the other half of the participants were asked on every trial to choose the person who preferred Klee. If the targets shared the same artistic preference as the participant, then the participant was selecting ingroup faces, whereas if the targets did not share the artistic preference with the participant, then the participant was selecting outgroup faces. We used the same set of face stimuli from Study 1a--450 pairs of face images generated from the grayscale neutral male average face of the Averaged Karolinska Directed Emotional Faces Database. We presented inverse noise faces equally on the left and right sides of the screen in a random order. We used the same pairs of faces for all participants. Face representation data processing. We used the same reverse correlation analysis from Study 1a to generate visual renderings of different groups by averaging noise patterns of selected faces--Klee and Kandinsky group faces, ingroup and outgroup faces, and Klee-ingroup, Klee-outgroup, Kandinsky-ingroup, and Kandinsky-outgroup faces. We generated both participant-level classification images and group-level classification images (refer back to the Method section of Study 1a for a more detailed description of this procedure). To test whether the Klee and Kandinsky version showed the ingroup positivity effect found in Study 1a, we created ingroup (n 100) and outgroup (n 100) classification images (see Figure 7). Second, to examine the differences between Klee and Kandinsky groups, we created Klee (n 100) and Kandinsky (n 100) classification images collapsed across ingroup and outgroup (see Figure 8). Finally, we examined the interaction between ingroup\\/outgroup and Klee\\/Kandinsky distinctions by creating four classification images by crossing the two dimensions: ingroup-Klee (n 50), ingroup-Kandinsky (n 50), outgroup-Klee (n 50), and outgroup-Kandinsky (n 50). All four classification images can be seen in Figure 9. Phase 2: Assessing impressions of face representations. In Phase 2, we assessed how different face images elicited different trait impressions. Independent samples of participants who were not aware of the face generation phase from Phase 1 rated the eight group-level classification images. To assess relative differences between ingroup and outgroup (Group), Klee and Kandinsky (ART), and Group ART images, we obtained ratings from three different samples of participants. That is, participants only rated ingroup and outgroup images, Klee and Kandinsky images, or Group ART images. Participants. We recruited a total of 150 participants (Mage 35.08, SD 11.15; 96 female, 54 male) through the TurkPrime website (www.turkprime.com) to complete an online survey administered through Qualtrics (www.qualtrics.com). Fifty participants rated ingroup and outgroup classification images, 50 participants rated Klee and Kandinsky classification images, and 50 participants rated ingroup-Klee, outgroup-Klee, ingroup-Kandinsky, and outgroup-Kandinsky classification images. The racial and ethnic breakdown of our sample of raters was 103 White, 26 Black, six Latinx, five Asian, one Native American, one Pacific Islander\\/Hawaiian, and eight multiracial participants. Participants were expected to complete the study in 10 min. All participants did not know about the face categorization stage of the study. They were compensated with $1 for their participation. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. After providing informed consent, participants rated the classification images on 13 trait dimensions (i.e., To what extent is this face. . . trustworthy, attractive, dominant, caring, sociable, confident, emotionally stable, responsible, intelligent, aggressive, mean, weird, and unhappy?;). Each face was presented by itself in a random order. Ratings were made on scales from 1 (not at all) to 7 (extremely). The order of each trait presentation was also random. Results For each sample of raters, we conducted a repeated-measures multivariate analysis of variance (rMANOVA) followed by a univariate analysis of variance for each trait. We show the results below separated by sample. Group membership (Group). A rMANOVA comparing the trait ratings of ingroup and outgroup classification images was significant, Pillai's Trace.83, F 14.05, df (13, 37), p.001, indicating some difference in trait ratings between ingroup and outgroup classification images. The univariate F tests showed that all trait ratings of ingroup and outgroup images were significantly different from each other at the.05 significance level. The means, F values, p values, and effect sizes for each comparison are presented in Table 4. The ingroup face was rated significantly more trustworthy, attractive, caring, emotionally stable, responsible, intelligent, and sociable; the outgroup face was rated significantly more dominant, aggressive, mean, weird, and unhappy. Artistic preference (ART). A rMANOVA comparing the trait ratings of Klee and Kandinsky classification images was significant, Pillai's Trace.60, F 4.32, df (13, 37), p.001, indicating some difference in trait ratings between Klee and Kandinsky classification images. The univariate F tests showed that the majority of trait ratings of Klee and Kandinsky images were significantly different from each other at the.05 significance level. The means, F values, p values, and effect sizes for each comparison are presented in Table 5. The Klee group face was rated significantly more caring, confident, emotionally stable, and sociable; the Kandinsky group face was rated significantly more aggressive, mean, and unhappy. Trustworthy, attractive, dominant, responsible, intelligent, and weird were not significantly different between Klee and Kandinsky face images at the.05 significance level. Group ART. We used rMANOVA to test the effects of group, ART, and the interaction between the two on trait ratings. A significant multivariate effect was found only for Group (Pillai's Trace.66, F 20.57, df (13, 135), p.001). The effects of ART (Pillai's Trace.09, F 1.04, df (13, 135), p.41) and Group ART (Pillai's Trace.07, F.75, df (13, 135), p.71) were not significant. Similar to the Group results reported earlier, ingroup faces were rated more trustworthy, attractive, caring, confident, emotionally stable, responsible, intelligent, and sociable, whereas outgroup faces were rated more dominant, aggressive, mean, weird, and unhappy for both Klee and Kandinsky face images. The differences found between the Klee and Kandinsky groups dissipated when the category labels were crossed with group membership. The univariate F test results including the means, standard deviations, F values, p values, and effect sizes (comparing ingroup and outgroup within Klee and Kandinsky) for each trait are presented in Table 6. Discussion In Study 2a, we investigated the generalizability of our findings from Study 1a to a different minimal group paradigm. First, we replicated the ingroup positivity effect in face representations: Ingroup faces elicited overall more positive trait impressions compared with outgroup faces. It is also notable that the magnitude of ingroup positivity was greater for a majority of the traits in the Klee and Kandinsky version compared to the overestimator and underestimator version. That is, the effect sizes were greater for trustworthy, attractive, dominant, caring, emotionally stable, aggressive, mean, and sociable, indicating that in the Klee and Kandinsky version compared with the overestimator and underestimator version, the ingroup face elicited even more positive trait impressions than the outgroup face. This is despite the fact that our sample sizes were smaller in this study compared with Study 1a. We also found some support for the generalizability of the Study 1a category label findings: The Klee group face was rated more caring, confident, emotionally stable, and sociable, whereas the Kandinsky group face was rated more mean and unhappy. We did not expect to find more favorable trait impressions for the Klee group face compared to the Kandinsky group face, so we are hesitant to interpret why this pattern emerged. Nevertheless, our findings still demonstrate that different minimal groups are represented differently regardless of whether they are ingroup or outgroup, supporting the idea that people may imbue meaning to minimal groups when they are visually representing faces of ingroup and outgroup members. Interestingly, when we crossed category labels with group membership, the differences between the Klee group and the Kandinsky group were no longer statistically significant. This may be in part due to strong group effects overshadowing the effects of minimal group labels, but also because people might have focused more on whether the target shared the same group membership with them or not, rather than reading into category labels, which was the case for participants in Study 1a. Regardless, our findings show that people may infer different traits from category labels in different types of minimal group paradigms, but that the inductive potential afforded by the category labels may moderate the extent to which ingroup positivity biases are expressed (i.e., more inductive potential leads to less ingroup bias). Study 2b In Study 2b, we used multiple regression RSA to examine unique contributions of minimal group labels (ART) and whether the target shared the same group membership with the participant or not (Group) in how participants chose faces that belonged to one of four Group ART groups (i.e., ingroup-Klee, outgroup-Klee, ingroup-Kandinsky, and outgroup-Kandinsky) during the face categorization task in Study 2a. Participants had only two pieces of information to complete the task: (a) the minimal group label of the targets (Klee fan or Kandinsky fan); and (b) whether the targets shared the same group membership with them or not (ingroup or outgroup). Because we did not find any effects of minimal group labels on Group ART face images, we expected to find greater contributions of Group than ART in trait representations of Group ART images. Method Participants. The data from the same 150 participants recruited in Phase 2 of Study 2a were reanalyzed here. Fifty participants rated ingroup and outgroup classification images, 50 participants rated Klee and Kandinsky classification images, and 50 participants rated ingroup-Klee, outgroup-Klee, ingroupKandinsky, and outgroup-Kandinsky classification images. See the Participants section of Phase 2 of Study 2a for a more detailed description. Procedure. We followed the same steps of multiple regression RSA outlined in Study 1b to examine contributions of Group and ART in the face categorization task while controlling for each other: (a) we computed pairwise correlations of trait rating data for each group-level classification image; (b) vectorized unique pairwise correlation matrices (i.e., excluding duplicate correlation coefficients); and (c) predicted vectors of Group ART trait rating data with linear combinations of vectors of corresponding Group and ART trait rating data. Results Ingroup Klee. We used ordinary least squares multiple regression to predict the pairwise correlation vector of the trait rating data of the ingroup Klee face image with the linear combination of the correlation vectors of ingroup face trait rating data and Klee face trait rating data. We found that both the ingroup ratings ( .66, SE.09, t 6.08, p.001) and Klee ratings ( .24, SE.13, t 2.15, p.035) were significant predictors of ingroup Klee face image ratings. We also conducted linear hypothesis testing to test whether ingroup ratings and Klee ratings were significantly different from each other and found that ingroup ratings and Klee ratings did not significantly differ in predicting ingroup Klee ratings, F(1, 75) 1.50, p.225. Outgroup Klee. We used multiple regression to predict outgroup Klee trait rating data with the linear combination of outgroup face trait rating data and Klee face trait rating data. We found that the outgroup ratings ( .89, SE.05, t 14.71, p.001) were a significant predictor of outgroup Klee ratings, whereas the Klee ratings were not ( .03, SE.07, t.57, p.570). The linear hypothesis testing showed that outgroup ratings predicted outgroup Klee ratings significantly better than Klee ratings, F(1, 75) 36.76, p.001. Ingroup Kandinsky. We used multiple regression to predict ingroup Kandinsky trait rating data with the linear combination of ingroup face trait rating data and Kandinsky face trait rating data. We found that both the ingroup ratings ( .83, SE.04, t 16.45, p.001) and Kandinsky ratings ( .19, SE.08, t 3.74, p.001) were significant predictors of ingroup Kandinsky ratings. The linear hypothesis testing showed that ingroup ratings predicted ingroup Kandinsky ratings significantly better than Kandinsky ratings, F(1, 75) 15.34, p.001. Outgroup Kandinsky. We used multiple regression to predict outgroup Kandinsky trait rating data with the linear combination of outgroup face trait rating data and Kandinsky face trait rating data. We found that the outgroup ratings ( .88, SE.06, t 15.15, p.001) were a significant predictor of outgroup Kandinsky ratings, whereas the Kandinsky ratings were not ( .04, SE.11, t.76, p.45). The linear hypothesis testing showed that outgroup ratings predicted outgroup Kandinsky ratings significantly better than Kandinsky ratings, F(1, 75) 27.40, p.001. Discussion In Study 2b, we found that for all four Group ART face representations, participants seemed to have used the ingroup\\/ outgroup distinction more than the minimal group labels (Klee and Kandinsky) as indicated by larger trait representational similarities between Group ART images and the ingroup and outgroup face images than the Klee or Kandinsky face image. Although there is evidence that a distinction was made between Klee and Kandinsky in the representations of the ingroup faces, our current findings are unlike our findings of Study 1b, in that participants in all conditions seemed to have focused more on whether the targets shared their group. Thus, we argue that although minimal group labels can be meaningful when visualizing faces (e.g., the overestimator label in Study 1), minimal group labels with less inductive potential might reveal less label effects, even during a task (e.g., face visualization) that demands forming a concrete representation of the target. The same limitations of Study 1b apply to the current study. Our interpretations of the multiple regression RSA results were drawn from trait rating results of group-level classification images, thus may be prone to human bias and only representative of the most typical (i.e., average) face representations. Additionally, it is not clear why Study 2a showed no interaction between Group and ART, but the current study suggested that representations of the labels contribute to representations of ingroup but not outgroup faces (i.e., category labels were significant predictors only for ingroup faces). This may simply be due to strong effects of group membership (ingroup\\/outgroup distinction) overshadowing the effects of category labels (Klee and Kandinsky), but it is also possible that the minimal group labels in this version of the minimal group paradigm are only weakly represented. Study 2c Study 2a and 2b showed that there might be some differences between face representations of Klee and Kandinsky groups, but whether the targets were ingroup or outgroup played a bigger role than the minimal group labels, which is in opposition to the findings of Study 1. Furthermore, we found that the magnitude of differences between ingroup and outgroup was larger in this version of the minimal group paradigm, indicating that different degrees of meaningfulness of minimal group labels may moderate intergroup bias in face representations of ingroup and outgroup. In Study 2c, we examined the representational differences in participant-level classification images of ingroup and outgroup as well as Klee group and Kandinsky group faces by using the support vector machine classifiers. Unlike the findings of Study 1c, we expected the algorithms to perform better at classifying between ingroup and outgroup faces than Klee and Kandinsky group faces based on larger differences found between ingroup and outgroup trait ratings than Klee and Kandinsky trait ratings. Method Stimuli. We used 200 participant-level classification images from Phase 1 of Study 2a. Each image had three dimensions: (a) group (ingroup or outgroup); (b) ART (Klee or Kandinsky); and (c) Group ART (ingroup-Klee, outgroup-Klee, ingroupKandinsky, or outgroup-Kandinsky). Procedure. We followed the same steps to conduct the machine learning analyses as described in Study 1c. To recap, we (a) vectorized and down sampled pixel intensity data of each image, (b) standardized the data, and (c) performed classification using support vector machines (SVM) with a radial basis function kernel (with default cost and gamma hyperparameters). We used 10-fold cross-validation for each analysis (i.e., classifying between ingroup and outgroup faces, Klee and Kandinsky faces, and Group ART faces). That is, each fold yielded a training set (90% of data 180 cases) and a testing set (10% of data 20 cases), both of which were evenly divided between classes (e.g., approximately equal numbers of ingroup and outgroup images). The SVM algorithm then learned the relationships between features (pixel intensity data) and class labels (e.g., Klee and Kandinsky) from the training set, and classified images from the testing set. We repeated this step 10 times. We then computed accuracy scores by averaging accuracies from these 10 folds. Next, we tested whether our classifiers performed better than chance by using 1000 permutation tests for each analysis. We also compared the accuracy of classification between ingroup and outgroup faces with that of classification between Klee group and Kandinsky group faces using the 5 2-fold cross-validation paired samples t test. Please see the Procedure section of Study 1c for more details. Results We were able to classify between ingroup and outgroup images from pixel intensity data significantly better than chance (accuracy 80.00%, p.001). However, we failed to classify between Klee and Kandinsky group face images better than chance at a.05 significance level (accuracy 57.00%, p.08). Next, we compared classification accuracies for Group and ART using the 5 2-fold cross-validation paired samples t test. This resulted in slightly different accuracy scores for each classification from 10fold cross-validation accuracy scores (Group 79.40% and ART 55.30%). The t test result showed that our model performed significantly better classifying between ingroup and outgroup faces than between Klee and Kandinsky group faces, t(9) 12.76, p.001, two-tailed, Cohen's d 4.04, 95% CI [19.83, 28.37]. Finally, multiclass SVM results showed that our model performed significantly better than chance (accuracy 41.00%, p.001). Unlike the previous two cases, the chance accuracy for the current classification was 25% (one out of four). The confusion matrix of results showed that our model misclassified within Group (78\\/160) more than within ART labels (22\\/104), such as misclassifying ingroup Klee face images as ingroup Kandinsky rather than outgroup Klee. Discussion In Study 2c, we investigated whether the pattern of results found in Study 2a and 2b (e.g., slight differences between Klee and Kandinsky, larger differences between ingroup and outgroup) holds true at the participant level using a machine learning analysis. We were able to classify between ingroup and outgroup participant-level classification images but failed to classify between Klee and Kandinsky group images better than chance (i.e., p.08). However, the multiclass results suggest that the aesthetic preference was represented to some extent in the pixel intensity data. Considering both Study 1 and Study 2 together, the fact that differences between face representations of ingroup and outgroup exist across different types of the minimal group paradigm at both individual and group levels suggests that the ingroup positivity effect in face representations is robust and unlikely to be paradigm specific. The finding that SVM classified between ingroup and outgroup face images significantly better than Klee and Kandinsky group face images, is contrary to the findings of Study 1c that the SVM classified between minimal group labels (i.e., overestimator and underestimator) significantly better than ingroup and outgroup. This discrepancy between the two studies supports the idea that different minimal group labels have different degrees of meaningfulness (e.g., NEST labels are more meaningful than ART labels) and that when the minimal group labels have less meaning, people may differentiate between ingroup and outgroup more. This interpretation is also consistent with our finding of more misclassifications within group labels than within ART labels of Group ART participant-level classification images. Although descriptive, it seems to suggest that people might have used the ingroup\\/ outgroup distinction more than the minimal group labels when visualizing faces during the face categorization task, leading to more consistent face representations of ingroup and outgroup than Klee and Kandinsky groups (e.g., ingroup Klee and ingroup Kandinsky are more similar than ingroup Klee and outgroup Klee). Study 3 Studies 1 and 2 served as a deep dive into how the representation of ingroup and outgroup faces in minimal group contexts might be influenced by category labels such as the overestimator\\/ underestimator and the Klee\\/Kandinsky labels that were previously believed to be arbitrary and trivial. We showed that these labels could matter, and the meaningfulness of a minimal group label may moderate intergroup bias. Although face representations of different groups approximated by the reverse correlation method could reveal how group membership might influence behavior, such as trusting behavior in an economic trust game , the implications of the effects of meaningful (or lack thereof) minimal group labels for intergroup behaviors remain unclear. Do people still differentiate between overestimators and underestimators when they do not need to visualize their faces? If so, do they ascribe different traits to these groups? Study 3 sought to address these questions by using the classic Tajfel matrices task and the overestimator\\/underestimator minimal group paradigm. In this study, we first assigned participants into one of two groups (overestimator or underestimator) and asked them to allocate resources (points) to two anonymous individuals. Critically, the only information participants had was their group membership and that one individual was an overestimator and the other was an underestimator. We then asked our participants to rate overestimator and underestimator on 13 trait dimensions that we used in earlier studies to examine trait impressions elicited by faces of different groups. By doing so, we examined whether people ascribe different traits to different groups, even when they were not required to visualize their faces. Thus, Study 3 examined (a) whether people discriminate between overestimator and underestimator during a resource allocation task and (b) whether people associate overestimator and underestimator with different traits when they do not need to visually represent their faces. Method Participants. We recruited 200 MTurk participants (Mage 38.01, SD 10.66; 103 female, 96 male, one other) via the CloudResearch website (; www.cloudresearch.com\\/) to complete an online survey administered through Qualtrics (www.qualtrics.com). Because we used the methods and procedures of Loersch and Arbuckle (2013), we attempted to recruit a comparable number of participants as them (n 76). We simply rounded this number to 100, and because we were interested in effects of both Group and NEST (; only examined the group effect), we then doubled the sample size. We preregistered this sample size before data collection (https:\\/\\/osf.io\\/s9243\\/?view_only92afae84a38548e8a 9412e8353f30905). The racial and ethnic breakdown of our sample was 137 White, 25 Asian, 17 Black, 11 Latino\\/Hispanic, nine multiracial, and one other participant. Participants were expected to complete the study in 15 min but given up to 30 min to finish. All participants were compensated with $2 for their participation. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. The current study used the resource allocation task from Study 1 of Loersch and Arbuckle (2013); however, we used a minimal group paradigm to assign participants to different groups instead of using real-world groups. First, as was the case with Study 1a, we used the NEST, a variant of the classic \\\"dot estimation\\\" procedure (Experiment 1 from Tajfel et al., 1971), to assign participants to either the overestimator or underestimator group. See the NEST procedural details from Study 1a for more information about this group manipulation. Resource allocation task. After completing the NEST, participants completed a series of six Tajfel matrices adapted from Loersch and Arbuckle (2013). Each matrix consists of 13 columns and two rows. On each trial, participants chose a column of two numbers to indicate points they would allocate to two anonymous individuals. Critically, the only information they had about these individuals was their numerical estimation style (overestimator or underestimator). If the target individual shared the same group membership as the participant, then they were allocating points to an ingroup member, if the target had a different group membership, then they were allocating points to an outgroup member. Although different point options allowed us to examine different strategies people could have used in this task, such as parity (allocating equal points) or maximizing group differences (giving more points to the ingroup member than the outgroup member), we simply summed the overall amount participants allocated to each individual and examined the effects of target category label (overestimator or underestimator), whether the target shared the group membership with the participants or not (ingroup or outgroup), and the interaction between the two. By doing so, we examined whether people showed ingroup favoritism (i.e., on average giving more points to the ingroup member than the outgroup member) as well as whether they allocated different points to overestimator and underestimator individuals regardless of their own group membership. Trait ratings. After completing the resource allocation task, participants rated both overestimator and underestimator on the same 13 trait dimensions that we used to assess trait impressions elicited by faces of different groups in Phase 2 of Study 1a and Study 2a (i.e., trustworthy, attractive, dominant, caring, sociable, confident, emotionally stable, responsible, intelligent, aggressive, mean, weird, and unhappy;). We simply asked them to rate a typical overestimator and a typical underestimator on the 13 traits. Ratings were made on scales from 1 (not at all) to 7 (extremely). The order of each trait presentation was random. Likability ratings. Lastly, we asked our participants how they felt about overestimators and underestimators on a 5-point scale from 1 (very negative) to 5 (very positive). Results Resource allocation task. To examine the effects of minimal group labels on how people allocate resources to various individuals, we conducted a mixed-design ANOVA with Group (ingroup, outgroup), NEST (overestimator, underestimator), and the interaction between the two as factors. We found a main effect of Group indicating that participants allocated more points to the ingroup member (M 15.72, SD 4.62) than to the outgroup member (M 14.17, SD 4.95), F(1, 198) 46.50, p.001, Cohen's d.42. We also found a main effect of NEST: Participants allocated more points to the overestimator (M 15.23, SD 4.74) than to the underestimator (M 14.65, SD 4.94), F(1, 198) 6.62, p.01, Cohen's d.08. The interaction between Group and NEST was not significant, F(1, 198) .26, p.61. Trait ratings. We used rMANOVA to test the effects of Group, NEST, and the interaction between the two on trait ratings of overestimator and underestimator. Significant multivariate effects were found for Group (Pillai's Trace.08, F 2.73, df (13, 383), p.001) and NEST (Pillai's Trace.25, F 9.92, df (13, 383), p.001). The interaction term did not yield a significant multivariate effect (Pillai's Trace.05, F 1.63, df (13, 383), p.08). We then conducted univariate F tests examining the effects of Group and NEST separately. The results showed that the ingroup was rated more trustworthy, attractive, caring, responsible, and intelligent, whereas the outgroup was rated more mean and unhappy. We also found that the overestimator was rated more dominant, confident, aggressive, and sociable, whereas the underestimator was rated more responsible and unhappy. The univariate F test results including the means, standard deviations, F values, p values, and effect sizes for effects of Group and NEST are presented in Table 7 and 8. Likability ratings. We conducted a mixed-design ANOVA with Group (ingroup, outgroup), NEST (overestimator, underestimator), and the interaction between the two as factors on people's ratings of overestimator and underestimator. We found a main effect of Group indicating that participants rated their ingroup (M 3.48, SD.63) more favorably than their outgroup (M 3.10, SD.59), F(1, 198) 42.59, p.001, Cohen's d.46. We did not find a significant main effect of NEST, F(1, 198) .19, p.66. The interaction between Group and NEST was also not significant, F(1, 198) .48, p.49. Discussion In Study 3, we investigated the effects of minimal group labels on intergroup behavior when people are not required to visualize faces of different groups. First, we replicated the classic ingroup favoritism finding with the resource allocation task. Specifically, participants allocated more points to a member of their ingroup than a member of their outgroup. More interest ingly, participants also allocated more points to the overestimator person than to the underestimator person, regardless of their own group membership (i.e., no significant interaction between Group and NEST). It is also important to note that allocating more resources did not mean that the overestimator was liked more than the underestimator. That is, although people rated their ingroup as more likable than their outgroup, there was no difference in the likability ratings between overestimator and underestimator. The resource allocation results suggest that participants might have imbued overestimators and underestimators with different qualities and this influenced their decisions to allocate more points to overestimator than underestimator, as opposed to simply favoring overestimator over underestimator. The trait rating data support this interpretation. First, we found ingroup positivity bias in trait rating data similar to the face representation studies: People rated their ingroup as more trustworthy, attractive, caring, responsible, and intelligent, whereas they rated their outgroup as more mean and unhappy. Second, we found differences between overestimator and underestimator on a number of traits, mirroring the findings of Study 1a: People rated overestimators as more dominant, confident, aggressive, and sociable, and rated underestimators as more unhappy. These findings suggest that the traits that people associate with overestimators are different from the traits that they associate with underestimators, and further supports the idea that people imbue meaning to the overestimator and underestimator labels. It is possible that differential trait attributions to the overestimator and underestimator labels could have led to differential resource allocations. Future research should be designed to examine the possibility of this mediation pathway. Despite an influence of NEST labels on trait impressions and resource allocations, there was not a significant interaction between Group and NEST (i.e., p.08 for trait ratings; p.61 for resources allocation). Study 4 Study 3 showed that people differentiate between the overestimator and underestimator labels even when they do not need to visually represent their faces. Is this phenomenon specific to the overestimator\\/underestimator paradigm? Do people differentiate minimal group labels that have less inductive potential (e.g., Klee\\/ Kandinsky)? The final study of the current research investigated the generalizability of our findings from Study 3 with the Klee\\/ Kandinsky paradigm (Experiment 2 from Tajfel et al., 1971). Study 4 used the same set of methods from Study 3 to examine whether people differentiate between people who like Klee paintings and those who like Kandinsky paintings during a resource allocation task. We again used the Tajfel matrices to assess how people allocate resources to others who are Klee or Kandinsky fans. Because we found less distinction between the Klee and Kandinsky groups in Study 2 compared with the overestimator and underestimator groups in Study 1, we expected to find little to no difference in how people allocate resources to Klee and Kandinsky groups. We also expected to find stronger ingroup favoritism (i.e., more discrimination between ingroup and outgroup) because lack of meaning in minimal group labels was associated with larger differences between mental representations of ingroup and outgroup faces in Study 2. Method Participants. We recruited 199 MTurk participants (Mage 37.03, SD 10.97; 102 female, 96 male, 1 other) via the CloudResearch website (www.cloudresearch.com) to complete an online survey administered through Qualtrics (www.qualtrics.com). Using the same rationale as Study 3, we tried to recruit 200 participants but due to an unidentifiable error in the recruitment process we ended up with 199 participants. We also preregistered this sample size before data collection (https:\\/\\/osf.io\\/s9243\\/?view_ only92afae84a38548e8a9412e8353f30905). The racial and ethnic breakdown of our sample was 137 White, 20 Black, 19 Asian, 13 Latino\\/Hispanic, nine multiracial, and one other participant. Participants were expected to complete the study in 15 min but were given up to 30 min to finish. All participants were compensated with $2 for their participation. Participants provided written informed consent approved by the UCSB Human Subjects Committee. Procedure. The current study followed the same procedure as Study 3 except for the version of the minimal group paradigm that assigned participants to different groups. As was the case with Study 2a, we used a classic aesthetic preference procedure (Experiment 2 from Tajfel et al., 1971) to assign participants to either the Klee or Kandinsky group. Resource allocation task. Next, participants allocated points to two anonymous individuals in a series of six Tajfel matrices similar to the ones used in Study 3 except that the only information they had about these individuals was their aesthetic preference (Klee fan or Kandinsky fan). If the target individual shared the same aesthetic preference as the participant, then they were allocating points to an ingroup member, if the target had a different aesthetic preference, then they were allocating points to an outgroup member. We summed the overall amount that participants allocated to each individual and examined the effects of target category label (Klee fan or Kandinsky fan), whether the target shared the same aesthetic preference as the participants or not (ingroup or outgroup), and the interaction between the two. Thus, we examined whether people would show ingroup favoritism as well as whether they allocated different points to Klee and Kandinsky fans regardless of their own aesthetic preference. Trait ratings. After completing the resource allocation task, participants rated both Klee and Kandinsky fans on the same 13 trait dimensions (i.e., trustworthy, attractive, dominant, caring, sociable, confident, emotionally stable, responsible, intelligent, aggressive, mean, weird, and unhappy;). We simply asked them to rate a typical person who likes paintings of Klee and a typical person who likes paintings of Kandinsky on the 13 traits. Ratings were made on scales from 1 (not at all) to 7 (extremely). The order of each trait presentation was random. Likability ratings. Lastly, we asked our participants how they felt about Klee fans and Kandinsky fans on a 5-point scale from 1 (very negative) to 5 (very positive). Results Resource allocation task. To examine the effects of minimal group labels on how people allocate resources, we conducted a mixed-design ANOVA with Group (ingroup, outgroup), ART (Klee, Kandinsky), and the interaction between the two as factors. We found a main effect of Group indicating that participants allocated more points to the ingroup member (M 16.19, SD 4.69) than to the outgroup member (M 13.69, SD 5.04), F(1, 197) 104.523, p.001, Cohen's d.63. We did not find a main effect of ART, F(1, 197) 1.88, p.17. The interaction between group and ART was also not significant, F(1, 197) .77, p.38. Trait ratings. We used rMANOVA to test the effects of Group, ART, and the interaction between the two on trait ratings of overestimator and underestimator. The only significant multivariate effect was found for Group (Pillai's Trace.10, F 3.37, df (13, 381), p.001). The multivariate effect of ART was not significant (Pillai's Trace.01, F.39, df (13, 381), p.97). The multivariate effect of the interaction term was also not significant (Pillai's Trace.03, F.82, df (13, 381), p.64). We thus followed up with univariate F tests examining the effects of only Group. The results showed that the ingroup was rated more trustworthy, attractive, caring, confident, emotionally stable, responsible, and intelligent, whereas the outgroup was rated more aggressive and unhappy. The univariate F test results including the means, standard deviations, F values, p values, and effect sizes for effects of Group are presented in Table 9. Likability ratings. We conducted a mixed-design ANOVA with Group (ingroup, outgroup), ART (Klee fan, Kandinsky fan), and the interaction between the two as factors on people's ratings of Klee fans and Kandinsky fans. We found a main effect of Group indicating that participants rated their ingroup (M 3.94, SD.73) more favorably than their outgroup (M 3.35, SD.69), F(1, 197) 86.52, p.001, Cohen's d.66. We did not find a significant main effect of ART, F(1, 197) .11, p.74. The interaction between Group and ART was also not significant, F(1, 197) .03, p.86. Discussion Study 4 investigated whether people differentiate between the Klee and Kandinsky groups during a resource allocation task as they do with overestimator and underestimator (Study 3) even when people are not required to represent faces of different groups. First, we replicated the classic ingroup favoritism finding with the resource allocation task again : People allocated more points to the individual who shared the same artistic preference as them (ingroup) than to the individual who did not share the same artistic preference (outgroup). However, participants did not discriminate between the Klee and Kandinsky fans when allocating resources. As predicted, however, this was related to stronger ingroup favoritism on the resource allocation task in Study 4 as indicated by the larger effect size (d.63) compared with the results on this same task in Study 3 when participants were influenced by the minimal group labels (d.42). We also replicated the finding from Study 3 that people liked their ingroup more than their outgroup. The effect size from this finding was again greater (d.66) than the one found in Study 3 (d.46), providing another piece of evidence that when people do not differentiate between minimal group labels, they may differentiate between ingroup and outgroup more, thus showing greater intergroup bias. Furthermore, we found no effect of minimal group labels on any of the 13 traits, indicating that participants did not differentiate between Klee and Kandinsky fans when they were not representing their faces. However, we replicated ingroup positivity effects in these trait rating data. That is, participants rated their ingroup as more trustworthy, attractive, caring, confident, emotionally stable, responsible, and intelligent, whereas rated their outgroup as more aggressive and unhappy. We not only found significant differences between ingroup and outgroup on a greater number of traits in this paradigm compared with the overestimator\\/underestimator paradigm, but also that a majority of the significant traits in both paradigms yielded larger effect sizes in the current version of the minimal group paradigm than the one used in Study 3. These findings further support the idea that when the meaning of the minimal group labels is diminished, people may focus more on the ingroup\\/outgroup distinction, which leads to greater intergroup bias. General Discussion Despite the popularity of the minimal group paradigm and its long history, no previous studies have rigorously tested the foundational assumption that minimal group labels have no consequential meaning to participants. The goal of this article was to empirically examine whether the classic minimal group category labels that have been used in many research studies over the past 50 years are ever imbued with meaning and whether such associations have consequences for various intergroup responses. We probed this question across four studies. We first used an overestimator\\/underestimator minimal group paradigm to replicate Ratner et al.'s (2014) finding that people form mental representations of ingroup faces that are associated with more favorable traits than are outgroup face representations. Beyond successfully replicating their ingroup positivity main effect, we showed that category labels also mattered, in that, faces of overestimators and underestimators are represented differently. Our initial evidence for these conclusions was drawn from statistically comparing whether the experimental conditions influenced the trait rating means of the classification images, which was a strategy used by Ratner et al. (2014) to examine differences between ingroup and outgroup. We then conducted representational similarity analysis on the trait rating data, and found that the overestimator label contributed to the pattern of trait ratings more so than the ingroup\\/outgroup distinction, but this was not the case for the underestimator label. We further corroborated these conclusions with a machine learning analysis. The machine learning analysis suggested that the NEST labels (overestimator vs. underestimator) might in fact contribute more to the face representations than the ingroup\\/outgroup distinction as indicated by our algorithm's better accuracy for classifying the NEST labels than the ingroup\\/outgroup labels. Next, we sought to understand the generalizability of the findings from Study 1. We did so by using a different minimal group paradigm (i.e., Klee vs. Kandinsky preference), which on its face has less inductive potential than does the overestimator\\/underestimator minimal group paradigm. We found that Klee and Kandinsky fans are represented differently at the group-level, consistent with the overestimator\\/underestimator results. However, the representational similarity analysis revealed that the Klee and Kandinsky labels contributed to the face representations far less than the ingroup\\/outgroup distinction. This in turn was accompanied by larger differences between ingroup and outgroup. We corroborated these findings with the machine learning analysis and showed that there were larger differences between faces of ingroup and outgroup than Klee and Kandinsky groups at the participant level. Despite using multiple analysis techniques to demonstrate effects of minimal group labels on category representation with different minimal group paradigms, we next sought to examine whether these category label effects translated into meaningful overt evaluations, impressions, and behaviors. To this end, we replicated Tajfel et al.'s (1971) classic overestimator\\/underestimator minimal group study with a resource allocation task adapted from Loersch and Arbuckle (2013). Consistent with Tajfel et al.'s (1971) original finding, participants allocated more resources to their minimal ingroup than outgroup. However, we also found that people allocated more resources to an overestimator than to an underestimator, although they did not evaluate overestimators more favorably than underestimators. We also showed that people associated different traits with overestimators and underestimators, suggesting that people imbue minimal group labels with meaning even when they do not need to visually represent faces. In our final study, we examined the effects of the Klee and Kandinsky labels on resource allocations, given that logically the Klee and Kandinsky labels provide less to read into than do overestimator and underestimator labels. We found that participants did not differentiate between Klee fans and Kandinsky fans when it came to allocating resources, evaluating them, and ascribing different traits. This lack of differentiation between the Klee and Kandinsky groups was accompanied by stronger ingroup favoritism on the resource allocation task as well as more favorable evaluations of ingroup than outgroup. It is intriguing that the reverse correlation procedure used in Study 2 identified representational differences between the Klee and Kandinsky groups, and these differences did not emerge in explicit trait ratings, evaluations, and behavioral responses, but studies consistently revealed stronger intergroup bias in the Klee\\/Kandinsky version than the overestimator\\/underestimator version of the minimal group paradigm. What does this all mean for Tajfel et al.'s (1971) foundational assumption? It appears that category labels are flimsy in some ways, but not in others. On one hand, intergroup bias reliably emerges irrespective of category label. On the other hand, category labels are represented differently and can moderate intergroup bias in representations when labels are high in inductive potential, such as the overestimator\\/underestimator distinction. Moreover, when high in inductive potential, these labels can carry more weight in the overall representations than the ingroup\\/outgroup distinction. However, the impact on downstream impressions, evaluations, and behavior appears to be limited. Impressions of and resource allocations to overestimators and underestimators differed, but these label effects did not significantly interact with the group effects, which supports the validity of the intergroup bias interpretations in the literature. Lessons Learned for Minimal Group Research Not all minimal groups are the same. This seems obvious, but this detail has been largely ignored in the minimal group literature. As Pinter and Greenwald (2010) point out, there has been oddly little development in group assignment techniques over the years. The old adage \\\"if it ain't broke, don't fix it\\\" probably contributed to this inertia, given that the paradigm has been successful, and to our knowledge, minimal group research has not suffered from replicability problems that have plagued other psychological paradigms. From a methodological standpoint, however, our work suggests that researchers need to recognize that careful attention to counterbalancing and full reporting of category label differences is important to prevent interpretative slippage. Slight differences between category labels could lead to assuming ingroup versus outgroup differences that are really driven by the category labels. Because category label differences might not be the same from one version of the minimal group paradigm to another, it should not be assumed that they are all interchangeable and will cause exactly the same effects. Our research also provides insight into how the implied meaningfulness of the labels influences various forms of intergroup bias. A priori, it was not obvious to us whether reading into the labels should increase intergroup bias or attenuate it. On one hand, the field has gravitated toward using contrived group distinctions instead of purely random ones. This suggests that some inferential grist on the labels could be important for making participants view the novel categories as entitative groups. From this perspective, the overestimator\\/underestimator paradigm should lead to more intergroup bias than the Klee\\/Kandinsky version because the inductive potential of the dot estimation labels provides a rationale to identify with one's ingroup. On the other hand, to the extent that one is reading into the dot estimation labels (particularly the overesti mator label) then these associations could obscure the ingroup\\/ outgroup distinction. Our results support this latter possibility. One implication is that if one is trying to manipulate minimal groups that are mostly devoid of meaning, then the Klee\\/Kandinsky paradigm might be a better option. However, as we expand on below, many real-world novel groups actually have labels that imply characteristics. To the extent that a researcher is interested in modeling this dynamic in the laboratory, then maybe the overestimator\\/underestimator paradigm is more appropriate. All of this said, the differences between these two paradigms should not be overstated. Despite their differences, they both generally support the claim that separating people into novel groups leads to ingroup positivity bias. We found intergroup effects on all of the dependent variables we examined. Moreover, on the resource allocations and explicit ratings we did not see significant interaction effects between label and group. This lack of an interaction suggests that label effects are less of a concern when outcome variables of interest are behavioral and selfreported impressions versus outcomes that are more sensitive to mental representations of category information (e.g., face representations using the reverse correlation technique). It is also clear from this research that simply telling participants that groups do not differ cannot be relied upon. Overestimator and underestimator were represented differently more than ingroup and outgroup, even though we told participants that numerical estimation style was not related to any other cognitive tendencies or personality traits. Klee and Kandinsky were also represented differently even though we gave participants the same instructions, albeit this distinction was represented to a lesser extent than ingroup and outgroup. Why would people ignore the instructions designed to constrain their inferences about the underlying essence of the groups? As we state earlier, Grice's (1975) Maxim of Quality states that people generally believe what they are told. From this vantage, telling participants that they should not read into the labels should be sufficient. However, people are motivated to make meaning of the world around them and as Bruner taught us long ago, people go beyond the information given. It is also the case that priming could contribute to the label effects. Affect and semantic misattribution research uses paradigms that instruct participants to not let a prime influence them, yet participants are still influenced by the prime. In a related vein, perhaps telling someone not to read into the meaning of a category label backfires in the same way that people struggle not to think about a white bear. Ironic process theory of mental control  argues that the act of telling people to ignore a concept makes them think about it more because it keeps the concept active in their minds. It is still peculiar, though, why participants represent Kandinsky and Klee groups differently. There is no reason to assume meaningful differences between these groups, unlike is the case for overestimators and underestimators. Maybe stereotypes associated with the Kandinsky and Klee surnames and their respective ethnicities are automatically transferred to the groups even though it is not logical to assume that fans of the abstract painters share their ethnicities. It is also possible that sound symbolism plays a role. Sound symbolism assumes that vocal sounds and phonemes carry meaning. Recent research suggests that more sonorant phonemes are associated with high emotionality, agreeable ness, and conscientiousness, whereas names with voiceless stop phonemes are associated with high extraversion. Maybe the way that category labels are pronounced can bias trait inferences. Additional research is necessary to explore these possibilities. Our research only focused on the two most famous minimal group paradigms. It was beyond the scope of our research to catalog effects of all possible minimal group labels. Although future studies are necessary to understand how broadly the current findings generalize, our evidence of category label effects suggests that a revision is necessary to the account of how categorization occurs in minimal group settings. The minimal group paradigm emerged in the early 1970s when similarity-based models dominated cognitive psychology's understanding of categorization. Not surprisingly, categorization during the minimal group paradigm was largely thought to straightforwardly involve matching characteristics of the perceiver with characteristics of the target. From this vantage, the category label served as a vehicle for ingroup versus outgroup classification but nothing more. However, cognitive psychology researchers outside of social psychology soon began to argue that similaritybased models were not adequate to explain categorization. They suggested that categorization may be \\\"more like problem solving than attribute matching\\\" . This so-called theorybased view of categorization suggested that perceivers take note of the attributes that correlate with category membership, but categorization ultimately results from generating an explanatory principle of how these attributes are interrelated. This conceptual development was recognized by some social perception researchers who used the theory-based view of categorization to explain racial essentialism. However, such insight was never integrated into theories designed to explain minimal group effects. Theory-based categorization could help minimal group researchers account for category label effects. We think it is likely that the explanatory vacuum created by the lack of meaning attributed to the minimal group labels prompts participants to wonder what produces the categorical distinction and why exactly they (and other people) are in one category versus the other. Therefore, it should not be a surprise to researchers if their participants attempt to read into the category labels and infer meaning from them. Significance for Reverse Correlation Research in Social Psychology The goal of the current research was to understand whether category labels influence minimal group responses. Because the reverse correlation method is sensitive to category representation it seemed like an ideal tool to use in our research. In the process of maximizing the utility of this method to answer our research questions, we developed several novel ways of analyzing reverse correlation data. To our knowledge, our work is the first to use machine learning to analyze participant-level classification images. Assessing participant-level classification images with traditional social psychological methods can be challenging because of the number of trials that are often needed. Although large numbers of participant-level classification images can take a long time to process with machine learning methods, this processing time is easier to manage than the fatigue and boredom of processing all of these images for human participants. Thus, the machine learning technique we used could prove useful to other social psychologists addressing different research questions with reverse correlation methods. We also demonstrate how representational similarity analysis is useful for analyzing patterns of trait ratings of aggregate classification images. This method is used frequently in fMRI research and only recently has been used by social psychologists to analyze behavioral data. We are hopeful that the application of machine learning and representational similarity analysis to analyzing reverse correlation classification images will be helpful for social psychology researchers addressing a widerange of topics. Why It Matters if Category Labels Matter Many everyday social categories are not natural kinds, in the sense that they are human creations that started with a seemingly arbitrary distinction. Yet, as is the case with minimal groups created in the laboratory, group distinctions in the real world are often given labels that are not completely devoid of meaning. When people are assigned to groups in organizations, those groups typically have team names that give them an identity. Sports teams, street gangs, nation states, and many other groupings have names that provide strong social significance, even though group membership is mostly a product of where you happen to have been born or currently live. Even military units that are officially defined in an arbitrary manner have nicknames. For instance, members of the 101st airborne division of the United States Army are called the \\\"Screaming Eagles.\\\" Members of the 34th infantry division are called the \\\"Red Bulls.\\\" These labels give their groups a specific character. If contrary to conventional wisdom, group labels matter when making sense of novel groups, the arbitrary distinctions from which intergroup biases are thought to start might not always be arbitrary and could actually provide grist to infer traits and power and status differences. People are active meaning-makers and associations that come to mind when processing category labels might be the impetus that gets the ball rolling down the hill toward entrenched biases. The possibility that the labels given to novel categories could have this effect has never been seriously considered in the literature. Yet doing so, makes clear that the labels that people use to categorize could have implications for understanding real-world groups. Moreover, the type of processing one is engaged in while thinking about such groups could influence the extent to which category labels are functional for serving people's inferential goals. One of the clearest real-world implications is for visualizing novel group members because the reverse correlation studies revealed the most pronounced category label effects. An example of such a situation is corresponding with another person electronically without knowing what they look like. In this situation, one might spontaneously visualize the other person and this visualization, even if not grounded in reality, could bias their evaluations and impressions. It is also frequently the case that we are tasked with finding someone when we do not know what they look like. In these situations, we might know something about their group membership and use this knowledge as a cue to infer what they look like. Our work suggests that in both of these situations, people might grasp onto any knowledge that is tangible and then relate this knowledge to some concept that they have experienced. Once people connect a novel group distinction to a concept that they have experienced, then they can retrieve perceptual details from memory to populate their visualization. It is possible that established group distinctions that now have considerable social meaning, such as race, at least partially developed their meaning through such a process. For instance, skin color is one of the most salient indicators of racial group. However, skin color is a physical attribute and does not have inherent social meaning. It is possible that associations that people made with light and dark contributed to how people initially formed mental representations about race. This last possibility is purely speculative, but highlights how forming theories of what causes group distinctions could affect how people mentally represent groups in everyday life. How category labels influence intergroup behavior and impressions during everyday life when visualization is not required is less clear. We did not find evidence that intergroup bias in such circumstances are moderated by category labels. However, the main effect of category label on the resource allocation task when participants were divided into overestimators and underestimators suggests that category labels can have important effects on behavior. For instance, when separated into novel groups in real-world situations, distinctions that signify group membership could imply traits about dominance or likability in ways that affect behavior distinct from mere group membership. Whether subtle cues like category labels and other attributes have any influence in real-life scenarios largely depends on the broader context. Unlike in a laboratory setting, which is highly controlled, everyday life is complex. Many groups are embroiled in intractable conflicts, are marked by stigmatizing stereotypes, and embedded in unequal power structures. In these situations, category label effects likely contribute negligible variability to intergroup responses. However, in the absence of these weighty factors, seemingly trivial factors such as category labels could have a surprisingly disproportionate influence. Recent research by Levari et al. (2018) shows that as the prevalence of a stimulus decreases people seek out other ways to distinguish categories. For instance, they find that when threatening faces become infrequent, participants start to view neutral faces as threatening. In relation to our research, this work suggests that if the usual drivers of intergroup conflict are not a factor, people might latch onto any attributes that differentiate one group from the other. As we state earlier, most groups are not labeled by random number or letter strings. Their names contribute to their identity. Thus, category labels could consciously or nonconsciously provide associations that feed intergroup differentiation. Conclusion The minimal group paradigm gained prominence because it showed that people discriminate even when group boundaries are meaningless. Our research makes the point that we should not assume that seemingly arbitrary group distinctions are meaningless from the perspective of the people in the groups. People are motivated to find meaning in their situations and also are passively influenced by priming and spreading activation so they might latch onto associations to imbue their groups with meaning. This could especially be the case when people are tasked with information processing goals that are most easily accomplished with access to concrete information (e.g., visualization). By challenging conventional understanding of the minimal group paradigm, our work provides new insight into the circumstances when category labels might influence intergroup responses.\",\"1135746714\":\"INTRODUCTION \\\"Performance\\\" has been an aspect to be discussed and widely studied since the 1980s. It is represented as an \\\"act of doing work or duty\\\" assigned to an individual in a work setting. Academic performance is viewed as a responsibility and outcome of the effort within their authority level. Performance is measured by several criteria, namely quantity and quality, cost-effectiveness and timeliness, and the need for interpersonal impact, which includes supervision. With the growth of the knowledge economy, universities are faced with intensive competition on both domestic and global levels toward rating and performance. The lecturers' performance is the key contributor to the academic quality of universities and colleges. Therefore, universities and colleges should empower their human capital to gain practicability, deliver state-of-the-art academic services, and achieve first-class academic status. It is noteworthy that the complex nature and difficulties in measuring the outcome have become the major drawbacks in assessing individual and institutional performance in higher education. Currently, academicians face highly competitive and demanding workplaces in higher educational institutions. The academicians are prompted to have higher workloads in the form of classroom teaching and learning duties, engaged in research, publications, grants, training, supervision, invigilation, administrative tasks, and social commitments. As a result, lecturers lose interest in their jobs and consider academic work to be less interesting. Academics who are constantly unhappy and dissatisfied will notice a change in their work execution and nature. Other tasks may satisfy them, but the entire workload does not. Furthermore, this may result in staff turnover, which can be costly to the institutions. The institution may experience a loss of personnel investment, staff replacement costs, training costs, and a delayed work process. As a result, assessing and improving motivational factors among academics is necessary to prevent unproductive conduct and improve work performance. Hence, the assessment and improvement in motivational factors among academicians are needed to reduce unproductive behavior and improve work performance among academicians. Academicians are responsible for shaping young generations who are the assets of the country. Low morale and unmotivated performance may negatively influence the students , which leads to the utmost importance in maintaining the academicians' motivation. The motivational theory suggests the psychological (intrinsic) and survival (extrinsic) needs. To be specific, intrinsic motivation is defined as a type of motivation based on the natural interest of individuals in various activities with challenges and uniqueness. It does not involve external rewards, but rather the individuals' expression regarding themselves and their interests. In contrast, extrinsic motivation is triggered by external factors that are primarily financial. It is also known as the outcome of the performance of an activity, which includes financial reward (FR), promotion, and performance appraisals (PALs). These factors have been adapted for the study from the literature, with only the three aforementioned factors being selected. Intrinsic motivation is strongly linked with the natural well-being of the teaching and learning process, which is systematically compromised by common practices among teachers and parents. However, although extrinsic motivation is on the contrary to intrinsic motivation, self-determination theory (SDT) suggests that some forms of extrinsic motivation are inadequate, while some forms are effective. Malaysian higher education offers a unique combination of learning opportunities that attract local and international students. A total of 20 public and approximately 450 private universities and higher education colleges offer eight levels of higher education programs in Malaysia under the Malaysian Qualification Framework (MQF 2.0) . Currently, 30% of university students are international students. The Malaysian higher education system offers education to attract reasonable foreign exchange for the country rather than uplifting the social, economic, and political economy. Quality teaching is at the heart of the higher education system, and it can only be achieved if higher education faculties are satisfied and wellfunctioning. The higher number of private higher educational institutions in Malaysia demonstrates that private investors are progressing toward gaining the lucrative higher education market. The PHEI industry share would have reached USD 0.85 billion by 2021 and USD 1.50 Billion by 2026 . However, quality education needs quality academicians working in a suitable work environment in a motivated manner. The current study seeks to analyze the motivation leading to job satisfaction and performance among academicians. It also aims to explain the motivational factors contributing to the lecturer's job satisfaction and performance. A questionnairebased survey has been employed among 343 academicians working across peninsular Malaysia to address the following research questions; (1) to what extent does the motivation (extrinsic\\/intrinsic) affect lecturers' job performance? (2) how does lecturers' job satisfaction intercede the relationship between motivation (extrinsic\\/intrinsic) and lecturers' job performance? (3) how does lecturers' self-efficacy influence the relationship between their job satisfaction and job performance. Following that, the remaining sections present the pertinent literature, the method adopted for data analysis, and the discussion of the results. LITERATURE REVIEW Theoretical Foundation Many theories are perceived as universal for the prediction and understanding of the needs categories that employees attempt to achieve within their motivation and fulfillment as a guide of priority or pre-potency within their work. Motivation is constantly linked with numerous prominent theories, each with different concepts and circumstances that impact performance and satisfaction. Some of the remarkable theories are Maslow's Hierarchy of Needs (MHN), Herzberg's two-factor theory (HTFT), social cognitive theory (SCT), Vroom's Expectancy theory (VET), and TwoFactor Theory (TFT) . The paper content theories include Herzberg's two-factor theory and social cognitive theory. Herzberg's theory has also evidenced that individuals are not satisfied with the lower workload. However, they gain satisfaction through the achievement of psychological needs, recognition and growth, responsibility, promotion, and the nature of the work. In parallel, social cognitive theory posits that two cognitions, namely outcome expectancies and self-efficacy, are essential in self-regulation. It has been suggested that intrinsic and extrinsic motivations are the most efficient methods of motivating an individual. A correct and effective motivation leads to individual self-efficacy, which is followed by job satisfaction and success in terms of organizational efficiency. Hypotheses Development Financial Rewards Financial rewards refers to monetary incentives that an employee receives in return for the appropriate performance in line with organizational objectives. The FRs denote the types of sessional earnings, bonus pay, pay increment, indirect costs, and additional reimbursement. Since the 1980s, the professionalism of lecturers has received relatively notable attention for enhancement, mainly by improving their motivation and job satisfaction through the FR system. Koo et al. (2020) added that FR does not only motivate employees, but also increases job satisfaction and performance of employees in an organization. Correspondingly, Pham-Thai et al. (2018) highlighted an increase in employee productivity upon the increase in the pay structure. The recent empirical work demonstrated that FRs positively influence job performance, while the employees are more concerned about the extrinsic reward systems that include salary, bonuses, or commissions that could increase their satisfaction and the organization profits. Accordingly, the following hypotheses are suggested: Hypothesis 1 (H1): FRWs positively affect the lecturer's job satisfaction (LJS). Hypothesis 2 (H2): FRWs positively affect the lecturer's job performance (LJP). Promotion Employee performance compensation is not always cost-effective. Employees respect managerial titles more since they have a formal status and may be put on resumes. The Promotion (PRN) denotes the opportunity offered to an employee based on the upward career movement provided by the employer to the high-performing employees. Promotion is offered in the form of job title, seniority, or pay raise across the board that follows the hierarchy. However, the remuneration for motivating workers' performance would not be as cost-effective. Employees value managerial titles due to their link with promotion, which allows them to consider their status. Bognanno and Melero (2016) postulated that promotions reveal different dimensions of skills and capabilities of different types of workers. From the academic settings, promotions are perceived as the most perceptible indicator of scholarly academic status. In the education systems, lecturers' promotions remain essential as they increase academicians' job satisfaction and performance. Hence, the following hypothesis is established: Hypothesis 3 (H3): PRN positively affects the lecturer's job satisfaction (LJS). Hypothesis 4 (H4): PRN positively affects the lecturer's job performance (LJP). Performance Appraisals Performance appraisals is utilized with various names, which include performance evaluation, performance review, personnel rating, employee evaluation, and employee appraisal. Lee et al. (2022) outlined various criteria based on three main groups: teaching, research, and service for the academic professional. A focus was placed on certain education processes, such as input (e.g., staff qualification, nature of students, and material resources), processes (e.g., teaching approaches and student involvement including feedback), and output (e.g., students' qualifications, rates of employment, and staff publications) . Despite the lecturers' tasks and responsibilities, the PAL process is viewed as the guide for the lecturers to improve their teaching ability and put their utmost effort. Individual performance may now be easily tracked, and feedback is more global than ever before. Companies have started to modernize their performance management systems by implementing advanced tools such as staff monitoring software, workplace tracking devices, feedback-tracking apps, and changing their performance feedback approaches. Mwangi and Njuguna (2019) stated that technology allows managers to communicate and refresh input on a more frequent and flexible basis than traditional approaches, which are reviewed on a monthly, quarterly, and yearly basis. It was emphasized that PAL is an organizational tool to satisfy the employees and increase overall individual and organizational performance. Thus, the following hypothesis is suggested: Hypothesis 5 (H5): PAL positively affects the lecturer's job satisfaction (LJS). Hypothesis 6 (H6): PAL positively affects the lecturer's job performance (LJP). Classroom Environment Classroom environment (CET) denotes the physical characteristics of the classroom and a combination of the lighting, temperature, and other aspects such as the ventilation system, floor, walls, room size, desks, chairs, rugs, whiteboards, and computers. Universities exert effort to create an attractive classroom environment by building a strong impression and directly influencing lecturers' perceived teaching quality, which increases the overall performance. Adding safety feathers to the classroom reduces the likelihood of accidents and mishaps, which is essential for maintaining a positive learning environment. In the context of the current study, HTFT is pertinent for clarifying that a decent CET contributes to worthiness and satisfaction among educators and students during their teaching and learning processes. The HTFT also asserts that a conducive environment motivates lecturers in their daily teaching activities. Both academicians and students expect a conducive, comfortable, and attractive CET to participate in. HTFT does not only accommodate the secondary level or the primary level, but it could also be an essential factor at the university level. Therefore, the following hypotheses are proposed: Hypothesis 7 (H7): CET positively affects the lecturer's job satisfaction (LJS). Hypothesis 8 (H8): CET positively affects the lecturer's job performance (LJP). Code of Conduct Code of conduct (CCT) refers to principles and rules regulating the social institution actions toward their stakeholders and the stakeholders' (especially employees) actions toward the institution. The CCT plays a dominant role in the teaching career. Teaching comprises an exclusive set of ethical ideas and professional values that describe the ethical responsibility of conduct, which include due process, intellectual honesty, integrity, respect for privacy and dignity, and personal achievement. In the academic context, most of the research works revealed that the CCT offers guidelines for college presidents, institutional advancement officers, academic officers, and individual college and faculty members of the university on how to perform their respective roles. According to Munyengabe et al. (2017b), Rwandan academics are dedicated to their jobs, but they do not avoid using CCT to improve job satisfaction and performance. Hence, the following hypotheses are proposed: Hypothesis 9 (H9): CCT positively affects the lecturer's job satisfaction (LJP). Hypothesis 10 (H10): CCT positively affects the Lecturer's Job performance (LJP). Autonomy Autonomy (ATM) is defined as the perception of independence to use personal and professional competence at work. Some teachers view autonomy as their freedom to develop their academic qualifications for managing the classroom, while others view autonomy as freedom from obstruction and control. The HTFT positions the practices of autonomy with a sense of responsibility and accountability, which contributes to excellence in the institution academic, government, and finance. Self-empowerment among lecturers has a considerable impact on intrinsic regulation as compared to self-determined regulation. Controlled regulation, on the other hand, is unaffected. Due to its impact on lecturers' overall performance and happiness, self-directed behavior among professors should be prioritized. Therefore, the following hypotheses are suggested. Hypothesis 11 (H11): ATM positively affects the lecturer's job satisfaction (LJS). Hypothesis 12 (H12): ATM positively affects the lecturer's job performance (LJP). The Effect of Job Satisfaction on Job Performance Lecturer job satisfaction presents the employees' actual perception of the job and could exhibit the real performance at the workplace. The literature supported the argument highlighting that employees are more productive and able to play a significant role in higher organizational effectiveness. However, contradictory evidence has presented that job satisfaction may not necessarily lead to job performance. It would be noteworthy to explore the impact of job satisfaction on the lecturers' job performance in higher education. Therefore, the following hypotheses are suggested: Hypothesis 13 (H13): LJS positively affects the lecturer's job performance (LJP). The Mediating Role of Lecturer Job Satisfaction Lecturer job satisfaction estimates the feelings and attitudes of employees toward their job. It also depicts the emotional state of pleasure as a result of the judgment over individuals' jobs as achieving or enabling the achievement of their values. However, job satisfaction among lecturers is dynamically significant in impacting the success of a university vision and mission. Extrinsic motivation (FRW, PRN, PAL) and intrinsic motivation (CET, CCT, ATM) have been demonstrated as the stimulus for overall job satisfaction. Based on the above discussion, the following hypotheses are proposed: HM1-HM3: The relationship between intrinsic motivation (FRW, RPN, PAL) and job performance is significantly mediated by job satisfaction. HM4-HM6: The relationship between extrinsic motivation (CET, CCT, ATM) and job performance is significantly mediated by job satisfaction. The Moderating Role of Self-Efficacy Self-efficacy denotes individuals' innate understanding of their capability and functions as an activation force to engage in a particular situation. In the academic context, lecturers with high self-efficacy could gain achievement, use creative teaching techniques, and create a perfect classroom environment for better performance. Self-efficacy empowers the lecturer to believe in themselves, offer organized teaching efforts, and confidently engage in superior work performance. Due to their influence on how lecturers comprehend their core duties and enhance their planning and organizing abilities related to such consequences, these aspects are critical for work success. This is in line with the assumptions of cognitive theory by Albert Bandura , who believes that self-efficacy is an essential characteristic of common causation because of the impact these beliefs have on the accomplishment of tasks and related goals. Bandura (1977) stated that self-efficacy could be changed according to the situation and it varies depending on the context and task. From the lecturers' point of view, self-efficacy certainty is defined as individuals' beliefs in their capabilities to execute their teaching tasks effectively and efficiently. Based on the discussion, the following hypotheses are suggested: H: SEY positively affects the lecturer's job performance (LJP). HM7: The relationship between the lecturer's job satisfaction and job performance is significantly moderated by selfefficacy. All the associations hypothesised in the above section are presented in Figure 1. RESEARCH METHODOLOGY The current study assumed an explanatory study design established on the deductive method. A cross-sectional surveybased strategy was utilized for the current study, followed by data collection to explore job satisfaction and performance among the Malaysian lecturers working in PHEI. The target population of this study's interest comprised the academicians from Private Universities throughout Peninsular Malaysia (see Supplementary Appendix 1). Despite the fact that there are more colleges in Malaysia than universities, the focus of this study is on university lecturers; the primary rationale for this choice was the disparity in job criteria between colleges and universities. The job descriptions highlighted the differences in total teaching hours per week and the diverse roles and duties of lecturers (e.g., research, publication, consultancy, administration tasks, and academic service-related activities). Despite the fact that college and university instructors are often academic equals, university lecturers' workloads and academic service-related activities are relatively greater than those of college lecturers. Based on the following table, Malaysia's total PHEIs comprised 448 entities comprising 51 universities, 10 international branch campuses, 38 college universities, and 349 colleges. Therefore, to generalize the population of PHEIs' lecturers based on the above justification, the universities in Peninsular Malaysia (51 entities, excluding 1 university from Sabah) were selected as the representative population sample of the overall PHEIs population. The Ministry of Higher Education Malaysia [MOHE] (2018) and Jabatan Pendidikan Tinggi (2019) directories were used to compile a list of universities for this study. As demonstrated in Supplementary Appendix 2, the content in both guides is divided into categories based on the types of campuses and operations. Non-probability sampling with a purposive sample technique was used to select the total number of respondents. Calculation of Sample Size Due to the unavailability of PHEIs' lecturer directories as a whole, the overall population of PHEIs' lecturers in Malaysia was assumed to be 1,000,000 and above. As indicated through Krejcie and Morgan's sample size, a sample size of 384 was advocated for an assigned population of 1,000,000. Hence, the sample size of this study was indicated with a minimum of 384 lecturers from PHEIs in Malaysia. However, this figure was rounded up to 400 respondents for ease of analysis purposes. Next, the sample size for the current work was estimated using G-Power 3.1 with a power of 0.95 and an effect size of 0.15. The mandatory sample size for the model amounted to 160 with eight input variables. PLS-SEM required a minimum threshold of 200 samples. Data collection was conducted using an online survey from the Malaysian PHEIs lecturers with qualifying queries and acquirement of the respondents' consent. This process took place from September 2021 to November 2021. The final analysis was conducted with 343 valid responses. Research Instrument The survey instrument was developed from previous research with minimal changes to fit the study's setting and scope. FRs perception was estimated with six items derived from Chiang and Jang (2008), while the perception of the promotion was obtained with six questionnaire items adopted from Munyengabe et al. (2017b), the perception of performance appraisal was evaluated with six statements from the work by Al-Ashqar (2017), classroom environment perception was estimated with six items taken from Munyengabe et al. (2017b), the perception about the CCT was identified with six question items borrowed from the work by Munyengabe et al. (2017b), and perception of autonomy evaluated with the six statements from the work of Johari et al. (2018). The job satisfaction among the respondents was estimated with seven items taken from Munyengabe et al. (2017b), selfefficacy was gauged with the six question items borrowed from work by Sharp et al. (2013), and the job performance was evaluated with the six statements from the work by Sukirno and Siengthai (2011). The question items were measured with the five-point Likert scale. The questionnaire items for the current work with the source are presented in Supplementary Appendix 3. Common Method Bias The single factor accounted for 37.18%, which was below the suggested threshold of 50%. Therefore, the insignificant influence of CMB in the current study was approved. Another CMB test was conducted to estimate the latent factor correlation for the current model. As a result, the correlation among the latent factors was less than 0.90, which indicated the absence of the issue of CMB in the current work. The CMB evaluated the current study by testing the full collinearity of all study constructs. All the study constructs were regressed on the common variable. Variance inflation factor (VIF) values are presented in Table 1. Overall, all VIF values were less than 5.5, which demonstrated the absence of bias in the data collected from a single source. Multivariate Normality Multivariate normality for the study data was assessed with the Web Power online tool1. Based on the calculated Mardia's multivariate p-value, the study data showed a non-normality issue as the p-values were below 0.05 (Mardia's multivariate skewness = 37275, p = 0.00; and kurtosis = 43.320, p = 0.00) . Following the multivariate non-normality, the current work employed the partial least square-structural equation modeling (PLS-SEM). Data Analysis Method The current study employed the PLS-SEM procedure to confirm the projected model and assess the proposed hypotheses using the SmartPLS 3.2 tool. The PLS-SEM is suitable to be used with a small data set and to reveal the casual-predictive association between the model variables. The path model hypothesis was tested with path beta (coefficient), confidence interval, t-values, and p-values. Artificial Neural Network Analysis Artificial neural network analysis is a pseudo-investigative method encompassing three layers: input, output, and hidden. The input and output neurons are linked via the veiled layer. The hidden layer functions in the same manner as the human brain block-box. The ANN analysis is a non-compensatory diagnostic method that uses a deep learning method with three layers: input, output, and hidden. The information is separated into three categories, namely training, testing, and holding out the sample. The predictive score for the model was calculated through the comparison of the Root Mean Square Errors (RSME) for the training and testing of the model. The minor difference between the RSME scores during the training and testing of the model demonstrated the high predictive and difference of the RMSE scores between the training and testing of the model, including the low predictive accuracy. The estimation of normalized relevance for the model latent factors was identified through the sensitivity analysis. The following formula was applied in the study to gain the goodness-of-fit index: R2 = 1 RMSE SSE. DATA ANALYSIS Demographic Characteristics In the current study, 63.0% of the respondents were female. While 5% of the respondents were Bachelor's degree holders, 61.5% of the respondents obtained the Master's degree, and the remaining respondents obtained the Doctor's degree. Most of the respondents (39.9%) were 26-35 years old, 35.6% of the respondents aged between 36 and 45 years old, 16.6% of the respondents aged between 46 and 55 years old, 6.4% of the respondents aged over 55 years old, and the rest of the respondents were under 25 years old. Moreover, 75.8% of the respondents held the permanent position, while the remaining respondents had the contract position. Following that, 42.0% of the respondents received less than 5 years of working experience, 33.2% of the respondents gained 5-10 years of working experience, 14% of the respondents had 11-15 years of work experience, 8.7% of the respondents gained 16-20 years of work experience, and the remaining respondents gained over 20 years of working experience. A total of 51.3% of the respondents carried the lecturer position, 18.4% of the respondents held the senior lecturer position, 15.2% of the respondents held the senior lecturer position with a PhD, 5.8% of them carried the associate professor position, 1.7% held the tutor position, and only 0.9% of the respondents had assistant professor position. The results are shown in Table 2. Reliability and Validity The suggestions by Hair et al. (2019) and the accomplished latent construct reliabilities were assumed and assessed with the Cronbach's alpha (CA), DG rho, and composite reliability (CR). The results are presented in Table 3. The CA values for each construct were above the minimum value of 0.70, while the minimum value of the acquired CA score amounted to 0.704 . Furthermore, all the DG rho scores of each construct were above the threshold of 0.70, where the lowest value of DG rho was 0.708 . The CR scores exceeded 0.70, while the lowest CR value was 0.818 . The average value extracted (AVE) for all items for each construct should be above 0.50 to justify the suitable convergent validity and withstand the uni-dimensionality of every construct. Based on the items, the constructs had acceptable convergent validity (see Table 4). All the value inflation factor (VIF) scores of each construct were less than 5.5, which indicated that the issue of multicollinearity was not present in the current model. The discriminant validities were assessed with the Fornell and Larcker, 1981, Heterotrait-Monotrait (HTMT) ratio, and loading and cross-loading. Fornell-Larcker criterion results suggested that the current model square root of AVE for a specific construct was more significant than the correlation between the other constructs and confirmed the discriminant validity. The HTMT ratio values for the study constructs showed satisfactory scores of lower than 0.900, which depicted an acceptable discriminant validity (see Table 2). The item loading and cross-loading enabled the appropriate level of discriminant validity for study constructs (see Supplementary Appendix 4). Hypothesis Testing The model measurement assessment was conducted to examine the study hypotheses. The adjusted r2 value for the six exogenous constructs (e.g., FRs, promotion, performance appraisal, classroom environment, CCT, and autonomy) on the job satisfaction elucidated 70.2% of the variance in the individual job satisfaction. The predictive relevance (Q2) score for the part of the model amounted to 0.399, which represented a large predictive relevance. The adjusted r2 value for the job performance (e.g., FRs, promotion, performance appraisal, classroom environment, CCT, autonomy, self-efficacy, and job satisfaction) on the job satisfaction amounted to 65.4. The predictive relevance (Q2) score for the fragment of the model was 0.433, which indicated a large predictive relevance. The model standardized path values, t-values, and significance levels are presented in Table 5. The path coefficient between FRW and LJS represents a significant and positive effect of the FRs on job satisfaction. The result offered considerable statistical sustenance to accept the H1. Furthermore, the path value between the PRN and LJS indicated that the promotion positively and significantly created job satisfaction, which provided the statistical support to accept H3. The path between PAL and LJS, which demonstrated the influence of the performance appraisal on job satisfaction, was positive and significant. Thus, the support to accept the H5 was offered. The path coefficient between the CET and LJS signified the classroom environment positive and significant impact on job satisfaction. Overall, the result presented the support to accept H7. The path from CCT and LJS demonstrated a positive and significant impact of the CCT on job satisfaction, which created the support to accept the H9. Following that, the path between ATM and LJS indicated a positive but insignificant impact of autonomy on job satisfaction, which created no statistical provision to accept the H. The path coefficient between FRW and LJP represented a significant and positive effect of FRs on job performance. The result offered substantial statistical support for not accepting the H2. The path value between the PRN and LJP demonstrated that promotion positively and significantly encouraged job performance, leading to the statistical provision for not accepting H4. The path between PAL and LJP demonstrated the positive but insignificant influence of the performance appraisal on the job performance, leading to the support for not accepting H6. The path coefficient between the CET and LJP indicated the positive but insignificant impact of the classroom environment on job performance, leading to the absence of support for accepting the H8. The path from CCT and LJP demonstrated a positive and significant impact of the CCT on the job performance, which offered the sustenance to accept the H. Moreover, the path between ATM and LJP demonstrated a positive and significant impact of autonomy on job performance, which created statistical support to accept the H. The path from LJS and LJP presented a positive but insignificant effect of job satisfaction on job performance, which offered the support to not accept the H. Lastly, the path from SEY and LJP showed a positive and significant impact on self-efficacy on the job performance, leading to the acceptance of H. Mediating Analysis of Job Satisfaction The mediational analysis for the study demonstrated that the relationship between the FRW and LJP was insignificantly mediated by the LJS, leading to no support to accept HM1. Following that, the relationship between the PRN and LJP was insignificantly mediated by LJS, showing no sustenance to accept HM2. The following mediating hypothesis evaluated the relationship between the PAL and LJP, which was mediated by the LJS. However, the analysis presented no support to accept the mediation of LJS between the PAL and LJP and not to accept HM3. Moreover, the relationship between the CET and LJP was insignificantly mediated by the LJS, which showed no support for accepting HM4. The association between CCT and LJP was insignificantly mediated by LJS and offered no sustenance for accepting HM5. Then, the mediating hypothesis evaluated the relationship between the ATM and LJP, which was mediated by LJS. Overall, the analysis showed no support to declare the mediation of LJS between the ATM and LJP and not to accept HM6. Moderation Analysis The moderation analysis result demonstrated that the relationship between the LJS and LJP was significantly moderated by self-efficacy and offered evidence to admit the HM7. Multi-Group Analysis The study assessed the measurement invariance using the measurement invariance of composite models (MICOM) procedure for two groups (Group 1. Work Experience<= 10 years, and Group 2. Work Experience > 10 years). The permutation p-values for all variables exceeded 0.05, which confirmed the partial measurement invariance. Therefore, the study was able to compare the path coefficients between two groups using PLSMGA. The results of the two groups (see Table 6) based on work experience showed no significant differences in all associations hypothesized in this study, except for the effect of promotion on job performance. The effect of promotion on job performance among academicians with working experience of 10 years or less was positive and statistically significant. However, the job performance among academicians with working experience of 10 years or longer was negative and statistically significant. The difference between academicians with working experience of 10 years or more and less was also statistically significant. Overall, the results indicated that the promotion had a more significant impact on performance among young academicians compared to senior academicians. Artificial Neural Network Analysis (Model 1 and 2) The multi-layer perception (MLP) ANN was employed for the current work, which involved three layers: input, hidden, and output. The feed-forward-back propagation (FFBP) MLP ANN was employed for the study. The tenfold ANN model in the SPSS neural network algorithm was determined to curtail the overestimated issue of the ANN. While 70% of the data was utilized for training, 30% was utilized for testing as per Gbongali et al.'s (2019) suggestion. The prediction accuracy was evaluated with the RMSE score of the model. As shown in Table 7, the results exhibited high predictive accuracy as the RMSE values of training and testing segments of data, which were close to each other. The relative values of RMSE for training and testing Model A and Model B demonstrated that the data achieved higher predictive accuracy. Model A was able to predict the intention to use the MWD by 98.3% through the goodness of fit. In Model B, the goodness of fit amounted to 98.4%, with the intention to use MWD being the most significant contributing factor for the use of MWD. The evaluations are presented in Table 7. Sensitivity analysis (see Table 8) was employed to evaluate the impact of each input variable in model A to develop job satisfaction for the lecturer. The normalized importance scores for every input construct are gauged with the percentage fraction of the relative importance of every input neuron divided by the highest relative importance. As a result, five most significant contributing factors for job satisfaction were FRW, PRN, CCT, CET, and PAL, while the five most contributing factors for Model B included SEY, CCT, PRN, ATM, and LJS. DISCUSSION AND CONCLUSION The findings revealed that financial incentives, promotions, and performance evaluations had no impact on the lecturer's job performance. The current outcome was consistent with Ong et al. (2020) finding that FRs aided professors' financial needs while also fostering job happiness. However, this contradicts Koo et al. (2020) finding that financial incentives motivate employees to perform better. Similarly, the findings supported Benson et al. (2019) assertion that career advancement empowers people and fosters a sense of success, which leads to workplace satisfaction. According to Benson et al. (2019), promotion is a great approach for academics to achieve job satisfaction. This is challenged by the findings of Ekundayo and Ayodele (2019), who found that providing promotions may not be the best way to improve academicians' job performance. Promotion may no longer be a viable approach for achieving job success in academic environments. This research also suggested that performance appraisals are beneficial for promoting job satisfaction rather than job performance. The current study's findings add to the empirical evidence that Herzberg's two-factor theory of job satisfaction is applicable to boosting job satisfaction among PHEIs professors. Although extrinsic and hygienic elements are important in predicting job happiness, they are not appropriate for predicting job performance. However, this present study suggests that intrinsic rewards, classroom environment, and CCT significantly impacted the lecturer's job satisfaction. The current study sought agreement with the work by Basalamah and As'ad (2021) that the classroom facilitated the delivery of teaching instruction while the academic felt empowered and satisfied. However, the study's results emphasized that the classroom environment was insignificantly related to the lecturer's job performance. The current result was not in line with Wargocki et al. (2020). Professional conduct can be improved with the right resources available in academic settings. However, a CCT empowers the job routines as a structure that simplifies the job duties and sequences that create the lecturer's job satisfaction, which is in line with the statement by Alizadeh et al. (2020). The CCT also offers detailed work customs and requirements established to simplify job performance. It was also confirmed that the code of behavior contributes to job performance. The current result was in line with the consequence established by Maxwell (2020) that the CCT empowered the lecturer's job performance. Contrary, autonomy insignificantly facilitates job satisfaction, which is not in line with the statement posted by Muhammad et al. (2020) that autonomy is not vital for job satisfaction. However, the study demonstrated that autonomy empowers the lecturers mentally and promotes a sense of job performance. The conclusion of this study was in line with the result presented by Muhammad et al. (2020), that the perception of autonomy was related to the employees' job performance. The lecturers are knowledge workers and require autonomy as an empowerment tool. The perception of autonomy offers the control that leads to the sense of personal responsibility and accountability, which controls the perception of self-control and regulation. The current study added to the literature that self-efficacy plays a significant role in exhibiting job performance and transforming job satisfaction into job performance. The recent research also adds to the literature establishing self-efficacy as a social-cognitive force that facilitates job performance. Moreover, the current work offers the practical implications that higher educational intuitions are required to harness self-efficacy, which allows the achievement of job performance. However, workplace autonomy is not a good predictor of lecturers' job satisfaction. Therefore, lecturers require the appropriate directions and guidelines to understand the job requirements and achieve enjoyment and performance in a teaching job. The result demonstrated that the lecturer's job satisfaction insignificantly boosted the lecturer's job performance. Job satisfaction may not lead to job performance, according to the current finding, which contradicts Ong et al. (2020).'s conclusion that job satisfaction does not harness job performance. The study also found that lecturers with high self-efficacy have higher job satisfaction. The findings were consistent with those of Matos et al. (2021), who found that self-efficacy is a prerequisite for work performance. In the mediation analysis, the lecturer's job satisfaction was found to insignificantly mediate the association between intrinsic and extrinsic incentive components and job performance. The moderation study indicated that self-efficacy regulated the connection between job satisfaction and job performance. Finally, the ANN analysis was conducted to estimate job satisfaction and job performance factors. As a result, the ANN model for job satisfaction has proven that three significant factors instigating job satisfaction are FR, promotion, and CCT. The model for job performance also confirms that the three essential factors harnessing job performance are selfefficacy, promotion, and autonomy. The management of PHEIs and the Malaysian higher education ministry must monitor academicians' satisfaction and performance to increase the quality of education in the Malaysian higher education sector. This initiative would help maintain the quality of education in Malaysia and contribute to the realization of the USD 1.5 billion mark by 2026, thereby achieving Malaysia's ambition of becoming a center of educational excellence and competitive international education hub in Southeast Asia. HEIs in Malaysia are now expected to achieve higher academic excellence. Henceforward, it is also important for the HEIs to meet the international academic trends by raising the overall academic standards as well as the quality of the education. Three key limitations highlighted in this study. First, the study only employed the limiting factors as the motivational factors to develop job satisfaction by harnessing job performance. Therefore, it is suggested that future studies incorporate more relevant motivational factors promoting job satisfaction and job performance, such as emotional intelligence, mindfulness, and other personal attributes. Second, job satisfaction in work settings also requires top management support and good relationships. However, this study only presented opinions about the limited personal factors that contribute to job satisfaction. The performance expectation should be formed with the mutual consent of the parties involved, while the execution of the job role requires support from the top management. Finally, the current research assumed a quantitative stance, which led to limited generalization and demonstration of the phenomenon under study. Thus, it is suggested that future research incorporates a mixed-method approach or multi-respondent (LecturersDeans) approach to understand the lecturers' job satisfaction and performance.\",\"1135746882\":\"Introduction Intimate partner violence (IPV) is a pervasive social challenge with severe health and demographic consequences. While IPV can be perpetrated against men or women, research suggests that women are disproportionately affected. Global statistics indicate that more than a third of women have experienced IPV at some point in their lives, although there are regional and country-specific differences. In South Africa, IPV is considered a significant contributor to the country's broader problem with violence, which has sparked debates leading to law reforms and widespread activism. The 2016 South African Demographic and Health Survey (SADHS) indicates that approximately 20% of ever-partnered women have experienced physical violence, while 17% have reported emotional violence and 6% sexual violence. Overall, 26% of South African women have experienced some form of IPV at some point in their lives. IPV is also considered the leading cause of mortality among South African women and the second-highest burden of disease after HIV and AIDS. As many as 5.6\\/100,000 women are killed by their intimate partners, a rate which is perhaps the highest in the world. These statistics are troubling, considering many studies have demonstrated the adverse effects of IPV. Violence against women has been linked to several physical, mental, sexual, and reproductive health consequences. Victims of IPV are also likely to report physical injuries, psychological morbidities such as depression and post-traumatic stress disorder, gynecological disorders, as well as HIV and AIDS. Several sociobehavioural factors, most of which are linked to power relations between men and women  have been associated with IPV. There is a substantial evidence that social norms and attitudes that support violence in general  and specifically against women have contributed to the high levels of IPV. In many patriarchal communities, women are considered as subordinate to men and violence is seen as a normal way of resolving conflict where societal expectations of culturally assigned gender roles are transgressed. Thus, permissive attitudes towards gender-based violence not only condone the perpetration of IPV but also influence victims responses to violence. The risk of IPV has also been linked to socioeconomic factors. Several studies have demonstrated that women from low socioeconomic backgrounds are at higher risks of IPV , while economic autonomy has been shown to be endogenously associated with IPV. In some instances, a correlation between witnessing interparental violence and vulnerability to IPV has been reported. Specific to South Africa, a history of violence, having multiple partners, as well as the level of education, are some of the factors that have been linked to the risk of IPV. IPV has many contributing factors, and globally, there are several studies which have investigated the effect of these on determining one's vulnerability to IPV (; Chikhungu et al., 20 1;). However, most of these earlier studies have relied mainly on traditional regression analyses to identify the correlates. While regression analysis can test a priori specified effects, it cannot capture unspecified inter-relationships across factors. Machine learning (ML) methods can address these limitations since they utilize a variety of \\\"statistical, probabilistic and optimization techniques\\\" to identify hidden and complex patterns and relationships in the data. ML is increasingly being used to create algorithms that have shown to have relatively more predictive reliability than the conventional methods. Recently, ML models have been applied in many areas of medicine and public health ; however, the approach is yet to be adopted by IPV researchers using population-based data. For instance, in our literature search, we came across only two studies that utilized ML models for risk prediction and achieved acceptable predictive accuracies. Ghosh (2007) used classification trees and random forests (RF) to predict the vulnerability of ever-married women aged 15-40 years to domestic violence incidents in India. In another study where the focus was on IPV perpetration, Petering et al. (2018) used several supervised ML algorithms, including logistic regression (LR), support vector machines, RF, and neural networks, to build an IPV perpetration triage tool which could be used to identify young people who are at high risk of perpetrating IPV. The aim of this article is two-fold: first, to build predictive models that can efficiently classify women based on their likelihood of experiencing IPV; second, to identify significant factors associated with the risk of IPV among these women. We use population-based data to build an IPV vulnerability model. We build tree-based ML models, including decision trees (DT), RF, gradient boosting (GB), as well as the conventional LR model, to predict the occurrence of IPV. Models developed in this study can be used to effectively identify subgroups of vulnerable women with a high risk of IPV and improve the efficiency of interventions for such women. Data and Methods We used data from the 2016 SADHS, which was undertaken by Statistics South Africa, in collaboration with the South African Medical Research Council (SAMRC). The SADHS is a cross-sectional and nationally representative survey of the health and demographic indicators for women aged 15-49 years. Data were collected from all nine provinces using a master sampling frame (MSF) which was delineated using the enumeration areas (EAs) of the 2011 South African Census. These provinces were further stratified into the urban, farm, and traditional areas, resulting in 26 sampling strata. From these, 750 primary sampling units were designated; 468 in urban areas, 224 and 59 in traditional and farm areas, respectively. Study Participants Overall, 8,720 women aged 18 years and older, who were eligible for the domestic violence module were selected for individual interviews. Although 2,442 women agreed to be interviewed for domestic violence, we only included ever-married women who also had complete information used to generate the IPV variable. Our final analytic sample was made up of 1,816 women. Outcome Variable We defined the principal outcome variable, IPV, as any form of physical, sexual, and emotional abuse perpetrated against women by their partners. Each of the three forms of abuse (physical, sexual, and emotional) were determined by response to a set of questions (see Table 1) as measured by the SADHS. A \\\"yes\\\" response to any of questions 1-7, 8-10, and 11-13 constituted physical violence, sexual violence, and emotional violence, respectively. For our analysis, we considered any woman who responded yes to at least one of the 13 items as being a victim of IPV. We selected risk factors of IPV based on causal assumption derived from subject matter knowledge and literature review, which comprised respondents' demographic, social, economic, union, and household characteristics. These included current age, age at first cohabitation\\/marriage, spousal age difference, marital duration, place of residence, ethnicity, level of education for both the respondent as well as the partner, household wealth index, religion, number of household members, sex of household head, employment status, number of living children, partner's alcohol and drug usage, history of abuse, empowerment variables, and variables of attitude to wife beating. A full description of these variables can be found in Table 2. Statistical Analysis We first explored the associations between IPV and baseline characteristics. We utilized chi-square tests categorical variables, while t-tests were used to determine the relationship between IPV and the continuous variables (Table 2). We reported two-sided p values and statistical significance was determined at p < .05. Analysis was done in Stata v. . We developed models for IPV prediction using four ML algorithms: DT, using the classification and regression trees (CART) algorithm , RF , GB , and LR. Tree building algorithms offer a variety of flexible methods which build a common framework for CART and ensembles such as RF and GB. RF and GB algorithms were chosen for their resiliency to overfitting, relative ease in the implementation, and general acceptance in the ML community. DT was selected due to its intuitiveness and easily interpretable pictorial evidence, despite its ability to reveal complex nonlinear associations. LR, which is the commonly used discriminatory model in applied studies, was chosen as a baseline comparison. The implementation of tree-based algorithms is straightforward and intuitive. Tree-based methods generally involve stratifying the predictor space into a number of simple regions. Predictions on a given observation are typically made using the mean or mode of the observations in the region it belongs to. In its simplest form, an equivalent tree-based method, which grows only one tree, is termed a decision tree. A decision tree is a simple structure that represents how we make decisions, like an \\\"if this, then that\\\" game. The process of growing a decision tree involves taking a recursive splitting approach to tree building. From the first split (known as the root node), which contains the entire data for model fitting (also called training data), the splitting process is repeated sequentially, while achieving a topdown tree structure and the most homogeneous subregions at each variable and cut point. The splitting is repeated along the child nodes (tree branches) until a terminal node (leaf) is reached. Though simple and highly intuitive, a single decision tree does not have prediction power as high as some of the other regression and ML algorithms. RF and GB models, also known as ensemble models, build upon DT by aggregating many trees to construct more powerful prediction models. RF involve simultaneously building simple DT using subsets of randomly selected data (with replacement). Further, each tree uses only a subset of the predictors. GB, on the other hand, does not involve random sampling of data; the trees are grown sequentially: previously grown trees are used to grow each tree. Seven variables had missing data with a percentage of missingness ranging from 0.2% to 5.7%. Thus, before applying LR, missing values were imputed using tree imputation with surrogate splitting rules. The tree-based methods, including DT, GB, and RF can handle missing values internally. We developed the models using the full set of variables described in Table 2. For LR, the least absolute shrinkage and selection operator (LASSO) variable selection technique was used in conjunction with the Akaike Information Criteria (AIC) to include only variables that avoid overfitting and maximize the potential usefulness of the final model. For performance comparison, we primarily reported for each algorithm, the area under the curve (AUC) of the receiver operating characteristic (ROC) curve. Additional reported statistics included balanced accuracy, sensitivity (recall), specificity, precision, and F1 score. To avoid overfitting, we incorporated a three-way data split: all models were trained, validated, and tested on a ratio of 60:20:20 random partition of the data. The test set was chosen for evaluating performance on unseen data. Finally, in line with the study objectives, it is essential to construct an interpretable classification model. Although we are ultimately interested in the model with best predictive performance, discriminative features or characteristics for IPV can be interpretable with by algorithms such as the DT or LR. Here, we focus on the DT model. Thus, interesting splitting rules were generated using if then statements and displayed in a decision tree for decision-making. To avoid overfitting of the training data that results from very large trees, we introduced the stopping rule of a fewer than 10% of the training sample in order to limit the tree size. We performed all predictive modelling in the environment of SAS enterprise miner 14.2 software. Results Table 2 shows the descriptive statistics and univariate analysis of selected characteristics stratified by the outcome variable, IPV. Women who experienced IPV accounted for 21.9% of the total sample. Our initial analysis identified risk factors that give useful insights into IPV vulnerability. Risk factors that were significantly associated (p < .05) with IPV include ethnicity, history of spousal abuse of respondent's mother, household wealth index, attitude to wife beating, whether husband\\/partner takes drugs, whether husband\\/partner drinks alcohol, whether the respondent is afraid of husband\\/partner, husband\\/ partner's education, whether respondent decides her health care, and whether respondent decides large household purchase. The comparative performance for the prediction models is presented in Table 3. With AUC values ranging from 0.704 to 0.758, the four models were able to discriminate between respondents who are victims of IPV and those who are not. RF had the highest AUC value (0.758) and is superior to the other methods in terms of discriminatory power. In terms of specificity, RF also produced the highest value (99.7%), closely followed by GB (99.0%). In terms of balanced accuracy (65.3%), sensitivity (36.7%), and F1 score (46.5%), DT achieved the highest prediction performance. GB outperformed the other methods in terms of precision. Based on the results of the prediction performances, we observed that DT and RF models had superior performance in most of the considered metrics. However, to construct an interpretable classification model for identifying important risk factors and complex interactions among variables, we favored DT over RF. Figure 1 shows the rules generated by the DT model. Figure 1 shows that the data was first split using the variable \\\"fear of the husband\\/partner,\\\" suggesting that this was the most critical factor in determining a woman's risk of IPV. Women who feared their husbands\\/partners were more likely (49%, node 3) to experience IPV relative to 17.0% (node 2) of those who did not. Among women who did not fear their husbands\\/partners, those who justified wife beating had a higher risk (57.7%, node 4) of experiencing IPV compared to 14.8% (node 5) of those who did not. For women who did not fear their husbands\\/partners and did not justify wife beating, those whose mothers had a history of IPV were more likely (29.2%, node 8) to experience IPV. In comparison, only 11.0% (node 9) of those with the same characteristics, but whose mothers did not have a history of IPV had experienced IPV. Among women who did not fear their husbands\\/ partners, did not justify wife beating and had mothers who had a history of IPV, those whose husbands\\/partners took drugs had a higher risk (81.8%, node 15) of experiencing IPV relative to 26.1% (node 14) of those with a similar profile, but whose husbands\\/partners did not take drugs. Among women who did not fear their husbands\\/partners, those whose husbands\\/partners drank alcohol had a higher likelihood of experiencing IPV (68.9%, node 7) than 28.4% (node 6) of those whose husbands\\/partners did not drink alcohol. Again, for women who did not fear their husbands\\/partners, had husbands\\/partners who do not drink alcohol, having a mother with a history of IPV increased the risk of IPV (85.7%, node 10). In contrast, only 19.3% (node 11) of women with the same characteristics, but whose mothers did not have a history of IPV had experienced IPV. Discussion In this study, we built ML models that can efficiently identify women who are vulnerable to IPV using data from a nationally representative survey in South Africa. As far as we are aware, this is the first study to utilize ML to predict vulnerability to IPV in a South African context, as well as using nationally representative data. Our findings demonstrate that, based on all considered metrics, LR, had inferior predictive performances to the other methods. Our study results also showed that the DT and RF models outperformed the other methods. Though the RF model had a higher specificity, however, since the DT model had higher accuracy and sensitivity, it was desirable for prediction. This is because, from a health perspective of IPV, a model with higher sensitivity is more important as it can correctly identify women who are truly vulnerable to IPV. In other words, the consequence of not rescuing a true IPV victim outweighs the consequence of identifying a woman who is not a victim of IPV. Of course, the preferred model could change, keeping in mind what might be of ultimate interest for pragmatic interventions. Our findings from the DT model demonstrate informative interaction patterns that profile the characteristics of the IPV victims. In summary, the following covariates were associated with IPV: fear of the husband or partner, attitudes towards violence, history of abuse, alcohol and drug use. Essentially our findings confirm previous studies which have shown a link between IPV and most of these covariates. For instance, multicountry studies and metaanalyses have shown the relationship between substance use, including alcohol and other drugs, and IPV  due to its inhibiting and instigating forces. In one South African study, approximately 65% of the victims of IPV reported that their husbands\\/partners were drunk prior to the abuse. As mentioned earlier, there are also studies which have shown that children who have witnessed violence between their parents are more likely to grow up with views about the appropriateness of marital aggression. The splits from the root node to the branches also show the variables that have a more significant effect on IPV, in this case, the fear of the husband\\/ partner was the most significant path. The risk of IPV also varied according to these variables from 17% for women who had non-permissive attitudes towards IPV to 85% for those who had permissive attitudes, had a partner who drinks beer as well as a history of abuse. Thus, by using this ML algorithms such as DT, researchers can detect the specific combinations of factors that constitute the highest (or lowest) risk for IPV. Such models can also be used to develop interventions by stakeholders such as social workers, policymakers or other interested partners. The most striking result to emerge from our analysis is that the fear of the husband or partner was the strongest predictor of IPV amongst South African women. However, this has been the least investigated correlate in the IPV literature, especially in South Africa. Only a few studies, which were undertaken in Nepal, included fear of the husband or partner as a covariate of IPV. In general, some scholars have started speculating that fear might not be an unintentional consequence of violence, but rather something perpetrators recognize, use, and play on. It has also been opined that in the South African setting, and Africa generally, men are more likely to take advantage of women who are timid and are naturally afraid of them. Fear of the husband or partner is also common among women with low socioeconomic status, and who marry late or are fearful of divorce. When men notice this fear, they tend to take advantage of their partners and perpetrate violence because they are confident that their partners can neither leave them nor report to the police authorities. As plausible as these reasons maybe, we also acknowledge the possibility of endogeneity between the fear of a husband or partner and IPV due to reverse causality. For instance, IPV is likely to create a climate of fear where victims live in constant fear of when the next violent episode might occur. However, our analytical approach did not control for this; hence the results must be interpreted with caution. Regardless of our findings, it is crucial to emphasize that our cross-sectional study cannot guarantee accurate predictions. Hence, our findings need to be validated using additional studies such as longitudinal or cohort studies. Second, the outcome variable was measured based on self-reported responses, thus there is the possibility of recall bias on the part of the study respondents. Third, the DT model is not robust to bias resulting from other essential risk factors that were not included in the model. Indeed, several studies have identified diverse characteristics that are associated with IPV, including mental health, criminal history and so on. It is recommended that future studies might include more factors to improve the predictive power of the model. Our findings should thus be interpreted in light of the aforementioned explanation.\",\"1135746893\":\"1 | INTRODUCTION The COVID-19 pandemic changed lives. Many cohabiting couples had less contact with others, began working from the same household, and had increased household duties. From early on, researchers warned that such stressors could alter dyadic processes and lower relationship satisfaction. The Vulnerability-Stress-Adaptation model (VSA), a key model of stress and relationship processes, may help us understand relationship satisfaction during the pandemic. This model suggests that stressful events may negatively affect couples' adaptive processes and enduring vulnerabilities may amplify these effects. Researchers adapted this model to the COVID-19 context and suggested that enduring vulnerabilities-both individual (e.g., personality) and contextual (e.g., household income), pandemic-related experiences, stress, and adaptive relationship processes were all likely to shape relationship satisfaction during the pandemic. However, it is unclear which of these factors mattered most for relationship satisfaction during this historic time. The current research examined the predictive importance of these factors at the beginning and over the first year of the pandemic using machine learning. In light of VSA, we focus on five broad domains covering vulnerabilities, stressors, and adaptive processes that may have been associated with relationship satisfaction during the pandemic and its changes over time: (1) demographic and environmental factors (e.g., gender, size of home), (2) intrapersonal and interpersonal pandemic-related experiences (e.g., getting COVID-19, partner understanding of COVID-19-related worries), (3) health and wellbeing (e.g., anxiety, exercise), (4) stress and coping (e.g., perceived stress, dyadic coping), and (5) relationship processes (e.g., prior week conflict, attachment). For more details on our theorizing, see Supporting Information S1. In Study 1, we aimed to examine the importance of various constructs in these five domains in predicting relationship satisfaction. Previous research using machine learning identified attachment, age, and conflict as the top predictors of relationship quality during the pandemic. We extended this research in a large sample by examining a wider range of predictors, evaluating their predictive power within five broad domains, as well as including two indicators of relationship satisfaction: perceived changes in relationship satisfaction since the pandemic started and relationship satisfaction in the past week. In Study 2, we aimed to examine whether the machine learning-identified predictors from the beginning of the pandemic were associated with changes in relationship satisfaction across the subsequent year. 2 | METHODS 2.1 | Participants & procedure Figure 1 summarizes the data collection procedure and analytical samples for Studies 1 and 2. We collected two samples: Sample A and B. Data collection methods differed, but the survey items were nearly identical. Sample A was cross-sectional and collected primarily between April 2020 and 2 May. 1 We shared a survey on social media (e.g., Twitter, Reddit) to recruit volunteers from the U.S. and Canada. We also used paid Facebook ads to reach potential volunteers. Interested individuals completed an eligibility survey. Eligible participants were 18 or older, in a cohabiting relationship, and living in the U.S. or Canada. Eligible participants completed the survey and were encouraged to invite their partners to complete the same survey. Sample A participants were not compensated and we did not collect any contact information. We aimed for 1000 individuals and 100 couples based on feasibility. A total of 1255 participants completed the study; 957 participated in the survey individually and 149 couples participated (298 individuals). Sample B was longitudinal and collected at six time points between April 2020 and May 2021. We shared an eligibility survey through Prolific.co and invited eligible participants to complete the same survey as Sample A. Eligible participants were 18 or older, living in the U.S., and sheltering-in-place with their romantic partner at baseline (i.e., both partners not leaving home except for essential business and exercise and not working outside the home; if one partner worked outside the home, the couple was not eligible). 2 Sample B participants received $3 for participating in this survey and were encouraged to invite their partners to complete the same survey (and receive $3). Sample B participants were informed that they would be contacted through Prolific for follow-up surveys. Those who participated in follow-ups received $1.50 for each additional survey. Because we aimed to collect longitudinal data from these participants, we determined the sample size based on a priori power analyses (see Supporting Information S1). The baseline survey was collected in April 2020 (Time 1; N = 618); 316 participants completed the study individually and 151 couples completed it (i.e., 302 individuals). They were invited to participate in follow-ups starting in May 2020 (Time 2; N = 558) and every 3 months thereafter: August 2020 (Time 3; N = 429), November 2020 (Time 4; N = 365), February 2021 (Time 5; N = 297), and May 2021 (Time 6; N = 222). Participants who completed the first four surveys received a $1.50 bonus. Across samples A and B, at baseline, 1273 individuals and 300 couples participated (N = 1873). The study was reviewed by the University of Michigan Institutional Review Board and received exempt status. 2.2 | Measures 2.2.1 | Relationship satisfaction We measured perceived changes in relationship satisfaction using the single item 'Your overall relationship satisfaction' in response to the prompt: 'How has your relationship changed since you and your partner have been sheltering-in-place together...?' Responses were measured on a scale using anchors relevant to multiple items (0 = Less\\/Lower, 5 = No change, 10 = More\\/Higher). In Sample A, participants not sheltering-in-place together were instructed to consider changes since the pandemic began. We measured prior week relationship satisfaction with the single item: 'In the past week, how satisfied have you been with the following...Your relationship overall?'; 1 = Not at all, 5 = Completely). These two indicators were correlated at.55 (p < .001). Single items were used to minimize survey length and participant attrition. Single-item measures of relationship satisfaction are widely used and perform well. 2.2.2 | Predictors See Table S1 for a summary of all 90 predictors included in the models. 2.3 | Analytic plan 2.3.1 | Machine learning at baseline In Study 1, we used Sample A data and Sample B baseline data (Time 1) to understand which variables had the greatest value in predicting relationship satisfaction at the beginning of the pandemic. We used Random Forests (RF): a machine learning method that repeatedly samples random subsets of predictors and participants to build classification or regression trees. It tests the predictive value of each predictor through a process called recursive partitioning and builds decision trees using the strongest predictors. A decision tree is a set of rules that predict the outcome, such as 'if age <25 and parent = 1, then average satisfaction = 4'. It repeats this tree-building process many times using bootstrapping and then averages the resulting trees. Results reveal the predictive metric of each variable and predictors can be rank ordered based on those metrics. We used the Gini Importance metric to select the top predictors. There are several advantages of using RF: It can handle many predictors at once while minimizing overfitting. It is nonparametric, which allows it to capture nonlinear relationships, such as interactions among predictors and complex splits of predictors involving cut-points. Furthermore, it examines the overall predictive power using cross-validation, that is, the resulting tree is evaluated on a subset of data that was not used in the construction of the trees. We used RF to understand which factors predicted variability in our relationship satisfaction measures at baseline along with their relative predictive values. We ran separate RF on each relationship satisfaction indicator for five categories: (1) demographic and environmental factors, (2) pandemic-related experiences, (3) health and wellbeing, (4) stress and coping, and (5) relationship processes. 3 To account for differences in relationship satisfaction across samples, we included a code for sample (A vs. B) as an additional predictor in all models. Similar to previous work , we used a subset of predictors identified by a variable selection algorithm. This algorithm may select different numbers of predictors for each model. For more information on the variable selection and RF procedures, see Supporting Information S1. 2.3.2 | Longitudinal growth curve models In Study 2, we used Time 2 to Time 6 Sample B data to examine whether the top predictors identified in Study 1, that is, the important predictors of relationship satisfaction during the early stages of the pandemic influenced relationship satisfaction over the following year. We took the baseline measures that emerged from the RF analyses and used them as predictors in longitudinal growth curve models assessing changes in satisfaction across new T2-T6 (;-May 2021) data. The models included the linear and quadratic effects of time, and their interactions with the top baseline measures (for more information, see Supporting Information S1). Consistent with prior studies , we focused on predictors that consistently emerged as important across RF models. We used actor-only models  because the data included individuals as well as couples. 2.4 | Study 1 results 2.4.1 | Machine learning analyses Study 1 used a relatively large dataset to understand which variables had the greatest value in predicting relationship satisfaction at the beginning of the pandemic. Figure 2 summarizes important predictors in each domain, their rankings, and model performance metrics. Demographic and environmental predictors This model explained little to none of the variance in perceived changes in relationship satisfaction since the pandemic began (-1.36%) and prior week relationship satisfaction (1.21%). For both outcomes, age was the top predictor. Pandemic-related predictors This model explained some variance in perceived changes in relationship satisfaction since the pandemic began (9.87%) and prior week relationship satisfaction (28.06%). For both outcomes, interpersonal aspects of the pandemic experience were most predictive, specifically partner's perceived understanding and dismissiveness of pandemic stress. Health and wellbeing predictors This model also explained some variance in perceived changes in relationship satisfaction since the pandemic began (12.50%) and prior week relationship satisfaction (26.07%). For both outcomes, positive and negative emotions were most predictive. Stress and coping predictors This model explained some variance in perceived changes in relationship satisfaction since the pandemic began (14.68%) and half of the variance in prior week relationship satisfaction (50.65%). Top predictors differed across outcomes, but for both outcomes, important predictors included perceived stress, perceived partner stress, and multiple aspects of dyadic coping. Relationship processes predictors This model explained the most variance, explaining 41.54% of the variance in perceived changes in relationship satisfaction since the pandemic began and 70.86% of the variance in prior week relationship satisfaction. Top predictors differed across the outcomes, but for both outcomes, important predictors included feeling appreciative of one's partner and satisfaction with quality time spent with the partner in the past week. 2.5 | Study 2 results 2.5.1 | Longitudinal growth curve models Study 2 aimed to examine whether the machine learning-identified predictors at the beginning of the pandemic were associated with changes in (prior week) relationship satisfaction across the subsequent year. Therefore, we used key predictors from T1 (identified in Study 1) to predict relationship satisfaction across T2-T6. In our RF analyses, two relationship processes predictors consistently emerged as important for relationship satisfaction: feelings of appreciation and satisfaction with quality time spent with one's partner. Thus, these two variables were included as simultaneous predictors of change in satisfaction over time in the subsequent growth curve models. The findings are summarized in Table S3. Overall, those who were more appreciative of their partner or satisfied with quality time at baseline were more satisfied with their relationship across timepoints. There was also a significant interaction between appreciation and time. Specifically, those who were more appreciative at baseline had high levels of prior week relationship satisfaction that remained high throughout the year, whereas those who were less appreciative at baseline had increased prior week relationship satisfaction over time that later plateaued (Figure 3). There was also a significant interaction between satisfaction with quality time and time (Figure 3). Those who were more satisfied with quality time at baseline experienced increases followed by decreases in prior week relationship satisfaction. For those who were less satisfied with quality time, changes in prior week relationship satisfaction were not significant. 3 | DISCUSSION We used machine learning to identify factors that explained cohabiting couples' relationship satisfaction (i.e., perceived changes in satisfaction since the pandemic began and prior week satisfaction) at the start of the COVID pandemic. The identified top predictors were then used in longitudinal growth curve analyses. Variability in relationship satisfaction was best explained by proximal relationship processes such as appreciation and satisfaction with quality time, explaining up to 70% of the variance. This finding suggested that adaptive processes, as outlined in the VSA model, had the highest predictive importance for relationship satisfaction. Although there was overlap in the top predictors across the two indices of relationship satisfaction, there was also strong evidence for differentiation. For example, in the stress and coping model, perceptions of own and partner stress were the top predictors of perceived changes in satisfaction, whereas dyadic coping was more predictive of prior week relationship satisfaction. Similarly, in the relationship processes model, negative relationship processes (e.g., conflict, irritation) were more predictive of perceived changes in relationship satisfaction, whereas prior week relationship satisfaction was best predicted by positive relational processes (e.g., being on the same page, satisfaction with sex life). These findings suggest that the two outcomes differed in a meaningful way: people may focus more on negative aspects of their relationship when reflecting back on the past, but be more influenced by the positive aspects when evaluating their relationship in the present. The factors not identified as top predictors may be as revealing as those that were. For example, despite the attention paid to it , division of labor during the pandemic did not emerge as an important predictor of relationship satisfaction in these samples. This may be because the association between division of labor and relationship satisfaction depends on how appreciated people feel , a factor that did emerge as an important predictor. Our findings are also inconsistent with another machine learning project that identified attachment as a top predictor of relationship quality at the beginning of the pandemic. This discrepancy could be because our top predictors are more proximal relationship processes that can be influenced by attachment, capturing the mechanisms through which attachment ultimately shapes relationship satisfaction. Using longitudinal growth curve models, we also examined changes in the prior week satisfaction over the first year of the pandemic. Feeling more appreciative of one's partner and more satisfied with partner quality time at baseline were both associated with high relationship satisfaction. In considering these findings, there are a few limitations to note. First, some of the top predictors shared similar wording with the corresponding measure of satisfaction (e.g., changes in irritation predicted changes in satisfaction). This makes conceptual sense, but may also represent a measurement issue, particularly since similar items were presented together in matrices. However, there were also relationship processes with consistent wording that did not appear as top predictors (e.g., change in quality time was not an important predictor of change in relationship satisfaction). Moreover, some processes emerged as consistently important across outcomes regardless of wording. The current RF methods have previously been used with dyadic data , however newer methods that account for interdependence (e.g., mixed-effects RF) may be used in future research. Also, we used single-item measures to maximize participation; however, multiple-item measures may perform better. As with any survey, patterned responses are also a potential limitation. To minimize this possibility, we randomized the presentation of items within question matrices. Finally, despite being large, our sample was majority White, educated, higher-income, and Western. This limits the generalizability of our findings to other demographics and countries that experienced a more severe pandemic or had stricter lockdown measures. In sum, in a large sample of people living with a romantic partner during the COVID-19 pandemic, relationship-relevant processes explained the most variance in relationship satisfaction. Feeling appreciative of one's partner and satisfied with the quality time spent together emerged as important predictors of both indicators of relationship satisfaction, and being more appreciative of one's partner and more satisfied with quality time spent with one's partner were associated with higher relationship satisfaction over time.\",\"1135746960\":\"1 INTRODUCTION The purpose of this article is to illustrate some of the current uses of machine learning (ML) to reduce the subgroup differences by race and gender in selection decisions. Subgroup differences are critically important in personnel selec tion because they can result in differences in passing\\/hiring rates by subgroup (called adverse impact), which is illegal if the selection procedures are not job related according to civil rights laws in the United States. Evenwhen selection procedures can be shown to be job related, the presence of adverse impact may be viewed as problematic by organizations attempting to increase the diversity of their work forces. As such, some researchers are attempting to use ML to help reduce subgroup differences. In order to present several relevant studies on the topic, only brief summaries are presented in this article. Interested readers should con sult each study's Online Supplement for additional information on the study background, method, and supplemental analyses. This article presents three complementary studies of this important problem. First, using mathematical proofs as well as simulated and real organizational data, Study 1 by Zhang and colleagues shows that (nonlinear) ML algorithms that make statistical adjustments to reduce subgroup differences must create predictive bias (also called differential prediction, which is the definition of unfairness in selection science), which may actually reduce validity and penalize high-scoring racial minorities. Study 2 by Hickman et al. illustrates one approach to reducing subgroup differences that involves adjusting the input data to be equivalent between races by oversampling higher-performing minorities during ML model training. The study shows that by statistically removing subgroup differences in the training data, one can only slightly reduce the differences in the resulting ML model but at the cost of slightly reduced accuracy. Third, attempting to increase the validity of statistical predictions and reduce subgroup differences at the same time is very difficult or impossible because the two outcomes are in conflict (i.e., increasing validity often increases sub group differences of the predictor composites). Research in recent years has used Pareto-optimal analytic techniques to attempt to find the best compromise thatmaximizes both outcomes to the extent possible. The difficulty is that cur rent techniques are limited to two outcomes, while there may be three (e.g., validity, subgroup differences, and cost). Study 3 by Song et al. presents a tool for achieving optimization for up to three objectives, which hasmany applications in selection. 2 STUDY 1: ARE FAIRNESS-AWARE ML ALGORITHMS REALLY FAIR? PREDICTIVE BIAS OF USING ML IN PERSONNEL SELECTION1 Thepast decadewitnessed remarkable advances in the development ofMLalgorithms that automate the construction of prediction models. These advances attracted interest from practitioners in applying ML to orga nizational decision-making processes, amongwhich personnel selection is a prominent example. Recognizing the importance of limiting adverse impact ,ML researchers devoted considerable attention to the development of fairness-awareML algorithms , which are designed to optimize for predictive accuracy while limiting the adverse impact of predictions. When used in personnel selection, these algorithms could offer mathematical guarantees in terms of an upper bound on the adverse impact of selection outcomes. Yet what has received little attention is whether the predictions made by fairness-aware ML algorithms could suffer from predictive bias (also known as test bias or differential predictions;), that is, whether the relation ship between the ML-predicted score and the criterion of interest (e.g., job performance) could be different for one demographic group than for another. Given the considerable attention afforded to predictive bias in personnel selec tion , this omission represents a significant issue in research and practice, and is thus the focus of this study. The goal of this study is to assess the potential for predictive bias in predictions made by fairness-aware ML for personnel selection.We start with mathematical analysis showing that, unless a \\\"plain\\\"ML algorithmwith no fairness constraint already satisfies theorganizational requirement on adverse impact, predictionsmadeby fairness-awareML are almost always biased even when every predictor is free of predictive bias. Our mathematical findings also reveal a peculiar result. Contrary to the intuition that a racial minority candidate always stands to gain from the inclusion of fairness constraints, the opposite could be true for some racial minorities. Specifically, when an ML algorithm is designed to satisfy a fairness constraint, it could be inherently incentivized to \\\"guess\\\" whether an applicant belongs to a protected group. As a result, these ML predictions tend to unfairly penalize those racial minority candidates who \\\"look like\\\" racial majorities according to the predictor battery. When the mean criterion score of racial minorities is lower than the racial majorities, those racial minority candidates who \\\"look like\\\" the racial majorities could be those who are highly qualified for the job. In this case, the predictive bias of ML predictions could lead to the exclusion of these candidates who would have been selected had there been no fairness consideration in ML. In other words, the predictive bias of ML predictions could distort the selection outcomes so much that a fairness-aware ML algorithm introduces its own fairness issues in the process of reducing adverse impact. After discussing the mathematical findings, we present Monte Carlo simulation results and a case study with real world data that confirm ourmathematical findings and demonstrate the prevalence of predictive bias in the predicted scores generated by a variety of fairness-awareMLalgorithms.We conclude the studywith a discussion of its practical implications. 2.1 Preliminaries 2.1.1 Fairness-aware ML algorithms We note at the outset a distinction between the design of selection systems for personnel selection  and that of fairness-awareML algorithms.ML researchers, who aremostly computer scientists, rarely design an algorithm exclusively for one purpose such as personnel selection. Instead, they often cite \\\"non-discriminatory hiring\\\"  as one of the most important goals of fairness-aware ML, while keeping open the possibility for the algorithm to be used for other purposes such as loan allocation. Thus, while we review the existing fairness-aware ML algorithms in the context of personnel selection, it should not be interpreted as implying that they cannot be used in other relevant contexts. In general, any algorithm can be characterized by its (1) input, (2) output, (3) requirement on the output, and (4) technical design for mapping the input to the output (;, p. 5). In the passages that follow, we first review the input, output, and requirement on the output of fairness-awareML, before briefly summarizing the existing algorithms for fairness-awareML. Input. A fairness-aware ML algorithm takes as input an incumbent dataset--known as training dataset in ML- collected from current or past employees of an organization. The composition of this dataset is similar to what is required for a local validation study in personnel selection. That is, for each incumbent employee, the dataset typically includes the predictor battery, a criterion score, and whether the employee is part of a protected group. Output. The algorithm's output is a prediction model that relates the predictor battery of a candidate to a numeric predicted score, which we refer to as the ML prediction. The functional form of the model could vary widely, from a support-vector machine  to a Gaussian process. Regardless of the functional form, the prevailing assumption in ML is that a prediction model serves as a drop-in replacement for the selection process. That is, once an organization applies the prediction model to a pool of applicants, it selects those applicants with the highestML predictions. Requirement on output. A key requirement on the output prediction model is to meet an organization's desired level of validity-diversity tradeoff. That is, it needs to balance between (1) maximizing the expected criterion of selected candidates, and (2) minimizing the adverse impact of selection outcome.While adverse impact has been assessed with measures such as the adverse impact ratio , the Fisher exact test , the ZIR test , and so on, once an applicant pool and selection rate are given, a threshold on one measure can be converted to another. Thus, we focus on the measure prescribed in the Uniform Guidelines, the Adverse Impact Ratio (AIR), which is the ratio between the selection rate of the racial minority group and the racial majority group. Algorithmic design. From an algorithmic perspective, fairness-awareML falls under the general paradigm of learn ing with privileged information. Whereas the algorithm does have access to the protected variable (e.g., race) of incumbents during training, it cannot include such a variable in the prediction model because, in the context of personnel selection, the use of protected variables in prediction is generally prohibited due to legal constraints in the United States. This makes the protected variable privileged information that is only available during training. The key technical challenge in algorithmic design then becomes how to leverage such privileged information in training the predictionmodel. It is important to note the similarities in howprotected variables are used in fairness-awareML vis-a-vis traditional selection systems. Traditionally, human experts often evaluate the potential adverse impact of a selection-system design based on incumbent data (which includes protected variables for the assessment of adverse impact), andmake the appropriate adjustments, such as revising the inclusion\\/exclusion of certain predictors or changing their weights. Yet, once a selection system is put into production, it has no access to any applicant's protected variables. If we draw an analogy between the design process for a selection system and the training of an ML model, then their use of pro tected variables is almost identical. That is, protected variables are usedduring training (manual training for traditional selection systems, algorithmic training forML) but not when the selection system orMLmodel is deployed in practice. To address this challenge, the general idea in fairness-aware ML is to revise a \\\"plain\\\" ML algorithm by assigning a penalty to a potential prediction model if it violates the fairness constraint (e.g., an upper bound on AIR). The more serious the violation is, the higher the penalty would be. Since this penalty can be assessed at training time using the privileged information, a fairness-aware ML algorithm would then be incentivized to adjust the output predic tion model to avoid the penalty and satisfy the given fairness constraint. For example, Kamiran et al. (2010) revised a decision-tree algorithm, specifically the rules used by the algorithm to determine how to grow a branch, in order to minimize adverse impact. Similarly, researchers have integrated fairness constraints by revising algorithms for repre sentation learning , support-vector machines , and natural language processing. To the best of our knowledge, however, the predictive bias of predictions made by either plain ML or fairness-awareML algorithmhas not been systematically studied in the literature. Even though the selection of pre dictors has been examined, the prevailing view is to include all available predictors and leave feature selection to the fairness-awareML algorithm. 2.1.2 Predictive bias Testing for predictive bias typically involves a moderated multiple regression framework known as Cleary's (1968) method. Its precise description requires the introduction of a few mathematical notations. Let the criterion variable be Y, the (vector representation of) predictor battery beX, the prediction generated by the fairness aware ML algorithm be f(X), and the group membership be G. For the sake of simplicity, we focus on two groups, with G = 0 being the racial majority andG = 1 being the racial minority (i.e., protected) group. Consider the following linear models where a0, b0 and c0 are the intercepts; a1, b1, b2, b3, c1, and c2 are unstandardized regression coefficients; and , ', and '' are random error terms: Y = a0 + a1 f (X) + , Y = b0 + b1 f (X) + b2G + b3 f (X)G + ', Y = c0 + c1 f(X) + c2G + '', Predictive bias exists if (1) b, indicating a slope difference between groups, and\\/or (2) b, indicating an intercept difference. The third equation further specifies whether a common regression line would, on average, over (c2 < 0) or under-predict (c2 > 0) the criterion scores of racialminority candidates,with either indicating the existence of predictive bias. Statistical significance tests may be conducted directly over the regression coefficients  or over the difference in R2 between the first and second equations. In empirical literature, intercept differences are found to be more common than slope differences, with a common regression line typically overpredicting the criterion scores of racial minority candidates. Note that, even though the Cleary's method tests linear models while ML may learn nonlinear functions, it remains an appropriate method for testing the predictive bias of ML predictions because, within each group, an applicant with a higher criterion score should be assigned a proportionally higher predicted score. 2.2 Predictive bias of fairness-aware ML algorithms 2.2.1 Key source of predictive bias: Prediction target Designing apractical systemwithML is a complexprocess ; andpredictivebias could arise inmany steps along the way, from making an improper selection of the ML algorithm to a lack of sufficient training samples. Since the purpose of this study is to investigate whether the introduction of fairness constraints could induce predictive bias inML predictions, we need to ensure that our findings generalize to different implementations of fairness-awareML regardless of their specific technical design. To this end, it is helpful to consider an idealized scenario in which fairness-aware ML produces the least possible amount of prediction error. If we could identify a source of predictive bias even in this idealized scenario, then the bias would likely generalize to all practical implementations of fairness-awareML.We construct this idealized scenario with two assumptions as follows. First, the ML algorithm being used should produce prediction models that are sufficiently complex to address the prediction task at hand. For example, we would not consider the use of linear regression to fit a nonlinear predictor-to-criterion relationship, the problem of which was already noted in the literature.With this assumption, any predictive bias we identify could not be easily fixed by switching to a more complex ML algorithm such as a non-parametric Gaussian process with unlimited model capacity , which always satisfies this assumption. Second, we assume the training dataset to be sufficiently large and drawn from the same distribution as the appli cant pool. Doing so allows us to sidestep a frequently arising issue in ML called covariate shift, which happens when a prediction model trained on one dataset is used for predicting over samples drawn from a different probability dis tribution. Covariate shift may potentially incur an increase of prediction error known as generalization error. While the reduction of generalization error is an important problem inML and has been treated with methods such as importance sampling , it is tangential to our work because such errors are typically assumed to be independent and identically distributed Gaussian noise with no statistical difference between groups (;, p. 29). In other words, they are unlikely to alter the predictive bias of ML predictions. Like the first assumption, this one ensures that any predictive bias we identify cannot be easily fixed by improving (e.g., increasing the size of) the input training dataset. In this idealized scenario, an ML prediction model should be able to approximate any prediction target function that defines, according to the desired validity-diversity tradeoff, what the ML-predicted scores should look like for each given value combination of the predictor battery. Whether such a prediction target exhibits predictive bias thus becomes the key question for assessing the bias of fairness-awareML.We address this question next. 2.2.2 Existence of predictive bias To assess predictive bias, we first need to derive amathematical model for the prediction target of fairness-awareML. To this end, it is helpful to start with a \\\"plain\\\" ML algorithm in which the sole objective for candidate selection is to maximize themean criterion score u(S) = 1 |S| xS E (Y|X = x) of the selected candidates S for a given selection rate (i.e., a fixed |S|). For such an algorithm, Rambachan et al. (2020) proved that its prediction target function, denoted by f0(x), is simply the expected criterion score for the input predictor battery f0(x) = E (Y|X = x) because selecting those candidates with themaximum f0(x) is guaranteed tomaximize u(S). In other words, the overall task of candidate selection can be decomposed into the individual tasks of approximating the prediction target f0(x) for each candidate. Comparedwith this \\\"plain\\\" algorithm, the prediction target for fairness-awareML ismore complex because it needs to balance the validity-diversity tradeoff. A common strategy is to pursue Pareto-optimal  selec tion outcomes, that is, those that no other possible outcome can dominate on both expected criterion and AIR. To do so, u(S) has to be maximized under a fairness constraint that Smeets a given lower bound r on the AIR. Clearly, f0(x) is no longer a proper prediction target because selecting the top |S| candidates with the maximum f0(x) might result in AIR< r. Since it may not be possible to assess whether AIR>= rwithout first assembling S, the introduction of the fair ness constraint brings into question whether the task of candidate selection is still decomposable into approximating a prediction target for individual candidates. Fortunately, as proved in the following theorem, the Lagrange-multiplier method  provides an elegant solution that enables such a decomposition. Theorem 1. For any >= 0 and any selection rate, selecting the candidates with themaximum f(x) = E(Y ||X = x) + Pr{G = 1||X = x} is Pareto-optimal on the validity-diversity tradeoff. Conversely, given any selection rate and any lower bound r on AIR, there must exist >= 0, such that the selection outcome that maximizes f(x) also maximizes expected criterion score under the constraint of AIR>= r. Further, there is E ( Pr{G = 1 ||X = x}||G = 1 ) > E ( Pr{G = 1 ||X = x}||G = 0 ) unless Pr{G = 1|X = x} is constant for all x. Themathematical proof is available in the SupplementalMaterials. The theorem yields two insights. First, the over all task of seeking Pareto optimality is still decomposable to the individual tasks of approximating a prediction target f(x) for each candidate, where , the Lagrangemultiplier, is a function of theAIR requirement. In otherwords, Theorem 1 states that, to make a Pareto-optimal tradeoff, fairness-aware ML should produce predicted scores that resemble the prediction target f(x). Interestingly, the only difference between f(x) and the prediction target for plain ML is an additive term of times Pr{G = 1|X = x}, that is, the likelihood for a candidate to be a racial minority given the observed predictors inX. Second, the theorem suggests that, even when every input predictor is unbiased, predictive bias could still emerge in the predicted scores of fairness-aware ML because the additive term in the prediction target function, Pr{G = 1|X = x}, is systematically larger for the racial minority group in almost all cases. There are only two exceptions: 1) when = 0, or 2) when Pr{G = 1|X = x} is constant for all x. Either exception would make f(x) equivalent with f0(x)--because their difference would be either zero or the same across all candidates--implying that plain ML would achieve AIR>= r anyway. To understand themechanism throughwhich a between-group difference in the additive termmanifests as predic tive bias, consider a few idealized examples. The first is when is set to eliminate adverse impact by ensuring an equal mean ofML-predicted scores, say f (x) = F, for both groups. Since a least-squares regression line linking f(x) and cri terion Y always passes through the center-of-mass point (f(x), Y), the two groups' regression lines pass through (F, Y0) and (F, Y1), respectively, where Yi are their mean criterion. According to the inequality in the theorem, there must be Y0 > Y1 when > 0, meaning that a common regression line has to overpredict the criterion score of racial minority candidates in this example. Figure 1a provides a graphic illustration of another example where a predictor variable has no predictive bias but a slight mean difference between groups. Due to this mean difference, the additive term in f(x) (i.e., Pr{G = 1|X = x}) becomes a reverse sigmoid function with x, as shown in Figure 1b. Figure 1c shows how this reverse sigmoid function \\\"bends\\\" the prediction target f to form a nonlinear relationship with the criterion. Since this bending is, by definition, more concentrated on racial minority candidates, the resulting nonlinearity is also more pronounced for them, resulting in the predictive bias shown in Figure 1d. In this specific example, a common regression line features a smaller slope and a larger intercept than the regression line for the racial minority group, leading to, on average, an overprediction of the criterion score for racial minority candidates. The example also points to a negative consequence of predictive bias. As illustrated in Figure 1e, when > 0, fairness-aware ML has to exclude from selection some racial majority candidates who would have been selected if = 0. This exclusion is not a problem in and of itself, because it is necessary for achieving the given bound on AIR. What is problematic is that ML has no access to the group membership of a candidate. As such, it has no choice but to \\\"guess,\\\" based on the observed predictors, whether a candidate is a racial majority who needs to be excluded. Recall from Figure 1b that the likelihood for a candidate to be in the racial majority increases with their criterion score. Thus, when fairness-aware ML needs to \\\"guess\\\" the candidates to exclude, it tends to pick some candidates with a higher criterion score. Unfortunately, such a guess is imperfect, meaning that some racial minorities could be inadver tently excluded too. The exclusion of these candidates is detrimental to fairness because it means that the adoption of fairness-aware ML leads to the exclusion of some qualified racial minority candidates who would have been selected had there been no fairness consideration in the first place. In other words, in attempting to reduce adverse impact in personnel selection, fairness-aware ML could inadvertently raise its own fairness issue through the introduction of predictive bias. Next, we present a simulation study and a case study to verify the findings based on Theorem 1. 2.3 Simulation study In the passages that follow, we describe the data-generating process for the simulation study, the fairness-aware ML algorithms tested, the simulation conditions, and the simulation results, respectively. 2.3.1 Data-generating process We generated two sets of data: (1) a varying-size training (i.e., incumbent) dataset, which was used for ML to learn its predictionmodel, and (2) a 1000-record testing dataset, which was used to assess the predictive bias ofML predic tions. In terms of features, we followed Finch et al. (2009) to simulate (1) a battery of five predictors: biodata, cognitive ability, conscientiousness, integrity, and structured interview, (2) a criterion variable, which is job performance, and (3) abinary (i.e., racialmajority orminority) groupmembership representingWhite andBlack applicants, respectively. The detailed procedure and its potential limitations are discussed in the SupplementalMaterials. 2.3.2 Fairness-aware ML algorithms As detailed in the Supplemental Materials, we tested four ML algorithms, Gaussian process (GP), support vector machine (SVM), regression-tree ensemble with least-squares boosting (BOOST), and a feed-forward, fully connected neural network (NN). While no qualitative difference emerged in results across algorithms, we found the first two (GP and SVM) to consistently outperform the latter (BOOST and NN) in terms of predictive accuracy. Thus, we focus on GP and SVMwhen reporting the simulation results. To set , we performed an iterative optimization like Google's TensorFlowConstrainedOptimization. 2.3.3 Simulation conditions We varied five parameters: algorithm, AIR bound, selection rate s, training dataset size N, and between-group dif ference ratio on predictors. The last parameter (0 < <= 2) served as a multiplicative factor for the standardized between-group mean difference for each predictor. We simulated twoML algorithms: SVM or GP; four levels for the selection rate: .1, .3, .5, .8; three for the lower bound on AIR: .5, .8, 1.0; three for the training dataset size: 1000, 2500, and 5000; and three for : .5, 1.0, 2.0. Overall, our simulation design consisted of 216 unique conditions or a 2 (algo rithm)x 4 (s)x 3 (N)x 3 ()x 3 (AIR) factorial design.We repeated each condition 20 times, leading to a total of 216x 20= 4320 runs. 2.3.4 Simulation results Table A2 in the online supplement shows themarginal statistics for the prevalence of predictive bias in fairness-aware ML. In light of the different implementations of Cleary's method, we reported in the table both the regression coefficient estimates for Race and the interaction between Race and ML prediction  as well as DR2, the increase in R2 from adding Race and the interaction term as regressors. Both implemen tations identified predictive bias in anoverwhelmingmajority of simulation runs. Specifically,DR2 (M= .026, SD= .017) was statistically significant (p < .05) in 3871 (89.61%) out of all 4320 runs. Similarly, the coefficient estimates for Race (M = -.350, SD = .183) and the interaction term (M = -.054, SD = .196) were statistically significant in 3572 (82.69%) and 461 (10.67%) runs, respectively. Notably, the marginal means for both coefficients were consistently below zero across all conditions, echoing our earlier discussions that a common regression line constructed from the ML predictions would likely overestimate the criterion scores of racial minority candidates. To further examine how the simulated factors affect predictive bias, we conducted a five-way analysis of variance (ANOVA)with the dependent variable beingDR2 and the independent variables being the five simulation factors. Due to the space limit,we include thedetailed results ofANOVA in theonline supplement, and summarize themain findings here. Due to the large sample size (4320), we followed Steinley (2006) to only consider (main and interaction) effects with effect size 2 >= .05. In terms of main effects, ANOVA identified AIR (F (2,4104) = 1819.98, p < .01), selection rate s (F (3,4104) = 953.03, p < .01), and between-group difference ratio (F (2,4104) = 557.49, p < .01). TheML algorithm and the amount of training data, on the other hand, do not have a pronounced effect on DR2, consistent with our earlier finding that the predictive bias results from the intrinsic design of fairness-aware ML, specifically its prediction target, rather than the specificML implementations. In terms of the directions of the main effects, observe from Table A2 in the online supplement that DR2 clearly increases with AIR and decreases with s. In other words, as the fairness constraint becomes more stringent with a larger AIR and\\/or in a \\\"select in\\\" scenario with a smaller s, fairness-awareML has to \\\"bend\\\" the prediction target more to achieve a Pareto-optimal outcome, increasing its predictive bias. The relationship betweenDR2 and is subtler and best qualified by a three-way interaction identified by ANOVA, s x x AIR (F (12,4104) = 97.34, p < .01), as shown in Figure A1 in the online supplement. The figure yields two observations. First, DR2 was surprisingly small when = 2, seemingly \\\"capped\\\" by an upper bound of around.025. This contradicts the intuition that, to reduce adverse impact when the between-group differ ence is large, fairness-aware ML has to increase its predictive bias. Interestingly, the contradiction speaks to a limit of using DR2 to quantify predictive bias. When is so large that the prediction target is dominated by its second term, that is, the likelihood of a candidate being a racial minority, the prediction target becomes an approximate of the group membership rather than the criterion of a candidate. In this case, making the true group membership (i.e., race) a regressor alongside the prediction target adds little to the explanatory power (i.e.,R2) evenwhen its coefficient is nonzero. Indicatively, Table A2 in the online supplement shows that, despite the low DR2, all 1440 simulation runs with = 2 returned statistically significant estimates for the coefficient of Race (M=-.464, SD= .091). Second, an increaseof from.5 to1actually reducedDR2 when the fairness constraintwas loose (e.g., a lowAIR= .5 under amoderate selection rate s= .3 or.5). Upon further examination, we found a key reason to be how the Lagrange multiplier responded to an increase of : Under a loose fairness constraint, even though the higher reduced the selection rate of racialminorities, this reduced rate still met theAIR requirement. Thus, an increase of did not require an increase of the second term in the prediction target, that is, Pr{G = 1|x}. On the other hand, since a higher made it easier to distinguish between the two groups, the likelihood function Pr{G = 1|x} became higher for the racial minorities. These two changes in combination drove down and therebyDR2 in a loose-fairness regime. In sum, the simulation results showed the prevalence of predictive bias for multiple fairness-aware ML algorithms across diverse simulation settings. The results further suggested that ML predictive bias was the largest in a select-in scenario (s= .1) with a stringent AIR requirement (AIR= 1); and the smallest in a select-out scenario (e.g., s= .8) where the AIR requirement and the between-group difference in predictors are both small. 2.4 Illustration of practical impact To further illustrate the practical implications of using a fairness-aware ML algorithm in personnel selection, we examined three additional issues: (1) the existence of a validity-diversity tradeoff in the selection outcomes of fairness-awareML; (2) the consequences of the predictive bias of fairness-awareML, specifically the number of racial minorities who would have been selected by a plain ML algorithm but are excluded from selection by fairness-aware ML; and (3) the reliability of our findings over a real-world dataset. Figure 2a demonstrates how the criterion-related validity of ML-predicted scores varies with the reduction of adverse impact when a fairness-aware ML algorithm is used over simulated datasets. As can be seen from the figure, fairness-aware ML lowered adverse impact at the expense of a potential decrease in criterion-related validity. Consistent with the understanding for traditional selection systems , themagnitude of this validity diversity tradeoff was more pronounced when the input data contained substantial between-group differences. For example, with a between-group difference ratio of = 1, the criterion-related validity dropped from.48 to.37when the AIR requirement increased from.3 to.7. While the existence of this tradeoff is technically obvious (e.g., given the Lagrangian objective function in Theorem 1), it indicates that fairness-awareML is far from a silver bullet, but instead subject to the same validity-diversity \\\"dilemma\\\"  and its associated practical challenges  as traditional selection systems. To illustrate how the use of fairness-aware ML could hurt rather than benefit certain racial minorities, Figure 2b depicts the number of \\\"deselected\\\" racial minorities, meaning those who would have been selected by a plain ML algorithm but were excluded by fairness-aware ML. As can be seen from the figure, the number of such candidates increasedwith AIR, reaching over 20% of all racial minorities being selected when AIR>= .7 ( = 1). This suggests that the deselected racial minorities were \\\"sacrificed\\\" by fairness-aware ML in pursuit of a lower adverse impact because, as discussed earlier, they \\\"look like\\\" racial majorities according to the predictor battery. Another observation from the figure is that the number of deselected racial minorities was higher when the input data contained larger between group differences. This is consistent with our earlier discussions that, the more pronounced the validity-diversity tradeoff becomes (thanks to the larger between-group differences), themore likely it is for fairness-awareML to incur predictive bias when reducing adverse impact. Finally, to examine the reliability of our findings over a real-world dataset, we tested fairness-awareML algorithms over a dataset containing the results of pre-employment tests used for entry-level positions in a Fortune company. While we defer details of this case study to the online supplement, themain finding was that predictive bias remained prevalent over real-world data, evenwhen thedataset features racialmajority andminority candidateswith very close criteriondistributions (specifically, a standardizedbetween-groupmeandifferenceof.11).While the smaller between group differences led to a milder validity-diversity tradeoff and, in turn, a smaller magnitude of predictive bias (e.g., as measured by DR2), the predictive bias identified over the real-world dataset featured a more pronounced slope difference than the simulation results, suggesting that fairness-aware ML likely had difficulty \\\"guessing\\\" the group affiliation of high-criterion candidates, leading to a flatter regression line for racial minorities. 2.5 General discussion Our findings are important because they speak to a substantive problem of using fairness- aware ML as a drop-in replacement for selection systems. Such a practice, while convenient, could lead to unintended consequences such as predictive bias. As summarized in Table 1, an important reason is the complexity of prediction models produced by ML. With a traditional selection system, prediction models (e.g., a weighted sum of predictors) take functional forms with only a few parameters (e.g., weights). This allows researchers and practitioners to directly inspect a model and identify potential problems. For example, if regressing criterion scores over predictors returned a negative weight for a cognitive ability test, a natural responsewould be to scrutinize the test design rather than to just deploy themodel in practice. With an ML algorithm, however, a prediction model may easily contain millions of parameters learned from data. While such added complexity affords ML models with better accuracy, it also makes the model prohibitively expensive to scrutinize manually. This is compounded with the prevailing view in ML that tasks like predictor selection should be done automatically by the algorithm rather than manually by experts. As a result, an ML prediction model could easily circumvent desirable, fairness-related, properties--for example, the absence of predictive bias--so long as these properties are not explicitly specified in the optimization goal. This is exactly what we observed in this study. There are twoways to address this problem.One is to further thework of InterpretableML (e.g., LIME;) to make ML algorithms and models open to manual scrutiny. Unfortunately, a wide gap exists betweenwhat is deemed \\\"interpretable\\\" inML andwhat could be properly examined in personnel selection. For exam ple, theprevailing view inML is that decision trees are interpretablebecause they canbeexpressedas (long) sequences of if-then-else statements. Yet the adoption of decision tree in personnel selection means that the predictor battery used for different candidates may contain different predictors, be applied in different orders, and feature different cutoffs. All these variations raise new research questions in terms of their legal defensibility in the personnel selection context, for which our understanding is still nascent. Thus, to pursue this direction, we believe the participation of I-O psychologists is urgently needed in the future development of interpretableML for personnel selection. The second way to address the problem is by formulating fairness notions, like the absence of predictive bias, as mathematical constraints that can be formally entered into an ML algorithm. Recent work in ML has already started formulating fairness constraints beyond adverse impact, to include notions such as the statistical parity of predictive accuracy between groups , the assurance that no protected group under one selection system would overwhelmingly prefer another system (i.e., \\\"envy-freeness\\\";), and so on.While handling mul tiple such constraints is technically feasible (e.g., the method of Lagrangemultipliers could assign a different to each constraint), the feasibility does not mean one could achieve all these constraints without increasing adverse impact or reducing the expected criterion score of selected candidates. Indeed, many of these constraints are conflictive with each other  or other legal requirements (e.g., privacy;), even without considering criterion. In personnel selection, this conflict could lead to thorny questions. For example, is it better to condone predictive bias when removing it leads to an increase of adverse impact? Like the first solution, the future research for this one is also in urgent need of I-O psychologists' participation, not only in using ML to further organi zational research , but also in developing fairness-related constraints and understanding the mathematical tradeoffs between them. As both solutions require long-term research and development, we offer some suggestions that may apply in the short term. A popular belief among ML researchers is that it is always better for an ML algorithm to take in as many predictors as possible--regardless of whether a predictor exhibits predictive bias--and count on the algorithm to sort out the proper use of these predictors and to achieve the desired fairness properties. While this belief is mathemat ically correct when adverse impact is the only fairness concern , the literature of personnel selection has long noted the limitation of having a singular focus on reducing adverse impact when seeking a diverse work force. Our study further shows the danger of relying on fairness-awareML for adverse impact reduction. For example, our simulation results indicate that predictive bias tends to be greater when the AIR requirement is more stringent. This suggests that, while fairness-aware ML can indeed reduce the adverse impact of selection outcomes, in doing so it might also be forced to incur other types of fairness concerns, such as predictive bias. Our study also suggests that one way to address these concerns is to carefully attend to characteristics of the selection system, such as the selection rate and the composition of the predictor battery. For example, our simula tion results indicate that predictive bias could be greater in highly selective scenarios, or when there are considerable between-group differences in predictors, especially under stringent AIR requirements. To this end, we submit thatML researchers and practitioners should not consider fairness-aware ML algorithms as silver bullets that work on any and all predictors, but carefully study the empirical evidence in the I-O psychology literature in determining which predictors to use in the context of personnel selection. 3 STUDY 2: OVERSAMPLING HIGHER-PERFORMING MINORITIES DURING MACHINE LEARNING MODEL TRAINING REDUCES ADVERSE IMPACT SLIGHTLY BUT ALSO REDUCES MODEL ACCURACY2 \\\"Is it going to have a disparate impact on different protected classes? That is the number one thing employers using artificial intelligence should be looking out for.\\\" - EEOC Commissioner, Keith E. Sonderling  Organizations are rapidly adopting tools that use artificial intelligence andML formanypurposes, including person nel assessment and selection (;Hickman et al., 2022;). However, significant concerns have been raised throughout society regarding the fairness and ethicality of ML assessments. In the United States, a key legal concern for ML assessments is that person nel selection decisions that cause adverse (or disparate) impact--substantially different hiring rates between groups that disadvantage a legally protected group --constitute prima facie evidence of employment discrimination. Several algorithmic solutions that adjust models to achieve equal group outcomes have been proposed to address group disparities in ML assessments , but many provide the final ML model with demographic information explicitly (e.g., by using demography as a predictor) or implicitly (e.g., by creating separatemodels for each group) during test administration. Both are likely illegal in the United States because they constitute disparate treat ment  and\\/or subgroup norming  during test administration. Therefore, there is a pressing need to advance our understanding of the causes of and potential (legal) remedies to ML model adverse impact. MLmodels tend to reflect subgroup differences in applicant attributes in the training data, which are then reflected in the ML model predictions. We investigate whether this tendency can be used to our advantage by examining whether removing (i.e., equal selection ratios) or reversing (i.e., selection ratios flipped to favor disadvantaged group members) subgroup differences in the training data reduces ML model adverse impact without sacrificing accuracy. To do so, we utilize a data preprocessing approach known as oversampling--techniques for resampling observations to address class imbalances --to manipulate adverse impact ratios in the train ing data. Then, we systematically examine how this affects the adverse impact and accuracy of ML models that use self-reports and interview transcripts to predict historical screening decisions. The present study contributes to the literature on employment discrimination in several ways. First, we answer the special issue call to investigate adverse impact in artificial intelligence andMLpersonnel selection systems. Second,we answer calls to test the effects of oversampling minority groups to enhance diversity in training data. We do so in a real-world, high-stakes dataset where adverse impact and group representation can be directly evaluated and altered. Oversampling to balancemeans and sample sizes has been shown to have small positive effects onMLmodelmeasurement bias  defined as equal accuracy across groups , butwe are unaware of any studies of oversampling's effects on adverse impact. By doing so with both self-reports and interview transcripts, our study addresses the fairness of both traditional andmodern selection systems. Further, we investigate the effects across a variety of text mining vectorization techniques andML algorithms. This allows us to estimate the effect of oversampling on adverse impact across a variety of MLmodeling approaches, reducing the chances that any observed effects are algorithm-bound. Third, we compare multiple oversampling strategies to inform future research andpractice. Specifically, we compare the effects of (a) adjusting training data adverse impact versus adjusting training data adverse impact and equalizing sample sizes, as well as (b) oversampling real versus synthetic applicants. Doing so provides nuanced answers regarding how different oversampling methods affect the adverse impact and accuracy of MLmodel screening decisions. 3.1 Indices of adverse impact Adverse impact is often operationalized as an adverse impact (AI) ratio--or the ratio of the selection ratios of two subgroups. Selection ratios (SRs) are calculated as the number of applicants hired in a subgroup divided by the total number of applicants from that subgroup. The AI ratio is calculated by dividing one subgroup's SR by another subgroup's SR. Adverse impact is concernedwith equality of outcomes. Themost common standard for identifying practically sig nificant adverse impact and prima facie evidence of discrimination is the four-fifths rule, or that the SR of members of one legally protected subgroup should not be less than four-fifths the SR of members of another subgroup.3 Therefore, AI ratios should exceed.80. TheAI ratio indicates the effect size of group differences in SRs and is commonly used, although significance testing is also relevant to discrimination claims. We chose to focus on the AI ratio because even minor subgroup differences in SRs become statistically significant when sample size is in the thousands, as in the present study. 3.2 Origins of discrimination in ML models MLmodels and their predictions reflect existing patterns in their training data. Therefore, to the extent that discrim ination and\\/or adverse impact exist in the personnel data used to train ML models, the ML models may reflect those historical patterns. We now turn to summarize the standard ML model development and evaluation process, as illustrated in Figure B1 in the online supplement, and then explain the relevant sources of ML adverse impact that motivate our oversampling approach. 3.2.1 Supervised ML in personnel assessment Most ML assessments rely on supervisedML, which involves training an algorithm to predict some known individual level outcome, such as historical screening or hiring decisions. To do so, individual behavior must be observed in an evaluative situation. Human observers then, either using the in situ behavior or amore holistic process involving addi tional information (e.g., resumes, cover letters), rate applicants and\\/ormake selection decisions. Amachine \\\"perceiver\\\" then observes and quantifies individual behavior, whether this behavior is performance in an evaluative situation (e.g., an interview), on a self-report scale, or on a test (e.g., of cognitive ability). For example, in automatically scored inter views, theunstructured, natural languageof interviewee responses is transcribed, vectorized, andused in an algorithm to predict the outcome of interest. DuringMLmodel development, researchers often test multiple predictor-algorithm combinations. For example, in textmining, researchersmay try outmultiple vectorization techniques (i.e., methods for quantifying unstructured text data, such as closed and open vocabulary;). To do so, the data are split into training and test datasets (e.g., Year 1 and Year 2, or k-fold cross-validation), the algorithm is fitted (or trained) on the training data, and the resultingMLmodel's accuracy is estimated on the test dataset. The predictor-algorithm combination with the highest cross-validated accuracy is often trained on all available data (i.e., both the training and test data). This final MLmodel is applied to future, unseen cases. However, group differences in training data may affect theMLmodel, as reflected in the model parameters and its predictions. Two training data disparities affect ML models: (1) group mean differences on the outcome variable; and (2) differential representation, or underrepresentation of a subgroup. Regarding groupmean differences, the concern is that if groupmean differences in the training data are not representative of the population of applicants to which the model will be applied, this may alter themodelweights in away that favorsonegroupof applicants over another. In our study, groupmeansequal their SRs, and therefore, groupmeandifferences represent adverse impact.Weexpect that training data AI ratios will affect ML model AI ratios, such that ML models trained with equal subgroup SRs (AI ratios = 1) or with subgroup SRs favoring the subgroup with the lower SR in the observed data (AI ratios > 1) will likely exhibit less adverse impact thanmodels trained on data where AI ratios< 1. Differential representation occurs when subgroups are unevenly represented in training data. In cases where the predictor-outcome relationships differ across groups, unequal representation in the data will cause the algorithm to primarily reflect the most prevalent patterns in the training data--or those of majority group members. Therefore, we also investigate the effects of equally representing subgroups in training data, as it has been proposed elsewhere as away of enhancing fairness and validity. 3.2.2 Diversity-validity tradeoff: ML edition Making adjustments during ML model training to enhance fairness may negatively affect the model's convergent validity (in our study, accuracy;). The so-called diversity-validity tradeoff is analogous to this concern. The tradeoff occurs because some highly valid predictors of job performance (e.g., multiple choice cognitive ability tests) exhibit large subgroup differences, whereas selection procedures with smaller subgroup differences (e.g., personality traits) tend to less validly predict job performance. One common suggestion for addressing the so-called diversity-validity tradeoff is to find equally valid selection procedures with smaller subgroup differences. Therefore, if oversampling to remove adverse impact in training data enhances ML model fairness without sacrificing convergence\\/accuracy, doing so may be preferable to training an ML model on raw historical data. With these considerations in mind, our study addresses the following research questions: Research Question 1a-b: How does adjusting the training data AI ratios affect ML model AI ratios when using (a) self-report scales, (b) text mined interview transcripts, or (c) both self-reports and interview transcripts to predict screening decisions? Research Question 2: How does equally representing subgroups (i.e., equalNs) in training data affectMLmodel AI ratios? Research Question 3: How does oversampling real versus synthetic observations affectMLmodel AI ratios? ResearchQuestion 4: Howdoes oversampling to remove adverse impact in training data affectMLmodel accuracy in the test data? 3.3 Method Figure 3 summarizes the present study's methods, andmore detail is provided below and in the online supplement. 3.3.1 Sample Participants in our sample applied for US-based positions in a female-dominated service industry. The sample consists of 2501 applicants (71.9% female, 36.6%White, 28.3% Black or African American, 19.1%Hispanic, 6.3% two or more races, 4.1%Asian, and the remaining demographic groups each comprised<1% of the sample). 3.3.2 Machine learning model predictor variables Text mined interview transcripts. Participants recorded their answers to five interview questions using an online video platform, and computer software transcribed their responses.We applied six common vectorization techniques to convert the interviews to vectors for use as predictors in the ML models, as detailed in Figure 3 and the online supplement. Self-report survey scores. The self-report survey included 16 proprietary multi-item, bipolar scales developed for the purposes of job selection that measure constructs including sociability, work ethic, and analytical mindset. These self-reports were scored in twoways: (1) as raw numerical scores and (2) as percentile scores based on norms derived by the survey vendor. The online supplement reports the scales' reliability and validity. 3.3.3 Outcome variable (screening decisions) Weused the organization's screening decisions as the outcome variable. The decision is binary: applicantswho passed proceeded to the next stage of the hiring process (screened in), and applicants who failed did not (screened out). The overall SR= .494,White applicant SR= .60, non-White applicant SR= .43, Black applicant SR= .46, andHispanic appli cant SR = .37. These SRs result in Non-White\\/White AI ratio = .72, Black\\/White AI ratio = .77, and Hispanic\\/White AI ratio = .62. A baseline model that always guesses \\\"screened out\\\" would have accuracy = .506, and this forms the 'baseline' accuracy against whichMLmodel accuracy is judged. 3.3.4 Train and test data splits for machine learning models To compare multiple predictor-algorithm pairs, we created a stratified three-fold split of the raw data to conduct k-fold cross-validation. Specifically, we split the data into k = 3 folds such that White, Black, and Hispanic appli cants had consistent Ns and SRs in each fold, thereby maintaining the original data's properties. Across the three folds, White SR = .60, Non-White SR = .43, Black SR ranged from.45 to.47, and Hispanic SR ranged from.36 to.38. The three folds ranged in size from N = 832 to 835. For all experiments, we trained ML models on two folds then assessed them on the third fold and repeated the process three times, using each fold only once for testing. In total, we trained and tested: 5 (the adjusted training data AI ratios) * 2 (adjusting SRs or SRs and Ns) * 2 (over sampling real or synthetic observations) * 154 (11 algorithms * 14 sets of predictors) + 154 (the 11 algorithms * 14 sets of predictors on the raw data) = 3234 models in each of the three folds, or 9702 models trained and tested. The output of our experiments used in all analyses in the manuscript and online supplement is available on OSF: https:\\/\\/osf.io\\/c46sp\\/?view_only=2ffb2172f8274968bf720429812deae.3.5 Algorithms We trained a variety of common machine learning algorithms, as detailed in Figure 3 and the online supplement. No one algorithm is optimal for all tasks (i.e., no free lunch theorem;), and these represent a sample of commonly used ML algorithms. We conducted hyperparameter tuning for each algorithm in each set of training folds of the original data, as detailed in the online supplement. We fully crossed these algorithms with the two methods for scoring the self-reports, the six text mining approaches applied to the interview transcripts, and the combined predictor set of the six text mining approaches plus the raw self-report scores. This allowed us to estimate the effect of oversampling on model outcomes across a variety of predictor-algorithm pairs, thereby ensuring that our results generalize across many ML models. Tables B8-B in the online supplement report the average accuracy obtained on the raw data when SR= .50 for all algorithms, predictors, and predictor-algorithm pairs, respectively. 3.3.6 Oversampling ratios We used under- and oversampling to investigate the relationship between training data AI ratios and ML model AI ratios in the test data. Prior to model training, we under- and oversampled minorities in the training data to achieve Black\\/White and Hispanic\\/White AI ratios ranging from.60 to 1.40, stepping by.20. In all cases, we kept Black and Hispanic SRs in the numerator. To achieve training data AI ratio = .60, we undersampled passing Black and Hispanic applicants to reduce their SRs. To achieve AI ratios= .80 to 1.40, we oversampled passing (i.e., screened in) Black and Hispanic applicants until the desired AI ratio was achieved. 3.3.7 Oversampling strategies We investigated two oversampling strategies: (1) oversampling to adjust SRs and (2) oversampling to adjust SRs and equalize sample sizes. The former case is described in the \\\"Oversampling Ratios\\\" subsection. In doing so, Black and Hispanic sample sizes increased by the number of cases added to achieve themanipulated AI ratio. To equalize sample sizes, we multiplied the White N by the desired SR (as determined by the desired AI ratio), then we (a) oversampled passing Black and Hispanic applicants (respectively) to reach those values and (b) oversampled (or undersampled, if necessary) Black andHispanic applicants who failed untilWhite, Black, and Hispanic applicantNs were equal. 3.3.8 Oversampling techniques We, (1) oversampled real observations with replacement or (2) oversampled synthetically generated observations. We used the Synthetic Minority Over-Sampling TEchnique (SMOTE;) to generate synthetic (a) screened in Black applicants, (b) screened in Hispanic applicants, (c) screened out Black applicants, and (d) screened out Hispanic applicants. Figure B2 in the online supplement illustrates how SMOTE works. We used the first two categories to adjust AI ratios and all four categories when adjusting AI ratios and equalizing sample sizes. 3.3.9 Test data selection ratio As a robustness check, we analyzed our results at different overall SRs in the test data: .10 and.50. To do so, we had the ML models output class probabilities (i.e., continuous values ranging from 0 to 1) instead of binary predictions. Then, to achieve overall SRs= .10 and.50, we set the highest 10% and 50%, respectively, of the class probabilities to 1 (pass\\/screen in) and the remaining values to 0 (fail\\/screen out). AI ratios are more likely to violate the four-fifths rule as the overall SR decreases. SR = .50 is very similar to the observed SR in our data, and SR = .10 represents amore competitive (e.g., later stage) selection procedure. 3.4 Results To investigate our research questions, we treated the 9702 sets of algorithmic predictions as observations for analysis and measured their accuracy and AI ratios at overall SR= .10 and.50. In the raw data, the models that used interview transcripts to predict screening decisions tended tobemore accurate thanmodels that used self-reports as predictors, and themodels that used both interview transcripts and self-reports tended to be nomore accurate than the interview models. Amongmodels that used: interview transcripts as predictors, AccuracyMax =69.6% (averagedacross the three folds); self-reports as predictors, AccuracyMax = 60.2%; and combined predictor sets, AccuracyMax = 70.6%. ResearchQuestion 1 concerns the effect of training dataAI ratios onMLmodel AI ratioswhen screening applicants in the test data. Table 2 reports the average ML model accuracy and AI ratios in the raw data and at each manipu lated training data AI ratio for models that used self-reports (top), interview transcripts (middle), and both interview transcripts and self-reports (bottom) as predictors (Tables B5-B7 in the online supplement report the same informa tion at overall SR = .10). On average, among models that used self-reports as predictors, changing training data AI ratios from.6 to 1.4 caused the ML model AI ratios to increase from a minimum of.11 (Hispanic\\/White AI Ratios) to a maximum of.16 (Black\\/White AI Ratios). Among models that used interview transcripts as predictors, the average increases were smaller, ranging from a minimum of.04 (Hispanic\\/White AI ratios) to a maximum of.07 (Black\\/White AI Ratios). On average, amongmodels that used both sets of predictors, the AI ratios increased from aminimum of.06 (Black\\/White AI ratios) to amaximumof.08 (Non-White\\/White AI ratios). These findings that the effects were largest among models that used self-reports and smallest among models that used interview transcripts as predictors align with themagnitude of correlations between training data AI ratios andMLmodel AI ratios reported in Table B1 in the online supplement for each predictor set. Thus, for all three predictor sets, training data AI ratios affect ML model AI ratios. Notably, however, the effects were sometimes small in magnitude. Table 3 and Figure 4 report the ML model AI ratios for the most accurate model from each predictor set because these are the models likely to have been selected for subsequent use. For example, the most accurate model included the Latent Semantic Indexing (LSI) operational ization of interview transcripts plus self-reports as predictors and used linear discriminant analysis for prediction. When it was trained on the raw data where Black\\/White AI ratio = .77, and Hispanic\\/White AI ratio = .62, it exhib ited an average Black\\/White AI ratio= .715 and Hispanic\\/White AI ratio= .611, whereas when it was trained on data where AI ratios were adjusted via oversampling to equal 1, it exhibited an average Black\\/White AI ratio = .747 and Hispanic\\/White AI ratio= .649. Research Question 2 regards whether equalizing subgroup Ns further enhances ML model AI ratios. Tables B2-B4 in the online supplement report the average ML model accuracy and AI ratios, respectively, for ML models that used self-reports, interview transcripts, and the combined predictor set in each experimental condition at overall SR = .50 (Tables B5-B7 in the online supplement report the same at overall SR = .10). AI ratios tended to increase by about.01 (although did not always do so) frommanipulating SRs and equalizingNswhen compared to onlymanipulating SRs. These findings alignwith the correlations between a dummyvariable forwhether SRs or SRs andNsweremanipulated and theMLmodel AI ratios, in that the correlations show aminimal positive effect of equalizingNs beyondmanipulat ing SRs. Overall, equalizing sample sizes tended to exert minimal, positive effects beyond manipulating training data AI ratios. Research Question 3 regards whether different effects are observed from oversampling real versus synthetic observations generated by SMOTE. As reported in Tables B2-B4 in the online supplement, these effects tended to be even smaller in magnitude than the effect of manipulating SRs versus manipulating SRs and equalizing Ns. Further, as reported in Table B1 in the online supplement, the effects were mixed across predictor sets, such that synthetic observations increased Non-White\\/White and Black\\/White AI ratios among models that used self-reports as predic tors but decreased themandHispanic\\/WhiteAI ratios amongmodels that used interview transcripts or both interview transcripts and self-reports as predictors. Therefore, oversampling real or synthetic observations provided similar effects. Research Question 4 addresses the tradeoff between ML model accuracy and adverse impact. Table 3 reports the most accurate models' accuracy when trained on the raw data versus when training data AI ratios = 1, and Figure 4 illustrates the tradeoff between accuracy and Non-White\\/White AI ratios for these models when training data AI ratios = .6, .8, 1.0, 1.2, and 1.4 (on average across the other conditions). As Figure 4 shows, oversampling to adjust training data AI ratios tended to slightly decrease model accuracy. For example, for the models that used LSI and self reports as predictors, AI ratios increased by.100 when training models on training data AI ratios = 1.4 compared to training on the raw data, and accuracy decreased by.027. Among models that used only LSI as predictors, AI ratios increased by.091 when training models on training data AI ratios = 1.4 compared to training on the raw data, and accuracy decreased by.006. This aligns with trends reported in Table 2when all models' outputs were examined. 3.5 Discussion Adverse impact is a foundational concern for ML-powered selection tools, as they receive heightened scrutiny from applicants and policymakers. The present study investigated the effects of oversampling high-performing minorities, a technique being explored by data and computer scientists , on ML model AI ratios. Removing or reversing adverse impact in training data increased ML model AI ratios while reducing ML model accuracy, although the effect sizes were small. 3.5.1 Theoretical and practical implications Although adequate representation in training data is important for developing ML models that are equally accurate across demographic groups , equal representation had veryminor, positive effects onMLmodel AI ratios in our study. This may be because oversampling minority success already affected differential representation in our training data. Indeed, to create training data AI ratios = 1.0, 1.2, and 1.4, we necessarily oversampled many minority applicants, thereby increasing their sample size beyond the number ofWhite applicants. Therefore, equal representation may have independent, positive effects, but our adjustments to training data AI ratios may have suppressed them. Further, the observed effectswere similar regardless ofwhether real or synthetically generated observationswere oversampled. This is encouraging, because although binary classification problems tend to benefit more from over sampling synthetic than real observations , there is something uncanny about using synthetic observations for personnel assessment. We encourage future work to continue to check if this holds true in other studies, but the current findings suggest that practitioners can reduce ML model adverse impact to a similar degree regardless of whether they oversample real or synthetic observations. Although oversampling to adjust training data AI ratios slightly increasedMLmodel AI ratios, doing so also tended to slightly reduce ML model accuracy. This issue is analogous to the so-called diversity-validity dilemma  and a known limitation of methods of enhancing algorithmic fairness. Such decreases in accuracy limit the potential practical value of oversampling. Importantly, however, ifMLmodels perfectly replicate historical human decisions, they could not reduce adverse impact. Future work is needed to determine how such adjustments affect validity and predictive bias, as these are more important than replicating historical decisions and our data did not include workplace outcomes. 3.5.2 Limitations and future research The generalizability of our findings to other personnel selection situations is limited by two primary properties of the dataset. First, our study focused on a subset of potentially useful predictors for personnel selection (i.e., self-reports and interview responses), yet many selection systems may have a broader array of predictors available, such as bio data and cognitive ability. Second, the sample size in our study is rather small for real-world ML applications, as they may be based, in practice, on tens of thousands of observations. The relatively low accuracy of our ML models may have enhanced the magnitude of oversampling's effects, whereas more accurate models may exhibit smaller effects from oversampling. Both a broader array of predictors and a larger sample size could potentially increase ML model accuracy and alter the effects of oversampling. Due to the small effects of oversampling, future research should investigate additional approaches for addressing MLmodel adverse impact. For example, removing predictors that are predictive of groupmembership holds potential for reducing adverse impact , as does reducing the weight given to such predictors. Future research is needed to determine the effects of these and other approaches in high-stakes settings. The effects of oversampling high-performing minorities rely on an assumption that subgroup differences in pre dictors will be consistent in new data. If, however, subgroup differences on predictors were inconsistent between the training data and subsequent applicants, then training data AI ratios may have a weaker relationship with ML model AI ratios. This suggests another route to addressing adverse impact, regardless of whetherMLmodels are used for assessment: enacting societal change to reduce subgroup differences in job-relevant qualifications (i.e., predic tors). Mean racial subgroup differences in job-relevant qualifications begin at a young age , and without change, they may persist in society for another 90 years or more. Addressing sub groupdifferences in job-relevant qualifications is themost direct route for reducing adverse impact across assessment methods. 4 STUDY 3: MULTI-OBJECTIVE OPTIMIZATION FOR PERSONNEL SELECTION: A GUIDE, TUTORIAL, AND USER-FRIENDLY TOOL7 Organizations oftenwant to optimizemultiple hiring objectives simultaneously, yet doing so can be difficult, especially for conflicting objectives (e.g., cost and effectiveness; job performance and diversity). As a result, there is a growing need to develop robust methods for making hiring decisions that consider multiple objectives. To date, however, most analytic approaches are limited to optimizing just one objective at a time (e.g., cost or effectiveness, instead of cost and effectiveness). Multi-objective optimization (MOO; a.k.a., Pareto-optimization) is a promising machine learning approach that can help organizations optimize multiple objectives. It generates predictor weights that optimize the value of one objective at given levels of the other objective(s); organizations then choose the set of predictor weights that best fulfills their needs and values. MOO has been used in personnel selection to address the diversity-validity dilemmaby deriving hiring solutions that can asmuch as double the proportion ofminority hireswhilemaintaining the expected job performance of the new hires. Nonetheless, existing MOO applications that are readily available for personnel selection are limited in two ways: (a) they have only been applied to optimize two objectives, and (b) those two objectives have always been task per formance and diversity (in the form of adverse impact;). Yet, organizations are often concernedwithmultiple hiring objectives beyond task performance and diversity, such as the likelihood of early turnover  and organizational citizenship behavior (OCB;). This study aims to provide a generalized MOO guide and tool that help users optimize multiple objectives in per sonnel selection. By doing so, we aim tomake two contributions. First, we provide a guide to generalize the application ofMOO to a wide range of objectives and enable the use ofMOO in situations beyond the diversity-validity dilemma. The guide details how to useMOO for different personnel selection applications by explainingwhat types of problems are best addressed with MOO, how to define MOO problems, and how to implement, evaluate, and monitor MOO selection systems. Second, we introduce a user-friendly online application (Multi-Objective Selection Tool, MOST; https:\\/\\/orgtools.shinyapps.io\\/MOST\\/) to help awide range of users exploreMOOwithout requiring complex computer programming ormathematical knowledge ofmachine learning algorithms.We hope this studywill foster the adoption ofMOO and, thereby, improve hiring outcomes. 4.1 Multi-objective optimization and current organizational applications As the term \\\"multi-objective optimization\\\" suggests, the MOO approach consists of a variety of algorithms that simultaneously optimize multiple objectives. MOO is useful any time the objectives are in conflict--or whenever the objectives cannot be simultaneously optimized. Buying a used car, for example, can be consid ered a MOO problem. Suppose our objectives for a car purchase are to (1) minimize price and (2) minimize mileage. Because price tends to decrease as mileage increases, there is a conflict between the two objectives, making it a typi calMOOproblem.Our purchase decisions are also often bounded by some conditions. For instance, wemay onlywant to consider minivans; in MOO, this is called an equality constraint (i.e., body type = minivan). Also, we may only want to consider cars with a fuel efficiency of at least 16 miles per gallon; in MOO, this is called an inequality constraint (i.e., gas mileage >=16 miles per gallon). Constraints reduce the range of feasible solutions (e.g., car options) to ones that are most likely to meet our needs. Thus, a typical MOO problem consists of a set of objectives (e.g., minimize price, minimizemileage) and, often, some equality constraints (e.g., body type=minivan) and inequality constraints (e.g., gas mileage>=16miles per gallon). The goal of MOO is to identify Pareto-optimal solutions. A Pareto-optimal solution optimizes one objective, at a certain level of the other objective(s). In our car buying example, the Pareto-optimal solutions include minivans with the lowest price given amileage, aswell asminivanswith the lowestmileage given a price. The choice of a final solution depends on the preference of the buyer. For instance, among all the Pareto-optimal minivan options, the buyer might decide to select theminivan that has the lowest mileage among all themedium-priced options. The example above concerns discrete choices of used cars. Inmany applications (e.g., personnel selection), the final solution is a set of predictor weights. For example, MOO has been used to address the diversity-validity dilemma in personnel selection. Thediversity-validity dilemmaconcernshowcommonpersonnel selec tion predictors and procedures (e.g., cognitive ability tests) that validly predict job performance also tend to engender adverse impact. This creates aMOO problemwhere organizations have to optimize two conflicting objectives (in this example, diversity andvalidity).Oneway toaddress this problem is to assigndifferentweights topredictors (e.g., struc tured interview, personality assessments) so that the resulting weighted predictor composite exhibits high validity in predicting job performance and low adverse impact. MOO uses a data-driven approach to generate multiple sets of predictor weights, each of which optimizes one hiring outcome (e.g., adverse impact ratio [AI ratio]) at a given level of the other outcome(s) (e.g., job performance). The final choice of predictor weights depends on organizational needs and values. For instance, organizations focused on complying with the four-fifths rule  may select the solution that maximizes the expected job performance with expected AI ratio greater than or equal to.80; while organizations focused on enhancing social equality may select the solution that maximizes the expected job performance with the expected AI ratio equal to 1.00 . In summary, MOO is useful when the objectives are in conflict with each other, which occurs when two or more objectives \\\"cannot be optimized by exactly the same weighting of the available selection predictors\\\" (;, p. 1382). By analyzing the relationships between the predictors and multiple objectives simultaneously, MOO provides a set of optimal solutions that organizations could choose from based on their specific needs and values. 4.2 Growing demand in personnel selection for optimizing multiple objectives There is a growingdemandamongorganizations to simultaneously optimizemultiple objectives in personnel selection. Recently, interest has increased in performance criteria beyond task performance, such as OCBs and CWBs , and non-performance criteria, such as turnover and employee well-being. OCBs con tribute to, and CWBs detract from, positive organizational functioning ; high rates of voluntary employee turnover harmorganizational outcomes ; and employeewell-being influences job satisfaction, job performance, and retention. However, it is often difficult to optimize these objectives simultaneously, as they are not perfectly related and sometimes are negatively or non-linearly related (e.g., curvilinear relationship between job performance and retention;). Thus, a strategy that optimizes one objectivemight not be optimal for, and could even hinder, another objective. MOO can help organizations develop selection systems that optimize multiple hiring objectives. In the sections below, we provide a guide, a point-and-click R Shiny app, and an R package for obtaining MOO solutions for general personnel selection purposes. 4.3 Guide for implementing multi-objective optimization for personnel selection MyriadMOOalgorithms exist for obtainingPareto-optimal solutions,which are summarized in the online supplement. In this study, we focus on the normal boundary intersection (NBI) algorithm developed by Das and Dennis (1998), which has been most commonly used in personnel selection. Table 4 provides a checklist with the key steps for adoptingMOO in personnel selection, and the online supplement provides a step-by-step example demonstration. 4.3.1 Stage 1. Define the MOO problem The first stage is to define andoperationalize the hiring objectives. Hiring objectives can be informedbyorganizational needs and values; for example, an organization may aim to improve employee diversity, retention, and performance. Stage 1 also typically involves setting the optimization goal (i.e., to what extent each objective should be optimized). For instance, some organizations may choose to select solutions that allowmore favorable retention rates only to the extent that expected task performance is not substantially decreased; other organizations may choose to maximize new hire task performance andOCB only to the extent the adverse impact risks are low. Thehiringobjectives then inform the choiceof predictors. Eachpredictor should individually demonstrate evidence of job-relatedness, through job analysis and content or criterion-related validity.8 As each predictor can be assessed in multiple ways, the most suitable method of assessment can be determined with validation studies, (lack of) overlap with other assessments, and practical considerations. For example, should interpersonal skills bemeasured using situational judgment tests or structured interviews? Are there assessment tools that are already available, or do they need to be developed? One should also examine practical and legal considerations related to the selection predictors, such as constraining predictor weights to be non-negative to properly reflect job analysis results. 4.3.2 Stage 2. Obtain MOO solutions The second stage focuses on obtaining MOO (or Pareto-optimal) predictor weighting solutions. Multiple algorithms are available for implementing MOO, which could be broadly classified as a priori and a posteriori algorithms. If the optimization goal is clear, a priori algorithm should be used; if it is not clear, then a posteriori algorithm should be used (see online supplement for details). MOO algorithms are supervised machine learning algorithms that train models (i.e., develop predictor weighting solutions) using calibration\\/training sample data. For example, when MOO is used for hiring, the calibration\\/training sample consists of employees with criterion data (e.g., job performance ratings), and the target\\/testing sample is the applicants. MOOmodels generate multiple sets of predictor weights that optimize each objective at given values of the other objective(s). Fromthem, theuser chooses the solution (i.e., predictorweights) that best satisfies theoptimization goals. In other words, MOO generates possible solutions; and from those solutions, the organization selects one based on their values, goals, and business necessity. Even when the optimization goal is loosely defined (e.g., to improve all three objectives to a reasonable degree), one can still narrow down the solution space based on the goal. They can, for instance, identify a subset of solutions with higher expected new hire job performance, retention rate, and AI ratio than the current practice, which can be presented to the organizational decision-makers for further consideration. When possible, users should conduct a pilot trial to (a) evaluate the predictor weights in the target sample of inter est and (b) identify any preparations needed to integrate theMOO system into selection practice. Specifically, collect predictor information from a pilot sample (e.g., applicant sample), use theMOOpredictor weights to identify individu als who might be selected with theMOO selection system (but not yet use them to make actual hiring decisions), and evaluate the hypothetical hiring outcomes (e.g., whether the AI ratio satisfies the four-fifths rule). In addition, examine practical needs such as communication (e.g., how to explain to stakeholders) and training (e.g., how to train recruiters and hiringmanagers). 4.3.3 Stage 3. Implementation The MOO solution is then implemented to make hiring decisions. Specifically, collect predictor information from the job applicants and, for each applicant, use the MOO predictor weights to calculate weighted predictor composites. The weighted predictor composite scores can be used to rank order job applicants to aid compensatory selection. They can also be used in conjunction with non-compensatory methods (e.g., minimum cut-score requirements), for instance, by first selecting out applicants that did not meet the minimum cut-score for certain predictors (e.g., edu cation requirement, licenses, work experience requirements) and rank ordering the remaining applicants based on weighted predictor composite scores. 4.3.4 Stage 4. Maintenance The MOO selection system should be maintained through continuous validation. In each round of implementation, the hiring outcomes should be evaluated to determine whether they still satisfy the optimization goals and detect any changes in the hiring outcomes. For example, are the new hires' job performance and 6-month retention rates similar to previous implementations? Does the AI ratio still satisfy the four-fifths rule? Are there any changes across different locations or times of the year? When the validation suggests a need to revisit the selection system, the selection sys temmust be re-evaluated, starting from Stage 1. This procedure supports the continued effectiveness of the selection system. 4.4 Multi-objective selection tool (MOST): A user-friendly tool to implement MOO The MOST online application (https:\\/\\/orgtools.shinyapps.io\\/MOST\\/) is a user-friendly and freely available R Shiny application that uses theNBI algorithm  to estimate predictorweights for optimizing three hiring objectives. We also provide a corresponding R package, \\\"rMOST\\\" that is available via the Comprehensive R Archive Network (CRAN; https:\\/\\/cran.r-project.org\\/) repository. Figure 5 provides an example MOST Shiny app interface and the online supplement provides a detailedmanual for using the app. 4.4.1 Procedures to use the MOST Shiny app Step 1. Define the MOO problem. The first step in implementing MOO in personnel selection is to define the hir ing objectives (see Table 4). Common hiring objectives generally fall into two categories: adverse impact objectives and non-adverse impact objectives. Adverse impact objectives relate to the proportion of selected applicants from legally protected groups (e.g., women, racial\\/ethnic minorities) relative to the proportion of selected majority appli cants, and they are commonly operationalizedwith AI ratios. Non-adverse impact objectives include all other objectives, such as dimensions of employee performance (e.g., task performance, OCB, CWB); they are commonly operationalized as supervisor ratings, peer ratings, andwith objective employee records. The MOST app can optimize predictor weights for: (1) three non-adverse impact objectives (\\\"No Adverse Impact Objectives\\\"), (2) two non-adverse impact objectives and one adverse impact objective (\\\"One Adverse Impact Objec tive\\\"), or (3) one non-adverse impact objective and two adverse impact objectives (\\\"Two Adverse Impact Objectives\\\"). Use the \\\"Optimization Problem\\\" drop-down menu to select one of the three options (see Figure 5). As the optimiza tion function does not allow negative predictor weights , MOST requires that the predictors and non-adverse impact objectives be operationalized such that greater values indicate more desired outcomes (e.g., emotional stability instead of neuroticism; retention instead of turnover). Step 2. ObtainMOO solutions. Prepare input statistics. MOST (which implements NBI) takes the predictor intercorrelations and the predictor objective relationships as input statistics, both of which can be estimated from the calibration sample. The predictor intercorrelations are represented by a correlation matrix of the predictors. The predictor-objective relationships are criterion-related validities (for non-adverse impact objectives; i.e., the correlation between each predictor and the measure of an objective) or subgroup difference (for adverse impact objectives; i.e., the standardized predictor mean scoredifference [Cohen'sd] betweenminority andmajority groups). For personnel selection applications, the intercor relations and criterion-related validity need to be corrected for range restriction and criterion unreliability to reflect the relationships in the applicant sample (; see online supplement for recommended resources for range restriction correction). To specify the inputs in MOST, begin by entering the number of predictors to be used in the field labeled \\\"Number of Predictors.\\\" Next, enter the predictor intercorrelations in the table labeled \\\"Predictor Correlations.\\\" The values entered into the table must be between-1 and 1. Simply click on a cell in the table and enter the relevant correlation; the predictors will be labeled \\\"P\\\" (for predictor) followed by a number representing their order in the table. Then, enter the relationships between the predictors and the objectives in the \\\"Predictor-Objective Relation ships\\\" table. The non-adverse impact objective will be labeled \\\"C\\\" (for criterion) followed by a number representing their order among the non-adverse impact objectives in the table (e.g., \\\"C1\\\"); these predictor-objective relationships should be entered as correlations between -1 and 1. The adverse impact objectives will be labeled \\\"AI\\\" (for adverse impact) followed by a number representing their order among the adverse impact objectives in the table (e.g., \\\"AI1\\\"); these predictor-objective relationships should be entered as standardizedmean subgroupdifferences of the predictor scores (subgroup d), with positive values favoring the reference group. If there are two adverse impact objectives, the subgroup ds entered should be the standardized mean difference between a minority group and the same reference group (e.g., both beminority groups compared to theWhite group, such as Black-White andHispanic-White subgroup d's). If the MOO problem has \\\"One Adverse Impact Objective\\\" or \\\"Two Adverse Impact Objectives,\\\" MOST will display additional fields requesting further input information. In both cases, theuser needs to enter the expectedoverall selec tion ratio of the selection system (a value between 0 and 1) in the \\\"Overall Selection Ratio\\\" field. In addition, if the problem has \\\"One Adverse Impact Objective,\\\" the user needs to enter the expected proportion of the minority sub group in the applicant pool (a value between 0 and 1) in the \\\"Proportion Minority for AI1\\\" field. If the problem has \\\"Two Adverse Impact Objectives,\\\" the user needs to enter the expected proportions of the twominority subgroups in the applicant pool in the \\\"ProportionMinority for AI1\\\" and \\\"ProportionMinority for AI2\\\" fields. ObtainMOOpredictor weights. After all the inputs are entered, click the \\\"Get Solution!\\\" button.MOSTwill visualize theMOO solutions in a 3-dimensional plot on the top right section of the screen and provide the predictor weights as well as the expectedhiring outcomes (expectedAI ratio and composite validities) associatedwith each solution, both in the plot and in a table on the bottom right section of the screen. The solutions in the table can be sorted in descending or ascendingorderbasedoneachcolumn,which facilitates solution comparisonand selection. In addition to the results for the three-objective solutions, for each of the three pairs of objectives, MOST also provides the results of two objective optimization solutions generated via NBI (e.g., C1 & C2, C2 & C3, C1 & C3). The user can obtain more information on each solution by hovering the cursor over a point on the plot. For each point on the plot,MOSTdisplays the solution number (which corresponds to the solution number in the table), predictor weights, and the expected hiring outcomes. The user can use both the plot and the table to select a solution that best fits their needs. 4.5 Discussion In this study, we highlighted organizations' growing demand to optimizemultiple objectives in personnel selection and described howMOOcan address this demand.We then introduced a guide for implementingMOO in personnel selec tion and provided a user-friendly online application and R package to help organizations implementMOO to optimize three hiring objectives. 4.5.1 Utility of MOO over common personnel selection practices To help readers consider the utility of MOO given their specific hiring scenarios and needs, we conducted a sup plemental exploratory study to investigate the factors influencing the utility of MOO. Details of the study and recommendations are provided in the online supplement. Although the utility ofMOOultimately depends on the spe cific hiring scenario and the user's needs, the results of our exploratory study suggest that MOO tends to be more useful when predictors are differentially related to the objective. That is, MOO tends to be more useful over other predictor weighting methods when predictor-objective relationships vary widely across predictors; and MOO tends tobe less useful over othermethodswhenpredictor-objective relationships donot varymuchacross predictors.When the predictor-objective relationships moderately vary across predictors, different users may find MOO useful to dif ferent degrees. Importantly, even in conditions where MOO does not seem to add much value, users can still choose to useMOO, asMOOgenerally performs (at the least) similarly to other predictorweightingmethods.MOOsolutions outperform unit weighting solutions; andMOOprovides the same set of predictor weights as regression weighting at the endpoints (in addition to a range of optimal solutions that onlyMOO [but not regression] can provide). 4.5.2 Shrinkage considerations with MOO and recommendations When applying optimizationmethods such asMOOand regressionweighting, wemust consider shrinkage. Shrinkage refers to decreases in the weighted predictor composite's validity when predictor weights derived from one sam ple (calibration sample) are used in another sample (validation sample). In personnel selection, predictor weights are oftenobtained froman incumbent sampleor archival data (calibration sample) andused to select applicants (validation sample). Previous studies  suggested that shrinkage exists forMOOsolutions.MOO shrinkage is influencedby calibration sample size andmagnitudeof theexpectedhiringoutcome. First, shrinkage tends to be largerwhen the calibration sample size is small. Song et al. (2017), examining optimization of two objectives--job performanceanddiversity--found that shrinkagewas sizable for a compositeof commonselectionpredictors (biodata, cognitive ability test, conscientiousness, structured interview, integrity test)when the calibration sample sizewasat or below 500; and shrinkagewas sizable for a composite of cognitive subtest predictorswhen the calibration sample size was at or below 100. For a composite of common selection predictors, when calibration sample size was 500, validity shrinkage (difference in calibration and validation sample job performance validity) ranged between.00 and.01, and diversity shrinkage (difference in calibration and validation sample AI ratio) ranged between.00 and.08. In contrast, when calibration sample sizewas 100, validity shrinkage ranged between-.01 and.03, and diversity shrinkage ranged between.00 and.43 (; Table 3). Second, for a particular objective, across MOO solutions, shrinkage increases to the extent the objective is being maximized. In other words, there tends to be the most shrinkage for objective C1 in the solution where C1 is maximized and the least (or no) shrinkage for C1 in the solution where C1 is least maximized. As an example, for a composite of common selection predictors, the diversity shrinkage (in terms of AI ratio) was as high as 1.24 (calibration sample size= 40, from AI ratio= 2.15 to.91; Table 3) for the solution where diversitywasmaximized butwas approximately 0 (across all calibration sample size conditions) for the solutionwhere diversity is least maximized (or where job performance was maximized; Table 3). Because the MOO solutions that maximize certain (single) objectives (i.e., the endpoints) are akin to regression solutions maximiz ing that same objective, compared to regression solutions, MOO solutions tend to be less or similarly susceptible to shrinkage [for the objective maximized by regression]. Specifically, compared to regression solutions that maximize a certain objective, MOO solutions at the endpoints are similarly susceptible to shrinkage [for that objective] while MOO solutions between the endpoints are less susceptible to shrinkage. Based on these findings, we provide several recommendations regarding shrinkage when using MOO. First, when possible, use large calibration samples (e.g., with more than 100 or 500 individuals, depending on the predictors;). Large calibration sample size can help reducemodel overfit (or capitalizing on chance), thus reducing shrinkage. Second, consider complementingMOO in conjunctionwith other approaches to improve hiring outcomes. For example, to ameliorate the diversity-validity dilemma in personnel selection and enhance orga nizational diversity, users could seek to develop and use predictor measures with smaller subgroup differences (;Hough et al., 2001) and adopt recruitmentmethods that enhance diversity. Finally, consider using shrinkage formulas to approximate cross-validated hiring outcomes. MOO shrinkage for mulas are recently developed to approximate cross-validated MOO (NBI) outcomes when optimizing two objectives. The shrinkage formulas are developed on the basis that (1) classic shrinkage formulas  can be used to approximate cross-validated outcomes on the MOO solutions at the endpoints (e.g., solution where C1 is maximized); and (2) the solutions between the endpoints can be interpolated (based on the specific MOO algorithms). Although previous studies suggested that shrinkage formulas are effective for NBI-based MOO applications optimizing two objectives, future studies are needed to examine whether the MOO shrinkage formulas could be generalized to other scenarios (e.g., other MOO algorithms and optimizing more than two objectives). Such work will be instrumental for selection practices usingMOO. 4.5.3 Future directions MOO is a promisingmachine learningmethod to advance both organizational practice and research. In addition to the personnel selection applications exemplified in this study,MOOcould be used in a number of otherworkplace applica tions. In practice, organizations often face a tradeoff between the cost and validity of a selection system.While some selection procedures (such as structured interviews and assessment centers) have high validity in predicting job per formance, they could also be costly to develop and administer; and other inexpensive procedures (such as personality assessments) may have lower validity. MOO could be used to address this practical concern by reducing the cost of a hiring design while optimizing diversity and validity. Specifically, one can useMOO to optimize three objectives: cost, job performance, and diversity hiring outcome. The cost objective can be operationalized as the sum of the cost of the predictors with non-zero weights, and the validity and diversity objectives can be operationalized as described ear lier (e.g., job performance validity, AI ratio). With appropriate algorithms and operationalizations, MOO can provide solutions with a select set of predictors that minimize cost, at a given level of job performance validity and diversity outcomes. In addition to improving organizational practice, MOO could be used as a research method to advance the the oretical understanding of workplace phenomena. Examples include expanding the predictor space--to explore new predictors that contribute to optimizing multiple organizational objectives. Existing methodologies (e.g., regression) that can only analyze one outcome at a time have restricted the historical focus in personnel selection to predictors that have high correlationwith task performance, such as cognitive ability, structured interviews, and biodata. The vast majority of our understanding of personnel selection predictors are informed bymeta-analyses, regression, and structural equation modeling studies that examine the criterion-related validity, composite validity, and\\/or incremental validity of predictor(s) in predicting a single workplace outcome--for example, job performance or retention. Our field as a whole has limited knowledge of how different predictors influence multiple workplace outcomes simultaneously-- for example, job performance and retention.MOO,with its ability to systematically exam ine multiple objectives, holds promise to unveil more holistic predictor-criterion relationships. With MOO, we can identify novel predictors of important work outcomes and develop methods to enhance multiple hiring objectives simultaneously--a broader, diverse collection of predictors holds potential to improve overall hiring outcomes.\",\"1135747006\":\"1. Introduction In this article, we compare machine-learning-based, ordinary least squares, and summative approaches to scoring a forced-choice image- based assessment of personality, which we previously reported on the creation and validation of. While in recent years new ways of scoring forced-choice assessments have been developed that can overcome issues associated with traditional forced-choice scoring approaches , these are typically for multidimensional measures. Since our measure has a combination of unidimensional and multidimensional items, these methods have limited applicability. As such, we previously used machine-learning-based scoring algorithms to overcome these challenges. Here we extend this work, examining how the use of different predictor combinations in different models impacts the validity of the measure. We begin by examining the significance of personality and how it is measured, both using traditional and more contemporary approaches, before narrowing our focus to image-based and forced-choice measures. We then describe the development of the models and evaluate their performance in terms of convergent and discriminant validity with the IPIP-NEO-120 and generalisability from the training to test data. In line with prior research , we conclude that machine-learning-based approaches outperform other scoring approaches and that they are a viable alternative option for scoring forced- choice assessments. 1.1. Measuring personality An individual's personality has significant implications for many aspects of their life, including their wellbeing, social relationships, health, and career success. Indeed, the Big Five personality traits (openness to experience, conscientiousness, extraversion, agreeableness and neuroticism or emotional stability) are routinely tested in pre-employment screenings due to their ability to predict future job performance. While self-report methods, such as the International Personality Item Pool (IPIP;) scales and the NEO-PI R , have been the default method of assessing personality until recently, self-report scales are associated with poor response quality  and incomplete responses due to respondent attrition , particularly if scales are lengthy. Self-reported measures of personality are also associated with social desirability bias or faking , especially in high-stakes contexts, where respondents inflate their scores more compared to respondents completing the assessment in low stakes contexts. This has implications for the use of personality assessments in high stakes contexts like recruitment, where candidates may attempt to inflate their scores to appear more favourably. 1.2. Alternative ways of measuring personality To overcome some of the issues associated with self-report measures, some have proposed using daily adjective-based measures of specific personality traits to avoid issues with one-time measurements  while others have proposed more contemporary measures to predict personality from a wide range of sources. For example, personality has been predicted using the facial expressions of individuals in YouTube videos  and video interviews  and audio (speaking activity and prosody) and non-verbal cues (looking activity, pose and body movements) of vloggers in YouTube videos based on annotations of personality. Others have inferred personality, based on observational ratings, from video resumes using features such as speaking activity, prosody, head motion, full face events, and overall motion. Moving away from video analysis, personality has also been predicted from mobile phone data including calls and text frequency, GPS data and text response rate , as well as through eye movement while running errands  and Facebook Likes. Others have used a text-based approach, using language to predict personality. While this is nothing new given that the Big Five model of personality was derived from language analysis , contemporary approaches use non- traditional sources of language such as social media posts instead of essays or descriptions of people and combine them with natural language processing computational techniques. For example, based on the frequency of word use and clusters of topics mentioned in Facebook status updates, personality has been predicted using latent Dirichlet allocation, a natural language processing technique used to cluster words into related topics. Such approaches, therefore, move away from the need for self-report, reducing the influence of faking and allowing personality to be measured automatically , although impression management on social media is not uncommon  so measures could still be affected by social desirability bias. Something that is also gaining traction is image-based assessments of personality, which ask respondents to select images from a predefined set and use these choices to predict personality through machine- learning-based algorithms. Assessments of this format are being offered by commercial providers (e.g., HireVue and Traitify), and are also being investigated in the literature. For example, Leutner et al.' (2017) image-based measure of creativity presented respondents with sets of images and asked them to indicate which image was most like them. Using predictive algorithms, they predicted creativity scores on traditional scales, finding that the assessment could accurately measure curiosity (r = 0.35), cognitive flexibility (r = 0.50) and openness to experience (r = 0.50). Although this measure only assessed the openness to experience personality trait, more recent image-based measures have explored measuring all five traits through image choices. For example, Krainikovsky et al. (2019) presented respondents with 300 images tagged with information relating to the behavior, objects, emotions, and scenery in a set of images and asked them to select 20 to 100 preferred images. Based on the percentage of chosen pictures which related to specific tags, they predicted Big Five scores, with convergence with the NEO PI ranging from r = 0.06 for neuroticism to r = 0.28 for agreeableness. More recently, an image-based assessment of personality designed for use in selection used image choices from pairs of images mapped to the Big Five traits to predict personality, with convergence with the IPIP-NEO-120 ranging from 0.60 for agreeableness to 0.78 for extraversion. The advantage of image-based assessments is that they are language neutral, meaning that they can remove language barriers to make the assessment more accessible to individuals like those with dyslexia. They can also be taken by people speaking different languages without having to create language specific items , like is needed with questionnaire-based measures. Images appear to elicit stronger preferences from respondents than questionnaire-based measures , which could potentially contribute towards more rapid measurement if less time is spent deliberating. In addition, if the image- based measures are gamified  by adding features like sound effects and progress bars , this can have additional benefits since game-based assessments elicit less test- taking anxiety , are generally quicker to complete , and can be more engaging for respondents. Assessments in this format can, therefore, overcome some of the issues associated with traditional self-report measures. This is particularly beneficial in the domain of recruitment since applicant perceptions of a selection process can influence how likely they are to accept a job offer , which can have implications both for candidates and hiring managers who could potentially miss out on top talent if they have an unengaging recruitment process. 1.3. Forced-choice assessments Forced-choice assessments vary from traditional self-report assessments in that they ask respondents to indicate the responses that are most and least like them, instead of asking them where they lie along a scale. Assessments of this type typically have two or four response options, with the most common format being multidimensional i.e. showing blocks of response options with statements from multiple constructs. In the context of personality, a respondent could be shown statements relating to multiple traits and asked to identify the statements they identify with most and least. Assessments of this format can prevent central tendency and extreme response styles  since there is no midpoint, as well as acquiescence responding, where respondents select both positive and negative statements, since they are not able to endorse all of the statements presented to them. Further, they are more resistant to faking than traditional measures , which is particularly important in high- stakes contexts like recruitment. The most typical way of scoring two-item forced-choice measures is by allocating a score of 1 when a positive item is selected (e.g., high extraversion) and a score of 0 when a negative item is selected (e.g., low extraversion). When the measure is multidimensional, featuring positive statements about two different traits, the selected statement is given a score of 1 and the unselected a score of 0. Therefore, the total score for that trait is calculated by summing the number of positive items selected relating to that trait. When the blocks have more than two statements and respondents are asked to indicate the most and least like them, the item selected for most is given 2 points, the unselected 1 point, and the least favoured 0 points. Again, scores for each construct are calculated by summing the number of points relating to each trait. Due to the fact that multidimensional forced-choice measures result in ipsative scales, where the score on one dimension is relative to another dimension and the overall score for each respondent across the constructs is the same, concerns have been raised about how well individuals can be compared since it is impossible to score above or below the mean score for all constructs. However, mixed-dimensional forced-choice formats, or those combining multidimensional and unidimensional items, are not prone to ipsative scoring in the same way as fully multidimensional measures as they behave more like a traditional questionnaire-based method. To combat the issue of ipsative scores with multidimensional measures, alternative ways of scoring forced-choice measures have been proposed, based on Item Response Theory (IRT) and Thurstone's framework for comparative data. These models, which are estimated by structured equation modeling, are more similar to traditional models that would be seen with Likert-scale-based measures, with structured factor loadings and uniqueness , allowing the scores to be better compared between individuals (; for an overview of the approach). Indeed, comparisons of traditional and IRT-based scoring of forced-choice measures found the IRT scoring to perform better across multiple types of forced-choice measures in terms of the correlation between the true score and that estimated by the traditional and IRT approaches. However, such approaches are typically focused on fully multidimensional measures due to their ipsative nature, with fewer efforts being focused on scoring approaches for measures that use a combination of unidimensional and multidimensional items. 1.4. Predictive scoring In contrast to questionnaire-based measures, many contemporary measures of personality use predictive scoring algorithms, using data either from alternative assessments or unstructured data from free text or videos, for example, to predict personality scores on traditional measures. Models of this type, that predict a specified outcome, are said to be supervised. This is in contrast to unsupervised learning, where algorithms are used to identify clusters in the data, with no specified target variable (; for an overview of the types of learning). Since the personality scores are known, these approaches therefore use supervised learning. While it is possible to use ordinary least squares regression (OLS) to predict scores, predictive measures typically have a large number of predictors, meaning there is a small n\\/p ratio. As a result of OLS being designed to minimise the sum of the squared difference between the actual score and predicted score, this can lead to overfitting of the model to the data it was trained on, particularly at small n\\/p ratios. Since the assessment will be taken by individuals other than those the model was trained on, this can result in the model performing poorly when applied to other samples, limiting its usefulness as a scoring algorithm. Machine learning approaches to prediction can help to overcome this. One approach used in machine learning is least absolute shrinkage and selection operator (Lasso) , which introduces bias into the model, therefore reducing the impact of variance between datasets on the performance of the model due to the bias-variance trade-off. Therefore, compared to OLS regression, Lasso produces a model that is more generalisable to datasets other than the one it was trained on, being more suitable for a scoring algorithm. Another advantage of Lasso regression is that as a result of the regularisation parameter lambda (l), the coefficient of some predictors is reduced to zero, removing them from the model. Consequently, only the predictors that are most powerful are retained in the model, reducing the complexity of the model and increasing its interpretability. Previously, we reported on the use of Lasso regression to create scoring algorithms for a forced-choice image-based assessment of personality, which presented respondents with pairs of images (items) mapped to the Big Five personality traits and asked them which image in the pair is most like them. After refinement, 150 item pairs (300 images) were retained in the assessment, with these predictors being binarised to represent whether a respondent selected the image or not. All 300 predictors, regardless of which trait they were designed to measure, were entered into the models and used to predict Big Five scores on the IPIP-NEO-120 . This study aims to compare the convergent and discriminant validity, as well as generalisability, of multiple approaches to scoring the image-based assessment: a) Lasso regression using all 300 predictors, b) Lasso regression using only the items mapped to each trait (i.e. image pairs that were mapped onto the Big Five traits by the team of I-O psychologists who designed them), c) OLS regression using all 300 predictors, d) OLS regression using only the items designed to measure each trait, and. e) the summative approach to forced-choice measures (again using image pairs as mapped by I-O psychologists). These models are elaborated on below. Convergent validity will be measured as the correlation between the image-based score and the score on the traditional personality test and discriminant validity will be measured as the inter correlations between the five personality traits and compared to inter correlations on the traditional personality test. Generalisability will be determined by comparing the correlation for the training and test sets, as this can be used to establish how well the model can be applied to unseen data. It was expected that the machine-learning-based approaches would perform the best in terms of convergent validity and generalisability, followed by the OLS approaches and then the summative approach. 2. Method Our previously developed image-based measure of personality  presents respondents with pairs of images and asks them to indicate which image in their pair is more like them, with the image pairs, or items, being intended to map on to the statements from the IPIP scales. Items are either single trait (unidimensional), featuring high and low levels of a single trait, or mixed-trait (multi-dimensional), with the images showing high levels of two different traits (; for an example). The measure, therefore, contains both unidimensional and multidimensional items. Previously, we examined the validity of the measure based on scoring algorithms created using Lasso regression, where all 300 images (150 pairs) were entered as predictors into the model for each trait. This data driven approach was chosen to maximise the predictive validity of the measure. In addition, supervised machine learning approaches can reflect variance from items that contribute to but are not designed to measure a trait since some facets from different traits can be similar  (e.g. excitement-seeking from extraversion and adventurousness from openness to experience). This study extends our previous findings, comparing the performance of multiple scoring approaches, including our initial scoring algorithms, in terms of convergent and divergent validity with the IPIP-NEO-120  and generalisability beyond the training data. 2.1. Participants For the purpose of this study, we use the same sample as our previous research, namely 431 compensated respondents recruited using Prolific Academic (222 female; 356 under 40 years old; 209 White, 73 Black, 66 Asian, 56 Hispanic, 14 Mixed Race). Respondents took the 150-item image-based assessment along with the IPIP-NEO-120 . The 150 items described in this study were previously selected from a larger item pool based on a sample of 300 compensated respondents (Mage = 31.14, SD = 9.26, 69 % female) who took the IPIP- NEO-120 along with 100 of the image-based items. Based on this sample, the 150 best-performing items were selected based on Cohen's d values, which represented the difference in the Big Five scores of respondents selecting image one versus image two , where items with higher Cohen's d values performed better and were therefore selected to be retained. These values also enabled the mapping of items to the trait they did measure, instead of what they were intended to measure, by assigning the item to the trait corresponding to the highest Cohen's d value. 2.2. Procedure In our previous study , we created a separate scoring algorithm for each Big Five trait, with neuroticism being reversed to emotional stability. Specifically, binarised responses to all 300 images were used as the predictor variables and IPIP-NEO-120 scores for the relevant trait as the outcome variable in Lasso models. This approach was selected as the regularisation parameter l results in some predictors being removed from the model, therefore producing a model with fewer predictors that is more interpretable , allowing it to be examined whether the items intended to measure each trait were indeed the most predictive of that trait. To extend our previous research, in the current study we compare the performance of multiple scoring approaches using both different models (Lasso, OLS and summative) and different combinations of predictors. Since the models are designed to be used as scoring algorithms and therefore need to be generalisable beyond the sample they were trained on, a train-test split was used, with the models being trained on 70 % (n = 323) of the data and tested on the remaining 30 % of the data. This approach allows the generalisability of the models to be examined  and is common in machine learning since it offers an opportunity for cross-validation. We describe each of the models below. 2.2.1. Lasso models a) Lasso 300 - regression using all 300 binarised predictors in the model for each trait , with a separate model being created to predict each Big Five trait. This approach was taken as it allowed examination of whether the items designed to measure each trait were predictive of scores for that trait through examining the items retained by each model. b) Lasso intended - regression using only the items designed to measure each trait (from both the single- and mixed-trait pairs) for each model. 2.2.2. OLS models c) OLS 300 - OLS regression using all 300 predictors, using the same approach as a), resulting in a low n\\/p ratio as no predictors were removed from the models. d) OLS intended - regression using only the items intended to measure each trait (from both the single- and mixed-trait pairs) for each model, resulting in a higher n\\/p ration than c). 2.2.3. Summative approach e) Summative - Since the measure contains both single- and mixed-trait pairs (both unidimensional and multidimensional pairs), the traditional summative approach to forced-choice measures was used as this approach would not result in ipsative scores and the popular IRT model  is only suitable for fully dimensional measures. For the single trait pairs, the positive (high levels) image was assigned a score of 1 and the negative a score of 0. For the mixed-trait pairs, the selected image was assigned a score of 1 and the non-selected 0. Scores were calculated by summing the number of points for each trait. 2.3. Analysis Model accuracy was determined by correlating the actual and predicted scores  for the training set, while convergent validity with the IPIP-NEO-120 was determined using the correlation for the test set. Although the summative approach is not trained in the same way that a predictive approach is, scores were still grouped into training and test sets to allow for better comparison with the regression models. Further, generalisablity of the model was determined by examining the disparity between the correlation for the training and test sets since this can give insight into how generalisable the models are beyond the training data. 3. Results The descriptive statistics for each model can be seen in Table 1, where all predictive models result in a similar distribution of scores regardless of the predictors included in or retained in the model and have a similar mean value compared to the IPIP-NEO-120 scores. Since the number of items designed to measure each trait varies (openness: 49, conscientiousness: 75, extraversion: 67, agreeableness: 62, emotional stability: 47), the maximum score for each trait differs accordingly for the summative scoring approach, resulting in lower mean scores and smaller ranges for the summative models compared to the predictive models. Further, the total score across all five traits for the summative approach ranged from 98 to 140 (M = 121.59, SD = 8.50), demonstrating that the measure is not prone to the issues resulting from ipsative scores that fully multidimensional scales are. The performance of each model, for both the training and test set, can be seen in Table 2. Since the summative model does not use a regression line, the mean squared error (MSE) and R2 statistics cannot be calculated using this measure. In general, the Lasso models had the highest convergent validity with scores on the IPIP-NEO-120, with convergent validity for model a) (Lasso 300) ranging from 0.60 for agreeableness to 0.78 for extraversion and model b) (Lasso intended) ranging from 0.58 for agreeableness to 0.77 for extraversion, performing similarly to the prior model. In contrast, the convergence between model c) (OLS 300) ranged from 0.45 for agreeableness to 0.58 for extraversion and model d) (OLS intended) ranged from 0.52 from agreeableness and emotional stability to 0.72 for extraversion. Finally, convergence for model e) (summative) ranged from 0.41 for emotional stability to 0.73 for extraversion. Further, the convergence for the training and test sets for the Lasso models, while lower for the test set, were similar for the training and test sets, indicating that the models are generalisable beyond the data they were trained on. The OLS models, particularly model c), have greater disparity between the correlations for the training and test sets, illustrating how OLS can be prone to overfitting, especially with small n\\/p ratios. Finally, model e) had no issues with generalisability since it is not a predictive model, although the range of correlations was large and the only model that performed particularly well was the model for extraversion, which has a considerably higher correlation compared to the other traits. Based on these findings, the Lasso models performed the best in terms of both convergent and generalisability, with the full Lasso models (Lasso 300) performing better than the Lasso intended models, followed by OLS models. The OLS intended models performed better than the OLS intended models, likely due to overfitting with the latter model due to the large number of predictors. The summative approach performed least well, as only moderate convergent correlations were present for the majority of the traits. As a result of the regularisation parameter l, some predictors were removed from the Lasso models. Table 3 shows the number of predictors retained by each model. Since the Lasso models used the fewest predictors but had the highest convergent validity compared to the OLS and summative approaches and were more generalisable, this demonstrates the benefits of using machine-learning-based approaches in predictive measures when measuring personality through alternative formats. When all 300 images were entered as predictors, as can be seen in Table S1, some of the predictors were retained by multiple models, signalling that through using this approach, personality can be rapidly measured through a small number of items that are relevant to multiple traits. In contrast, similar (but lower) convergence is achieved through OLS regression using all 150 items, which would result in a longer completion time. Further, all models contained a mixture of both multidimensional and unidimensional images (see Table S1), retaining the structure of the assessment even when some images were removed from the models and therefore avoiding ipsative scores. The correlation matrix for the models is shown in Table 4. Using a multitrait-multi-method approach, the convergent and discriminate validity of the measures can be determined. There is greater discriminate validity for model b) (Lasso intended) compared to model a) (Lasso 300). Models c) and d) (OLS 300 and OLS intended, respectively) have greater discriminate validity than models a) and b), although this is at the expense of the convergent validity of the models, suggesting that the lower discriminate validity is due to weaker correlations overall. Model e) has the greatest discriminate validity compared to the other models, but also has much lower convergent validity for most traits. 3.1. Image intention versus image mapping When refining the measure, we examined whether the item mapped to the trait that it was intended to measure. Based on the Cohen's d values calculated in our previous study, 60 % of the items were mapped to the trait they were intended to measure (; for procedure). To examine whether the performance of the models improved when including only the items mapped to the trait, models b) (Lasso intended) and d) (OLS intended) were redeveloped using only the items mapped to each trait using the Cohen's d values , instead of the items intended to measure that trait, as determined by the I-O psychologists. As can be seen in Table 5, the convergence of these models is similar to that of models b) and d), with correlations ranging from 0.59 for agreeableness to 0.76 for extraversion for the Lasso mapped model and 0.56 for openness to 0.68 for extraversion for the OLS mapped model. The summative approach was not investigated for the mapped trait as the Cohen's d values only indicated that there was a large difference in scores, not whether the image that was designed to be positive was in fact positive for the mapped trait. The coefficients of items for all models can be seen in Table S1. 4. Discussion This study aimed to compare the performance of machine-learning- based, OLS-regression-based and summative approaches to scoring a forced-choice image-based assessment of the Big Five personality traits. In this section, we compare the performance of each model in terms of generalisability and convergent and discriminate validity with the IPIP- NEO-120. We then provide some potential areas for further research, such as an examination of the predictive validity of the different models for predicting job performance. 4.1. Model evaluation In this study, we compared the performance of a) Lasso regression using all 300 predictors (150 image pairs), b) Lasso regression using only the images intended to measure each trait, c) OLS regression using all 300 images in each model, d) OLS regression using only the images mapped to each trait in each model, and e) the summative approach to forced-choice assessments. In terms of convergent validity, the Lasso 300 model performed best, followed by Lasso intended, then OLS intended, then OLS 300, and finally the summative model. The two Lasso models performed similarly, and the use of items mapped to each trait, instead of the items intended to measure each trait, did not make a large difference. The OLS intended model performed better than the OLS 300 model, although not as well as the Lasso models. Again, the inclusion of the items mapped to the trait, rather than intended to measure the trait, did not make a big difference on the performance of the model. The worse performance of the OLS models compared to the Lasso models is likely due to the large number of predictors, which would have been a particular issue for the OLS 300 models since they had a much greater number of predictors than the models using only the items intended to measure the trait, likely leading to overfitting. While the summative approach appears to have a large range of convergence values, where the upper end is more in line with the convergence for the other models, this is due to a particularly high correlation for extraversion (r = 0.73). In contrast, the correlation for emotional stability is particularly low at r = 0.41 and for the remaining traits, convergent validity is around 0.51. Not only is the summative approach less consistent, but the correlations for the remaining traits are also relatively low compared to the correlations for the Lasso and OLS models. In terms of discriminate validity, the summative approach had smaller correlations than the other approaches and the Lasso models had the largest correlations, therefore following the inverse of the pattern for convergent validity. This suggests that the Lasso models have greater correlations overall, and the summative models have lower correlations. Therefore, none of the approaches we tested can both maximise convergent validity while also maximising discriminate validity. Finally, in terms of generalisability, since the summative models are not predictive, there is no issue with generalisability to unseen data. For the predictive models, generalisability, determined by comparing correlations for the training and test set since the test set acts as an unseen sample , was greater for the Lasso models than OLS models. However, for the Lasso models, model b) (Lasso intended) had more similar correlations for the training and test sets than model a) (Lasso 300). Models c) (OLS 300) and d) (OLS intended) had lower generalisability, with model c) having a particularly big disparity between the training and test sets, likely due to the small n\\/p ratio and overfitting of the model. Due to the generally better performance of the machine-learning- based scoring models, which confirms our hypothesis, we recommend that a machine-learning-based approach to scoring forced-choice measures of personality, particularly those of an image-based format, is a viable option. This finding is in line with previous findings that the convergent validity of forced-choice personality measures is stronger when they are scored using supervised machine learning approaches, as opposed to typical forced-choice scoring approaches. As well as the better performance of the machine learning models in our study, Lasso has the additional benefit of removing predictors from the models, leaving only those with the greatest predictive power and resulting in a more interpretable model , as well as allowing shorter measures to be derived. This therefore has implications for personality measurement, where more complex assessments based on non-traditional formats or data sources that traditional scoring approaches are not sophisticated enough for can be created and scored using machine learning techniques increasing opportunities for innovation. 4.2. Further studies Since studies investigating forced-choice, image-based assessments of personality are sparse in the literature, there are a number of directions that future research could take. For example, further studies could more comprehensively examine how the models perform in terms of their discriminant validity relating to other traits since only the IPIP- NEO-120 was used as a point of comparison in this study. Additionally, since other machine learning approaches have been suggested to be appropriate for scoring a forced-choice personality measure, including elastic net regression, deep neural networks, and random forest  future studies could compare different machine learning approaches to examine if the performance of the models can be improved further. Alternatively, future research could move away from machine learning based approaches and instead seek to develop an IRT- based scoring approach for unidimensional or mixed dimensional measures since current efforts have focused on IRT approaches for fully multidimensional measures. Moreover, since the Lasso models reduce the number of predictors retained,  further research could examine how these items can be combined to create a measure that rapidly, and accurately, measures personality in around 1 min. These new measures, along with the one described in this study and our previous work , could be examined for predictive validity, using job performance ratings to examine whether the different scoring approaches and combinations of items differ in their ability to predict future job performance. Additionally, future research could address one of the major limitations of the current study, where the measure was only investigated in relation to English-speaking respondents. Although it is claimed that image-based measures do not need to be redeveloped in the target language , the interpretation of image meaning may vary between cultures, meaning that the models may perform differently in other cultures. This could have a particular implication for the Lasso models since the images retained in the models for one language or culture may be less predictive of personality in another culture. Consequently, future research could use a cross-cultural approach to examine the performance of the measure and each model in different cultures. 5. Conclusion This study supports the use of machine-learning-based scoring models for forced-choice personality assessments, particularly those designed for high-stakes contexts like selection. We found that the machine-learning-based Lasso models performed the best in terms of generalisability and convergent validity, although discriminate validity was weaker than the OLS and summative models. The OLS models had acceptable performance, but resulted in less interpretable models that retained all predictors and were less generalisable to unseen data, likely due to overfitting , particularly with the model using all 300 predictors due to the small n\\/p ratio. The summative approach performed least well, although it does not have issues with generalisability like predictive approaches can. Based on these findings, we recommend that the best approach to scoring forced-choice personality measures, particularly if they have an image-based format, is through machine-learning-based predictive scoring algorithms. Based on our findings, we recommend the use of machine learning based predictive scoring algorithms for forced-choice assessments over OLS or summative approaches since machine learning algorithms can maximise the accuracy of the model and generalisability of models to unseen data. Through machine learning, shorter measures can be developed, allowing personality to be measures rapidly through forced-choice statements. While we did not examine whether machine learning can maximise the predictive validity of a measure, our findings show promise for machine learning as a viable scoring method for forced- choice assessments of personality and highlight the possibility for innovative measures of personality to be developed and scored by machine learning.\",\"1135747024\":\"INTRODUCTION Self-esteem can be defined as a person's appraisal of his or her qualities, attributions, and expectations , being of great relevance also in the academic university environment. A person with high self-esteem is a person with better socioemotional and cognitive functioning. Low self-esteem seems to be related to academic problems and poor emotional adjustment in both the personal and social spheres; for this reason, it is necessary to measure it and take it into consideration. Likewise, the interaction that is generated among the members of the educational community will also affect the levels of self-esteem. In line with it, self-esteem will play an important role in decision making and in the behaviors that are implemented at certain times in the university environment. Therefore, instruction in cognitive problem-solving strategies helps students improve their self-esteem ; thus, self-esteem can be modified and for this it must be previously analyzed. In relation to resilience, self-esteem seems to play a mediating role between this concept and other psychological variables. Self-esteem is also a variable that has acted as a predictor of resilience in regression analyses. Greater resilience is associated with more functional and healthier behaviors in college students. The emotional resources available to university professors to manage the classroom and their interaction with students should be seen as a limited resource in the sense that they can be diminished if they are overused. This point was observed when analyzing gender differences in the way of managing different situations in the classroom, finding that female teachers are more emotionally involved in interactions with students, which can also lead to greater wear and tear, while male teachers take a more formal and distant approach to relationships with a lower emotional cost that leads to saving emotional resources. This fact alone does not explain the resilience of teachers, but it helps to understand underlying processes that may be latent and that in some way may also influence or relate to levels of self-esteem. University can be seen as a crucial event since it involves the construction of a personal and professional life project. According to these authors, who are based on well-being model of Ryff (2014), at this stage it is essential to have adequate levels of resilience to face the challenges of this educational stage. In this line, positive experiences of repeated success can contribute to a better personal valuation and to an increase in the probability of choosing resilient behaviors to reach achievement. Among teachers, the search for meaning in teaching is one of the qualities that are most present in people with high levels of resilience. As can be inferred, the relationship between self-esteem and the implementation of certain behaviors to manage problems is complex. It appears that the presence of resilient behavior and high self-esteem is related to the ability to manage emotions and self-control them. One mechanism that explains the generation of behavioral coping strategies in which resilience can be included is found in the sequence: thought-feeling-behavior. In this sense, and following these authors, it seems that proactive thoughts generate pleasant feelings that are embodied at the level of observable behavior by choosing adaptive behaviors from their entire behavioral repertoire (including resilient behaviors in their case); basically, this mechanism describes a behavioral decision-making process and helps to generalize learning to natural contexts since there is a direct action of the subject with his environment. By contrast, low levels of self-esteem are associated with a greater use of avoidance behaviors that avoid confronting the problem , and in which it is difficult to implement resilient behavior. In addition to what has been said so far, regression analysis studies have found that self-esteem is involved in the process of predicting teachers' perceptions of their ability to solve problems effectively. Regarding the relationship between self-esteem and perceived stress, both concepts seem to be closely related to each other. Specifically, high levels of stress may be present in situations of depressive symptomatology, anxiogenic, and even suicidal ideation, so it is important to act on all variables that are related to stress, including low self-esteem. Low levels of self-esteem have also been linked to serious consequences in teaching and university performance  by affecting areas related to well-being. The relationship between self-esteem and stress seems to be inverse, so that high levels of stress at work are associated with low levels of self-esteem. However, not only work factors influence the genesis of stress in teachers, but also personal variables such as resilience. The absence of resilience plays a transcendental role in the explanation of the genesis of burnout due to its predictive value for teacher stress. In summary, the relationship between self-esteem and the other factors that may be affecting its levels is complex, and for this reason it is necessary to continue researching it. Precisely to study the complex relationship between variables, artificial intelligence (AI), including artificial neural networks, is a growing technology with great implications in the field of education. In particular, AI is able to reduce human error and perform predictive analytics with less data than traditional systems, and the education system is expected to benefit from the technology surrounding this technology through more realistic decision-making processes. Besides, artificial intelligence applications have a great scope for expansion, opening up new avenues for research and information analysis, exploring new paths in the field of innovation and the design of programs more in line with the educational reality. Today, the specialization of AI in the educational field is leading to the concept of IAEd, which is helping teachers to simulate hypothetical scenarios and manage a large amount of data related to the teaching-learning process, school coexistence, attention to diversity, among other aspects. This is even more critical in the wake of the pandemic resulting from COVID-19 , and the need to create online learning environments and to operate with a large amount of information. Because of the novelty and multiple applications of AI and big data, it is necessary to promote studies that make use of this technology and to train the educational community in the development of applications, in the improvement of research and even, ultimately, in decision making on issues related to educational policies. In addition to what has been said so far, sociodemographic variables such as age or gender can contribute to some extent to predict the cognitive processes of teachers, although their impact on artificial neural networks still requires further study. Without a shadow of doubt, it is a problem that needs addressing. The main objective of this research was, on the one hand, to study the relationship between resilience and stress and self-esteem and, on the other hand, to analyze the contribution of the levels of resilience and perceived stress on the levels of self-esteem of the university educational community through the design of an artificial neural network with predictive capacity. The following hypotheses were used as a starting point: (h1) Self-esteem and resilience will maintain a direct relationship so that the higher the self-esteem, the higher the resilience; (h2) self-esteem and stress will have an inversely proportional relationship so that the higher the self-esteem the lower the levels of perceived stress; and (h3) the levels of resilience and stress--at the input layer--will contribute to a greater extent to the predictive capacity of the artificial neural network of self-esteem--at the output layer. MATERIALS AND METHODS Procedure A quantitative and ex post facto design approach was used. Within this design, a battery of scales was administered through an online platform initially using educational communication channels to reach the target population, and once achieved, the link was allowed to continue to be shared through a snowball process, following the approach of previous studies. The questionnaires were administered between May 2019 and May 2020. Before starting the task, the objectives of the study were informed and consent to participate was requested. The completion time was approximately 15 min. At all times the research was voluntary, confidential, and anonymous, in line with the guidelines established in the Helsinki Protocol. The study was approved by an Ethics Committee of the University of Murcia (ID: 2478\\/2019). Participants A total of 290 people from the university educational community of two higher education centers in southeastern Spain participated in this research. Of these, 73.1% (n = 212) were women and 26.9% (n = 78) were men. Regarding their educational role at the university, 69.7% (n = 202) were students and 30.3% (n = 88) were professors. The mean age was 30.26 (SD = 10.784) with the mean age of females being 30.21 (SD = 10.746) and the mean age of males being 30.40 (SD = 10.954). Likewise, the mean age of the student body was 26.36 (SD = 8.144) and that of the faculty was 39.20 (SD = 10.810). Instruments The instruments administered were: Ad hoc sociodemographic questionnaire. For the development of this research, information was collected regarding gender (nominal variable with two options: female or male), age (continuous or scale variable), and role (nominal variable with two options: student or teacher). Rosenberg Self-Esteem Scale (RSES). Originally designed by Rosenberg (1965) and validated in Spanish by Atienza et al. (2000), it consists of 10 items with four response options where 1 means \\\"strongly disagree\\\" and 4 means \\\"strongly agree,\\\" with direct and inverse items. Example: \\\"In general I feel satisfied with myself \\\" (item 7). The sum of the items gives rise to a variable that can reach 40 points. In relation to the interpretation, the higher the score, the higher the self-esteem and vice versa. Similarly, this scale was administered in university population and obtained an alpha of 0.72 . In the current version, an a = 0.875 was found. Perceived Stress Scale (PSS). The Spanish version of the stress scale  devised by Cohen et al. (1983) was administered. The scale is composed of 14 items assessed through a six-option Likert-type scale where 0 means \\\"never\\\" and 5 means \\\"very frequently,\\\" and after considering the direct and inverse items and adding them together, a total score is obtained that is directly proportional to the level of perceived stress. Example of item: \\\"In the last month, how often have you been upset because of something that happened unexpectedly?\\\" (Item 1). Remor (2006) found a Cronbach's Alpha of 0.81. In the current study, 0.86 was found. Brief Resilient Coping Scale (BRCS). The Spanish version was administered  based in turn on the Sinclair and Wallston (2004) scale. It is rated based on a five-option scale where 1 means \\\"It does not describe me at all\\\" and 5 means \\\"It describes me very well.\\\" In total there are four items. For example, \\\"I believe I can grow in positive ways by dealing with difficult situations\\\" (item 3). In the original scale, a Cronbach's Alpha of 0.86 was obtained. In the Spanish version, 0.82 was obtained. In the current research, a = 0.78 was obtained. Data Analysis First, a descriptive analysis was carried out, analyzing the main trend and dispersion indexes. Next, an inferential analysis was performed which consisted of the application of Pearson's correlation for continuous variables to study the relationship between variables and Student's t-test to compare independent means and examine the existence of differences, and finally, an artificial neural network was designed to predict the levels of self-esteem. Regarding the design of the artificial neural network, a backpropagation algorithm was chosen. Five independent variables consisting of two factors of a nominal nature and three covariates of a continuous nature were selected. The dependent variable was self-esteem. The scale change of covariates was standardized. When programming the artificial neural network, it was noted that the program would try to achieve a distribution of cases around 60% for the training phase, 30% for the testing phase, and 9.3% for the holdout phase. Finally, the program came close to these values and the resulting network consisted of 171 subjects in the training phase (59%), 92 subjects in the testing phase (31.7%), and 27 subjects in the holdout phase (9.3%). Then, the automatic selection of architecture was chosen according to which the minimum number of units in the hidden layer would be one and the maximum 50, taking into account the principle of simplicity and optimization. The type of network training was batch and the optimization algorithm was the scaled conjugate gradient. As for the training options, we started from an initial Lambda of 0.0000005, initial Sigma of 0.00005, and an interval shift +-0.5. For both descriptive and inferential data analysis, the statistical package SPSS version 24  was used. RESULTS Preliminary Analysis of the Relationship Between Self-Esteem, Perceived Stress, and Resilience Participants obtained a mean level of self-esteem located at 31.88 (SD = 5.837); men obtained a mean of 31.19 (SD = 5.820) and women 32.14 (SD = 5.837) on this variable. With respect to the educational role, students obtained a mean in self-esteem of 31.80 (SD = 6.102) and teachers a mean of 32.07 (SD = 5.208). As for perceived stress, the mean score was 26.10 (SD = 9.030); the mean score in men was 25.53 (SD = 8.644) and in women was 26.31 (SD = 9.179). The mean score of the students on the perceived stress variable was 26.39 (SD = 9.386), and that of the teachers was 25.43 (SD = 8.167). In the case of resilience, they obtained a mean score of 14.993 (SD = 3.282); males obtained a mean score of 14.64 (SD = 3.369) and females of 15.03 (SD = 3.252). Regarding the educational role, students obtained a mean score in resilience of 14.77 (SD = 3.423) and teachers of 15.28 (SD = 2.920). The analysis of the correlations found between self-esteem, stress, and resilience is shown in Table 1. It can be seen how there are statistically significant direct and inverse relationships between certain variables. Age was not significantly related to any of the variables in Table 1 (p < 0.05). Design of the Artificial Neural Network Architecture Figure 1 represents the artificial neural network model designed to predict the levels of self-esteem in the university community based on the dependent variables exposed and the calculated bias. With respect to the information in the network, it was composed of three layers (input layer, hidden layer, and output layer). First, the input layer consisted of two factors of a nominal nature (gender and educational role) and three covariates (age, perceived stress level, and resilience). The total number of units was seven excluding the bias unit and the rescaling method for covariates was standardized. Second, there is only one hidden layer consisting of four units again excluding the bias unit. Activation function was tangent hyperbolic. Third, the output layer was formed by a dependent variable (selfesteem), the rescaling method for scale dependents was standardized, the activation function was identity, and the error function was sum of squares. In Figure 1, the lines connect the nodes and represent the relationship between them. This is called synaptic weight. Darker relationships represent values below 0, and lighter ones represent values above 0 (indirect and direct relationships respectively). The scores entered in the input layer and their interaction will determine the nodes generated in the hidden layer that will mediate the final result in the output layer. The estimated parameters of the synaptic weights of the relationships between units or nodes in the network are shown in Table 2. Regarding the summary of the resulting model, the sum of squares error of the training phase was 57.177, the relative error was 0.673, and the stopping rule used was to obtain a consecutive step without decreasing the error. In the testing phase, the sum of squares error was 32.151 and the relative error was 0.719. In the holdout phase, the relative error was 0.943. The dependent variable was the total self-esteem score understood as a scale or quantitative variable. The contribution of each independent variable introduced in the input layer of the artificial neural network to the final score of the dependent variable self-esteem is shown in Table 3. The importance of each variable is shown on a scale from 0 to 1 and the normalized importance from 0 to 100 (stated as a percentage). As can be seen, the sociodemographic variables (gender, educational role, and age) contribute less to the predictive capacity of the network than resilience and stress, the latter variable being the one that contributes most to explaining the subject's self-esteem. Figure 2 shows the comparison between what the model has predicted and what has been observed. As can be seen, an ascending cloud of points has formed in which there is a directly proportional relationship. DISCUSSION The main objective of this study was to analyze the relationship between self-esteem, stress, and resilience by designing an ANN capable of predicting the first variable as a function of the other two. With respect to hypothesis 1, it is confirmed that there is a direct relationship between self-esteem and resilience. The mechanisms that explain this relationship are varied, although it seems that resilience allows solving daily life problems and coping with high stress situations , which would have an impact on a higher self-esteem. In this sense, Servidio et al. (2018) affirm that those low resilient behaviors based on problem avoidance coping strategies are precisely associated with low levels of self-esteem in line also with what was raised by. In short, resilient behavior has been related to higher self-esteem and adjustment. Regarding hypothesis 2, it is confirmed that there is an inverse relationship between self-esteem and stress, finding that those teachers with higher levels of self-esteem had lower levels of stress and vice versa. This is in line with what was previously stated by Santos et al. (2017), who also found that self-esteem and stress maintained an inverse relationship. Specifically, this relationship could be mediated by the coping strategies that teachers put in place. In relation to hypothesis 3, it is confirmed that resilience and stress have a relevant role in predicting the levels of self-esteem given an artificial neural network backpropagation algorithm, having a high contribution to its predictive capacity, above the sociodemographic variables. In the present study, stress was found to have a significance of 0.422 (normalized significance of 100%) and resilience had a significance of 0.284 (normalized significance of 67.3%). These results are closely related to those found previously in which sociodemographic variables did not provide as much predictive information to the network as the others. In any case, within the sociodemographic variables not all contribute equally. According to ANN, age contributes to self-esteem levels to a greater extent than sex or education. In any case, it was stress levels, followed by resilience, which accounted for the network results. It appears that those with lower stress levels and higher resilience have higher self-esteem. Thus, to study this relationship, artificial intelligence-derived technology has proven useful for predicting complex relationships and interactions between variables. The complex relationship between these variables raises the need to provide resources to increase the levels of self-esteem of the educational community as well as their strategies to cope with stress. In this scenario, teachers are a key and decisive factor in the transition from a traditional education to a digital education based on artificial intelligence, at all levels, from teaching to information management and research being their basic training to achieve educational quality and excellence. Applicability and Implications Regarding the applicability of the study, knowing which variables contribute to the levels of self-esteem of the educational community is a valuable tool to promote those factors that are more closely related to high levels of personal self-concept, which in turn are closely related to higher academic performance and lower depressive symptoms according to recent studies. It is expected that higher self-esteem favors the motivation of university students , and this in turn leads to an increase in their academic performance. Likewise, the aim of this study is to provide useful information adjusted to reality in the design of intervention programs aimed at increasing self-esteem levels through a scientific and replicable approach. Such programs have been shown to have a beneficial effect on the educational community. Such programs should focus on improving the subject's adaptive capacity, enhancing coping strategies, and resilience. Because of their predictive value for teacher stress, emotional intelligence in particular, and resilience in particular, should be part of teacher training programs and their selection processes. For all these reasons, it is necessary to introduce the teaching of functional coping strategies in the curriculum itself. At this point, educational psychology has a wide field of intervention. It is also worth mentioning knowledge transfer actions (participation in Congresses and lectures) that seem to have allowed students to feel more valued and in touch with the educational community and their teachers and raise their self-esteem levels. Therefore, these types of events should also be encouraged. Finally, it should be noted that AI and methodology based on artificial neural networks, or more recently, AIEd, should be the basic working tool for teachers. Limitations and Future Lines of Research With regard to limitations, the network has focused on measuring self-esteem, perceived stress, and resilience, also taking into account a series of very specific sociodemographic variables, since that was the objective of this research. However, this does not mean that a person's self-esteem at university is solely influenced by these variables. The very concept and nature of self-esteem is complex  and further research is needed to clarify. Besides, the results should be interpreted with caution due to the number of participating subjects. In this regard, it is difficult to make generalizations to the entire university setting and ensure the representativeness of the sample, although it represents a starting point from which to continue to expand sample and encourage comparative studies across regions and countries. With respect to future lines of study, academic performance could be included as a variable since it could also be to some extent a reflection of student self-esteem. Likewise, further research is needed in the field of AI and in the methodologies derived from it. It would also be interesting to introduce coping strategies for their implication on stress and resilience levels. The educational field will benefit greatly from the use of AI because of the large amount of information that it must manage. There is no doubt that the main objectives of AI should be to help society to solve problems and prevent them, to promote innovation, just as these goals are also intended in the educational field. This multidisciplinary approach should involve the different sectors and social actors, including scientists, teachers, politicians, managers, etc., which requires a wide range of knowledge (i.e., financial markets, policies, education, medicine, legislation, among others;). Therefore, it will be necessary to consider what will be the new competencies of teachers in a digital world in which AI plays such an important role. CONCLUSION Self-esteem is a complex variable. In the current research, its study has been limited to the relationship that other variables such as resilience, perceived stress, and other sociodemographic variables may have with self-esteem. Of these, stress and the individual's resilience capacity are those that contribute most to explaining the levels of self-esteem in comparison with the sociodemographic variables. An inversely proportional relationship has also been found between perceived stress and self-esteem as well as a direct relationship between self-esteem and resilience. Gender and educational roles do not seem to have as great an influence as age when analyzing the contribution of these variables to the results in self-esteem. On the other hand, it is necessary to promote the use of methodologies based on artificial neural networks in the educational system in order to analyze the relationships between the variables that influence the teaching-learning process.\",\"1135747060\":\"1. Introduction 1.1. Background The human face is the basis for a wide variety of judgments. From just a single glance, we can recognize friends, family, and acquaintances. Faces also help us retrieve semantic or emotional information about others (e.g., name, career, relation). In addition to being the primary cue for identity, faces carry information useful for making visually derived semantic categorizations. For example, we spontaneously per ceive the age, ethnicity, and gender of others. Among these categorizations are the personality inferences we make from faces. Although the human tendency to infer personality traits from faces is well studied, much less is understood about the extent to which this information supports or interacts with the information used for face identification. From a computational perspective, Oosterhof and Todorov (2008) and Walker and Vetter (2009) showed that human trait judgments can be modeled and manipulated reli ably by computer graphics programs. These studies also demonstrated that faces contain quantifiable physical correlates for human social-trait judgments. In Oosterhof and Todorov (2008), participants labeled neutral-expression face images with words that related to social traits (e.g., sociable, mean, weird, confident). These responses were reduced to 13-dimensional social-trait vectors that described each face, which were then submitted to principal component analysis (PCA). This produced a trait space with two main axes. The first axis was interpreted as \\\"trustworthiness\\/valence\\\" and the second axis was interpreted as face \\\"dominance.\\\" Next, Oosterhof and Todorov (2008) gener ated a face-shape space using three-dimensional laser scans of faces. This space was used to produce three-dimensional models of faces that participants rated for trustwor thiness and dominance. Using the best linear fit, Oosterhof and Todorov (2008) found that the fiducial points in the face-shape model could be manipulated to alter trait judg ments. The degree of these manipulations predicted whether human participants rated faces as more or less trustworthy or dominant. Social information can also be inferred from highly variable images (e.g., large changes in viewpoint, expression, illumination, and age), albeit with a third important dimension in the trait space which is related to youthful\\/attractiveness. One limitation of Oosterhof and Todorov's (2008) model is that it considers only face shape and does not consider face reflectance or pigmentation. Walker and Vetter (2009) addressed this limitation with a model that included both the shape and reflectance of faces. They used 200 three-dimensional laser scans of faces to generate a three-dimen sional morphable model  and collected trait ratings on a sample of faces. Using these ratings, they found the locations of faces in the space that had high values on specific traits and calculated the average position of these faces, as well as the direction of the vector toward this average from the global average in the space. To generate faces with variable amounts of these traits, individual faces in the space were moved along this trajectory to alter (decrease or increase) the presence of the trait. Walker and Vetter (2009) showed that these manipulations could be controlled to affect social trait inferences made by humans. Combined, the work of Oosterhof and Todorov (2008) and Walker and Vetter (2009) demonstrates that it is possible to use computer graphics models, driven by human rat ings, to \\\"generate\\\" faces that elicit specific social trait judgments. Consistent with these generative findings, other studies have shown that for a limited number of traits, it is pos sible to learn a direct mapping from face images to trait judgments, without explicit con trol or knowledge of the underlying two- or three-dimensional structure of the face. For example, using deep convolutional neural networks (DCNNs), Lewenberg, Bachrach, Shankar, and Criminisi (2016) predicted social-trait judgments from face images. Lewen berg et al. (2016) crowd-sourced large numbers of \\\"attribute\\\" ratings from faces, on both objective (e.g., hair color, gender) and subjective (e.g., attractiveness, humor) aspects of facial appearance. Next, DCNNs were trained to classify images in a binary manner, according to the presence or absence of each attribute. Lewenberg et al. (2016) predicted objective attributes with very high accuracy (gender = 98.33%, ethnicity = 83.35%, hair color = 91.69%, makeup = 92.87%, age = 88.83%). Three subjective traits also were well-predicted, but to a lesser degree (attractiveness = 78.85%, humorous = 69.06%, chubby = 61.38%). In converging work, McCurrie et al. (2017) trained DCNNs to predict three subjective traits (trustworthiness, dominance, and IQ), as well as age. McCurrie et al. (2017) measured the proportion of shared variance (R2) between the trait predictions computed from their model and the trait ratings assigned by human participants. McCur rie et al. (2017) found significant agreement in all cases (trustworthiness R2 = 0.43, dominance R2 = 0.46, age R2 = 0.74, IQ R2 = 0.27). It has also been shown that the social-trait inferences made by humans can be pre dicted using the output from DCNNs trained for different tasks. Song et al. (2017) predicted human-assigned social traits from the princi pal components (PCs) of the feature responses produced by networks trained for object recognition, face identification, and facial landmark localization. They found that the highest correlation between human-assigned traits and predicted traits could be obtained using the features from a network trained for object recognition. This underscores the widespread availability of trait information in the visual features used to represent diverse images. However, the image set used by Song et al. (2017) did not control for emotional expressions or image characteristics (pose, illumination, etc.). These variables are known to influence the way humans perceive social traits. Thus, the social-trait prediction performance reported by Song et al. (2017) was based on a combination of features from face identity, expression, and image parameters. A more fine-tuned analysis is necessary to determine the extent to which each of these factors can contribute to the accuracy of the predictions. In this study, we focused primarily on prop erties of the face itself as a cue to social trait inferences. Overall, the above studies indicate that DCNNs are capable of learning human assigned social traits from face images, as well as attributes such as gender and age. The features that support these trait inferences were made concrete by the work of Oosterhof and Todorov (2008) and Walker and Vetter (2009). For example, wide eyes are seen as trustworthy, whereas thin eyes and lips are seen as untrustworthy. These features are an integral part of a person's appearance and thereby a cue to their unique identity. An empirical indication that identity and trait information may be related comes from a study by Hassin and Trope (2000), who found that faces were judged to be more similar to one another when they were rated as having comparable social traits. This returns us to the question we address here. How does the information in faces that gives rise to social-trait inferences relate to the information that specifies the iden tity of a face? To that end, we made use of a computational model trained exclusively to identify faces. We asked whether the face representations produced by this network retain the information needed to make inferences about a face's social-trait appearance. In other words, to what extent is it possible to use face representations optimized in an identity-trained neural network as a representation that supports human-like trait inferences? Before proceeding, we digress briefly to define and discuss DCNNs, a class of hierar chical neural networks introduced by Krizhevsky, Sutskever, and Hinton (2012). DCNNs have changed the state-of-the-art in machine learning and have proven especially power ful for visual tasks such as face and object recognition. These networks were designed originally to model the response properties of the primate ventral visual stream. DCNNs con sist of layers of simulated neurons that alternately convolve and pool input, while expand ing the representation in intermediary layers of the deep network. The final layer(s) of DCNNs are typi cally fully interconnected and serve to compress the representation at the top level of the network into an abstract set of emergent features. These features form a highly compact representational code--usually on the order of a few hundred elements. It is important to note that the success of deep learning is in part due to the availability of extremely large, open-source, identity-labeled training datasets. For face identification, a major accomplishment of DCNNs is their visual robustness to changes in viewpoint, illumination, expression, and appearance. This is a consequence of the fact that DCNN architectures for face identification are trained on large numbers of identities and use multiple, variable images of each identity. DCNNs trained in this manner have consistently yielded state-of the-art results on the well-tested Labeled Faces in the Wild and YouTube Faces datasets. For present purposes, the compact nature of the face representations produced by DCNNs, as well as the impressive ability of these representations to support identifica tion, makes them an ideal tool for probing the co-dependence of trait and identity infor mation in faces. 1.2. Overview We tested the interdependence of identity and social-trait information in the top-level features of a DCNN trained for face identification. To study this, we collected social-trait ratings from participants for a large number of face images. Next, we obtained identity descriptors for each of the face images using a state-of-the-art face-identification DCNN. We used the DCNN representa tions and the human-assigned trait ratings to address four questions. First, we asked whether the social-trait ratings we collected modeled a trait space similar to that described in previous research. This was tested by interpreting the first two axes produced by a multivariate analysis of the human trait ratings. Second, we verified that the network used in this study produced an identity code pow erful enough to support recognition. Specifically, we computed Receiver Operating Char acteristic (ROC) curves to measure identification performance on the faces in our dataset, using the top-level features produced by the network as the representation of each face. Third, we asked whether the top-level descriptors produced for each image by the face identification DCNN could be used to predict the human-assigned social trait ratings. We trained and tested a linear regression model with cross-validation and compared the similar ity between the predicted trait profile and the human-generated trait profile for each face. Fourth, we asked whether social traits could be predicted individually. To answer this, we measured the prediction error for each social trait. 2. Methods -- human ratings 2.1. Participants A total of 85 participants completed the data-collection portion of this experiment (20 males, 65 females, aged 18-30, M = 21). Participants consisted mostly of undergraduate students from The University of Texas at Dallas. Participants enrolled in a psychology course were compensated for their participation with one credit toward that course. Partici pants not enrolled in a psychology course received no compensation. Of the 85 participants, one male and one female were excluded from analysis, because their arrival time overlapped with additional participants and they completed the experiment in an alternate setting. 2.2. Face stimuli The face stimuli presented in this study were selected from the Human ID Database. To control for effects attributable to the race of the stimulus, only images of Caucasian identities were used in this experiment. All images showed individuals with a neutral expression. A total of 280 images were selected, depicting 192 distinct identi ties. Some identities (n = 83) appeared in the dataset multiple times to allow for verification of the network's face-identification accuracy (see Section 4.2). In total, 109 identities appeared once, 78 identities appeared twice, and five identities appeared three times. For the data collection task, to make the workload for participants manageable, the 280 images were divided into four sets containing 70 images each (19 male, 51 female). All participants were assigned randomly to rate one of the sets. There were no duplicated identities within each set. Participants were unfamiliar with the identities in the images they were shown. 2.3. Social traits The list of social traits used in this study was derived from the Big Five Factors of Personality. The social traits listed by Gosling et al. (2003) constitute a generalizable and representative selection of traits pertaining to indi vidual personalities. Each trait in the Big Five is classified into one of five personality domains -- openness, conscientiousness, extraversion, agreeableness, or neuroticism. For the data collection task, we selected 18 traits that were either from this list or were related to items from this list (see Fig. 1). We selected multiple traits from each of the five domains listed by Gosling et al. (2003). It should be noted that the traits we chose for this study are different than those used in previous work. The traits here were chosen to reflect a wide range of personality traits that fit the oretically into the Big Five axes, as well as face perception research that examines trait inferences. We consider the relationship of these terms to those used in previous work by constructing the trait space described in Section 4.1. 2.4. Procedure for human trait ratings Participants in this study were asked to decide whether each item in a list of traits applied to a given face. Every participant viewed 70 face images (19 male, 51 female). Each face was presented on a computer screen next to a list of 18 social traits. For each trait, the participant was asked to select whether the trait: (a) applied perfectly, (b) applied somewhat, or (c) did not apply, to the face being shown. Previous studies exam ining social-trait inferences have used scales ranging from two points  to nine points , indicating that researchers' range of the response options has varied substantially. We used a 3-point scale to limit variance that might occur based on individual differences in the way participants used the scale. There was no default selection. Participants were able to advance to the next face only after making a selection for each trait. Participants were allowed as much time as they needed to rate each face. Fig. 2 provides an example page from this data collection task. Trait ratings were collected from 19 and 23 participants per face image. The data com prised an n 9 m trait matrix, X, where n represents the number of faces and m represents the number of traits. Any given cell of the matrix Xi,j contained the mean of participants' ratings for the jth trait on the ith face. The columns of this trait matrix were then con verted into z-scores so that each face was represented by its deviation from the average ratings across the faces. The social-trait inferences assigned to different images of the same individual were more correlated than the trait inferences assigned to images showing different identities (same identity, r = .56; different identity, r = .13). Several trait labels were applied very similarly by participants, indicating that at a higher level of abstraction, some subsets of traits measure the same underlying variable. The dimensionality of the trait space was reduced to minimize the effect of these duplicated traits within the dataset. To do this, traits that correlated highly with one another (Pearson correlation coefficient, r > .80) were averaged together and treated as a single trait. This reduced the final number of traits from 18 to 11 by combining (a) talkative and energetic; (b) warm, sympathetic, softhearted, trusting, helpful, and reliable; and (c) efficient and thorough. The traits talkative and energetic both imply levels of activity and were combined and relabeled as talkative. The traits warm, sympathetic, soft-hearted, trusting, helpful, and reliable all imply positive valence and were combined and relabeled as warm. The traits efficient and thorough both imply capability and were combined and relabeled as efficient. The social trait space considered in this study was comprised of the 11 traits that remained. 3. Methods for DCNN trait analysis 3.1. DCNN network specification The neural network features for each face were generated from the DCNN described in Table 1. This network was trained for face identification and has achieved state-of-the art performance on the IJB-A dataset. Parametric rectified linear units (PReLU) are used to compute the activation function. The final output layer consists of 512 units. The network was trained on the CASIA Webface database, which consists of 494,414 images depicting 10,575 identities. 3.2. DCNN face representation To carry out the simulation, the DCNN processed the 280 face images rated by partici pants to produce a unique 512-dimensional feature descriptor vector for each image. These feature descriptors were considered as the face identification network's representa tion of each face. 4. Analysis and results 4.1. Consistency of social-trait space with previous findings First, we examined the underlying social-trait space from our human-assigned trait rat ings to compare with previous findings. This was important given that the social traits used in this study differed from those used in previous work. To this end, we submitted our participants' responses to a principal component analysis (PCA). PCA is a multivariate analysis tech nique that uses singular value decomposition to rotate the data and create a new space defined by orthogonal components that are ordered according to the percentage of vari ance they explain. Here, PCA was applied to the human trait inferences. The coordinates of each trait were plotted along the first two principal components and visualized to ensure parity with the existing literature (Fig. 3). The first component produced by a PCA of the human-assigned traits was interpreted as approachability and separates traits such as talkative, efficient, and warm, from traits such as anxious, and quiet. The second component was interpreted as dominance and separates traits such as assertive and impulsive from traits such as shy, quiet, and anxious. These results are generally consistent with previous studies, which identified two primary social-trait components corresponding to slight variations on the axes we found: affiliation and dominance , warmth and competence , and trustworthiness and dominance. 4.2. Face-identification accuracy To test the DCNN for identification accuracy, we computed the cosine similarity between the 512-dimensional feature descriptors for all possible pairs of the 280 face images in our stimulus set. This resulted in an n 9 n similarity matrix, where n repre sents the number of face images. Identification accuracy was analyzed using an ROC curve to measure the separability between same-identity (match) and different-identity (non-match) image comparisons. The same-identity distribution contained the similarity scores for all image pairs in which the same individual was shown in both images. In all cases, these were two different images of the same person, taken over a span of weeks or months. The different-identity distribution consisted of all pairs of images depicting two different individuals. We considered only scores from the upper triangle of the symmetric similarity matrix. The ROC showed near-perfect performance on our stimulus set, as measured using the area under the curve (AUC = 0.995). This illustrates the high quality of the identity infor mation in the top-level DCNN features. 4.3. Trait-profile predictions from DCNN face representations A regression model was trained to predict the human-generated social-trait responses from the face representation generated by the face-identification DCNN. The trait ratings were represented in an n 9 m matrix, Y, where each of the n rows represented one of the 280 stimuli and each of the m columns represented one of the 11 traits. The face recogni tion features were represented in an n 9 k matrix, X, where each of the n rows again rep resented one of the 280 stimuli and each of the k columns represented one of the 512 top-level features. The regression model used to train the mapping from DCNN features to traits was: Y 1\\/4 XBth E where B represents the learned k 9 m weight matrix and E represents the error matrix. This model was trained using cross validation, wherein the regression model was learned using all but one of the stimulus images and was tested on the left-out image. In cases where the identity in the test image also appeared in the training images, all images of that identity were held out from the training set. This produced an n 9 m output matrix, Y , similar to Y, but where all trait values were model predictions rather than human responses. Human-generated trait vectors and computer-predicted trait vectors were compared using their cosine similarity. To test for statistical significance, we compared the average cosine similarity between the human- and computer-generated social-trait vectors to a null hypothesis distribution. This null distribution was created by a permutation test where the values within each column of the n 9 m trait matrix, Y, were reordered randomly. This effectively destroyed the relationship between social traits and stimuli, but preserved the underlying distribution of scores within each trait. The regression model was then re-com puted. This was repeated for 1,000 iterations. After each iteration, the average cosine sim ilarity between the 280 human-generated trait vectors and the 280 computer-predicted trait vectors was calculated. The average cosine similarity between the human trait ratings and the computer-predic tions (i.e., true cosines) was compared to the cosine similarities from the null distribution. The results show that the vectors of social-trait inferences predicted using the actual model were significantly more similar (p < .001) to the human-generated traits than the social-trait vectors predicted from the null distribution. There was no overlap between the mean of the true cosines (M = 0.353) and any of the cosines computed in the null distribution (M = 0.078). Thus, we conclude that the top-level features produced by the face-identification network contain information that supports human-generated social-trait ratings. Next, we explored the degree to which social-trait information is distributed throughout the top-level features from the face-identification DCNN. This was tested by eliminating varying amounts of top-level DCNN features from the space and then re-computing the regression model. Deleting features containing information critical to social-trait predic tion should reduce the cosine similarity between the human-generated trait vectors and the computer predictions. Alternatively, deleting \\\"noisy\\\" features (i.e., features consisting of only information extraneous to the social-trait prediction task) should increase the accuracy of the trait predictions. The top-level features from the DCNN were ordered according to their contribution to the regression model. The contribution of each feature was assessed using its regression weights and was measured to be the sum of absolute values across the rows in the k (number of features) by m (number of traits) weight matrix. Rows with the lowest sums contributed the least to the regression model and rows with the highest sums contributed the most. Beginning with the lowest 5%, an increasing number of features were removed and the regression models were recomputed. This was repeated until the prediction accu racy, indicated by cosine similarity, began to decrease. A decrease in prediction accuracy occurred after removing 27.5% of the features. The remaining 72.5% of features represent the optimal set of predictors. The average cosine similarity between human-generated trait vectors from this optimized regression model was significantly higher (M = 0.533, p < .001) than the average cosine similarities computed using the null distribution (M = 0.078). There was no overlap between the average of the true cosines and the cosi nes computed in the null distribution. These results show that social trait information is not evenly distributed throughout the top-level feature space learned by the face-identifi cation DCNN. The importance of individual top-level DCNN features for either identifi cation or for predicting social-trait inferences should be explored in-depth in future work. Here, we report the finding that accurate trait-inference prediction requires just a subset of the features from the overall top-level DCNN feature space. 4.4. Predicting individual traits from face representations The previous regression experiment established that a profile of social-trait information is represented in the top-level features of the face-identification DCNN. We now address the extent to which individual social traits can be predicted in isolation. Regression-model predictions were computed for each of the 11 individual traits. The resulting n 9 1 dimensional trait vectors were compared to the corresponding columns from the original n 9 m trait matrix using the coefficient of determination (R2). This provides a direct measure of the similarity between each predicted trait and its human-generated counter part. The coefficients of determination for predicting each trait individually are presented in Fig. 4. In Fig. 5, we plot the prediction accuracy for each trait contrasted against a null distribution. This figure shows that all of the social traits considered in this study were predicted at levels significantly above chance. The best-predicted traits here were impulsive, anxious, and warm. These social traits were predicted at levels comparable to the subjective trait predictions reported for the three traits examined in McCurrie et al. (2017). In the present case, however, predictions were made from a network trained for face identification, whereas in McCurrie et al. (2017), the network was trained explicitly for trait prediction. 5. Discussion Human faces provide a rich source of information that can be used to identify individ uals, as well as to form first impressions of strangers. Both topics have received consider able attention in the literature, but they have never been investigated simultaneously. In this study, we examined the co-existence of identity and trait information in faces. The main findings are as follows. First, we demonstrate that information related to social and personality traits is retained in the top-level features of a DCNN trained for face identifi cation. This finding complements previous work illustrating that DCNNs retain specific information about images that is not directly relevant for object\\/face recognition. Second, we show that DCNN face \\\"identity\\\" representations predict human-generated social-trait inferences, both at the indi vidual-trait level and at the level of full trait profiles. Third, we show that the DCNN used in this study identifies faces with high accuracy, allowing us to consider the reten tion of social trait information in the context of a high-performing identification system. Fourth, we verify that the particular social trait ratings we collected in this study, which differ somewhat from previous studies, produce a social-trait space similar to those reported in previous work. To understand the implications of these findings, it is useful to return to what we know about the nature of the physical information that supports trait inferences and to expand our discussion beyond simple face structure. The literature offers evidence that three sources of information contribute to social-trait inferences : face structure , emotional facial expressions , and image characteristics. As noted previously, evidence for the role of face structure in trait inferences comes from findings indicating that the manipulation of face structure, using computer graphics models, affects human trait judgments in predictable ways. The fact that face structure con tributes substantially to both face identity and trait inferences is perhaps not surprising, but makes it clear that information relevant for one of these tasks may also be relevant for the other. In addition to face structure, both posed facial expressions and expressions that are derived from the structure of neutral faces have been shown to influence trait ratings. For example, Montepare and Dobish (2003) showed that deliberate changes in the emotional expression of a face influence social trait ratings. These differences in the perception of social traits reflect the structural resemblance of a face to a specific emotion. Further, a recent study by Todorov and Porter (2014) showed that image characteristics, which include photometric variables such as illumination and image quality, are linked to social-trait perception. It has also been shown that DCNNs trained for face identification retain information pertaining to these image characteristics. Together, these studies provide further evidence that identity and social traits can coexist in a unitary representation. In this study, face emotion and image characteristics were controlled to reduce the noise associ ated with social-trait judgments. Because of this, the present results may underestimate the extent to which it is possible to predict social-trait inferences from more naturalistic stimuli (; for an approach that allows face expression to support trait classification). Returning to the question of how trait and identity information are related, it is impor tant to remember that face structure is relevant for face identification, but that facial expression and image characteristics are not. The finding that facial expression and image characteristics contribute to trait inferences makes it clear that first impressions are based not only on the face itself, but also on the specific circumstances in which a face is encountered. Although the behavioral and neural literatures reflect a conceptual division between social-trait and identity processing , advances in modeling face identification with DCNNs demonstrate that this division is not computationally necessary to account for the creation of a unitary face representation that can support both identity and trait perception. DCNNs are designed to model the computational processes seen in the primate ventral visual stream. There are some questions con cerning the validity of the claim that these networks are good models of ventral stream visual processing, given that their architectures are not well-suited for complex, sequen tial behavior. However, research has shown that the response properties of units in the intermediate layers of a DCNN predict the response properties of neurons in primate visual area V4. Moreover, units in the top level of the DCNN predict the responses of neurons in the inferotemporal (IT) cortex. Although interpreting the similarities between the ventral visual stream and DCNNs requires caution, the finding that these networks can accommodate both identity and social-trait information is consistent with recent evidence suggesting specific modifi cations to the distributed model of face processing. The distributed model posits a functional division of identity and social information from faces, whereby invariant identity information from faces (e.g., face structure) is processed in ventral stream face areas, and social (e.g., changeable aspects, including emotion, gaze, and head rotation) information is processed in the dorsal stream face areas (posterior Superior Temporal Sulcus). Although this theory is consistent with much exist ing data on the neural processing of faces , recent work suggests that the ventral pathway is also sensitive to some changeable information from faces. For example, Duchaine and Yovel (2015) suggest that the ventral temporal cortex processes emotional expression. More concretely, it has been shown that the fusiform face area (FFA) responds to faces regardless of whether people attend to the expression of the faces they view. In addition, the intensity of observed facial expressions modulates neural responses in the FFA. Further work is required to determine whether these responses arise purely from faces, or from faces and bodies in motion. However, combined with the present results, these neural findings advance support of the distributed model modification suggested by Duch aine and Yovel (2015). Specifically, this modification posits that ventral temporal repre sentations support both identity and social judgments about faces. This modification does not alter the distributed model's original hypothesis that social information from facial motion is processed in the dorsal stream (pSTS). Rather, it suggests that both ventral and dorsal stream face areas can play a role in the social processing of faces. In considering DCNN properties in this context, it is worth noting that there is now strong evidence that DCNNs retain specific information about face images, in addition to face categories (e.g., identity). That DCNNs are able to generalize well is paradoxical, given their enormous degrees of freedom. The topic of why DCNNs generalize is discussed in depth in O'Toole, Castillo, Parde, Hill, and Chellappa (2018) and C. Zhang, Bengio, Hardt, Recht, and Vinyals (2017). Presently, the surprising generalizability of DCNNs implies that these networks have the potential to model trait inferences based on expression and image-char acteristics in addition to facial structure. Future work should consider ways to dissect the sources of face-identity and face-image information that allow DCNNs to predict social trait inferences. In conclusion, we present the novel finding that social-trait information and identity information are not independent from one another. Understanding how social traits and identity are linked can provide clues to the structure of high-level visual information pro cessing in general face perception. The simple and direct linear classification methods used in this study underscore the fact that trait information is readily accessible within the representations produced by DCNNs trained for face identification. The presence of this trait information may constrain theories of how neural codes for faces can serve mul tiple face processing tasks.\",\"1135747104\":\"INTRODUCTION Among the central questions of personality research are associations of personality characteristics with life outcomes, defined as phenomena that could potentially be influenced by personality. From an applied perspective, understanding these associations may allow for the identification of people at risk of negative life outcomes, such as unemployment or diabetes, and for the discovery of potentially modifiable risk factors related to these outcomes (e.g. specific academic difficulties or health-related lifestyle aspects). From a psychological-theoretical point of view, delineating these associations allows for understanding where, to what extent, and perhaps even how personality may play out in people's lives. We argue that achieving these aims can benefit from the most accurate possible description of how personality is associated with outcomes, even if this means very nuanced patterns of associations--in which case, this very realization is telling. Most commonly, personality-outcome associations are represented using the five broad domains of the five-factor model  or the Big Five : conscientiousness, extraversion, agreeableness, neuroticism, and openness. Based on this representation, the associations are ubiquitous. However, we argue, they tend to be relatively weak and are often unspecific, such that even very different outcomes are related to similar trait combinations. One potential way to improve the accuracy and specificity of the associations may be to investigate them using a larger set of narrower, more specific personality traits than the Big Five. Although personality facets  can be useful for this, recent work suggests that even single-personality questionnaire items contain unique information that may be lost in aggregation (; Mottus et al., in press). In this study, therefore, we explore the usefulness of single questionnaire items as personality markers (analogous to genetic markers in molecular genetics research) in accounting for the variance in 40 outcomes representing individual differences in a variety of life domains. Specifically, we focus on items' predictive accuracy for these outcomes in comparison with the Big Five traits and address the possibility that Big Five-outcome associations tend to be at least partly driven by the items that happen to be included in them rather than (only) by the latent traits purportedly underlying the Big Five scores. Domains The Big Five personality domains are robustly associated with a wide range of broad life outcomes, such as physical and mental health  or success in education, career, and relationships. In addition, several of the domains are linked to more specific outcomes, such as drinking or smoking , relationship satisfaction , volunteering , exercising , or voting choices. Although ubiquitous, the associations are often modest in strength, with most effect sizes (r) well below.30 and often even below.20 or.10 (especially in larger samples). And because the Big Five traits are intercorrelated, the unique associations are often smaller still. Of course, individual effect sizes are expected to be modest in psychology--and they are generally small --because any behavioural phenomenon is likely to be linked with a huge number of causal factors. There is nothing inherently wrong with small effects. However, we argue that personalityoutcome associations may often be at least somewhat stronger than can be estimated by means of the Big Five domains and that quantifying the full magnitude of these associations when and where they are, in fact, stronger than is observed based on the Big Five can be useful for advancing our understanding of how personality intersects with life outcomes. Also, while the Big Five domains do relate to a wide range of broad and narrow life outcomes, the associations are often rather unspecific. Positive outcomes such as higher educational level and income, relationship quality, lack of antisocial behaviour, healthy lifestyle, or longevity tend to be associated with low neuroticism on one side and high conscientiousness and agreeableness on the other, and they often share smaller links with high extraversion and\\/or openness (;Mottus et al., 2012). The opposite pattern tends to characterize negative outcomes. Because this pattern corresponds to the social desirability of the Big Five domains , it suggests that the domains may be to a substantial extent associated with the general valence in outcomes rather than with their specific aspects--the aspects that make even similarly valenced outcomes distinct. To the extent that this applies, it limits the informativeness of the associations beyond suggesting that, perhaps accurately, positive things hang together. Facets Albeit far less frequently, personality-outcome associations have also been investigated at narrower levels of the personality hierarchy. Each Big Five domain, for instance, has been suggested to be made up of six facets , although this particular facet model is not necessarily based on empirical work  and there exist alternative facet models for the Big Five domains. Besides contributing to the domain particular facets are intended to measure, they also capture unique variance that is not shared with other facets of the same domain and that corresponds to distinct aetiological mechanisms. This facet-specific variance is also associated with life outcomes, and therefore including facets into prediction models of these outcomes can increase their predictive power. In addition, facet-outcome links are likely to be more specific than those based on the Big Five domains. Therefore, using facets can entail not only more accurate but also more diverse representations of personality-outcome associations. For example, both body mass index (BMI) and aggressiveness have small positive associations with neuroticism, but the two outcomes differ substantially in their links with neuroticism facets; for the former, the association only pertains to the impulsivity facet, whereas the latter has the strongest link with the angry hostility facet. When this applies, arguably, personality-outcome associations should be interpreted at the level of facets and not be generalized to domains at all. Personality-outcome associations from nuances (items) Facets may not be the most specific personality characteristics. Recently, McCrae (2015) suggested that the hierarchy of personality traits extends even below facets, to narrow personality characteristics that he called nuances. Because of a lack of proper classification, nuances have thus far been operationalized as individual personality questionnaire items. Nuances contain unique variance that is not shared with Big Five domains and their facets and tend to have trait-like properties of cross-rater agreement (; Mottus et al., in press), stability over time, and a non-zero level of heritability (Mottus et al., in press;). Also, nuances often display varying gender differences and age trends from the domains and facets that they are intended to be indicators of (; Mottus et al., in press). Some of this unique variance is filtered out when items are aggregated into domain and facet scores, but it may be useful for outcome prediction. For example, BMI is associated with the unique variance of items related to overeating  and giving up on self-improvement programmes , among a range of other items, and such associations tend to replicate across samples from different countries (Mottus et al., in press). Likewise, items' residual variance (after adjusting for the variance of the Big Five traits and their facets) has meaningful associations with people's interests in various life domains. For other outcomes, nuance-specific variance may appear less relevant: Mottus, Kandler, et al. (2017) found that item residuals were not significantly correlated with life satisfaction after Bonferroni correction for the number of correlations they had investigated. In fact, there may be systematic regularities in where items' unique variance provides incremental predictive value. It has been suggested that for optimal prediction, predictors and outcomes should be matched in terms of their breadth. If so, items should be better predictors of more specific outcomes (if outcome-relevant items have been included in the personality measure) whereas composite traits that aggregate multiple behavioural, affective, and cognitive tendencies should out-predict items for broader outcomes that also aggregate the cumulative results of multiple behaviours, thoughts, and feelings. On the other hand, a finding that items' unique variance does not significantly correlate with an outcome in a given sample  does not necessarily mean that it does not relate to the outcome at all. For instance, residuals of some items may well be associated with life satisfaction, but their individual effect sizes may be too small to reach significance, especially when stringent significance criteria (adjusted for large numbers of associations being tested) and not very large samples are used. Cumulatively, however, outcome prediction models including a number of questionnaire items may predict outcomes better than the Big Five domains even when individual links between the items' unique variance and outcomes are weak. We appreciate that this reasoning may seem surprising to some readers, especially given the currently widely prevalent concerns around replicability--one has to be mindful of the dangers of over-fitting models to data, either accidentally or deliberately. But rest assured, we take this danger very seriously. In order to illustrate our thinking, it may help to draw a parallel with molecular genetics. A parallel with molecular genetics Geneticists have come to realize that complex phenotypes tend to be linked to hundreds or thousands of genetic variants each conferring only a tiny effect, rather than to a few genetic variants with strong effects that can be easily detected; this is known as the fourth law of behaviour genetics. Such phenotypes-that is, most if not all phenomena personality psychologists are striving to learn about--are called polygenic. To study the individually small but potentially numerous genetic effects, researchers atheoretically link millions of genetic markers, single-nucleotide polymorphisms (SNPs) designating allelic variations in specific regions of the genome, to a given phenotype, a method known as genome-wide association studies (GWAS;). Given large enough samples (which means tens of thousands of participants or even more), predictive models built on the basis of the GWAS results can often explain substantial amounts of variance in complex phenotypes such as intelligence , schizophrenia , BMI , or educational attainment , even when applied to independent samples of people. Such models can be used to create polygenic scores: individuals' estimated genetic propensities for a given phenotype, derived by summing weighted allelic counts across large numbers of SNPs, with the weights taken from a GWAS carried out in independent samples. In addition to simply allowing for phenotypic prediction from genomic information, GWAS have also started to unravel the multifaceted biological aetiology of complex phenotypes such as BMI , depression , or intelligence. Importantly, including even very small effects of the genetic markers that, individually, are not statistically significantly associated with the phenotypes--and an overwhelming majority of them are not--typically contributes to the amount of variance explained by polygenic scores. For example, SNP-intelligence associations (from a GWAS meta-analysis based on up to about 280 000 people in total) with p-values of up to.26 contributed to the prediction of observed intelligence in four independent samples. This means that even tiny and, by conventional criteria, non-significant effects are often in fact real effects in that they contribute to the predictive signal. Realizing this is important not only because it allows for more accurate predictive models but also because this tells geneticists something very important about the very nature of the genetic aetiology of complex phenotypes: genetically, they are so multiply determined that singling out any one--or even dozens or hundreds--genetic variant(s) as the gene(s) for any one phenotype may often not make much sense. For an accessible account of recent progress in GWAS and polygenic scoring research and its major implications for understanding behavioural phenomena (with intelligence as the focal trait), readers are referred to Plomin and von Stumm (2018). Analogously, just as complex phenotypes are highly polygenic, we suggest that complex outcomes may turn out to be polynuanced. In other words, their associations with personality variations may be driven by a large number of specific personality characteristics in addition to, or perhaps sometimes even instead of, a small number of broad 'underlying' constructs that composite personality traits ought to represent. To address this possibility, questionnaire items could be used as personality markers of (yet unknown) nuances. What is more, full sets of items could be atheoretically linked to outcomes in questionnaire-wide association studies (;, although their rationale for and procedures of questionnairewide association studies differed from ours). If and when outcomes turn out to be polynuanced, this will have implications for both prediction per se and, more generally, for our understanding of how personality intersects with phenomena outside the personality domain. As for prediction, this could suggest that optimal models should be built based on large numbers of narrow personality characteristics (nuances). Such predictions could be called polynuance scores, analogous to polygenic scores. Polynuance scores are also similar to empirically constructed personality scales such as those of the Minnesota Multiphasic Personality Inventory  and the California Personality Inventory , with the difference that polynuance scores comprise weighted (by regression coefficients) contributions of items, whereas this is not necessarily the case for scores of empirically constructed personality scales. Such polynuance scores could be used wherever personality characteristics are used for prediction--for example, for selecting best candidates based on predicted future performance or identification of people at future risk for certain negative outcomes --but they would provide higher predictive value than predictions based on domains and perhaps also facets. As for understanding, in addition to singling out specific aspects of personality that are linked with particular outcomes, investigating the sheer numbers of such links may be informative regarding how personality and outcomes tend to intersect (e.g. how polynuanced they tend to be). Furthermore, if outcomes are indeed correlated with ranges of nuances, this may also increase the likelihood that different outcomes correspond to a more varied set of personality profiles than the all-positive-goes-together pattern that often emerges when the Big Five domains are used. If so, there would be more to personality in relation to outcomes than a desirable personality tagging along with other desirable characteristics. Let us be clear: we are not postulating that all outcomes are necessarily polynuanced--they may well not be at all, or only some of them may be. But we do suggest that this is an entirely realistic possibility with potentially broad implications and that it is therefore something worthwhile exploring. To exactly this end, we systematically compared the degrees to which item-based models could predict a range of outcomes compared with the degrees to which the outcomes could be predicted from the Big Five domains. We also explored the possibility that the Big Five-outcome associations may be driven by the items that had been included in the domain scores. Methodological considerations Adding more predictors to a model tends to increase the amount of variance the model can explain in the sample where the model is fitted, but such a model might perform poorly when applied to a different sample of the same population: this is known as model over-fitting. Therefore, item-based models may tend to out-predict domain-based models for purely statistical reasons. In order to mitigate this danger, geneticists (and all adopters of machine learning principles) often create (train) the prediction models in one sample and apply (cross-validate) them in independent samples. Personality researchers could also do this. In fact, the training and cross-validation samples do not necessarily have to be independent in the sense that they are collected by different researchers and\\/or at different times\\/sites. A single, sufficiently large sample can be split into two independent partitions, one for model training and the other for validation, and this procedure could be repeated multiple times, which yields a distribution for the parameter of interest such as the amount of variance in an outcome the model can account for in independent groups of people. In order to increase the likelihood of a model performing well in cross-validation, it is advisable to ensure that its parameters (e.g. regression coefficients) are estimated as well as possible in the training sample. When large numbers of intercorrelated predictors are included in a model, traditional least square or maximum likelihood regressions may struggle to produce coefficients that yield optimal predictions in independent samples because they are tailored to the idiosyncrasies of the particular sample (i.e. over-fit) and because of possible multi-collinearity among predictors. Increasing the sample size helps with the former, but not the latter. Therefore, it may be useful to employ regularized regression approaches such as ridge , least absolute shrinkage and selection operator (LASSO) or elastic net  for training models. These regression methods are designed to deal with large numbers of intercorrelated predictors and yield more parsimonious (compared to more traditional, non-regularized approaches) models that are less prone to over-fitting to start with. Specifically, these approaches penalize regression coefficients by shrinking them towards zero, because this counteracts the natural tendency of regression models to produce inflated (over-fit) coefficients. Of course, it is important not to over-penalize the coefficients: as a simple rule, the optimal penalization is one that maximizes a model's performance in cross-validation. With regularized regression approaches combined with cross-validation, prediction models can be based on from a few to tens or hundreds (in fact, even thousands) of intercorrelated predictors (domains, facets, or items). The present study In order to illustrate the ideas discussed above, we employed a large British adult sample (N ~ 8700) who had completed a 50-item Big Five personality questionnaire. Specifically, we predicted 40 outcomes reflecting a variety of life domains from the Big Five personality domains and then from the 50 items used to define the domain scores. Using such a wide range of outcomes yielded a generalizable pattern of findings, but it also allowed us to investigate systematic variations among outcomes in their predictability. Specifically, we examined whether differences between domains and items in the prediction of outcomes would track with the breadth of these outcomes as rated by independent judges. The use of a large sample allowed for training models with tens of predictors and validating them in independent and yet sufficiently large subsamples. However, the use of a personality measure with only 50 items, many of which were similar or almost entirely overlapping in content (e.g., 'I seldom feel blue' and 'I often feel blue'), meant that the prediction models could sample markers for only a limited set of nuances from the yet-unknown population of all possible outcome-relevant nuances. Therefore, we had to consider the extent to which items would out-predict domains as the lower-bound estimate of such a tendency. Most associations were longitudinal over about 5 years, which somewhat reduced the risk that personality ratings were 'contaminated' by outcomes, or the other way around. MATERIALS AND METHODS Participants This project used data from the National Child Development Study (NCDS), an ongoing longitudinal study of 17 634 individuals born in a specific week in March 1958 in Great Britain, and of a further 929 individuals born in the same week abroad who immigrated to Great Britain before age 16 . To date, a wide range of variables reflecting different aspects of the cohort members' lives have been measured in nine separate sweeps, at ages 7, 11, 16, 23, 33, 41\\/42, 46\\/47, 50\\/51, and 55. Here, data collected in Sweep 8 (2008\\/2009, age 50\\/51) and Sweep 9 (2013\\/2014, age 55) were used. Data in Sweep 8 were collected in a 55-minute face-to-face interview, as well as using a selfcompletion questionnaire posted to participants prior to the interview. Data in Sweep 9 were collected by first inviting cohort members to participate online and subsequently contacting non-respondents via telephone. Measures Predictors Personality data were collected as part of the Sweep 8 self-completion questionnaire, using the 50 items from the International Personality Item Pool (IPIP;), measuring Goldberg's (1992) markers for the Big Five (10 items for each trait). The items were answered on a 5-point Likert-type rating scale from very inaccurate to very accurate. Of the 9790 respondents in Sweep 8, selfcompletion questionnaire data were available for 8787. However, only the 8719 participants (4519 female) who had completed more than 80% of IPIP items were included in this study. After median replacement of 1326 missing item responses, individuals' item scores pertaining to each domain were averaged to calculate scores on the Big Five personality traits. There were no extreme cases of multicollinearity (r > .80) between any items. Correlations between the Big Five domains ranged from.06 to.38 in magnitude (median = .24). Outcome measures Variables collected in Sweeps 8 and 9 were screened as possible outcome candidates. For 7621 (3999 women) of the 8719 participants in this study, data from both Sweeps 8 and 9 were available. For the remaining 1098 (520 women) individuals, data were only available from Sweep 8. Most outcome candidates were selected based on previous literature and a theoretical rationale that they could be related to personality. A few outcomes, such as 'partner's age', 'attending concerts or theatre', and 'eating out' were included for purely exploratory purposes. During the outcome selection process, outcomes that were answered on an ordinal, interval, or continuous scale were given preference over binary ones. Similarly, where available, preference was given to outcomes measured in Sweep 9 over those measured in Sweep 8 to avoid criterion contamination, whereby an outcome measured concurrently with personality may have affected personality ratings or the other way around. For instance, how often one currently sees friends may influence extraversion ratings. Only five of the selected outcomes were based on data collected in Sweep 8. All 40 outcomes are shown in Table S1 of the Supporting Information (available from https:\\/\\/osf.io\\/2efnr), alongside the names of the original NCDS variables and a record of any changes made to these variables. The variables can also be seen in Table 1 of the main text. For detailed descriptions of how NCDS variables were measured, including exact wordings of interview or questionnaire items, see the NCDS documentation. Thirteen of the outcomes were derived from the original NCDS variables, either by creating binary variables from categorical variables with more than two levels or by combining two NCDS variables supposedly measuring the same outcome into a single variable (see Table S1). The remaining 27 outcomes were directly based on a closely corresponding NCDS variable. However, in many cases, minor changes were made to the variables (see Table S1). For 16 variables, the original coding of the NCDS variable was reversed in order to make the interpretation of personality-outcome relations more intuitive. For instance, the coding of the outcome 'volunteering' that originally ranged from (1) at least once a week to (4) never was reversed in order for higher values to correspond to volunteering more frequently. Following screening for outliers and normality, extreme values for a few cases were either excluded or capped at a certain value for the outcomes 'income', 'body mass index', and 'working hours per week'. In addition, three variables, 'alcohol units per week', 'income', and 'body mass index', were log-transformed to make their distributions more normal. Table S1 also provides information on the outcomes' measurement levels, and the coding of response options for binary and ordinal outcomes after any changes had been made. In total, 13 outcomes were binary and modelled using logistic regression. The remaining 27 outcomes had either been measured on interval or continuous scales (seven outcomes) or on ordinal scales with at least four levels (20 outcomes); 23 of these variables were fairly normally distributed and modelled using the Gaussian distribution, whereas four variables--'sport (R)', 'volunteering', 'internet use (R)', and 'number of children'--were heavily skewed towards smaller values and were thus treated as having a Poisson distribution when building outcome prediction models. This is the reason why for 'sport (R)' and 'internet use (R)' the original NCDS coding was not reversed, meaning that higher variable values correspond to using the Internet less and doing less sport [the label (R) is included to remind the reader of this counterintuitive coding]. Outcome breadth versus specificity In order to be able to test the hypothesis that items tend to confer more incremental predictive value for narrower outcomes, we asked 19 individuals (aged between 19 and 30 years; 12 had no formal connection with psychology, and the rest were either former or current students or had otherwise some experience of psychological research) to rate each of the 40 outcomes in terms of their breadth versus (behavioural) specificity. The raters were presented with the following instruction: 'Below is a list of variables that researchers may use to characterize individual differences in behavior, attitudes, socioeconomic performance, health, and so forth. To what extent does each of these variables represent a relatively specific type of behavior? At the other end of the spectrum, the variables may represent broad life outcomes, reflecting the contributions of multiple types of specific behaviors that could happen over longer periods of time. Please rate each variable on this dimension of specific behaviors versus broad life outcomes'. The outcomes were rated on a 5-point Likert scale, with endpoints marked as specific behavior (1) and broad life outcome (5). Across the 40 outcomes and 19 raters, the intraclass correlation of single raters was.18, whereas the intraclass correlation of average ratings was.81 (calculated using the psych package;). For each outcome, we averaged the ratings of the 19 raters. Statistical analyses The selection of NCDS variables as outcome candidates, outcome creation from these via recoding, initial checks for outliers and normality, and deletion of extreme values and log transformations as described above were carried out in SPSS, Version 21. All subsequent analyses were carried out using R 3.4.2 . Model building and outcome prediction In order to avoid overestimating models' predictive strength due to over-fitting, the sample was split into a training sample (three-fourths) and a validation sample (one-fourth). We chose to use a notably larger proportion of participants for model training, because the larger the training samples, the more precise models tend to be and thereby the higher the predictive accuracy in the validation samples they generally allow for. Prediction models were fitted in the training sample, from which regression weights were obtained and then applied in the validation sample for outcome prediction. Squared correlations between predicted values and observed values were used as estimates of model prediction strength. The process of model training and outcome prediction was repeated 100 times in random splits of the sample. Prediction models for each outcome were built using a penalized regression, which shrinks regression coefficients towards zero by imposing a penalty on their combined size. Penalized regression can be employed when the number of potential predictors in a model is high, such as in GWAS , and when the predictors have high intercorrelations. Two widely used forms of penalized regression are ridge  and LASSO. Ridge regression applies a penalty to regression coefficients that depends on the sum of the squares of these coefficients, whereas the LASSO penalty depends on the sum of the absolute values of these coefficients. Both approaches have limitations. Ridge regression tends not to increase model parsimony as it rarely shrinks regression coefficients to zero. In contrast, LASSO leads to sparser (with more zero coefficients) and thereby more parsimonious and readily interpretable models, but it has a downside of often randomly selecting one out of many correlated predictors, setting coefficients for others to zero (ridge regression tends to shrink coefficients for correlated predictors towards each other;). This means that LASSO solutions are not unique. We thus employed the elastic net penalty , which combines both ridge and LASSO penalties, mitigating limitations associated with each of them alone. Elastic net tends to yield parsimonious models (due to the LASSO penalty), in which groups of correlated predictors are treated in the same way (due to the ridge penalty), all either being included or excluded from the model. We modelled binary outcomes using the binomial link (logistic regression), outcomes with a Poisson distribution using the Poisson link, and the remainder of outcomes using the Gaussian link. The optimal regularization parameter lambda (l) was obtained using 10-fold cross-validation within the training sample such that it minimized crossvalidated error across the folds. These procedures were carried out by the glmnet package for R. Adjustment We did not control for age as all participants were born in the same week, but we adjusted for gender by including it as a covariate into the prediction models built in the training samples (we did not residualize the outcomes for gender outright because not all of them were continuous). When obtaining predicted values in the validation samples, the regression weight of the gender covariate was set to zero, in order to obtain predictions only from personality while having regression coefficients for personality variables that controlled for potentially spurious sex effects. This was done to avoid inflating what would be interpreted as personality's predictive share in the outcomes. All R scripts are publicly available at http:\\/\\/osf.io\\/z9pr2. NCDS data are available from the UK Data Service, University of Essex (http:\\/\\/www.ukdataservice.ac.uk). RESULTS Model calibration We carried out a simulation to test the degree to which our procedure of training models in one partition of the sample using the elastic net penalty and validating them in another partition would guard against more complex (item-based) models out-predicting more economical (domain-based) models for purely statistical reasons (e.g. over-fitting). We simulated data similar to our empirical data (N = 9000; 50 items grouped into five traits; 40 outcomes), but with a particular underlying structure: five latent traits [N(m = 0, s2 = 1)] contributed to 50 observed variables [10 for each latent trait; N(m = 0, s2 = 1)] and any number between one and five of them could also contribute towards the outcome variables [N(m = 0, s2 = 1)]. The factor loadings of items on their latent traits were in a realistic range, varying from about just under.40 to just over.70 (they ranged from.35 to.76 in our real data), and the contributions of latent traits towards the outcomes varied from close to zero to potentially about.60 (mostly <.20, very rarely >.40). We then applied the same procedure on these simulated data that would be applied on the empirical data (comparing the predictive accuracy of 'items' and the five 'domain' scores they would make up), with a focus on the degrees to which item-based and domain-based models predicted the outcomes. We repeated the procedure 100 times. The simulation R code is available at https:\\/\\/osf.io\\/2fnqm. In most cases (83%), domain models out-predicted item models (as per underlying data-generating model), with the average predictive accuracy of item-based models being about 96% of the accuracy of domain models (1% and 99% quantiles of the ratio of item model predictive power to domain model predictive power were 0.80 and 1.02, respectively). Therefore, if personality-outcome associations were indeed driven by the purported latent traits that the item composites were designed to measure, our statistical procedure was likely to correctly show that domains out-predicted items. By implication, this meant that when item models would out-predict domain models in real data, it would tend to correctly indicate that the associations were at least in part driven by the unique characteristics that the items reflect. Thus, the procedures employed in this study were likely to effectively guard against more complex models outpredicting more economical ones for artefactual reasons. Predictive strength of item and domain models On average, approximately two-thirds of IPIP items and four to five Big Five domains were included in item and domain models, respectively (i.e. had non-zero elastic net regression coefficients, which are given the Table S2 of the Supporting Information; https:\\/\\/osf.io\\/8sjyr). Table 1 shows the mean variance explained (R2) in each outcome by item and domain models across 100 replications; we also report the 99% confidence intervals (CI) for these average estimates, calculated based on their variance across the 100 replications. Domain models for 33 outcomes and item models for 35 out of the 40 outcomes predicted more than 1% of variance (R2 > .01). Mean variance explained by domain and item models varied considerably across the outcomes and was highest for outcomes such as 'sleeping enough', 'NVQ (education)', 'feeling in control', 'racist attitudes', 'optimism', and 'general health' (R2 > .10). Among the outcomes, the least predictable from personality characteristics were 'frequency of exercising', 'partner's age', 'divorced', 'stopped smoking', 'diabetes', 'number of children', and 'high blood pressure' (R2 < .01 for domains). On average, across all outcomes, item models explained 5.45% of variance, while domain models explained 4.18% (medians were lower, 3.36% and 2.46%, respectively, indicating a positive skew). In other words, the average item-level prediction exceeded that of domain-level prediction by about 30%. For 37 outcomes, item models tended to out-predict domain models; for 33 of these, the 99% confidence intervals of average item-level and domain-level predictions did not overlap. For three outcomes ('cigarettes per day', 'high blood pressure', and 'partner's age'), the predictions were roughly similar in magnitude (difference in R2 < .001). Although these outcomes were among the least predictable from personality characteristics in the first place, generally, the ratio of the strengths of item model and domain model predictions was not significantly linked with prediction strength of either domain or item models, suggesting that, as a general tendency, items were stronger predictors for outcomes regardless of the overall degree to which these could be predicted from personality. When only considering the 33 outcomes for which domain models on average predicted more than 1% of variance (R2 > .01) in order to avoid including inflated estimates where the outcome was not predicted very much at all, the prediction improvement of item over domain models ranged from 3.58% ('cigarettes per day') to 94.75% ('voted in last election'). For these 33 outcomes, a paired Wilcoxon signed rank test showed the difference in variance explained between item and domain models to be significantly different from zero (p = 2.33 * 1010). These results stand in stark contrast with the simulation results presented above, suggesting that personality trait-outcome associations cannot be fully accounted for by the (underlying) Big Five domains. The breadth of outcomes We correlated the outcomes' average breadth ratings, given by the 19 raters, to the degrees to which they were predicted by either domains or items, and the difference and ratio between the two kinds of predictions (r-to-z transformed Columns 2 and 5 of Table 1 or the ratio between them). None of the correlations was sizeable (Spearman's r = .09 to 0). For example, among the outcomes most strongly outpredicted by the item models were specific behaviours referring to attending cultural events, using the Internet, or consuming alcohol as well as broad outcomes, such as educational qualification, income, or BMI. This suggests that the variability among the 40 outcomes in their behavioural specificity versus breadth had little to do with how well they could be predicted from either items or domains, or with the degree to which items conferred incremental predictive value over domains. Items tended to out-predict domains regardless of the breadth of what was predicted. Domain-level predictions were at least in part driven by nuances The typical predictive advantage of item models over domain models was arguably only moderate (about 30%). But it is important to realize that this observed advantage was unlikely to have entirely accurately revealed the degree to which the nuances captured by the items tended to predict the outcomes on top of, or rather than, the latent traits purported to underlie the domain scores. This is because the predictive value of nuances was always included in the domain scores, likely inflating the predictive value of the domain scores compared with what it would have been without these nuances included. In other words, the Big Five domains per se, independently of any particular items that happened to be included in their operationalization (but could not have been included, had the test constructors chosen alternative items that were equally reflective of the domains but with different unique outcome associations), could have done worse in the prediction. This reasoning is of course based on the assumption that the underlying traits that the Big Five scales are supposed to measure exist independently of how particular questionnaires approximate them--but this is a de facto standard assumption in personality research anyway. In order to address this possibility, the predictive power of item models should be compared with domains operationalized independently of these particular items (e.g. by using another questionnaire with items that measure only the domains and not nuances). We did not have such data, and we doubt that anyone has. However, as a post hoc analysis, we could remove a few items that most strongly predicted each outcome from the Big Five scales and re-estimate their predictive power after this (R code available from https:\\/\\/osf. io\\/bce2h). Assuming that the underlying traits that the domain scores were designed to approximate indeed exist independently of the particular items aggregated into them , the reduced scores should have measured the same underlying traits as full scores, possibly barring a small drop in measurement reliability. Therefore, if the associations were driven by the purported latent traits, their predictive value should have dropped minimally when a few items were removed. For each outcome, therefore, we compared the predictive power of the model based on 50 items with that of five Big Five domain models, with domain scores calculated based on either 49, 48, 47, 46, or 45 items in total; that is, one to five of the most predictive items were removed from domain scores, regardless of which domain they fell into. As above, the models were trained and validated in independent samples, and the procedure was repeated 100 times in random sample splits for training and validation, respectively. Dropping only the most predictive item (identified using elastic net regression, as above) from the domain score this item initially happened to belong to reduced the average (across the 40 outcomes and 100 permutations for each) predictive power of domain models by about 6%, whereas also removing the second, third, fourth, and fifth most predictive items from the domain scores these items happened to belong to reduced the average predictive power of domain models by about 11%, 14%, 16%, and 19%, respectively. In most cases, removing the five most predictive items left the shortest scale (i.e., scale from which the most items had been removed) with eight items instead of 10, but for five outcomes up to three and for two outcomes up to four items out of 10 were removed from what would become the shortest scale. For reference, when we removed five randomly chosen items from among the 50 items making up the five domains, average amounts of variance accounted for by domain models only decreased by about 1%. Assuming that domain models with the five most predictive items removed from the domains (i.e. leaving them based on 45 items instead of 50) constituted at least a somewhat fairer comparison for item models than domain models with the most predictive items included in their variance (because the scores still measured the same domains, regardless of whether they contained 8 or 10 items), the average predictive power of item models (R2 = .0545) was about 61% higher than the average predictive power of the domain models based on fewer items (R2 = .0335). Of course, this could also be an underestimate, because other (than the 'top-five') nuances uniquely predictive of some outcomes were still included in the domain scores and potentially still inflated the estimated predictive power of domains per se. Specifically, Figure 1 shows the average predictive value of both domain and item models when up to 10 (i.e. 20%) most predictive items were removed from them (for 30 of 40 outcomes, the shortest scale retained six or more items; for eight outcomes, the shortest scale retained five items, and for two outcomes, only three or four items were retained in the shortest scale; note, however, that outcomes were mostly predicted by four or five scales and most scales contained more items than the shortest scale). In the figure, the predictive values are grouped into quartiles according to the degree to which models based on 50 items predicted the outcomes (i.e. the top quartile represents the average predictive value of the 10 outcomes most accurately predicted from personality characteristics, which roughly corresponds to the top 10 rows of Table 1). Both item and domain models tended to lose some of their predictive power as ever more items were removed, and this tendency was fairly similar regardless of the degree to which the outcomes had been predicted in the first place (across the four quartiles, removing 10 items decreased the average predictive value of domain models by 26% to 34%; the average across all 40 outcomes was 31%). However, while the predictive advantage of item-level models did not pertain to a few 'top nuances' for the most predictable outcomes (in the top quartile, the predictive advantage of items was 27% with all items included, and it was still 21% with top 10 items excluded), in outcomes where the overall amount of variance accounted for by personality was smaller, this seemed to be the case. For example, for the least predictable outcomes, removing only a few of the most predictive items resulted in domain models being on par or even outperforming item models. This tendency suggests that the more predictable from personality characteristics an outcome was, the more this prediction was driven by nuances. These findings indicate that domain-level predictions are often likely to be at least in part driven by the nuances that happen to be included in them. Item-level predictions were mostly not driven by domains In order to estimate the extent to which item-level predictions were driven by their common variance, ostensibly reflecting underlying domains, we carried out another post hoc analysis. Specifically, we reran the item-level predictions (as above, training models on 75% of the sample and validating the models in the remainder of the sample and repeating this procedure 100 times) using the residual variance of items (Mottus et al., in press;). Specifically, items were regressed on the scores of all five domain scores using linear regression, and the residuals were saved (the item being residualized was removed from its intended domain score at the time to avoid regressing the item on itself). The item residuals tended to correlate highly with item scores before residualizing, with r = .69 to.94 (M = 0.83). On average, the models based on item residuals explained 5.11% of variance in the 40 outcomes, and for 33 of them, the percentage exceeded 1%. Therefore, removing the domain variance from items only attenuated their average predictive value by about 6%, and the average item residual model still out-predicted the average domain model by about 22%. This result reinforces the notion that much of the predictive value of personality for outcomes tends to stem from the characteristics that are aggregated into traits rather than whatever underlying causal entities the aggregates (items' common variance) are purported to approximate. DISCUSSION We explored the usefulness of personality questionnaire items as personality markers (in a way analogous to genetic markers, SNPs) for representing personality-outcome associations. We found that prediction models based on questionnaire items (similar to prediction models built from GWAS data in genetics) accounted for a non-zero share of variance in most outcomes. In fact, the item models mostly showed greater prediction strength than models built from Big Five domains, with an average of 30% more variance explained. We also made a case that this percentage likely underestimated the degree to which the unique variance in items was predictive of outcomes on top of, or even rather than, the traits purportedly underlying the Big Five domains. This is because the domain-level predictions were inflated by the unique variance of the items included in them, and items that had been residualized for the domains predicted the outcomes only slightly worse than items that included the domain variance. Furthermore, we found that the degree of the incremental predictive value of items over domains did not depend on the breadth (versus behaviour specificity) of the outcome. These findings were based on a large sample of more than 8700 participants, mostly longitudinal associations, and a wide range of diverse outcomes. Moreover, we used rigorous statistical procedures that guarded against model over-fitting, as shown by the simulation. Importantly, unlike most studies that link psychological constructs with outcomes by fitting statistical models and estimating the performance of these models in the same sample(s), in this study, outcomes were predicted from models that had been trained in independent people. This means that we quantified genuine predictive power rather than just correlations. Collectively, the findings suggest that particular personality-outcome links often pertain to specific personality characteristics rather than the broad Big Five traits these characteristics are ostensibly only indicators of. Personality links are pervasive, although often expectedly weak in magnitude The present findings reinforce the conclusion that the associations of personality characteristics with life outcomes are ubiquitous. Even though the selection of outcomes was not entirely random in this study, we can hypothesize that most markers of individuals' socio-economic success, health, or social behaviour can, to some extent, be predicted from personality characteristics included in omnibus personality models. Of course, this does not mean that the links are necessarily reflective of causal contributions of personality to these outcomes , but sometimes they may be--patterns of behaviour can have consequences. However, the effect sizes were generally weak, with on average slightly over 4% of variance being predicted from the five Big Five scores and about 5.5% from their 50 items. This has several mutually non-exclusive explanations. One possibility is that the tendency for low effect sizes is an accurate reflection of reality: anything that people differ in is likely to have a myriad of causes--many of them possibly idiosyncratic--and personality differences at any single point may constitute only a fraction of them. Another possibility is that the 50-item measure used in this study covered only a small sample of potentially relevant personality characteristics, as either domains or nuances. It is only too likely that the prediction models--especially item-level models--could have performed much better had a more comprehensive measure been used; we will also discuss this issue below. A third explanation for the relatively modest effect sizes is that they resulted from out-sample predictions and were little, if at all, upwardly biased by over-fitting--unlike most results reported in the literature. And yet it should be noted that nearly a quarter of outcomes did share over 10% of their variance with 50 item-level personality characteristics. What do the findings tell us about how personality intersects with outcomes? We expected personality characteristics, especially the broad Big Five traits, to be stronger predictors of broad life outcomes that aggregate the accumulating consequences of a wide range of behaviours, thoughts, feelings, and aspirations , as opposed to more specific behavioural outcomes. This would have been consistent with the possibility of there being broadly acting underlying personality traits casting their non-specific influences through narrower characteristics (nuances) whose (unique) relevancy only depends on specific outcomes. Although the nuances would then be aetiologically more proximal to the outcomes and might therefore have stronger links with them, it is unlikely that personality questionnaires, especially short inventories such as the 50-item IPIP, cover the nuances specifically relevant for each and every outcome. For the most part, then, the nuances reflected in test items would serve as mere indicators (measurement devices) of the broader underlying traits. But this did not appear to be the case: the degree to which outcomes were predicted from personality did not track their breadth\\/specificity, and items tended to out-predict domains for broad and specific outcomes alike. But could it be that the items' incremental predictive validity results from item-outcome overlap ? This was unlikely for the present findings. For example, among the outcomes for which items made the biggest incremental contribution (in relative increase terms) were voting, being self-employed, time spent working, smoking, alcohol use, BMI, educational qualification, occupational social class, number of cars owned, income, and Internet use, but none of the IPIP items made any reference to them. Indeed, for most outcomes, it is hard to see how they would be more obviously connected with any individual IPIP item than with any Big Five domain. And yet items were collectively more strongly linked with the outcomes than the domains, even if the domainrelated variance had been removed from the items. Moreover, the more outcomes were incrementally predicted by items, the less likely this was driven by only a few items. For example, for the 10 most predictable outcomes, even after dropping the 10 most predictive items, item models tended to out-predict domain models by nearly the same ratio than the models with all items included. A plausible interpretation of these results is that the associations of personality and outcomes do not generally pertain to the ostensible underlying traits. Instead, outcomes may be highly polynuanced--linked with a wide range of specific personality characteristics--such as phenotypes are generally polygenic. If so, personality trait scores are correlated with outcomes because questionnaire items sample from among the nuances that are either directly linked with the outcomes themselves or linked with other nuances that are relevant for the outcomes. The latter possibility of indirect associations between items and outcomes contributing to predictive power recasts the idea that genetic markers (SNPs) can be linked with phenotypes not only because they represent genetic variants directly relevant for the phenotype but also because they are in linkage disequilibrium with the directly relevant variants (i.e. serve as proxies). In the personality context, such 'linkage' may arise from direct causal associations among the nuances or their links with overlapping motivational characteristics , among other reasons. To the extent that this scenario applies, because personality test items constitute samples of markers of potentially outcome-relevant nuances, aggregating them into broad traits almost inevitably filters out some of the outcome-relevant information--which is exactly what the present results tended to show. The present findings do not only point to where in the (descriptive) personality trait hierarchy the outcomerelevant ingredients may lay but also reinforce the concept of nuances as potentially useful descriptive--and maybe even explanatory--units of personality (; Mottus et al., in press;). But why do we aggregate in the first place? Specific personality characteristics (tendencies for specific behaviours, feelings, cognitions, and motivations), as reflected in single items, are typically aggregated into composite scales in order to increase the reliability of measurements, in order to allow for more parsimonious models, and because it is hoped that the aggregates reflect some underlying, aetiologically homogeneous and causally potent properties of the human mind. What has been proposed in this article may not seem in lockstep with these aspirations. So? Indeed, the ratio of measurement error to substantive (non-error) variance is larger in single items than in aggregate trait scores, which could limit the value of item-based analyses. However, this is primarily a problem for studies based on small samples where model parameter estimates are less stable and aggregation of observations per person helps to increase their reliability. In sufficiently large samples, such as the one used in this study, parameter estimates are more stable even with lower reliability of single measurements, as the aggregation of observations across persons compensates the low reliability of single measurements. But even with smaller samples (N ~ 1000) than used here, parameter estimates pertaining to single items (e.g. age or gender differences, or associations with outcomes such as BMI) tend to be consistent across studies and must therefore be reasonably reliable (Mottus et al., in press). Likewise, item-based models for predicting outcomes are apparently less parsimonious than those based on higherorder traits, simply because there are more items than their aggregates. Generally, science strives for simplicity and parsimony, ceteris paribus. But it is exactly this latter clause-all else being equal--that is important here. First, if and when items do allow for outcome predictions that are more accurate, it might appear that reliance on what appears as less parsimonious at face value has some benefits. It has been argued that it is exactly prediction that psychology should strive for, rather than grossly simplified explanatory models with commensurately low practical value for describing what is going on in the real world. Second, relying on composites for, say, causal explanations requires the composites to have an appropriate ontology: they need to reflect something real about individuals rather than just being summaries of psychological 'stuff' . If the composites were just convenient summaries of items, it would still be the items that have to carry the explanatory weight in the end, and we might just as well represent the associations using these. Doing so would not mean doing away with personality as all the information pertaining to composite traits--and some more-would be retained, even though modelled differently. Again, we may rely on a parallel with genetics. The realization that the genetic architecture of complex phenotypes is so complex that it does not lend itself for a priori hypothesizing is exactly what has finally allowed geneticists to predict phenotypic variance from genome-based observations. That is, it is exactly suppressing the strive for apparent parsimony that has been useful--because reality cannot always be represented parsimoniously. In a way, of course, realizing and accepting that things are complicated (e.g. polygenic or polynuanced) can allow for the emergence of new, higher-order principles that focus less on which specific elements of a complicated system are interlinked but look for some general organizational principles of the system. It could be argued that domain-level predictions are particularly useful because they allow for generalized explanation. For example, an observed correlation between conscientiousness and longevity can be explained by a variety of behaviours that conceptually fall under this domain (e.g. being mindful about one's health and able to resist urges to behave in unhealthy ways, adhering to medical advice and treatment), regardless of whether these have been directly captured in particular questionnaires. However, to the extent that personality-outcome associations are actually not driven by domains such as conscientiousness but the specific characteristics that happen to be captured by the questionnaires or somehow in 'linkage' with them, such generalizations may be particularly dangerous--they would need to be tested rather than assumed. Knowing the patterns of correlations (or 'linkage') among these characteristics (the basis for domains) can guide our hypothesizing as to which characteristics could be relevant in addition to those that have been directly measured and linked with any given outcome. But we do not necessarily need domains per se to explain the associations. But it is also important to realize that identifying itemlevel (or nuance-level, by inference) associations does not preclude aggregation. For example, outcomes could then be predicted from polynuance scores, which are weighted aggregates of items (weights being the associations of the items with the outcomes), exactly as phenotypic variance is being predicted from polygenic scores, which are weighted aggregates of genetic markers. In addition to the prediction of outcomes for which the polynuance scores were initially created, other outcomes could be predicted, or associations between polynuance scores created for different outcomes could be calculated. This would allow exploring the extents to which different outcomes either correspond to different personality profiles or are independent with respect to their personality-related mechanisms. Again, this recasts an extremely useful concept in genetics, genetic correlation , which quantifies the extent to which different phenotypes are linked with overlapping genetic variance. Geneticists study patterns of genetic correlations among phenotypes to learn about their genetic aetiology , and personality psychologists could study patterns of 'personality correlations' (e.g. correlations between polynuances scores for different outcomes) to learn more about how personality relates to outcomes. These patterns may inform us on some of the general principles regarding how personality intersects with phenomena outside the personality domain. What do the findings tell us about personality traits? As recently outlined by Baumert et al. (2017), one of the most important questions regarding the aetiology of personality traits--defined as correlated patterns of behaviour, thinking, and feeling--is whether each of them corresponds to a shared set of processes (amounting to a latent common cause) exclusive to this particular trait ('correspondence') or whether they arise from more complex interaction processes among some more basic components of personality ('emergence'). It is possible to think that these basic components represent what we have termed as personality nuances. To the extent that the former scenario applies and the Big Five domains approximate the shared processes constituting latent causes of particular traits, one could expect personality-outcome associations to be mostly driven by the domains. To the extent that behaviour, thinking, and feeling coalesce into what appear as traits because of more widespread interactions among them, there is less reason to think that the personality-outcome association should be driven by the traits per se, because the behaviours, thoughts, and feelings that give rise to them are causally autonomous and they are not exclusively aligned with any one trait alone. If so, the present findings may be more in line with the 'emergence' explanation of traits, although not directly supportive of it. Implications for behavioural interventions Findings that personality traits are linked with a range of positive and negative life outcomes have lead researchers to consider the possibility of intervening on relevant personality traits to obtain desirable changes in these outcomes. For example, Jokela et al. (2013) discuss the possible effect of increasing conscientiousness on improving life expectancy. When and to the extent that personality-outcome associations are driven by nuances rather than broadly acting underlying trait domains, potential interventions aiming to change outcomes by changing personality ought to first identify the most relevant nuances for these outcomes and then specifically target these. On the one hand, this could be easier than targeting domains, which would be a more attractive course of action if the associations were driven by domains per se. For example, changing a habit is likely to be easier than changing the whole collection of behaviours, thoughts, feelings, and motivations that conscientiousness encompasses. On the other hand, if the number of relevant nuances is large, selecting the best targets may be complicated. Nevertheless, our findings suggest that personality-based interventions may generally be more successful when focusing on more specific behavioural, cognitive, affective, and motivational tendencies. Limitations and future directions Probably the biggest limitation of this study is that the 50item Big Five questionnaire, IPIP, is likely to cover only a very limited set of nuances. Not only is the sheer number of items small, but these items also tend to overlap in their content, for example, 'I seldom feel blue' and 'I often feel blue' or 'I have a vivid imagination' and 'I do not have a good imagination'. As a result, the potential benefits associated with item model prediction may be much more substantial when more comprehensive personality measures are used. Moreover, future studies should go beyond a preconceived and contrived construct space. Specifically, most existing personality questionnaires are explicitly designed to include sets of items that each measure one of the Big Five domains and nothing else. Within these sets, items are selected to maximize their common variance in order to ensure questionnaires' internal consistency--that is, items are designed to overlap in content. Consequently, by design, most personality questionnaires are likely to measure only limited sets of nuances, even when they include large numbers of items. In addition to using a range of existing questionnaires, future research should investigate whether outcome prediction can be further enhanced by deliberately selecting a diverse range of items and covering as broad a range of nuances as possible. To do so, more nuances should be identified, for instance, by subjecting data from large personality item pools to clustering procedures. As item-level analyses appear to confer substantial additional predictive value, reliably detecting this will require large samples--another lesson we can learn from genetics. It is a common practice in GWAS studies to aggregate samples, often using harmonized or linked measures of the outcomes. Also, GWAS studies are often making their findings publicly available to facilitate collaborative efforts (e.g. the LD Hub; http:\\/\\/ldsc. broadinstitute.org). Similarly, personality researchers should begin publishing item-level raw data and outcomes, as this will facilitate the identification of item-level associations and predicting outcomes across multiple studies. For example, what we did across subsamples could be done across studies. At least, item-level association profiles should be published, so that they can be recycled (e.g. for predicting not-yet-measured outcomes) or meta-analysed in other studies (e.g. Mottus et al., in press). Here, if only implicitly, we have treated outcomes as dependent variables--something to which personality may potentially contribute to. Of course, what we conceptualize as personality may often partly result from variability in the outcomes such as educational qualification or occupational level, or they may spuriously correlate due to shared causal factors such as polygenic influences. However, regardless of the direction of the causality between personality and outcomes, representing their associations as accurately as possible is likely to contribute to a better understanding of them. Also, we should note that most associations were longitudinal in this study, with personality being measured about 5 years before outcomes: in some cases, this may have diminished the probability of outcomes (e.g. 'sleeping enough') 'leaking' into personality ratings. CONCLUSION We report that predictive models based on 50 items, treated as markers of personality nuances, tend to explain more variance in a wide range of outcomes than models based on the Big Five domains. On average, the predictive advantage was an admittedly modest 1.3%. Should anyone care? We think that there are reasons to heed these findings. First, although the difference between more parsimonious domain-level models and more complex item-level models is not large in absolute terms--most effect sizes are small to start with and probably for a good reason--outcomes are multiply determined. Therefore, the relative difference is more substantial: moving from domain to item models confers a 30% increase in the amount of variance accounted for in outcomes. Second, this is likely to be a lower-bound estimate of the incremental value of item models, given that the instrument used in this study was short and limited in item content, and it was deliberately designed to measure the Big Five traits and nothing else. With more comprehensive item pools, the incremental predictive value of item models will probably be larger. Third, our analyses lend credit to the hypothesis that personality-outcome associations, even if they are modelled using more parsimonious domains, are driven by the specific personality characteristics that individual items are markers of, either directly or by means of being in 'linkage' with the directly relevant characteristics. Therefore, even if and when domains do allow for a reasonable prediction of outcomes, they may often not be useful as explanatory units for the associations. In order to move from correlations towards explanations, item models may turn out to be more helpful in the end. To the extent that causal contributions from personality to outcomes are plausible at all, we now have evidence that outcomes may be highly polycausal (polynuanced) and that this is especially plausible for outcomes that are more strongly linked with personality. These findings alone are informative. In conclusion, where sample sizes are large enough, future personality research could routinely build prediction models from items, in addition to domain-based models. Importantly, this comes at no additional cost in terms of data collection. Ultimately, more accurate descriptions can help with more realistic explanations.\",\"1135747313\":\"INTRODUCTION Research Objective Allowing that socialized knowledge is embedded in the language also through the tendency of words to co-occur together across relevant documents, this study argues that such linguistic correlates can reveal much about trust and distrust--key socialization beliefs. That proposition is supported by projecting questionnaire items about trust and distrust and their familiarity antecedent and a behavioral outcome on a semantic space (discussed below) that was built out of a relevant corpus of three psychology textbooks , and then analyzing the resulting cosine distance matrix of those questionnaire items. The analysis shows that not only are expected theoretical correlations supported, but also that trust and distrust can be statistically differentiated in this manner--something that survey research using questionnaires had difficulty doing. The ability to mine such knowledge from language may be another tool to study human behavior through text analysis in cases where surveys cannot be given to human subjects, where the context is unknown to them, and where constructs that cannot be easily differentiated such as trust and distrust need to be studied. To clarify, we are not claiming that this method replaces surveys, only that it could complement survey research. The Importance of Trust and Distrust in Human Behavior Interpersonal trust is a key driver of human behavior and a key determinant of interpersonal relationships because it allows people to assume, rightly or not, that they know how those they trust will behave. At the core of trust theory  is the recognition that people are independent agents who cannot be fully controlled and that these people are not even consistently rational in their behavior. Therefore, contends trust theory, trying to understand how others will behave can introduce so much social uncertainty as to be cognitively overwhelming to the extent that people might refrain from interacting with others they do not trust because they do not understand what is going on. Knowing how the trusted party will behave, i.e., trusting them, allows people to reduce that otherwise overwhelming social complexity to more manageable levels by assuming that the trusted party will behave in expected socially acceptable manners and not in other unexpected socially unacceptable manners. Because it allows reducing the otherwise overwhelming social complexity to manageable levels, and in doing so allows people to assume that there is a common understanding of what behavior is permitted, interpersonal trust is a key driver of social and economic structures. Trust also determines the preference of one vendor or company over another in contracting relationships, again, presumably because the trusting party assumes it knows how the trusted party will behave , and whether any interaction will even occur because when the risk of not knowing what the trusted party will do is too big then people refrain from interacting. Because of those reasons, trust is also a key determinant in the adoption of new IT  of many kinds including ecommerce , virtual teams , online communities , online software marketplaces , online consumer marketplaces such as eBay , e-banking , e-government , among others. Trust is even a determinant of susceptibility to phishing. Basically, trust is a key construct in human behavior. Trust, as often defined in management papers, is about \\\"the willingness of a party to be vulnerable to the actions of another party based on the expectation that the other will perform a particular action important to the trustor, irrespective of the ability to monitor or control that other party\\\" (;, p. 712). This willingness to trust is based according to Mayer et al. (1995) on beliefs about the trustworthiness--ability, benevolence, and integrity--of the trusted party. That assessment of trustworthiness is modeled by Mayer et al. (1995) as the consequence of previous interactions with the trusted party. As research showed, that assessment of trustworthiness can also be the result of the trusting person's propensity to trust, often modeled as initial trust, that is based on lifelong socialization , a propensity that is influenced inter alia by socialization and national culture. In the technology context, for ecommerce as an example, this initial trust may be even more important than the perceived usefulness and ease of use of the IT. Distrust is closely related to trust and is an integral part of trust theory, but it is not just the opposite of trust. Even early on in the study of trust it was recognized that the breakdown of trust results in more than just a reduction in the level of trust in that such a breakdown often results in a transformation of the relationship to one of avoidance. Conceptually, distrust is a separate construct entirely from trust , dealing with negative beliefs about the other party. Although research based on survey data has found it hard to statistically differentiate between trust and distrust , neuroscience has shown that the neural correlates of trust and distrust are distinctly different  with trust being mostly associated with neural-correlates that are associated with rewards such as the putamen (the outer part of the lentiform nucleus of the brain) and with information processing such as the dorsolateral prefrontal cortex (DLPFC) while distrust is associated with neural correlates associated with aversion such as the insular cortex and with fear such as the amygdala. Thus, while trust brings people together based mostly on rational reasons, distrust separates them based on fear and aversion. The ability of neuroscience to identify this distinction where survey research could not do so has been one of the reasons suggested for adopting neuroscience into the mainstream of social sciences research. As this study will show, the ability of text analysis to also make this distinction is a point for consideration. Trust, Distrust, Familiarity, and the Objective of This Study A key reason why people trust or distrust, and the context of this study, is because people are socialized into trusting strangers , or a specific group of strangers , or distrusting them as the case might be , through socialization and the historical and social information that that socialization conveys. In a nutshell, socialization is \\\"learned\\\" familiarity with people at large or with a specific group of people one has not yet encountered. This kind of learning through socialization is typically portrayed as a lifelong experience starting at childhood through education and interaction with other people. People are taught whom to trust and whom to distrust sometimes even on a purely irrational and historically and socially totally irrelevant basis as an integral part of their \\\"education\\\" of learned prejudices and \\\"truisms\\\"1. Across business contexts, familiarity is a significant predictor of trust. Being familiar with the trusted party means that the trusting party knows better what to expect, what the rules of conduct are, how the trusted party might react, and has a reasonable idea of the trusted party's integrity, benevolence (or at least caring), and capability based on past performance. Being familiar with the other party taps into many of the reasons why trust is needed: being able to assess the trustworthiness of the trusted party as a way of reducing risk , being able to better understand what is happening and plan and respond accordingly , as well as reducing distrust across social group boundaries. Indeed, choosing a familiar party to contract with can be so compelling an argument that often people will prefer to contract with a party they are familiar with regardless of the price. This is not just that trusted vendors can charge a price premium. It is that in some cases, specifically low cost contracts to develop software and related services, the trusted party will always win the bid over unfamiliar parties regardless of price. And, when price does come into play, such as in large software contracts signed by a bank, then the familiar party will on average be given the contract on terms that require less oversight such as contracting on a time and materials basis rather than a fixed price contract. Socialization, and the familiarity it creates, is a powerful tool, but not all its teachings are direct and overt. Some of the messages that socialization broadcasts are subtle and hidden in the language we speak. Indeed, as immoral as it may be, the dictionary definition of many words, e.g., racial or social classifications, carry such social praise or stigma that make people feel that they are somewhat \\\"familiar\\\" with the other party based on what they were taught and thus leads them to trust or distrust total strangers based on this socialization. A rather innocuous example is the one Zucker (1986) gives of US banking in the early 1900 where people trusted bankers based on the social class of those bankers who, presumably because one was taught that they belong to a \\\"better\\\" social class, can be trusted. In other words, familiarity can also create distrust. The importance of familiarity in building trust, and by extension reducing distrust, seems to be true across business contexts. This applies in contracting between organizations  1As a demonstration of this trust-building or distrust-forming socialization process, think of how many times you heard, or maybe even gave, advisory or precautionary sociological \\\"truisms\\\" such as \\\"Don't talk to strangers\\\" or \\\"What do you expect of (fill in your preferred racial\\/religious\\/social\\/political etc. noun)? They are always right\\/wrong\\/racist!.\\\" as well as ecommerce  and ecommerce recommendation agents , as it is in daily life. Accordingly, the objective of this study is to argue for linguistic socialization and its implications in a new and expanded context. We argue that trust and distrust are registered into the very language we speak and that therefore some aspects of the socialization into trusting and distrusting can be studied through text analysis. To emphasize this registered socialized embedded knowledge, we label it linguistic correlates. Technically, it is the same as analyzing how words and vectors of words correlate (or co-appear), expanding on the logic of Gefen and Larsen (2017). The next sections will show that running text analysis on a semantic space that was built by analyzing a corpus created out of the paragraphs of three psychology textbooks --arguably a reasonable trustworthy repository of theories on human behavior--supports this proposition. This semantic space was chosen because it is accessible in the public domain at lsa.colorado.edu together with an interface that allows projecting combinations of entire sentences on that sematic space. The result of that projection is a matrix of cosine distances that can be extracted for further analysis. That further analysis in covariancebased structural equation modeling (CBSEM) will show that projecting sentences that comprise of survey measurement items dealing with trust, distrust, and related constructs allows the reconstruction of a statistical model based on the cosine distances among each pair of those sentences. And, that in doing so, known psychological relationships of trust and of distrust can be reconstructed. Deriving Linguistic Correlates of Trust and Distrust Through a Semantic Space Just as the conclusions being drawn about sociological events and the interpretation of social constructs will differ based on the sources being read, so too it is recognized that the results of text analysis will depend on the corpus being analyzed and its reliability and connection to the topic being studied. Accordingly, as the study of trust and distrust is clearly in the realm of psychology, and undeniably many other social sciences related to psychology, we chose a semantic space derived from a corpus based on textbooks in psychology. The \\\"psychology\\\" semantic space used in this study was created based on a total of 13,902 textbook paragraphs containing 30,119 unique terms. The approach depends on a bag-of-words representation where each paragraph's word order is abandoned and frequently used terms downweighed before the termdocument matrix is subjected to a singular value decomposition (SVD) as described in Larsen and Monarchi (2004). In general practice, 300-500 dimensions are retained. In the creation of this specific semantic space a 398-dimension space was created. This means that each word that is part of one of the textbooks is represented by a 398-dimensional vector of what that term means in the context of all the other words. The meaning of a sentence is inferred through the addition of the vectors for each of the words in the sentence, a process known as projection. That sematic space is available in the public domain through an interface at lsa.colorado.edu, shown in Figure 1. Specifically, survey items from previous research that dealt with trust were projected into this semantic space together with items dealing directly with distrust. The cosine distances among the projected survey items as produced by lsa.colorado.edu were then analyzed using CBSEM. The results discussed in the next sections are as theory predicts. Specifically, the questionnaire items were copied into lsa.colorado.edu, shown in Figure 1, and the derived cosine distances, shown in Figure 2, were then copied and arranged in a matrix form ready to be analyzed with Mplus, shown in Table 1. The questionnaire items appear in Table 2. The Potential of Studying Linguistic Correlates in the Study of Trust and Distrust Showing, as this study does, that studying the word associations of trust and distrust produces equivalent results as survey research on trust did, raises the possibility, and clearly more research is needed before such an argument can be made unequivocally, that studying the linguistic registration of trusting behavior in an appropriate source (a textbook on human psychology in this case) might allow new avenues for studying trust and distrust. Such avenues might allow the studying of trust and distrust also in contexts that cannot be studied or do not exist anymore. The context might have changed and the people not available anymore, but at least their study as they are registered linguistically can still be done. This might include studies such as how the meaning and importance of trust and distrust as registered through word associations changed overtime. Given that one cannot administer questionnaires to people who lived in London 150 years ago, but one has easy access to the books written by Charles Dickens and others of that period, such a possibility might open the door to new understandings. Such an approach to studying trust and distrust--and by extension other constructs, beliefs, attitudes, behaviors, etc.-might also reveal, in a broader context, why non-native speakers of English answer the same questions differently in English versus in their native language, even when the surveys are an exact translation of each other. This approach might potentially also point out possible reasons for social differences about trust and distrust, and provide support for the hypothesized effect of history on trust and distrust as portrayed by Fukuyama (1995). Indeed, comparing the word associations of trust and distrust and the meaning revealed through those in the books of Charles Dickens compared to Henrik Ibsen might be quite revealing. Moreover, and perhaps this is going on a tangent, if indeed part of our socialization as humans is registered in the language we speak through word correlations, then this might be especially important in predicting how people might understand the role trust and distrust play also in as of yet not quite there technologies. To put this into perspective, research on how we as people trust and distrust others has been about another party that is human or composed of a group of people. Specifically, in that past research the trusted party may have been a person [e.g., Blau (1964)], a community [e.g., Ridings et al. (2002)], a market populated by people [e.g., Pavlou and Gefen (2004)], an organization [e.g., Mayer et al. (1995)], a government [e.g., Warkentin et al. (2018)], or a human-like IT interface such as an avatar. But what about a trusted party whose intentions and intelligence are not human or related to people? Being able to understand, even if only through the knowledge embedded in language, why people trust or distrust in such a case may prove essential with the growing influx of AI into daily lives where AI is creating an environment that is sometimes beyond human understanding, as demonstrated recently in a case of a self-taught AI beating the world champion in gos without the world champion even understanding some of the strategies the AI applied. The linguistic correlates of trust and distrust might enable modeling human reaction also in such cases of interacting with an AI where the reasons cited above for the importance of trust and distrust do not readily apply. After all, there are no rational assessments of the behavior of an AI agent playing go, nor are there considerations of risk, familiarity, social strata considerations, social identification, etc. Nonetheless, being able to model in statistical terms the human response to such a world could be revealing. The next sections will describe the method we applied to study the linguistic correlates of trust and distrust, why theoretically one might expect there to be linguistic correlates, and some details about the method, and then report the statistical analysis and discuss the results and their potential. MATERIALS AND METHODS Replicating the established hypotheses that familiarity builds trust, and adding to it that familiarity may also lead to the opposite, i.e., distrust, as Fukuyama (1995) relates, and further extending into both trust and distrust as major considerations in the decision to purchase online , the research model is presented in Figure 3. This figure shows the output of the standardized Mplus analysis on the model. Boxes represent the measurement items, which in this case are the questionnaire items that were projected onto the semantic space. These items and their codes appear in Table 2. The covariance among all pairs of those measurement items is constrained in CBSEM so that only the covariance values associated with the paths that are shown in the model as arrows are expressed. All other covariance values are fixed at zero. Fixing those paths to zero frees enough degrees of freedom to include in the model also latent variables, i.e., constructs that while they cannot be measured directly are reflected by the explicit measurement items, as well as how those constructs relate to each other. In this formalization, each measurement item is a function of the latent variable it is assigned to, the circles, and of an error term. For example, fm1, being one of the familiarity measurement items, is predicted by the construct \\\"familiarity\\\" with a path estimate of 0.946 and standard error of 0.006 as well as by a random error term with a path estimate of 0.106 and a standard error of 0.012. The model of the paths leading to the measurement items is known as the measurement model. The paths among the latent variables is known as the structural model. The structural model is what the theory talks about. For example, that trust affects use is shown by the path between the circle labeled trust and the circle labeled use. Those latter paths represent the underlying proposition that the pattern of findings, i.e., supported hypotheses, as revealed in previous survey and archival data research methods can be extracted through linguistic correlates derived from an appropriate corpus. Preparing the Model for Study The model was tested by projecting the [Intended] Use, Trust, and Familiarity scales based on Gefen et al. (2003b) and ad hoc items of Distrust on the psychology semantic space at lsa.colorado.edu. These questionnaire items are shown in Table 2 with the subsequent Mplus estimated standardized loadings of each item on its related latent variable (construct). The first column contains the item code. This code appears also in Table 1 and in the Mplus code in the Appendix. The second column shows the wording of each item, with a header to make it easier to identify which items relate to which construct. The third column contains the standardized loading of that item on the latent variable, i.e., construct, as produced by the Mplus analysis. The lsa.colorado.edu site receives as input a set of sentences (or individual words) that are to be projected onto one of several preexisting semantic spaces. See Figure 1. It then builds the cosine distances matrix of each sentence from each other sentence by running a latent semantic analysis (LSA) process. See Figure 2. The process involves projecting each possible pair of sentences as two vectors, each comprising all the words in one of the sentences, on a chosen preexisting sematic space. The idea behind LSA is that words (\\\"terms\\\" in LSA parlance) that tend to appear together have shared dimensions of meaning. What LSA does is to first create a term to document [frequency] matrix (TDM) of the original corpus, possibly preparing the data beforehand through stemming and other methods, weighing the terms, and then applying SVD to the TDM to reduce the dimensionality of the data. It is then assumed that words that appear together on the same principal component (dimension) after this dimensionality reduction exercise share some meaning. Words can appear in many principal components thus showing the richness of language and that the same word can carry many meanings. The result of the SVD is known as a semantic space. The semantic space analyzed already exists on the lsa.colorado.edu site. The vectors of the sentences can then be projected onto this sematic space, even though the sentences themselves never existed in the original texts. The comparison of these vectors allows a calculation of the cosine distance between them. At its core, LSA is about word co-occurrences. It is a data-driven approach, and some therefore see it as more objective. As argued, certain words tend to be used together, such as \\\"trust\\\" and \\\"purchase,\\\" so words take on meanings both in terms of the words with which they co-occur, and in terms of words with which they do not co-occur frequently, such as \\\"sky\\\" and \\\"purchase.\\\" Words that co-occur frequently will tend to have a smaller cosine distance between them, and, by extension, two sentences where each contains words that tend to appear in the other sentence will also have a small cosine distance between them. Importantly, LSA works in cases of second and third-level relationships where words do not even need to co-occur, but both co-occur with the same words. For example, LSA will tend to recognize that terms such as \\\"distrust\\\" and \\\"trust\\\" are related even if the words never co-occurred in the text analyzed, for example because both may appear together with the word \\\"transaction\\\" or the word \\\"relationship.\\\" Because these co-occurrences reflect language used to describe the world, the LSA word vectors contain within them reflections of our shared perceptions of how the world works. Much work has gone into understanding how LSA works relative to the human mind, and Landauer (2007, p. 31) even argued that LSA \\\"demonstrates a computational method by which a major component of language learning and use can be achieved.\\\" The applicability of LSA to partially replicate through text analysis survey responses by people seems to support this contention. Without entering the debate of what LSA does or does not do [cf., for example, Valle-Lisboa and Mizraji (2007)], we use LSA to address a specific question in a way that is mathematically rigorous and that can be replicated by anyone with an understanding of statistical methods. More details on how to run LSA in R together with a discussion of the methodological and statistical validity consideration are available at Gefen et al. (2017). As LSA is now widely accepted as a research method, with hundreds of uses within Psychology and Information Systems, we will not go into further depth on the process. Readers interested in this process are referred to one of many detailed descriptions, ranging from mathematical introductions  to conceptual explanations. We chose LSA for several reasons. First, it is an established and tested method and has been so for the last two decades. Second, it has been shown to simulate human thought processes, producing survey results that sometimes correspond to how human subjects answer the same questionnaire items , including assessing the meaning of words through their association with other words , and even simulating priming effects through word choice. LSA has even been applied in this context to support the supposition that the meaning of a word is derived through its associations to other words , and supporting that supposition even by comparing the LSA semantic meaning of a word with eye tracking. And, third, the method we apply, running a CBSEM analysis on the correlations derived from LSA semantic spaces has been previously applied to show that the widely supported model of IT adoption, the technology acceptance model (TAM) , can be supported by projecting the existing scales of that model on a semantic space that was created out of unrelated newspaper articles. The Underlying Idea Behind Linguistic Correlates As specified, the idea being propagated in this study is that socialization knowledge is to some extent ingrained in the language that we speak and write. And that this applies also in word co-occurrence relationships. As a result of this engraining, analyzing word co-occurrence relationships in relevant text could reveal some of that socialization knowledge. Such an argument is supported by the significant and consistent replication of the relationships between the perceived usefulness and the perceived ease of use scales of TAM  in both the measurement model (how items load significantly only on their assigned constructs and not on other constructs) and the correlation between the constructs in the structural model by projecting its questionnaire items on two newspaper semantic spaces. The argument for ingrained knowledge in language, expanding on the proposition advanced by Gefen and Larsen (2017), is that if certain words or combinations of words tend to occur together, then these co-occurrence tendencies might be registering socialized knowledge linguistically. Thus, for example, if the word \\\"distrust\\\" and the word \\\"avoid\\\" tend to occur together considerably more than \\\"trust\\\" and \\\"avoid\\\" do, while \\\"trust\\\" tends to co-occur often with \\\"purchase\\\" than \\\"distrust\\\" does then this co-occurrence might be registering that people tend to avoid that which they distrust but tend to purchase from those they trust. This kind of analysis may actually have the potential to reveal self-censored knowledge too, addressing a known problem with questionnaires. It is well-known that people completing surveys, even anonymous ones, consider both what they think the survey administer wants to hear and what they themselves are implying by their answers. Thus, it would be rather hard to elicit honest non-politically correct prejudices because people completing a questionnaire know that expressing such ideas openly is shunned by society, meaning that there is a bias in such data if it is collected through surveys. However, because LSA analyzes also indirect associations among words, it might catch such prejudices. Indeed, indirect associations of terms identified by LSA has been shown to be beneficial in the case of analyzing medical records to reveal important patterns in the population being studied  as well as how IT design battles evolve in the press. Moreover, terms that are not easily distinguished from each other in the statistical analysis of survey questionnaire items filled by people, might nonetheless be differentiated in text analysis because they each have their own distinct associations with other terms. This differentiation will indeed be shown in the next section. This is not an argument for causation. It does not mean that people behave as they do because of that linguistically ingrained knowledge, as implied in the \\\"Sapir-Whorf hypothesis\\\"  that language determines thoughts and behavior or in an Orwellian control of thought through a newspeak language. Rather, the argument is for correlation. People behave as they do for a myriad of reasons, and the language they and others use reflects those tendencies. It may be that their behavior--and more accurately in this case their story-telling about their behavior--reflects their socialization through language, but it may just as well be that language registers the shared aspects of theirs's and many others' story-telling. ANALYSIS RESULTS Analysis Process The measurement items' cosine matrix produced by lsa.colorado.edu was entered as input to Mplus version 7.4 and analyzed as a reflective CBSEM. In our measurement model, the reflective CBSEM measurement items are modeled as reflecting a latent variable, known otherwise as a construct. Thus, DT1, DT2, and DT3 all reflect the latent variable (construct) Distrust, and no other construct, while USE1 and USE2 reflect the latent variable Use, and no other, etc. If there are significant cross-loadings, i.e., a loading of a measurement item on a construct it was not assigned to, then CBSEM will identify that cross-loading in the modification index table together with an assumed kh2 improvement as well as a noticeable change in the overall fit indices of the model. The measurement model part of a CBSEM model specifies that pattern of measurement items to constructs loadings. The structural model then specifies the relationship among those constructs. Mplus analyzes both the measurement model and the structural model together, highlighting any problems with unspecified covariance or with measurement items whose covariance overlaps. It is standard procedure in CBSEM to drop items that have such problems , but it should be reported  as we do here. Items TR5 and TR6 were dropped because the cosine distance between them and between each of them and TR4 was 1.000, meaning that as far as the maximum likelihood algorithm that CBSEM applies as a default for continuous variables these three items are practically indistinguishable from each other. Being indistinguishable from each other, results mathematically in an Mplus observation that \\\"the sample covariance matrix could not be inverted\\\" when those items were included. No other pairs of measurement items had a cosine of 1.000 between them. Item TR3 was dropped to improve model fit (including TR3 did not change the overall model pattern but resulted in an RMSEA of 0.138). It is long established as an acceptable practice to drop items in CBSEM because of such reasons. The Mplus analysis was run specifying that the sample size was 400, which is the rounded number of dimensions created by lsa.colorado.edu for the textbooks when creating the semantic space. As is standard in Mplus for continuous measurement items, we retained the default maximum likelihood analysis. Overall model fit was acceptable : kh = 187.853, RMSEA = 0.085, CFI = 0.985, TLI = 0.979. The Mplus code is available in the Appendix. Interpretation of the Analysis The standardized structural model showed that Use was significantly predicted by Trust (b = 0.52, p < 0.001), Distrust (b = 0.18, p < 0.001), and Familiarity (0 = 0.34, p < 0.001). That Trust is a stronger predictor of Use than Familiarity is consistent with anthropological studies where knowing the historical context determines levels of trust and distrust that, in turn, determine behavioral intentions [e.g., Fukuyama (1995)]. These significant predictors of Use are consistent with the literature cited above. Familiarity significantly predicted Trust (0 = 0.79, p < 0.001) and Distrust (0 = 0.82, p < 0.001). This too is consistent with the literature cited above. The CBSEM model modeled Trust and Distrust as being correlated on account of these two constructs being portrayed in theory as non-overlapping opposite beliefs\\/assessments of each other with non-overlapping opposite consequences on behavioral intentions. The theoretical distinction between the Trust and Distrust constructs is also supported by fMRI studies. The distinction between Trust and Distrust as separate constructs is supported in the CBSEM model through the very low modification index values among the items of the Trust and Distrust constructs. Trust and Distrust as constructs are significantly correlated (th = 0.32, p< 0.001). R2 values were 0.97 for Use, 0.72 for Trust, and 0.66 for Distrust. Cross-loadings were low, as also indicated through the 2In CBSEM notation, exogenous (independent) variables (latent constructs) are labeled b, while endogenous (dependent) variables are labeled x. Paths between exogenous and endogenous variables are labeled 0. Paths between endogenous variables are labeled b. Correlation paths between endogenous variables are labeled th. Thus, because Familiarity is modeled as affecting Trust, Distrust, and Use, all the paths leading out of Familiarity are labeled 0. The paths between Trust and Use acceptable levels of the RMSEA statistic. Notice that LSA does not specify the sign (plus or minus) of the cosine distances. Hence, the Mplus model shows that the relationships between Distrust and all the other constructs are positive. That is a known limitation of LSA in that it measures the semantic closeness of words, or vectors of words such as the entire sentences of a questionnaire item, as an angle but where the direction of that angle is immaterial. Ad Hoc Analysis As an additional ad hoc analysis to establish that differentiating between Trust and Distrust indeed produces a significantly better model, a model that unites these two constructs was compared with the original model. Specifically, the kh2 of the original model (kh = 187.853) was compared with the kh2 of an alternative model in which Trust and Distrust were united into one construct. The resulting kh2 of this alternative model (kh = 1073.722) was significantly worse (1kh = 855.869), showing that separating Trust and Distrust produces a significantly better model. DISCUSSION Summary of the Results The proposition advanced in this study was that socialized knowledge is also ingrained in language, and that this registered knowledge can be extracted through text analysis tools such as LSA and subsequent statistical analysis. These linguistic correlates, as we call them, can be analyzed to both reconstruct existing hypotheses, and do so purely through text analysis and without resorting to distributing surveys to human subjects, as well as be applied to additional analyses not easily performed through survey research. This proposition was demonstrated in the context of studying trust and distrust as they relate to familiarity as an antecedent and to purchase (labeled \\\"use\\\" in other studies) as an outcome. The analysis supports this proposition, but also highlights some text analysis nuances that should be considered. The analysis shows that linguistic correlates can be analyzed to support the measurement model, showing that the cosine distances between pairs of questionnaire items that are projected on a relevant semantic space can then be analyzed through CBSEM to support the expected significant loadings of those questionnaire items on the latent variable they theoretically reflect. The linguistic correlates also enabled the statistical differentiation between trust and distrust (see ad hoc analysis in section \\\"Interpretation of the Analysis\\\"), which has been hard to do with survey research  even though this distinction is suggested in theory  and has been shown in neural science. The analysis also supports the next part of the proposition that the correlation patterns among those constructs, i.e., the structural model, are consistent with theory. The analysis, and between Distrust and Use are labeled b, and the correlation between Trust and Distrust is th. however, also shows that the cosine distance between some pairs of items was 1.000, i.e., a perfect overlap, producing a result that is seldom seen in data collected through surveys administered to human subjects, and requiring dropping items accordingly. The conclusion is that some aspects of socialized knowledge about trust and distrust are ingrained in the language we speak, and that that the registration of this socialized knowledge can be extracted through linguistic correlates to the extent that allows recreating relationships that theory implies. Implications for Trust Theory and the Possible Role of Linguistic Correlates Trust theory and the English language clearly differentiate between trust and distrust, showing that although the two terms are related in their contexts, they are not the same and do not even overlap in their meaning. Such a difference is shown also in this study where both trust and distrust are correlated to familiarity and to use as well as to each other, but their items significantly do not reflect the same, one, latent construct. That studying linguistic correlates could show that difference when survey research that analyzes human subjects' responses to questionnaires could not, and thereby possibly creating a misinterpretation that trust and distrust overlap in meaning, shows a potential contribution for analyzing linguistic correlates, or at least that linguistic correlates can add significantly to knowledge acquired through survey research. More specifically from a trust theory perspective, that Trust had a stronger standardized effect on Use (b = 0.52, p < 0.001) than Familiarity (0 = 0.18, p < 0.001) did, suggests that, as previous models [e.g., Gefen (2000)] show, it is mainly that familiarity builds trust and that it is mostly trust rather than familiarity that determines behavior. Extending that line of logic, that the standardized effect of Trust is considerably stronger than that of Distrust (b = 0.34, p < 0.001) suggests that trust is more important in determining behavior than distrust is in the context of providing information online (see wording of the USE1 and USE2 items) as projected on this specific semantic space. Likewise, that Familiarity affects both Trust (0 = 0.85, p < 0.001) and Distrust (0 = 0.82, p < 0.001) with an almost equal standardized coefficient and that those coefficients are considerably higher than the standardized correlation between Trust and Distrust (th = 0.32, p < 0.001), suggests that familiarity affects trust and distrust through two mostly unrelated channels. Such an observation is consistent with how Fukuyama (1995) describes the evolution of trust and of distrust in different cultures differently based on their histories. What builds trust is not what creates distrust. Such an ability to differentiate between trust and distrust was brought a decade ago by the burgeoning NeuroIS discipline. (NeuroIS is a name given to the discipline and society that studies neuroscience as applied to information systems). NeuroIS used that same need to differentiate between trust and distrust. NeuroIS then used that verification of the trust-distrust distinction through neural correlates to argue that because neuroscience could do so while questionnaire data research could not, to advance a key argument for the importance of such neuroscience research. The same argument may be applicable to text analysis and to linguistic correlates too. Not only can the study of linguistic correlates support behavioral hypotheses through the patterns of word co-occurrences, but it can even support hypotheses that survey data may not be able to. Neuroscience and text analysis are clearly not the same and they undeniably measure different data. Nonetheless, building on that same argument about the ability to study if two constructs might not be the same even when survey research cannot show it, text analysis does have the advantage over neuroscience in that it is cheaper and faster. There are potentially many other such constructs of interest that could be studied. Broader Implications for Text Analysis in View of Linguistic Correlates As Gefen and Larsen (2017) previously suggested, analyzing linguistic correlates may also add another tool to the toolbox that social scientists apply to assess, and maybe statistically control for, priming , and the inevitable introduction of common method variance in data collected by surveys. Moreover, text analysis, even if its results do not fully overlap survey analysis given to live subjects, may also provide a cheaper option to pretest existing questionnaires before embarking on a more costly data collection endeavor with subjects. To that, this study adds also the ability to statistically show the discriminant validity, i.e., to differentiate, between constructs that theoretically and linguistically are not the same, but that survey research has not been able to show their discriminant validity. Moreover, this kind of a method might be especially applicable to the study of contexts that cannot be studied by surveys, such as those unrelated to current actual experiences. Studying linguistic correlates might allow a glimpse into how people in the past thought, and, hence, how concepts of interest changed in their linguistic meaning and associations over time. Clearly talking to actual people or studying actual responses to surveys has its advantages, but there is no known current technology that allows us to ask Charles Dickens or Henrik Ibsen about their take on trust. Studying their writings is an obvious alternative. This method allows doing so semi-automatically. Likewise, such a method could allow studying how these linguistic correlates changed over time by comparing current literature with that of the past. The comparison of linguistic correlates might also reveal hints as to why, as the Introduction brought, non-native speakers of English answer the same questions differently in English compared to answering the surveys in their native languages, even when the surveys are an exact translation of each other. It may well be that part of the answer is that the linguistic correlates of the constructs being studied in those surveys differ across languages. Studying linguistic correlates might also reveal partially how people in the present might respond to technologies of the future. That is, studying linguistic correlates could provide a partial picture of the socialized knowledge embedded in the language aspect of why people do what they do. It might be impossible to study how people will react to new technologies such as new aspects of AI that are not available yet--and why in the context of this study they may trust or distrust those-but, looking into people's linguistic correlates might reveal at least the socialized knowledge embedded language aspect of that question. It might also reveal some hints as to why some cultures might be more open than others to accepting and trusting such AI. Such a glimpse could be of much importance considering that current theories about trust are geared at a person, group of people, or an anthropomorphized party. Current theories of trust address such a target by discussing reasons such as controlling risk and understanding the social environment. It is questionable if and how any of those reasons might apply to an AI. Studying linguistic correlates might at least identify possible motivations and drives that are socialized into language. This also suggests an avenue for possible future research into why people might trust or distrust even when the reasons provided by current research, such as controlling risk  or simplifying the social environment to manageable levels , clearly do not apply. Possibly, such a study of trust and distrust through language usage patterns as revealed through text analysis of a reasonably expert source such as textbooks may allow assessing how people might trust and distrust also in contexts that are beyond their ability to assess risks in or to understand. Limitations The study demonstrated the linguistic correlates proposition through an admittedly simple model. But the very fact that the model could be replicated at all suggests that indeed at least some aspects of social knowledge are recorded in language through the association of words. Presumably, as discussed above, this ingrained knowledge corresponds to how people think either because they learned or socialized that language embedded knowledge or because that language embedded knowledge recorded how people behave. Obviously, replication with other relevant corpora is necessary, but that the analysis supported the proposition is revealing. Limitations that apply to CBSEM would apply to this method too. Had the model been too complex then the \\\"noise\\\" of covariances that are not included in the model would eventually result in overall poor fit indices. Likewise, many of the overall fit indices, such as kh2 and RMSEA are negatively affected as the sample size increases. As the tendency in LSA is to have about 300 to 500 dimensions, and therefore the analysis would be modeled as a sample size of between 300 and 500 data points, the risk of having overall fit indices that do not match the criteria we apply to survey research may become an issue. Likewise, as with other types of data collection, it is imperative that the source of data be a reliable, valid, and relevant one. This applies in this context much as it does to interviewing experts or giving out surveys. Choosing the correct population (or corpus in this case) is crucial. Possibly, the limitation that most limits this study and others like it is that the semantic distance, a cosine distance in this case, signifies the strength of the relationship but not its direction, i.e., whether the relationship is positive or negative. Thus, the path from Distrust to Use is positive while according to theory it should be negative. The current method does not address this. Refinements are needed to add a sign value to the cosine values produced by LSA or any other text analysis method that is applied to extract semantic distances. CONCLUSION This study demonstrated the ability to apply LSA and CBSEM combined to investigate the linguistic correlates of trust and distrust. The study also showed that analyzing linguistic correlates can be applied to differentiate between trust and distrust--something survey research had difficulty in doing. Clearly, the concept of linguistic correlates and the potential of modeling their role in human decision making, is not limited to trust and distrust alone. Nor is this potential limited to the study of only the present. Texts of the past could be just as readily analyzed in the method demonstrated in this paper, opening through linguistic correlates a view to the past and how people in long gone periods might have thought. Practically, this also opens the window to the possible study of how we as people of the present might respond to future technologies and contexts based on our current linguistic correlates.\",\"1135747390\":\"INTRODUCTION Whether employees are engaged in their work or not has important consequences for employees themselves, the organizations they work for, and the clients they work with. Engaged employees are full of energy, are dedicated toward work, and are often completely immersed in their work activities. They also experience more positive emotions, think in novel ways, and show better performance. Since the emergence of the concept of work engagement, organizational scholars have been studying its presence, predictors, and outcomes. However, most of the literature assesses work engagement with structured data, that is, measurement scales, and there have been few attempts to innovate measurements. Although structured data have allowed scholars to understand the phenomenon of work engagement, a drawback is the limited potential for new theoretical or applied discoveries. At the same time, within organizations, a vast pool of data, in the form of unstructured (non-predefined) text data, remains scarcely studied. For example, employees generate and share large amounts of written text with each other. Those qualitative data may potentially offer new insights in work engagement and add to the more traditional structured approaches to data analysis. This is because unstructured data are not limited to predefined categories, present the multidimensionality of a phenomenon, and allow to compare these dimensions simultaneously (e.g., combining linguistic and substantive patterns) . Text mining offers a unique approach to unlock these insights as it is a method to analyze large amounts of text in a relatively short timeframe. Its benefit compared with traditional quantitative or qualitative research is that it is able to analyze unstructured text, but on a large scale and replicable across studies. There are quite a few studies that show its potential in a variety of disciplines , but, although declared a future research avenue, few attempts have been made regarding organizational research. Therefore, the purpose of the current study is to explain work engagement through text mining methods by attempting to classify employees' survey-based self-narratives into high or low work engagement and analyze the text features that contribute to the classification. The research question that guided our study is as follows: To what extent can we explain work engagement by analyzing self-narratives through text mining? Using two samples, representing two waves of an annual survey among Dutch healthcare employees during 2 years of COVID, this paper conducts two studies to answer that question. We tested multiple text features: unigrams, bigrams, psychological features, and linguistic features. For the psychological features, we conducted a preselection based on the job demands-resources (JD-R) theory. Next, for the first study, we used exploratory sample 1 to explore which features explain work engagement. We then formulated hypotheses based on the main themes that emerged from the features. For the second study, we used both exploratory sample 1 and confirmatory sample 2 to analyze to what extent text features persist over time, across survey waves. Our study contributes to the literature by being the first to explain work engagement by text mining self-narratives. Theoretically, we increase the understanding of work engagement as a concept. Some of our results confirm the duality of the JD-R model when we find low-engaged employees tend to mention job demands whereas high-engaged employees tend to mention job resources. Yet because our analysis is exploratory, we are also able to extend and question extant findings. We find linguistic patterns may be markers of work engagement  and observe features that question the literature, like the finding that low-engaged employees mention their managers more often. At the same time, we also discuss how our application of text mining is limited in terms of the accuracy with which we are able to explain work engagement, as well as how particular sample characteristics like age and gender may influence the results. Second, methodologically, our findings open multiple avenues for survey-based and in vivo applications of text mining. We discuss how text mining could support or complement structured forms of data collection , or be used to analyze existing unstructured data in organizations like emails or intranet posts. Finally, we explore how our study may have practical implications in the screening and identification of groups of employees based on work-related well-being challenges. THEORETICAL BACKGROUND Defining, modeling, and measuring work engagement Work engagement is a work-related and positive state of mind, characterized by vigor, dedication, and absorption. Vigor refers to a high level of energy and preparedness to invest effort in activities. Dedication refers to enthusiasm and strong involvement with one's work. Finally, absorption is a state of complete immersion in one's work. Whereas vigor and dedication are considered core dimensions of work engagement, absorption is considered an additional dimension. Whereas our knowledge of work engagement has increased in the past years, there are several remaining questions, for example, on its social-psychological origins and the effectiveness of work engagement interventions. Antecedents of work engagement are often studied within JD-R theory, a theory within organizational psychology that explains how job characteristics affect employees through a dual process. In the health impairment process, demanding job characteristics--\\\"aspects of the job that require sustained physical, emotional, or cognitive effort\\\"--cause job strain (including burnout) and health complaints. Burnout refers to the state when employees experience chronic feelings of exhaustion and a cynical attitude towards work and the people with whom they work. In the motivational process, resourceful job characteristics--\\\"aspects of the job that help to either achieve work goals, reduce job demands (...), or stimulate personal growth\\\"--foster motivational outcomes (including work engagement) and job performance (;, p. 392). In addition, job demands and resources are proposed to interact: Resources may weaken the impact of demands on burnout, whereas challenge demands may strengthen the impact of resources on engagement. Over the years, many resources that stimulate work engagement have been identified. Generally, they have been classified into one of two categories: situational and individual factors. As explained above, antecedents of work engagement are mainly job resources. These include job characteristics like social support from colleagues, task significance, and autonomy as well as leadership-related factors like having a good relationship with supervisors and experiencing transformational leadership. In addition, individual factors have been found to explain work engagement. For example, employees with higher emotional stability, extraversion, and conscientiousness are more likely to report higher work engagement. Besides these higher-order personality factors, lower-order factors--factors that are more malleable--have been found to predict work engagement, for example, selfefficacy and optimism. And employees who are more proactive tend to be more engaged and even positively influence their co-workers through practices of job crafting. In turn, studies have shown that work engagement can have far-reaching effects. Research on work engagement shows positive relationships with more active positive emotions and more novel thinking. What is more, there is abundant research that shows work engagement increases task performance , although studies also indicate that we know relatively little about the boundary conditions of these effects. The studies described above commonly measure work engagement with multidimensional scales. The most used scale that defines work engagement as the combination of vigor, dedication, and absorption is the Utrecht Work Engagement Scale (UWES;). Other measures of engagement are very similar to the UWES , or measure concepts that are fundamentally different from engagement. For example, May et al. (2004) and Rich et al. (2010) developed the Job Engagement Scale, which includes cognitive, emotional, and physical engagement. According to Bakker et al. (2022, p. 285), \\\"the wording of the items shows a striking resemblance with those included in the absorption, dedication, and vigor subscales of the UWES, respectively.\\\" The latter authors also discuss other instruments to assess engagement, including the instrument by Soane et al. (2012) and Shuck et al. (2017). Bakker et al. (2022, p. 286) conclude that the items show considerable overlap with the vigor and absorption subscales of the UWES, whereas some of the alternative instruments that aim to assess engagement in fact assess affective organizational commitment and extra-role behaviors. Although new scales have been developed since , there have been few attempts to innovate measurement. For example, Bakker et al. (2014) point out that most of the research on work engagement has not attempted to link the concepts to observable outcomes. At the same time, there is some criticism on the UWES, including the fact that factor analyses have not always been able to distinguish between the three components of work engagement. Here, a new method like text mining may help optimize the measurement of work engagement by approaching it in a completely different way. Similarly, a particular bias of maintaining the same measurement methods is that these structured data limit the potential for new theoretical or applied discoveries. Text mining may allow new insights into what observable behaviors of employees are affected by work engagement. Below we address this issue further. Considering unstructured data to measure work engagement The vast majority of data in an organization are unstructured. Unstructured data refer to \\\"a single data unit in which the information offers a relatively concurrent representation of its multifaceted nature without predefined organization or numeric values\\\" (;, p. 558). For example, employees continuously exchange spoken or written text via conversations, email, or texting. These text data are seldom used in studies but may present new insights for the study of work engagement through three advantages over structured data. First, structured data (like survey scales) are always limited to the way they are defined and operationalized. In contrast, unstructured text data are neither predefined nor categorized, and this may lead to new insights. Second, unstructured data are multifaceted. There are multiple potential facets to unstructured data to be studied (e.g., there are linguistic and substantive properties to text). Third, unstructured data offer concurrent representation: Through analyzing facets simultaneously (e.g., the combination of linguistic and substantive patterns), we can learn about different phenomena at the same time. Although unstructured data offers new opportunities for research, structured data are important too. Unstructured data provide, besides its general format being either text-based or image-based, little certainty. This type of data does not allow for easy sorting, searching, analyzing, summarizing, or visualizing. Structured data, on the other hand, provide certainty in measurement and analysis as the data are set in predetermined categories or values. This type of data is easily stored, searched, analyzed, summarized, and visualized. The data contain exactly what could be expected and allow for accessible, unbiased analysis. It has been an important part of theory-based research as theory is operationalized into a specific measurable form. In comparison, unstructured data require a different, more thorough approach to hold value. Although unstructured data have been underused with regard to work engagement, previous studies have shown that free-form text can be a rich source of data that contains important insights about mental well-being and allows identification and screening for mental diseases. For example, research has shown that the content of the speech of schizophrenics differs substantively from non-schizophrenics. There are some specific examples of text mining in psychology and organizational research. Pang et al. (2020) succeeded in predicting 24 character strengths, like gratitude, zest, and leadership, based on Twitter language. This study indicated that one can use text mining to measure the character strengths of large populations. Similarly, La Bella et al. (2018) used text mining to track perceived organizational leadership styles almost real-time with Twitter messages. Examples in clinical settings include the screening of posttraumatic stress disorder in self-narratives  and the identification of trauma patients. When employing text mining for work engagement, we hope to explore whether and how employees high in work engagement may display different features from employees low in work engagement. One reason for the limited attention to analyzing textual data may be that traditionally analyzing text was a time-consuming endeavor as manual coding was the only option. However, new techniques derived from machine learning and statistics may enable to study work engagement and other concepts with unstructured data. One particularly promising avenue to innovate is by text mining as it is a general methodological framework to analyze large corpora of text. Hence, text mining offers large-scale text analysis in short timeframes, only bottlenecked by computing power and the fact that often large amounts of texts are required to generate insights. Its benefit, therefore, compared with traditional quantitative or qualitative research is that it is able to analyze unstructured text, but on a large scale and replicable across studies. Only recently organizational scholars have suggested that this approach could be used to assess organizational concepts like burnout. Text mining refers to the analytical process that aims to generate insights or test hypotheses using unstructured text data. The data are systematically collected, cleaned, and transformed (a process referred to as preprocessing), after which one of multiple text mining operations can be applied to generate insights from the text data. Texts can be analyzed based on textual patterns and linguistic features, as well as dictionary approaches, considering the words used in the texts. Finally, postprocessing requires the interpretation and evaluation of the results by applying specific domain knowledge to them and validating the data. There are many text features that can be analyzed through text mining. First, textual patterns such as bag-of-words approaches look at the occurrence of words in texts and try to understand the corpus based on word counts. Similar approaches include the use of n-grams, which refers to word combinations of two (e.g., \\\"working day\\\"), three (e.g., \\\"busy working day\\\"), or more words. The advantage of using bag-of-words and n-grams lies in their simplicity. The only information lost is the position of the words or n-grams in the text, which means it is a true-to-source feature to analyze. The downside, however, is that respondents with different background characteristics like educational background or social status may use different words to convey identical information. Patterns might emerge based on characteristics that are unrelated to the research at hand. Second, there are dictionary approaches, which tag words in the texts with categories the words belong to. In our analysis, we can use these categories to understand the texts. An example of such a dictionary approach is Linguistic Inquiry and Word Count (LIWC). LIWC counts words in linguistic and psychologically meaningful categories. Dictionary approaches resolve the issue of textual pattern features as the underlying meanings and categories of the words are analyzed, rather than the words themselves. However, the downside is a loss of information as the words themselves are not analyzed further. In choosing the features, one could either test all available features or apply some sort of a priori selection. A priori selection is often applied to prevent overfitting. Overfitting is a common problem in machine learning where a model performs well on the training data but fails to generalize to new, unseen data. This happens because the model is trained on a specific set of data that might not contain a fully balanced representation of all words that do or do not contribute to the classification of the variable. The model will fit to the training data as specifically as possible, even though there might be false patterns that do not hold over multiple samples. Overfitting can be avoided by using a larger and more diverse dataset for training, as well as using regularization techniques to prevent the model from learning overly complex patterns in the data. Our text mining approach was both theory and data driven. Specifically, we used the JD-R theory to select psychological features based upon their resemblance to any aspect of the definition, dimension or items of work engagement. In sum, in the present study, we hope to gain new insights into the concept of work engagement using text mining. Across two studies, we will compare bag-of-words, bigrams, and LIWC dictionary approaches (psychological process and linguistic features) to explore the possibilities offered by text mining. METHODS Procedure, samples, and data We used data from two samples, representing two waves of an annual survey among Dutch healthcare employees who are members of Stichting IZZ, a collective of healthcare employees and employers in the Netherlands. This foundation has over 400,000 members of which around 210,000 are healthcare employees, who make up a notable share of the population of around 1.7 million healthcare employees in the country. The annual survey, executed since 2018, is used to monitor how healthcare employees perceive their work and well-being. It presents an opportunity for employees to share their experiences, which are then shared (at the group level) with healthcare organizations, governments, societal partners, and media. The survey has, among else, been helpful in informing these parties about the challenges COVID-19 posed for healthcare employees. Besides, to add an extrinsic motivation to finish the survey, participants could choose to participate in a separately organized giveaway (with products that stimulate well-being). For the first sample, data were collected in May and June 2020. For the second sample, data were collected in May and June 2021. Table 1 shows how we arrived at our final sample. All members of the collective that provided an email address were sent an invitation to participate in the survey via email. For a response to be valid, respondents had to provide informed consent and indicate they were currently working in healthcare. At the informed consent page, respondents were informed about the goal of the survey, procedures of participation and opting out, data storage and usage, and the possibility to get in touch with the researchers. The survey itself consisted out of multiple open and closed questions on employee well-being in healthcare, including the questions presented in this study. We did not employ attention checks. All questions used in this study, except the text mining question, used forced response. The text mining question was placed at the end of the survey as it would take considerable time. Therefore, as Table 1 indicates, respondents with partial responses corresponded with respondents who did not fill in the text mining question--these were removed from the dataset. Besides, to be included in our final sample, we set the minimum number of written words at 20. Finally, as in our study, we will compare employees in the top 10% with those in the bottom 10% of work engagement scores. Table 1 also presents this subsample. The methodological choices made above will be elaborated on below. Table 2 presents the characteristics of the respondents in the final sample. The respondents are representative for the population of Dutch healthcare employees in terms of gender (84.3% of employees are female) but somewhat less representative in terms of age (employees in the population are younger: 34% are younger than 35 and 24.2% are older than 55) . Especially the gender composition, with a vast majority of female employees, is a typical (but not unique: e.g., in Dutch primary education, 87% of teachers are female;) characteristic of healthcare sectors. Although the majority in the Netherlands is very large, a WHO report shows across the world women constitute around 70% of the healthcare workforce. What is more, the same report indicates that the small minority of men in healthcare is more likely to hold leadership positions. This leadership gap has systemic roots in gender roles : Men and women in healthcare (are expected to) work in different jobs. We should take into account that such factors could affect work engagement. Besides, although our sample is older than the population of healthcare employees, the population of healthcare employees is aging rapidly (the group of healthcare employees aged 55 and oversaw a 9% increase in just 10 years;). Finally, nursing\\/home care, hospitals, and disabled care constitute the biggest healthcare branches within the Netherlands and are also the largest in our sample. However, although hospitals are the biggest group in our sample, within the population, nursing\\/home care is bigger (with a total of 28% of healthcare employees in the population;). In sum, our samples of healthcare employees are fairly representative for healthcare. It is important to note that there may be systemic reasons to expect differences in work engagement based on demographic characteristics. Table 3 shows that, to some extent, work engagement varies across age, gender, and healthcare branch. Most notably, work engagement is generally higher among women, among 46-55-year-old employees (not taking into account the youngest and oldest categories, which both have low N), and among employees in nursing\\/home care (Additional analysis in the Supporting Information elaborates on these differences). This may have consequences for our study's external generalizability. This study analyzes text-based features among a fairly representative sample of healthcare employees, but the particular sample characteristics (e.g., distribution of age and gender) and how work engagement relates to these characteristics may limit generalization to different sectors. Finally, to protect the privacy-sensitive information that participants provided in their selfnarratives, data are stored on secure servers in compliance with privacy regulations and not made publicly available. We do present multiple Supporting Information that provide extra information on the research process: an overview of the included features, the R script for our analyses, our approach to deciding the cutoff, an overview of all significant features, an overview of the relative feature importance to the models, additional analysis on the role of demographics, and an additional analysis using a different classifier (Naive Bayes) (accessible via https:\\/\\/osf.io\\/ jzdx5\\/?view_only=f981153538214470b2bd1a9e9a538009). Work engagement scale The UWES-9 work engagement scale includes nine items on three dimensions: vigor, dedication, and absorption. All dimensions were measured with three items on a 5-point Likert scale ranging from \\\"Never\\\" (1) to \\\"Always (daily)\\\" (5). Example items are \\\"At my work, I feel bursting with energy\\\" (vigor), \\\"I am proud of the work that I do\\\" (dedication), and \\\"I feel happy when I am working intensely\\\" (absorption). The items were summed to create an overall index of work engagement. The reliability of the overall scale was good, Cronbach's alpha was.908 for sample 1 and.910 for sample 2. Self-narrative question Below, we present the English translation of the question that was shown to respondents to write their self-narrative: We have one additional question about how you have experienced your work during this time of COVID-19. We would like to take a closer look at your personal experiences. Could you summarize what you have experienced? How have you experienced the past few months? What impact has this had? How are you feeling now, physically and emotionally? How do you view your work now? And how do you look forward to the coming months? This multifaceted question functioned as a writing prompt to guide the content of the selfnarratives. Writing prompts make writing easier when they, in our case, promote the structure of the story that respondents are expected to write. The question was drafted purposefully and in reiterative discussion between all the authors of this study and contained multiple subquestions that each served their own purpose. The first subquestion was general (Could you summarize what you have experienced?), after which the second subquestion specified the time period (How have you experienced the past few months?). Third, we asked about the consequences of these experiences (What impact has this had?). Fourth, we referred to the energy continuum of exhaustion versus vigor (How are you feeling now, physically and emotionally?). Fifth, we referred to the identification continuum of dedication versus cynicism (How do you view your work now?). Finally, we asked about participants' future perspective (And how do you look forward to the coming months?). After this, a text box provided participants ample opportunity to share their self-narratives. Analysis The process of text mining involves four basic steps: (1) data preprocessing; (2) training on a subset of the data; (3) testing on a different subset of the data; and (4) interpreting the results. We will explain these steps below. Data preparation In the first data preparation step, the corpus of all texts was cleaned, and features were extracted and selected to prepare for data analysis. Packages and code used can be found in the Supporting Information R script. We checked whether we needed to apply criteria for minimum or maximum number of words in the selfnarratives. An explorative analysis of the data indicated that respondents who replied with fewer than 20 words in their self-narratives commonly responded with variations of \\\"I do not have anything to share.\\\" This is not a substantive answer to the question prompt--yet it occurred many times in the initial dataset. We decided to require respondents to have written 20 words at minimum to avoid meaningless self-narratives, but we did not apply a maximum as no self-narrative appeared extremely long. The second part of the data preparation aimed to tokenize the text by removing punctuation, numbers, and capitalization and by splitting the texts word by word. The resulting list of words was spell-checked by one of the researchers for all words that occurred at least a total of 10 times to find spelling errors or gibberish that would be included in the model. One returning issue concerned the occurrence of abbreviations alongside the same abbreviations written out in full. This included both general abbreviations as well as job-specific abbreviations. As abbreviations were often unclear and seemed to vary on a text-by-text basis, these abbreviations were not manipulated to full terms. No other issues were found based on this quality check of the data, and after this quality check, we proceeded with the data. Further cleaning steps were the removal of frequently occurring stop words (e.g., \\\"the\\\" and \\\"a\\\") that do not discriminate texts or add little to no meaning to texts. The list of stopwords filtered is based on the Dutch stopwords list as compiled by the Snowball stemming project; this list is included in the corpus package. Finally, stemming the tokens, reducing all words to their stem, was done to ensure various inflections of the same word are counted together (e.g., \\\"working,\\\" \\\"worked,\\\" and \\\"work\\\" become \\\"work\\\"). For sample 1, this step reduced the total number of words by 48.44% (from 632,174 to 325,963) and the unique number of words by 29.41% (from 22,524 to 15,899). For sample 2, this step reduced the total number of words by 48.01% (from 443,668 to 230,653) and the unique number of words by 17.77% (from 15,573 to 12,806). The other steps in data preparation were feature extraction and feature selection. This study used features based on bag-of-words approaches and dictionary approaches to explain work engagement. Importantly, below, we describe how the features were selected for sample 1. For sample 2, we only used the features that contributed to the classification into high and low work engagement in sample 1. First, the bag-of-words approach assumes no relationship between the order of the words in a text and the meaning of the text. The words are as they are independent from other words in the text. This approach was used for generating unigrams (one word, e.g., \\\"happy\\\") and bigrams (two words, e.g., \\\"not happy\\\"). Second, the dictionary approach used the LIWC dictionary to tag words with categories belonging to Psychological Processes or Linguistic Dimensions. The translated Dutch version of LIWC 2015 has 67 categories for which the words can be matched. The features selected were both theory driven and data driven. For the Linguistic Dimensions, we explored all features available. For the psychological features, we selected features using the JD-R theory: We checked whether they reflected an aspect of the definition of work engagement, its dimensions, or items. We selected features from the affective, social, perceptual, and biological processes, drives, time orientations, relativity, and personal concerns. Although this means that there was some a priori selection of features, the selection was necessary to limit the scope of the study. Additionally, because of the theory-related nature of the LIWC psychological process features, a priori selection based on theory could prevent overfitting. Through a priori selection of psychological process features, we narrowed the amount of psychological process features from 54 categories (including all overarching categories and more specific subcategories) to 25 categories. This was done to remove non-work related features, as these would be less relevant for our purposes of explaining work engagement. That is, in this specific analysis, we were looking for factors related to work, not for other factors. That is not to say that work engagement is irrelevant for employees' private lives, as research suggests otherwise. It only means that we slightly narrowed the scope of our analysis. Comparing the features to the definition, dimensions or items of work engagement provided a good measure for selection. For example, the category \\\"Time orientations\\\" (including the categories \\\"past focus,\\\" \\\"present focus,\\\" and \\\"future focus\\\") was included as work engagement is related to time orientations. The dimension absorption includes the phrase \\\"whereby time passes quickly\\\" (;, p. 391). In contrast, some subcategories of \\\"Personal concerns,\\\" like the \\\"Leisure\\\" category with words like \\\"home,\\\" \\\"chat,\\\" and \\\"movie,\\\" were deemed less relevant for our endeavors. Additionally, to prevent overfitting for the Random forest model and to ensure robust features were kept, feature selection was applied for each of the feature representations using chi-squared tests. Features were kept based on the criteria of significant chi-squared outcome for features that occur at least 10 times in the first sample dataset. The Supporting Information Included features presents an overview of all included features and specifies why features were included. Training and testing The training phase consisted of learning from a first subset of the sample to understand how the text is related to the outcome variables. In the exploratory phase, our first study, we tested a number of settings and chose those in which the models performed better for sample 1. First, we used the full UWES-9 scale for the classification of employees. We did explore whether it would make sense to focus on the energetic component of work engagement by using only one of the dimensions of work engagement (vigor)--or a combination of vigor with exhaustion (a dimension of burnout;)--to classify employees. The argument here was that perhaps this would lead to better classification as studies indicate the energetic component of work engagement is more sensitive than the other components. We decided to focus on the UWES-9 as most studies of work engagement use this full scale. Second, as Forman mentions in his critique on text classification feature selection methods, classifying minority classes is a pitfall for feature selection methods that use scoring methods based on outcome variables (2004). We therefore carefully decided on the criteria for classifying employees into high or low work engagement (the Supporting Information Cutoff explains this process in more detail). We explored relative (percentages) and absolute groups (e.g., scores below 2 and above 4 on a 5-point scale). We found the sample is unbalanced: More healthcare employees tend to be relatively high engaged. Therefore, we chose to use a relative, 10% cutoff. For the confirmatory phase, our second study, the same cutoff was set a priori. Hence, to correctly classify whether an employee is high work engaged or not, we decided to select employees with a self-narrative of at least 20 words, who had a work engagement score in the highest 10% versus a work engagement score in the lowest 10% of each sample. For the first study, sample 1 (N = 1119) was split into a training set containing 80% of the selfnarratives and a testing set containing the remaining 20% of the self-narratives. For the second study, we wanted to analyze how the features persist over time and across survey waves, so we used sample 1 in its entirety as the training dataset and sample 2 as the testing set. Table 4 reiterates the way the two studies were set up. The purpose of splitting the sample into a training and testing set is to learn to recognize high versus low work engagement in the training set using the mentioned text features, after which the testing set can be used to assess its performance in recognizing high and low work engagement for previously unseen data. We used Random forest, a machine learning model that generates many decision trees trained and tested using resampling of the sample data. Each tree randomly samples a predefined number of features per split and decides based on the feature that best distinguishes the classes at that point in the tree. After all trees are built, Random forest calculates the best scoring features based on the classification scores per tree with and without the feature. If the trees classify worse when the feature is excluded from the tree, that means the feature has some explanatory power. Random forest allows for hyperparameter optimization, which is the process of modifying the model settings for better model performance. The Random forest model was applied using the randomForest package in R. The randomForest package allows for different settings for the nodesize, mtry, and ntree hyperparameters. For study 2, the Random forest hyperparameter optimization was done using the caret package by doing grid search for the minimum node size (nodesize) and the number of variables to sample as candidates for each split of a node (mtry) . A smaller nodesize hyperparameter value allows for more splits in the tree, resulting in a more complex tree. Additionally, the number of trees (ntree) was optimized by building the Random forest model with 100, 250, 500, and 1000 trees. No noticeable improvement was found after 250 trees for any of the models, test set error rate was lowest with 250 trees, and highest with 500 and 1000 trees, whereas OOB error rate only marginally improved. For comparison, in study 2, we also used a different classifier, Naive Bayes. Naive Bayes is a probabilistic machine learning algorithm that assumes features are independent of each other. The algorithm is well suited for highdimensional datasets such as text data because it is efficient due to scaling linearly with the number of predictors and data points. Despite the assumption of independence often being violated, Naive Bayes tends to deliver robust and accurate classification. Naive Bayes, Random forest, and other approaches such as support vector machines and logistic regression are commonly used in text mining classification problems. In data science, there is no consensus on the best method as it depends on the data at hand. This means that in practice researchers use a variety of algorithms based on the conditions for the used case and pick the best performing algorithm. The model results are evaluated primarily using a confusion matrix, the accuracy score, and P-value for accuracy score compared with the no-information-rate (NIR), which is an accuracy value that always predicts the most frequently occurring class in the dataset. The Supporting Information R script presents the R code for our main analyses. Interpreting In the fourth step, we evaluated the text mining results by comparing them to the domain knowledge on work engagement. For study 1, we used an exploratory approach to interpretation, by comparing the exploratory results to the existing literature on work engagement after conducting the analysis. We developed several hypotheses that explicated the main themes emerging from the analysis of study 1. We use both our observations (our data from study 1) and potential explanations in the existing theories in the literature to inform our hypotheses. For study 2, we used a confirmatory approach to interpretation, by assessing the hypotheses formulated in study 1. These hypotheses guided our discussion in study 2 and enabled to assess whether the same features contribute to explaining work engagement over time. RESULTS STUDY 1 In this section, we present our results in four steps. First, we described the groups of high- and low-engaged employees in the sample. Second, we counted all the features that we analyzed in the self-narratives of these employees, and we assessed whether features are significantly more frequently observed among high- or low-engaged employees. Third, we tested whether the features can be used in a Random forest model that can correctly classify employees into high or low work engagement (using the training and testing approach, as described in our methods). Fourth, we presented the features that contribute most to the accuracy of the model, as these features indicate best how self-narratives of high- and low-engaged employees differ. First, there are 5591 respondents who answered the text mining question with 20 words or more. The mean number of characters in the self-narratives was 667.24 (SD = 442.01), and the mean number of words was 113.31 (SD = 76.21). The mean work engagement score was 3.87 (SD = .63). Table 5 presents the respondents that are in the highest and lowest 10% of work engagement scores. The variance within the lowest 10% is notably larger than the variance in the highest 10%. This shows that our \\\"lowest 10 percent\\\" group is a varied of group employees ranging from very low on work engagement to moderately engaged. Second, we counted the features in the groups of employees. The Supporting Information Significant features presents all features that are observed significantly more frequently in selfnarratives of either high- or low-engaged employees. When we present the most contributing features in Table 7, we use the information from this step to indicate which feature is observed significantly more among high- or low-engaged employees. Third, we employed Random forest to test whether these features can be used in a model to classify employees in the highest 10% or lowest 10% of work engagement. Table 6 presents the results of four models: unigrams, bigrams, psychological features, and linguistic features. Table 6 first indicates how many features contributed to the models. Next, it shows how the models performed. We find that unigrams score best with a 62% accuracy score, whereas the other models have lower accuracy scores (for bigrams, 58%; for psychological features, 60%; and for linguistic features, 59%). Considering that a model based on randomization would have an accuracy score of 50% (an equal chance of true or false classification), we find that all models classify into high or low work engagement better than random. The model with the unigrams appears the most successful. Fourth, we analyzed the features that best classify into high or low work engagement in the different models. For that, we assessed the discriminatory value of the features based on the mean decrease in accuracy of the models when a specific feature is excluded. The Supporting Information Feature importance presents the importance of all the features in the models. Table 7 presents the (most) strongly contributing features. For the unigrams and bigrams, we translated the features from Dutch into English and provided them with a common stem to ease interpretation. For the psychological and linguistic features, the feature categories are presented, and if applicable, the overarching category is presented between parentheses. The Supporting Information Included features presents more information on the content of these categories. Table 7 should be interpreted as follows: The bigram \\\"goes well\\\" contributes most to the accuracy of the bigrams model, and this bigram is significantly more present among employees with high work engagement. In contrast, the bigram \\\"from house,\\\" which is the second most contributing feature, is significantly more counted among employees with low work engagement. Likewise, we find that the psychological feature \\\"positive emotion,\\\" an LIWC dictionary with words like \\\"safe,\\\" \\\"trust,\\\" and \\\"beloved,\\\" contributes most to the accuracy of the psychological features model and is significantly more counted among high-engaged employees. In contrast, the feature \\\"anger,\\\" a subdictionary of negative emotions, with words like \\\"aggression,\\\" \\\"stupid,\\\" and \\\"fight\\\" and the second most contributing to this model, is significantly more present among employees low in work engagement. In Section 5, we interpret what themes are presented in the features and how this relates to the extant literature. DISCUSSION OF STUDY 1 For study 1, we used an explorative approach to assess whether we can explain work engagement through text mining. We found that models with unigrams, bigrams, psychological features, and linguistic features can correctly classify healthcare employees into high or low work engagement with an accuracy of up to 62% (for the unigrams). Whether we will find similar results in the next study depends on two aspects of the features. First, it will depend on the translatability of the type of feature. Herein, we may expect differences between the types of features we use. A methodological explanation for the success of unigrams in this study is that the high number of features may allow for more discrimination between the groups. However, for study 2, the question is whether unigrams will translate over samples. Potentially, psychological and linguistic features will explain better across samples than unigrams or bigrams as they use dictionary approaches that measure underlying meanings and categories of words rather than the specific words themselves. Second, whether we find the same result in study 2 will depend on the translatability of the content of the feature. If we assume that healthcare employees' work engagement can be explained by factors that are time-insensitive, our models should perform similarly. We will therefore explore whether we observe a few grand themes among the features that contribute to our models. As the process of feature selection was partially data driven, there are many features, and not all are readily interpretable or categorizable. However, across the models, three prominent feature themes emerge from the self-narratives, which we named emotions (24 features), crisis (35 features), and affiliation (19 features) (the Supporting Information Feature importance presents the coding). Below, we introduce these themes, compare them with the extant literature, and formulate hypotheses for study 2. First, the models include strongly contributing features that address the positive or negative emotions in the self-narratives. For the unigrams, we find positive emotion words to be related to high-engaged employees (e.g., good) and negative emotion words to be related to low-engaged employees (e.g., burdened). For the bigrams, we find positive emotion word combinations to be related to high-engaged employees (e.g., goes well and very good) and negative emotion word combinations to be related to low-engaged employees (e.g., unsafe feeling). For the psychological features, we find positive emotions to be related to high-engaged employees, and anger and negative emotions to be related to low-engaged employees. These findings support the conceptualization of work engagement as a \\\"positive motivational state\\\" (;, p. 389) as well as the finding that positive emotions are positively related to work engagement. Additionally, employees who experience job strain are less able to regulate their emotions , and scholars have suggested that emotional instability may be a personal demand that affects work engagement. Hence, our first hypothesis is as follows: H1. Referring to positive emotions contributes to explaining high work engagement, whereas referring to negative emotions contributes to explaining low work engagement. Second, the models include features that refer to the crisis during which the study was conducted, the COVID-19 pandemic. For the unigrams and bigrams, we find references to the crisis related to low-engaged employees (e.g., high work pressure, meter distance, usual work, direct contact; face-to-face [contact], crisis) and references to the absence of the crisis to be related to high-engaged employees (e.g., family, allowed; goes well, allowed [to] come). A side note here is that we indicated we have reason to expect that unigrams and bigrams translate less well across samples. Nevertheless, the features echo an emerging stream of literature that shows the pandemic may in many cases have deteriorated work engagement  and other aspects of well-being. For many healthcare employees, COVID-19 caused higher stress levels and other negative health outcomes. There is an emerging literature on the effects of a crisis in the context of JD-R theory. Demerouti and Bakker (2023) argue the COVID-19 crisis has increased job demands. Besides, a crisis also tends to make resources scarce. They propose that during a crisis, employees who experience manageable job demands (and high job resources) will maintain higher engagement than employees who experience high job demands (and low job resources). At the same time, changes in engagement are likely not only caused by individual demands and resources but by a more complex interplay of individual and higher-level factors. In sum, our second hypothesis is as follows: H2. Referring to a crisis contributes to explaining low work engagement, whereas referring to a normal work context contributes to explaining high work engagement. Third, the models include features that refer to affiliation and social connection, which appears to explain high work engagement. For the unigrams, we find references to social contact (e.g., listening and talking) to be related to high-engaged employees. For the bigrams, we find multiple plural references to be related to high-engaged employees (e.g., we go, work we, and our resident). For the psychological features, we find social processes (e.g., \\\"talk\\\" and \\\"love\\\") and affiliation (e.g., \\\"friend\\\" and \\\"social\\\") to be related to high-engaged employees. And for the linguistic features, using 1st and 3rd person plural is positively related to high-engaged employees (in addition, the Supporting Information Significant features shows that low-engaged employees use significantly more singular forms, but these do not contribute to the models). This supports studies that show the importance of affiliation for work engagement. First, experiencing social support is an important predictor of work engagement, and in turn, employees who are engaged offer more social support. Another study explained how especially new employees' work engagement is highly affected by socialization in the organization. Additionally, scholars have studied the concept of teamwork engagement, which indicates that work engagement is not merely an individual process but also part of a team process. Work engagement is contagious, and employees can collectively experience high levels of work engagement. Therefore: H3. Referring to affiliation contributes to explaining high work engagement. Having defined our hypotheses, we submitted a preregistration at the Open Science Framework that described the hypotheses as well as the plan of analysis. For study 2, our primary aim is to select the features of study 1 and to assess whether these features contribute strongly to the models in study 2. We will evaluate the success of these features both in terms of specific features as well as the themes that they represent (as referred to in the hypotheses). The next section describes the results of study 2. RESULTS STUDY 2 In study 2, we repeated the analysis with both samples. Again, we present the results in four steps: We described the employees in the sample, we counted all features and tested whether features are significantly more frequently observed among high- or low-engaged employees, we built the Random forest model, and we presented the features that contribute most. First, we already introduced the first sample above. In the second sample that we add in this analysis, a total of 4470 respondents answered the text mining question (20 words or more). The mean number of characters in the self-narratives was 583.89 (SD = 397.33), and the mean number of words was 98.03 (SD = 67.99). The mean work engagement score was 3.79 (SD = .66). Again, we selected the respondents that are in the highest and lowest 10% of work engagement scores (Table 8). Like the first sample, the variance within the lowest 10% is notably larger than the variance in the highest 10%. Average scores also appear to be slightly lower for both groups compared with the first sample. Second, we counted the features. The Supporting Information Significant features presents all features that are observed significantly more in self-narratives of either high- or low-engaged employees. When we present the most contributing features in Table 10, we use the information from this step to indicate which feature is observed significantly more among high- or lowengaged employees. Third, we employed Random forest to test if these features can be used in a model to correctly classify employees in the highest 10% or lowest 10% of work engagement. Table 9 presents the models and shows that, compared with study 1, all but one model performed worse. The unigrams, bigrams, and linguistic features performed worse (accuracy scores of 52%, 53%, and 54%) and barely outperformed a random model. However, the model with psychological features still has an accuracy score of 60%. For comparison, we also conducted study 2 using the Naive Bayes classifier rather than Random forest. We find that for unigrams the results improve much (from 52% to 64%), whereas for the other features, the results are only slightly better (bigrams: 56% instead of 53%; psychological features: 61% instead of 60%; linguistic features: 55% instead of 54%). Supporting Information Naive Bayes presents all results for Naive Bayes. Fourth, we analyzed the features that best explain high or low work engagement in the different models. For that, we assessed the discriminatory value of the features based on the mean decrease in accuracy of the models when a specific feature is excluded. Table 10 presents the (most) strongly contributing features that were also significantly more present among either high- or lowengaged employees (the Supporting Information Feature importance presents the overview of all features). Notably, the total amount of contributing features decreased drastically for the unigrams and bigrams, indicating that many features that were used in the first sample were not used in the second sample. In the following discussion section, we compare the features to those of study 1. Additional analysis demographics We conducted additional analyses to investigate how the sample demographics (gender, age, and healthcare branch, as described in Table 3) may affect the results. First, Supporting Information Additional analysis presents some significant differences in work engagement across gender, age, and healthcare branch in both samples. In all cases, effect sizes were small. Second, we explored whether demographics play a role in explaining work engagement. We used the DALEX package  to analyze how gender and age relate to the text features in explaining work engagement. The results (in Supporting Information Additional analysis) show that, next to the text features, gender and age contribute to the models. This suggests that gender and age of healthcare employees contribute to explaining work engagement and that this may partially confound the effects in our main analysis. A next step in future research would therefore be to add these to the analyses. We discuss further implications in the discussion section. GENERAL DISCUSSION In this article, we aimed to explain work engagement by analyzing self-narratives through text mining. We compared unigrams, bigrams, psychological features, and linguistic features. After the explorative approach in study 1, for study 2, we used a confirmatory approach to assess whether the same text features, dependent on both the type of feature and content of the features, can explain work engagement across two samples. From both studies, we deduce three main findings that we want to highlight. First, psychological features can correctly classify healthcare employees into high or low work engagement with 60% accuracy across samples. Second, the features that contribute to the classification partly confirm the literature on the JD-R theory and work engagement. Third, the features also unlock new insights by extending and questioning work engagement theory. First, we find that the model with psychological features explained work engagement best in both studies, with 60% accuracy. In the first study, unigrams generated the best model (62% accuracy), but the unigrams performed worse in the second study (52% accuracy). This indicates that dictionary approaches, which measure underlying meanings and categories of words, have more success in explaining work engagement than bag-of-word approaches using specific words. A likely explanation is that although employees may write about similar topics, they may use different words. Second, some of the features that contribute to the classification partly confirm the extant literature on antecedents and outcomes of work engagement. Based on study 1, we proposed three hypotheses, supported by the literature, regarding prominent features that explained work engagement. In evaluating our hypotheses, we focus on the model with psychological features, as this is the only model that performed consistently. Drawing conclusions from a model that does not outperform a random model would not be appropriate (we will pay some attention to the model with linguistic features as it still performs slightly better than random). First, we expected that referring to positive emotions contributes to explaining high work engagement, whereas referring to negative emotions contributes to explaining low work engagement (H1). This hypothesis is confirmed in study 2 because, again, positive emotions were more present among high-engaged employees and negative emotions were more present among low-engaged employees. Second, we expected that referring to a crisis contributes to explaining low work engagement, whereas referring to a normal work context contributes to explaining high work engagement (H2). This hypothesis was only based on unigrams and bigrams that referred to the COVID-19 crisis. These models were not able to explain work engagement in the second study. Therefore, Hypothesis 2 was not supported. Third, we expected that referring to affiliation contributes to explaining high work engagement (H3). This hypothesis is confirmed too, because again referring to social processes and affiliation explains high work engagement. Besides the hypotheses, three other psychological features contributed across two samples: High-engaged employees referred significantly more to rewards (a dictionary with words like \\\"benefit,\\\" \\\"bonus,\\\" and \\\"promotion\\\"), and low-engaged employees referred significantly more to power (words like \\\"manager,\\\" \\\"attack,\\\" and \\\"dependent\\\") and work concerns (words like \\\"job,\\\" \\\"burden,\\\" and \\\"junior\\\"). Our third finding is that text mining unlocks new insights that extend or question common findings in the literature. First, we are able to uncover that work engagement is related to linguistic patterns. Mainly, across two samples, employees with high work engagement use more first-person plural (e.g., \\\"we\\\" and \\\"our\\\") than employees with low work engagement. Second, some findings are puzzling and allow to question the literature. For example, in the first study, there are multiple unigrams that refer to management, and across two samples, there is a psychological feature that refers to power. What is striking is that these features all contribute to explaining low work engagement, suggesting employees who are low engaged tend to mention their managers more. We also observe features that refer to certain subgroups of employees. For example, in both samples, the unigram \\\"retirement\\\" contributes to explaining low work engagement. Exploratory analyses of self-narratives that include this unigram suggest that these are employees who are close to retirement. And in sample 1, the unigram \\\"caregiver\\\" contributes to explaining high work engagement. Exploratory analyses suggest that these employees are voluntary caregivers besides their regular work. We should be careful to interpret these exploratory findings, and we provide potential explanations below. Scientific and practical implications Our findings have multiple implications. Regarding implications for theory, our study furthers the understanding of work engagement as a theoretical concept. First, text mining enables validation of findings in the extant literature and complements these findings with rich context due to a large-scale analysis of self-narratives. As we explained in our theory section, antecedents of work engagement are often studied within the JD-R theory, a theory within organizational psychology that explains how job characteristics affect employees through a dual process. Job resources foster a motivational process leading to positive outcomes like work engagement, whereas hindrance job demands cause a health impairment process and diminish the positive effects of job resources on work engagement. Our results confirm this duality because the features describe resources and demands. The features that high-engaged employees refer more often to, like affiliation and rewards, are often job resources. For example, experiencing social support positively affects work engagement. Even more so, work engagement can be a truly contagious process transferring between employees , and even from employees to partners  and home life. Likewise, positive emotions, another feature more present among high-engaged employees, can be considered personal resources. Contrarily, the features that low-engaged employees refer to, like power and work concerns, tend to be job demands. Power refers to words describing hierarchy or dependency, with words like \\\"manager,\\\" \\\"attack,\\\" and \\\"dependent.\\\" This resembles studies that have shown that abusive supervision or bullying is negatively related to work engagement , and may also point to the absence of autonomy, an important resource and antecedent of work engagement. Finally, negative emotions, another feature more present among low-engaged employees, may suggest that emotional instability be regarded as a personal demand that affects work engagement. Second, whereas some findings confirm the duality of JD-R theory, we also found remarkable linguistic patterns that extend it and relatively unexplored antecedents of work engagement that question it. These findings can increase our understanding of work engagement and how it is theorized and measured. First, the finding on linguistic differences between high- and low-engaged employees uncovers a new research area that may focus on work engagement markers within speech or writing. Until now, studies have mostly focused on linguistics in more clinical concepts, like schizophrenia. Studying linguistic patterns may increase our understanding of work engagement, especially in the context of diary studies, as these studies allow employees to provide unstructured data on a regular basis. Specifically, the finding that high-engaged employees use more first-person plural is a tangible indication of the social and contagious nature of work engagement. Second, in the self-narratives, low-engaged employees more often referred to their managers. This finding is puzzling. The literature shows that managers can have important, positive influences on employee well-being and often finds positive effects of \\\"good\\\" leadership styles or behaviors on work engagement. In contrast, managers can also have negative influence, when they bully or execute abusive supervision. Our results suggest that employees are more likely to mention managers if they are a negative influence. A potential explanation is that positive behaviors are more seen as a self-evident part of a managers' role. COVID-19 has been a tremendous leadership challenge , and especially employees who experienced failing leadership may have wanted to mention this in their self-narratives. In any case, the results suggest a vital role for managers in fostering employee work engagement. Third, some findings beg for further research. For example, a recent study suggests that \\\"mental retirement\\\" among older employees is non-existent. At the same time, in our study, employees who were low in engagement more often referred to retirement. One explanation is that working during COVID-19 has been especially burdensome for older employees. This emphasizes the need for interventions that support older employees in the workplace. In contrast, the finding that highengaged employees refer more to being a voluntary caregiver besides their work points to another avenue in which work engagement may affect home life and cause citizenship behavior. Our main methodological contribution is that, to our knowledge, this is the first study that succeeds in explaining work engagement by text mining self-narratives. The best-scoring model in the first sample uses unigrams (62% accuracy), and the best-scoring model across samples uses psychological features (60% accuracy in the second study). We argue that our study indicates that, for work engagement, classification by text mining cannot easily replace structured forms of data analysis as it is not precise enough yet. Nevertheless, there are multiple avenues in which text mining could support and complement more traditional data analysis. First, it could validate the relative importance of antecedents and outcomes of work engagement. For example, if a relationship between work engagement and another concept, for example, empowering leadership, is analyzed, additional text mining of open questions could indicate differences in the way employees in high or low categories of work engagement discuss their managers. Text mining could also perform a supporting role by being used in the validation of scales, for example, by analyzing what words employees use to describe being engaged at work. Scales that use this as input for wording may be more ecologically valid. Text mining could also complement structured forms of data analysis by using it as an exploration of what topics and concepts are associated with work engagement but may have received little attention in the literature. Besides, in situations where lengthy surveys are not preferred, text mining enables efficient analysis of an open question. Finally, and this is both a methodological and practical implication, text mining may present a new avenue for in vivo assessment of work engagement. Now that this study has found that survey-based self-narratives explain work engagement to some extent, future research could use existing data to attempt to do the same. Albeit for scientific or managerial purposes, existing texts (like shared diaries or intranet posts) or other forms of unstructured data within organizations may very well allow for the screening and identification of employees whose work engagement is challenged. In addition, studies could employ text mining techniques to present employees with a self-assessment of work engagement. Employees could, after providing a self-narrative, perhaps receive a comparative score and\\/or a personalized suggestion, like talking to a confidant. By making assessment easier, text mining could perhaps be a preventive HR tool, if employee privacy is maintained and the interest of employees is put first. Besides, the exploration of the features that contribute to explaining work engagement may help employees, (HR) managers, and (healthcare) organizations to more quickly recognize and act upon challenges to work engagement. The features that turned out to be important may indicate resources where organizations should invest in, like guaranteeing adequate social support systems and stimulating social contact between employees. Likewise, organizations should pay attention to employees' emotional state. Gauging healthcare employee work engagement has become increasingly relevant since the COVID-19 crisis, which has been challenging especially among healthcare employees dealing with COVID-19 patients. Limitations There are limitations to this study. First, the data we used present limitations. We compare selfnarratives to work engagement scores within the same survey, which may lead to common source bias. Using two survey waves has increased the strength of our design. Still, future research could go beyond survey-based analysis by employing human coders (e.g., psychologists) to assess self-narratives. Likewise, our text mining data were survey-based and created specifically for this study. This somewhat limits the external generalizability of our findings when discussing opportunities for text mining of existing, unstructured data. It is common, however, to begin with manufactured data and then expand to pre-existing data after. Therefore, future research could use such pre-existing data like email, intranet, or social media messages to address this limitation. Finally, the survey did not use attention checks, which may be regarded as a limitation. Nevertheless, there is considerable discussion in the literature about their effectiveness and necessity. Recent findings suggest attention checks do not harm scale validity but removing those who fail attention checks often does not alter substantive analyses either. Second, there are limitations related to sample characteristics. The dataset is unbalanced because there are more employees who score high versus low on work engagement (high work engagement: M = 4.87 for sample 1 and M = 4.84 for sample 2; low work engagement: M = 2.60 for sample 1 and M = 2.46 for sample 2). This limitation indicates that our analysis strictly explains the differences between very high work engagement and work engagement lower than the midpoint (i.e., 3) of the scale. One explanation is that healthcare employees are generally high in work engagement, so the limited generalizability of our results may be more pronounced in sectors with lower work engagement, like manufacturing. Nevertheless, future research may explicitly include employees with low work engagement by, for example, targeting employees who intend to quit their jobs (e.g., in exit interviews). Another limitation regarding sample characteristics is that our main analysis focused on text-based features and therefore ignored the role of demographics such as gender and age. However, the literature shows work engagement can vary depending on gender and age (although only to a limited extent;). Yet we also argued that gender and age may affect the results because of the particularities of our sample and, to some extent, the population of healthcare employees. Hence, we can expect gender and age to meaningfully relate to work engagement in our particular samples. Controlling for gender and age in additional analysis confirms that these variables do play a role. Although these findings should be taken into account, our goal was not to develop the best model to explain work engagement but the best fitting model with text features from a representative sample of healthcare employees. Having a fairly representative sample for healthcare is a strength of our study's generalizability within healthcare and comparable sectors but does limit generalization when it comes to sectors with different characteristics. With this restriction in mind, our results contribute to understanding what text-based features contribute to explaining work engagement. Future research may extend our findings by paying more attention to the role of demographics in text mining research and by repeating our methods in different contexts. A final limitation regarding sample characteristics concerns differences in respondent characteristics between the two samples. In comparison with the first sample, the second sample contains fewer respondents who also wrote shorter texts and had a lower mean work engagement. A potential explanation for the difference in participation rates is respondent fatigue: Respondents may have been more motivated to provide a self-narrative when the request for such a narrative was newly introduced compared with when it was repeated. However, the drop in work engagement may also point to another explanation: As the COVID-19 crisis continued, healthcare employees were exposed to persistent job stress, which may have caused the decrease in general levels of work engagement between the 2020 sample (#1) and the 2021 sample (#2) . Sample heterogeneity may have affected the translatability of text features across samples somewhat and may have decreased the reliability of the models. However, we did find that the samples were comparable when it comes to gender, age, and healthcare branch. Still, future research could attempt to collect samples with identical respondent characteristics to counter sample heterogeneity. Third, limitations apply regarding the methods used. This study aimed to explore the possibility of text mining for work engagement classification using Random forest and Naive Bayes. Our results showed that, with Random forest, we were able to classify, but in study 2, Naive Bayes performed better than Random forest. This may inform future use of classifiers for textbased features. Yet there are more possibilities for future research and further optimization of the methodology. Other approaches, including statistical methods such as LASSO feature elimination and OLS regression, and machine learning methods such as support vector machines (SVMs), could prove better suited to the data at hand. This should be decided on a case-by-case basis depending on the data and project goals. For our project, we primarily used Random forest as it presents feature importance information, which allows us to understand what features contribute most to the models. Besides, compared with regression models, it is able to handle the high dimensionality of text data better. Additionally, compared with regression, Random forest is more robust to outliers. Finally, Random forest is also suited for nonlinear relationships and categorical variables. SVMs share some of the advantages of Random forest but are heavier and harder to interpret. In sum, following up on our study, researchers could employ a variety of methods to provide new insights into the uses of text mining. Fourth, our results are promising but the models are not nearly 100% reliable. One of the reasons may be that the self-narratives were relatively short. Longer stories may lead to better explanations. Besides, there is the issue of the \\\"middle 80%\\\": We cannot readily make statements about all respondents in between the highest or lowest 10%. Our approach is a most likely-case scenario, if we do not find differences between these two groups, there most likely will not be any differences found for the 80%. If we do find differences between these two groups, these differences will most likely be more pronounced than the differences for the 80%. Our recommendation for future research is to look beyond binomial categorization. Different feature selection methods, data representations, or neural network approaches to text classification could improve model performance further. Likewise, taking inspiration from our approach to studying work engagement, scholars could expand our study and include different features to test their respective contributions to explaining work engagement. By doing so, scholars can continue to confirm, extend, and\\/or question the literature. For example, extension could take place by studying unexplored features. We also see opportunities to further question the literature if scholars find features that contribute more to the reliability of the models than the features that we studied, or if features suggest contrary relationships between work engagement and other variables compared with our results or established theories on work engagement. Besides, we explored a bag-of-words and dictionary approach to text classification for work engagement. This means that syntactic and contextual information is not taken into account. Modern approaches that focus on further understanding relationships in the text may help future research do enrich the analysis. Word embeddings such as Word2Vec  and GloVe  allow to keep syntactic information intact instead of considering each word as a standalone feature. Recently, approaches such as BERT  and OpenAI GPT  take this even further by incorporating contextual knowledge in the model using pretraining. Fifth, we only addressed the concept of work engagement. It remains a question of how other measures, like positive and negative affectivity, compare with our text mining approach. Future research could explore the comparative explanatory accuracy of such measures in the work context. Finally, the most important limitation regarding the results is inherent to text mining: \\\"text mining procedures in and of themselves cannot support causal inference (i.e. internal validity) unless the study design is such that, next to association, temporal precedence and isolation are also established\\\" (;, p. 148). We analyzed associations, not causal relationships. CONCLUSION In this paper, we aimed to introduce text mining as a methodological approach to study employee work engagement, and, more generally, text mining as a method in organizational research. Our study attempted to analyze work engagement, and the features that contributed to the models help explain what it means to be (or not to be) engaged in work. Text mining truly allows to assess the multidimensionality of a phenomenon , and so, like qualitative research, it offers a richer description of reality, but, like quantitative research, it is able to handle large amounts of data. In sum, text mining is an interesting and innovative approach that may be used to validate but also complement findings from studies with more structured approaches to studying work engagement.\",\"1135747420\":\"Introduction Our appraisal of the trustworthiness of strangers from their facial appearance is a very important aspect of our daily social life. A brief exposure to unknown faces is sufficient for us to evaluate their trustworthiness. This trustworthiness evaluation involves the activation of the amygdala, a subcortical brain region that plays a crucial role in fear response generation. For example, patients with bilateral amygdala lesions show impairments in discriminating between trustworthy- and untrustworthy-looking faces. Further, brain imaging studies report that amygdala activation increases linearly with a decrease in the trustworthiness of faces. Notably, amygdala's activity decreases in association with facial familiarity  and becomes completely inactive in a response to one's own face at either the subliminal or supraliminal level. If amygdala activation is negatively correlated with trustworthiness, we may perceive faces that resemble our own trustworthy. Correspondingly, many studies have demonstrated the impact of self-resemblance on inferences of trustworthiness by using faces created by digitally morphing self-face and other faces. For example, a composite face with self-face increased the prosocial behavior in a trust game  and biased candidate preference in a political election. However, in reality, there is no such thing as a stranger's face containing half of one's own face. There is a trivial possibility that the mere exposure effect of the self-face  or self-positivity bias  produces this effect. Therefore, it is necessary to investigate whether this effect is observed on natural unfamiliar faces in real life. Composite face stimuli with self-face have been used in the previous studies due to the difficulties involved in quantitatively assessing the similarity between the self-face and other faces. Facial similarity is affected by various factors, including the shape, size, and arrangement of facial parts. It is very difficult to determine the factors that contribute to facial similarity evaluation and the extent of their contributions. To address this problem, the current study used deep convolutional neural networks (DCNNs) for human face recognition to assess the similarity between self-face and other faces. DCNNs have been extensively studied in many application fields, such as industrial process monitoring and fault detection , credit default prediction , and facial recognition. Especially, for face recognition, DCNNs learn to reduce the distance between the feature vectors of a person as much as possible. In other words, the smaller the distance between the feature vectors, the more similar the faces to each other as a whole. A previous study compared estimation of couples' facial similarity between human judgment and the DCNNs' facial recognition algorithm. The study confirmed that similar results were obtained. Recently, some researchers succeeded in obtaining highly discriminative features for face recognition by incorporating a margin in the loss function , such as the additive angular margin loss (ArcFace) . By adding additive margin loss for optimizing the neural network, the DCNN learns to maximize the separation of variable faces, enabling more accurate person identification. In this study, we used this stateof-the-art DCNN for face recognition to compute the face dissimilarity distance on a large dataset of natural faces. To eliminate the effects of race and age on the appraisal of trustworthiness, we limited this study's models and evaluators to individuals of the same race, age, and social class and examined whether the people whose faces resembled one's own face were perceived more trustworthy than others. To the best of our knowledge, this is the first study to demonstrate the relationship between facial similarity estimated from the feature vectors of deep learning and trustworthiness perceived by people. The results of our study have the potential to be used for a wide range of applications in the online society. For example, the automatic estimation of feature vectors from faces will enable personalized suggestions and matching, such as finding partners in peer to peer (P2P) lending, member matching in SNS, and creating more reliable avatars. Methods We created a facial dataset comprising the faces of 200 Japanese college students (100 male and 100 female students aged 19-24 years). Images of full frontal face with neutral expression without any eyeglasses or accessories were taken using a smartphone-camera. Subsequently, the face area was trimmed to fit a square shape. All the photo images were 512 x 512 pixels in size and converted to gray scale, with a mean intensity adjusted to 128 and a standard deviation of intensity set to 50. Another group of 30 Japanese college students (15 male and 15 female students aged 19-24 years) participated in the study as evaluators. We created their facial photographs in the aforementioned manner with the facial dataset. The chosen sample sizes of the face datasets and evaluators are similar to those in previous publications related to trustworthiness evaluation of human faces. The review board of Osaka University approved the experimental protocol (FBS-4), and our procedures followed the guidelines outlined by the Declaration of Helsinki. All participants provided written informed consent prior to the experiment. The evaluators participated in a behavioral experiment to rate the trustworthiness of the 200 faces of the dataset (Fig. 1A). For this purpose, they were positioned in front of a monitor display (EIZO 24 inch, 1920 x 1080 pixels) at a distance of 65 cm. In each trial, following the presentation of the fixation cross for 1 s against a gray background, the facial photograph was presented for 0.5 s. Subsequently, the evaluators were asked to rate the face's trustworthiness on a scale of 1 (very untrustworthy) to 7 (very trustworthy) using a universal serial bus numeric keypad. The input number was converted to a range from -3 (very untrustworthy) to +3 (very trustworthy). To appraise trustworthiness, the raters imagined trusting the person whose face was presented with their money. Once they pressed the button on the display, the next trial started after 1.5 s. Half the number of raters first assessed a group of women and then a group of men. The remaining raters evaluated the facial images in the reverse order. Within the same group, the facial images were presented in a random order. The presentations of the stimuli were controlled using MATLAB 2019a (Mathworks). Facial similarity was calculated using the pretrained Deep Convolutional Neural Network (DCNN) for face recognition supervised by the ArcFace loss. The pretrained ArcFace model based on ResNet  and pretrained using MS-Celeb-1M dataset  is available at https:\\/\\/github.com\\/mobilesec\\/arcface-tensorflowlite. This neural network computes a 512-dimensional embedding vector for each face. We analyzed the vectors of 230 faces (the 200 faces in the dataset and 30 raters), calculated the L2 distance between vectors for each pair, and used it as the face dissimilarity distance. Here, photo images resized to 112 x 112 pixels were inputted to the model, and each embedding vector was L2 normalized. To visualize the high-dimensional data, we compressed them into two dimensions using t-distributed stochastic neighbor embedding (tSNE). The relationship between the face model's trustworthiness and the face dissimilarity distance between the rater and model was analyzed using the linear regression model (y= beta*x+ b, trustworthiness score for y and the face dissimilarity distance for x). Subsequently, the effect of sex on the slope was evaluated by analysis of covariance (ANCOVA). To examine whether the averaged face was evaluated to be more trustworthy than the face model, we calculated the dissimilar distance between each face model and the averaged face. Further, the averaged face was created by averaging the vectors among the same-sex group. Subsequently, we analyzed the correlation between the dissimilar distance of each face model from the average face and the mean trustworthiness score of the face model. All statistical analysis was conducted using the Matlab 2019a (Mathworks). Results Initially, we created a dataset of the frontal faces with emotionally neutral expressions of 200 (100 male and 100 female) Japanese college students (aged 19-24 years). A second group of 30 (15 male and 15 female) Japanese college students (19-24 years) rated the trustworthiness of the faces on a 7-point scale (Fig. 1A). The trustworthiness score was converted to a range from -3 (very untrustworthy) to +3 (very trustworthy) so that neutral was 0, for later analysis. The mean trustworthiness was 0.09 +- 0.82 for female faces and -0.02 +- 0.82 for male faces, and there was no significant difference between the two values (two-sample t-test, t198=-0.98, p= 0.33). In addition, there was no significant difference in trustworthiness values between female and male raters (female faces: female rater 0.14 +- 0.39, male rater 0.03 +- 0.35, t28= 0.8, p= 0.4; male faces: female rater 0.03 +- 0.3, male rater -0.08 +- 0.53, t28= 0.73, p= 0.47). Subsequently, to calculate the facial dissimilarity distance between the face dataset and the raters, we computed a 512-dimensional embedding vector for each face using the pretrained DCNNs for face recognition (ArcFace , see Fig. 1B). Further, we calculated the L2 distance between the vectors for each pair. Figure 1C depicts the L2 distance matrix for all face model-rater pairs. It further indicates that the face dissimilarity distance between the faces of opposite sex is greater than that between the faces of same sex. A tdistributed stochastic neighbor embedding (tSNE) map, which visualizes high-dimensional data in the low-dimensional map, revealed a clear difference in terms of sex in the 512-dimensional embedding vector (Fig. 1D), as well. In addition, the raters' faces were dispersed within the distribution of same-sex face models. Therefore, we utilized this L2 distance as an index of the face dissimilarity distance between the rater and face model and analyzed the relationship between self-resemblance and trustworthiness rating. Next, we analyzed a relationship between the face dissimilarity distance and perceived trustworthiness using linear regression model. A significant negative slope between face dissimilarity distance and trustworthiness was detected (beta=-0.18, t5,996=-2.34, p= 0.019, R2= 0.001). We further examined the effect of sex on this negative correlation. As shown in Fig. 2, the face dissimilarity distance between the face model and evaluator was greater when both were of opposite sexes (blue line) than when they were of the same sex (red line). In the same sex, perceived trustworthiness dramatically increased with a decrease in the face dissimilarity distance. Contrastingly, perceived trustworthiness did not change with the face dissimilarity distance for the opposite sex. Further, ANCOVA confirmed that a significant difference in slope between the same and opposite sex (F5,996= 3.85, p= 0.049, e2= 0.001; the same sex, beta=-0.36, t2,998=-3.15, p= 0.0016, R2= 0.003; the opposite sex, beta=-0.04, t2,998=-0.36, p= 0.72, R2= 0.0004). In addition, there was no significant difference in slope between the male and female evaluators (F5,996= 1.24, p= 0.27, e2= 0.000), and between the male and female models (F5,996= 0.95, p= 0.33, e2= 0.000). To examine the impact of the performance of DCNNs for estimating facial similarity on the correlation between facial similarity and trust level, we calculated the facial dissimilarity distance using FaceNet  (https:\\/\\/pypi.org\\/ project\\/deepface\\/), which does not introduce a margin in the loss function. As a result, the facial dissimilarity distance was not significantly correlated with the perceived trust level (beta= 0.01, t5996= 1.71, p= 0.09). We further examined the effect of center cropping of the dataset using the ArcFace network (https:\\/\\/ github.com\\/peteryuX\\/arcface-tf2) and found that without center cropping, the significant correlation between facial similarity and trust level disappeared (with center cropping, beta=-0.4, t5996=-2.40, p= 0.017; without center cropping, beta=-0.14, t5996=-0.8, p= 0.42). These results suggest that to examine the relationship of facial similarity with the trust level, it is critical to use a neural network that can more accurately represent the relative relationship of facial similarity, such as by introducing a margin in the loss function and centrally cropping the database. We further examined the effect of dataset size on the correlation between the face dissimilarity distance and perceived trustworthiness. From the original dataset consisting of 200 people, we randomly selected 50, 100, and 150 people 1000 times each, and calculated the correlation coefficient in each dataset. The results confirmed that when the number of people in the dataset exceeded 150, the 95% confidence interval of the correlation coefficient fell below 0 (from -0.06 to -0.005; see Supplementary Fig. 1). To examine the effect of the number of trustworthy levels on the correlation between facial dissimilarity distance and trustworthy level, we performed sensitive analysis. We merged 7-levels (-3 - +3) of trust level into 3-levels ([-3, -2], [-1, 0, +1], [+2, +3]) or 5-levels (-3, [-2, -1], 0, [+1, +2], +3), and analyzed Pearson's correlation coefficient R for each. As a result, we consistently observed a significant negative correlation for all scales (3-levels, r=-0.028, p= 0.028; 5-levels, r=-0.028, p= 0.03; 7levels=-0.03, p= 0.019); however, the negative correlation was the highest for 7-levels (see Supplementary Fig. 2). There is a trivial possibility that this phenomenon occurred because the face with the smallest average distance from all evaluators was rated the most trustworthy. To examine this possibility, we averaged the embedding vectors across the samesex face models for each sex and defined these vectors as the embedding vectors of average faces (indicated using greencolored triangles and circles in Fig. 1B). Then, we calculated the face dissimilarity distance from the average face to each face model and analyzed the association between this distance and the trustworthiness score of each face model (Fig. 3). No significant correlation was found between trustworthiness scores and the distance from the average face for both male and female faces (female: r=-0.08, p= 0.44; male: r= 0.02, p= 0.82). Discussion By examining a large number of unaltered natural face images and measuring their objective facial similarity using the state-of-art DCNNs, the current study demonstrated that people automatically calculate the similarity of a stranger's face to their own and perceive faces that resemble themselves to be more trustworthy than those that do not. However, this phenomenon is observed only when the individual and stranger are of the same sex. Further, our results suggest that, in real life, people flexibly change their social judgment strategies depending on the stranger's gender. Why does self-resemblance affect the perception of trust worthiness? Brain imaging studies report that the amygdala's activity increased when a person confronted an untrustworthylooking face. Moreover, researchers reveal that patients with bilateral lesions in the amygdala evaluate every face to be trustworthy. These findings demonstrate that when the amygdala is not activated, people perceive an object to be trustworthy. Accordingly, the results of the current study imply that the amygdala is activated on viewing faces that do not resemble us, whereas it is suppressed on encountering faces that resemble us. Therefore, one should consider why the face that resembles us does not activate the amygdala. According to a previous study, the extent to which a person looks caring and attractive accounts for more than eighty percent of the variance in trustworthiness judgments. On the other hand, the amygdala is activated when a person encounters fearful  or unattractive faces. Based on these considerations, we speculate that a face that resembles one's own produces positive valence, such as being secure and attractive, which suppresses the amygdala's activity and causes the face to be perceived more trustworthy. Many psychological arguments explain a mechanism whereby self-resemblance produces positive valence. First, the frequency of exposure to an object induces positive affective response to that object; this is known as the mere exposure effect. Since we frequently view our own face in mirrors and pictures, the self-face's mere exposure effect may increase our familiarity with the self-resemblance face and produce a positive bias toward it. However, in real life, we spend significantly more time looking at other people's faces than our own. Therefore, the average face that balances the facial characteristics of many people has a generalized mere exposure effect  and should be perceived trustworthy. However, the current study revealed that the similarity to the average face did not have any effect on trustworthiness ratings. The mere exposure effect could not adequately explain self-resemblance's impact on trustworthiness. Another argument is that self-similarity by itself increases attractiveness, independent of familiarity. Byrne proposed this similarity-attract theory based on his findings that people who have similar attitudes and beliefs are highly likely to be attracted to each other. We are attracted to those who are similar to us in terms of physical characteristics, social class, and personality, as well. Moreover, incidental similarities, such as the same birth date, same first name, and similarity in fingerprints, increase prosocial behaviors. According to Byrne's theory, we prefer people who are similar to us because we expect them to have similar thoughts and values and to feel secure when they are in our company. Since the sense of security is a particularly important consideration in trustworthiness assessments, our preference for people who resemble ourselves may increase our trust in such people. In the field of evolutional psychology, similarity-based preferences are explained by Hamilton's theory of inclusive fitness. Hamilton proposed that insects and animals exhibit numerous prosocial behaviors toward their kin to increase the survival chance of genes similar to their own. The theory also applies to human beings since they can detect kin using olfactory cues  and treat their kin preferentially in crisis situations. Earlier studies using composite faces of self-face with another face reported that selfresemblance promotes parental investment  and altruistic behavior in human beings. They further proposed that human beings use facial resemblance as a kinship cue. Since one-half of the composite face comprised self-face, the appearance of the composite face probably closely resembled a kin's face. However, the size of real-world human communities is so large that the likelihood of a stranger being one's kin is overwhelmingly small. Therefore, if we decide to trust strangers based on whether they are our kin or not, we will end up trusting only a very few number of people because most of the people we meet are not our kin. In this respect, the current study presented many strangers' faces of the same generation to raters, and the raters evaluated approximately half of these strangers to be trustworthy. Therefore, it is not likely that they judged trustworthiness based on the criterion of whether the faces were their kin or not. Finally, a positive association between self-resemblance and trustworthiness was observed for faces of the same sex alone. This is because another psychological process may be involved in the trustworthiness assessment of faces of the opposite sex. Regarding mate selection, Winch proposed the complementary needs theory that people tend to prefer a mate whose needs are opposite and complementary to their own. This theory is applicable to the face preference of people of opposite sexes, as well. For instance, women's masculinity preferences are stronger in cultures where poor health is particularly harmful to survival. Therefore, we suppose that the antagonism of psychological processes between self-similarity and self-dissimilarity preferences diminishes the impact of self-resemblance on the trustworthiness ratings for faces of the opposite sex. Irrespective of whether this inference is true, the current study provides a new insight how people flexibly switch strategies in their social judgment of a stranger depending on whether he or she belongs to the same or opposite sex. In this study, we limited the dataset to students aged 19-24 years to eliminate factors such as race, age, and social status that might influence judgments of trustworthiness. As a result, we successfully demonstrated a significant correlation between facial similarity and trust level in a dataset of 200 people. However, because of the small number of data, it is unclear whether the association between facial similarity and trust level is a general phenomenon across ages. Further investigation is required in the future with a larger dataset that includes a wide range of age groups. Conclusions Using facial similarity estimated from state-of-the-art deep learning, in this study, we demonstrated that people tend to trust faces that are similar to their own. We observed particularly strong correlations in judging the trustworthiness of same-sex faces. We also demonstrated the importance of carefully selecting a neural network that more accurately represents relative facial similarity between persons. If we can predict a person's perceived trustworthiness based on facial similarity using artificial intelligence, it may have a wide range of social applications, such as online P2P lending and the creation of trustworthy avatars.\",\"1135747471\":\"1 INTRODUCTION Societal polarization in the wake of the COVID-19 pandemic is a life and-death example of the impact of conspiracy theories. Conspiracy theories contribute to many issues of existential importance includ ing the erosion of democratic norms, vaccine hesitancy, and denial of climate change. In order to address these negative impacts, it is crucial to have a comprehensive understanding of the predictors of conspiracy theorizing. A significant limitation of the existing literature is that researchers have typically examined only a small number of predictors at a time, within one or a small number of national contexts. This is largely because relevant the ory tends to focus on low- tomid-level analyses, specifying only a small number of proximal predictors. This inevitably overlooks other possi ble predictors beyond the scope of these theoretical models , and the relative importance of these predictors. Machine learning offers a complementary approach- a data-driven, exploratory analysis of many candidate predictors at different levels of analysis, which identifies the most important pre dictors of conspiracy theorizing and holds the potential to reveal overlooked factors at the individual and contextual (country) level, thus inspiring new hypotheses. In the present study, we conducted a machine learning analysis of 115 potential individual and country-level predictors of conspiracy theorizing in a large inter national dataset collected during the early weeks of the COVID-19 pandemic. 1.1 What are conspiracy theories and where might they thrive? Conspiracy theorizing refers to a belief that two or more actors have coordinated in secret to achieve an outcome and that their conspir acy is of public interest but not public knowledge. In the past 15 years, research has focussed on the psycholog ical determinants of conspiracy theorizing (; for reviews). In a review of the literature, Dou glas et al. (2017) synthesised these psychological determinants into a framework of epistemic factors such as paranoia, existential factors such as feeling unsafe, and social factors such as perceptions of out group threat. Many of these same variables also predict conspiracy theorizing about the origins of COVID-19 , suggest ing that conspiracy theorizing about specific events (e.g., the origins of COVID-19) shares similar predictors with conspiracy theorizing about more general aspects of social and political circumstances (e.g., that politicians do not inform the public of the true motives behind their decisions). Despite the abundance of prior research on individual-level pre dictors of conspiracy theorizing, each study has typically focussed on only a small number of predictors. For example, some studies have focussed on the relationship between individual differences in feelings of control and conspiracy theorizing , whereas others have focussed on feelings of paranoia. Different individual-level predictors are rarely combined within a single study, which precludes assessing their relative impor tance in predicting conspiracy theorizing. There are also doubts about the cross-national generalizability of individual-level predictors since research has generally studied these predictors in one national context. It therefore remains to be seen how frameworks such as the one proposed by Douglas et al. (2017) can explain conspir acy belief amidst a range of other (previously un-tested) variables, and across a wide range of national contexts. Even less is known about potential contextual or country-level pre dictors of conspiracy theorizing. Conspiracy theorizing has typically been examined in smaller-scale studies conducted in one country (and typically in WEIRD samples) or has compared conspiracy theorizing across a limited number of countries. Studies have yet to examine how conspir acy theorizing may vary from place to place according to country-level differences. These limitations of the literature pose a challenge when seeking to understand conspiracy theories in the context of a global threat like COVID-19. There is some evidence at the within-country level suggesting that contextual factors may be important at the between-country level, too. Within countries, adverse socio-political circumstances such as low socio-economic status , discrimination , discontent , and victimization  are all associated with increased conspiracy theorizing. As countries typically differ in these contextual factors, we might expect conspiracy theorizing to be more prevalent in countries that have less well-functioning socio-political systems and where people have more reason to feel disillusioned. Fol lowing these findings, scholars have argued that conspiracy theorizing, and the deeper political and economicmalaise they signify, could there fore be attenuated by increased societal equity and transparency. There are, nonetheless, theoretical grounds to expect the opposite relationship between socio-political conditions and conspiracy the orizing. Conspiracy theories question and problematize power, and often refer to the actions of a powerful government and its agen cies. Thus, they may flourish in countries with powerful and effective states, whose governments are seen as more capable of conspiring. There is also reason to think that conspir acy theories may flourish in democratic states. Specifically, conspiracy theorizing has been framed as a way to attack and delegitimize polit ical opponents , allowing communities to construct alternative narratives to resist being erased or disempow ered by those in power. Conspiracy theorizing provides a 'heuristically indispensable' reminder that political power is concealed and exercised secretly (;, p. 78) and that inmodern capitalist economies, 'cor porations may make false claims; control certain markets unfairly; or manipulate Government support' (;, p. 63). In sum, some scholars have argued that conspiracy theories may be a tenet of democracy. There are therefore competing hypotheses about the societal condi tions that foster conspiracy theories. According to some research, they thrive in brutal, dysfunctional, or unjust social conditions in which peo ple are structurally disempowered and psychologically disillusioned. On the other hand, they may prosper in more stable and democratic social conditions where states are strong and where politicians, journalists, opinion leaders, and the pub lic have the political freedomand resources to express and disseminate their suspicion and criticism of powerful elites. Exploratory research using large-scale multinational data may determine which of these theoretical perspectives is most plausible. 1.2 Methodological approaches Existing research has largely been confirmatory, relying on theory to identify reliable predictors of conspiracy theorizing. Recently, it has been argued that machine learning analyses can complement exist ing theory by facilitating the rigorous exploration of large datasets, casting a broader net and identifying potentially overlooked relevant predictors. In a recent analysis of this type, Brandenstein (2022) analysed several predictors of conspiracy theoriz ing basedonDouglas et al.'s (2017) frameworkof epistemic, existential, and social needs. Brandenstein conducted a machine-learning analysis on a representative dataset of over 2000 UK citizens. This anal ysis revealed that the relationships between epistemic, existential, and social factors and conspiracy belief--which have been observed in previous research--were largely supported. Brandenstein (2022) therefore sets a precedent for using a machine-learning approach to study the predictors of conspiracy belief. In the current study, we go further by (a) examining a wider range of psychological predic tors including many that have previously been untested, (b) examining a range of country-level predictors, and (c) including a wide range of national contexts. Multiple individual-level variables are tested simultaneously alongside multiple country-level factors to establish a picture of themost important predictors of conspiracy theorizing. The machine learning algorithm random forests  is particularly suited to this endeavour  because it can accommodate a large number of candidate predictors and performs variable selection, usually offers very good performance at low computational cost, and affords straightforward interpretation of variable importance and marginal effects of the predictors. The algorithm intrinsically accommodates non-linear associations and interactions between predictors, which is advantageous in the absence of a strong theory about the shape of associations. Random forests also curtail spurious results and maximize the generalizability of the find ings by means of bootstrap aggregation. Specifically, many bootstrap samples are drawn from the original data, and a regression tree model is estimated on each bootstrapped dataset. The prediction error of this tree model is then estimated on cases not in the bootstrap sample, which provides an estimate of the model's generalizability. In model selection, this so-called 'out-of-bag' prediction error is minimized, thus ensuring a generalizable result. A further advantage of random forests is that it can accommodate both individual- and national-level predictors and is robust to measurement variance and cross-level interactions, where a given predictor has a different effect in different countries. In both cases (measurement variance or randomeffects), the model would include an interaction that accounts for different effects between countries. Random forests thus constitute a relatively flexible model that can identify potentially important predictors even in the presence of relationships of unknown complexity, while incorporating checks and balances to prevent false-positive findings and ensure generalizability. 1.3 The present study The present study used data collected by the PsyCorona consor tium (see https:\\/\\/www.rug.nl\\/sustainable-society\\/research\\/previous themes\\/psycorona\\/ for details), which was launched in 114 countries with over 60,000 participants in the weeks after the World Health Organization (WHO) declared COVID-19 a pandemic. A 20-min web based survey, which was translated into 30 languages, investigated the psychological impact of the COVID-19 pandemic. Data were col lected by a combination of convenience sampling, snowball sampling, and representative sampling by a professional service. Full details of the PsyCorona survey and all variables measured are available here: https:\\/\\/osf.io\\/qhyue. This study used data collected from 19 March to 17 May 2020. Individual-level variables included demographic factors (e.g., age, gen der, education, religion) and shortened self-reportmeasures of psycho logical factors, including some that pertain to the psychological needs associated with conspiracy theorizing (e.g., paranoia, feelings of strug gle, migrant threat;). The survey was broad in scope and thus included many individual-level psychological variables not currently known to be relevant to conspiracy theorizing. From the PsyCorona survey, we included 80 individual-level variables, of which 16 were multi-item composites. The survey data were enriched with country-level factors (e.g., political stability, the effectiveness of gov ernment), some of which were matched to the date of participation in the survey (pandemic severity, government policy response to COVID 19). In total, 35 country-level variables were included. The dependent variable of interest was the extent to which participants engaged in conspiracy theorizing. A table of all variables and their descriptions is available in the project OSF repository: https:\\/\\/osf.io\\/ev24r\\/. The present study is explicitly exploratory, and therefore no hypotheses are provided. Nevertheless, based on previous research we might expect some individual-level variables to emerge as impor tant predictors (e.g., feelings of struggle, paranoia, migrant threat). Furthermore, as this analysis is one of the first studies on conspiracy theorizing to use a large multinational sample, wemight expect results to shed light on the two contrasting perspectives we outlined earlier regarding country-level predictors. Specifically, if conspiracy theories are a consequence of poor socio-political conditions, then we would expect themachine learning results to show that conspiracy theorizing is related to country-level indices of negative life conditions includ ing lower political stability and higher deaths from COVID-19. On the other hand, if conspiracy theorizing is more characteristic of rela tively well-functioning societies with fewer social problems, we would expect it to be related to country-level indices of positive life condi tions such as higher political stability and lower number of deaths from COVID-19. 2 METHOD All data files, analysis code and secondary data used in this study are available in the project OSF repository: https:\\/\\/osf.io\\/ev24r\\/. 2.1 Participants The cross-sectional PsyCorona survey was approved by the Ethics Committee of the University of Groningen (study code: PSY-1920-S 0390) and New York University Abu Dhabi (study code: HRPP-2020 42). All participants gave their informed consent before taking the survey. Of the 60,192 participants who completed the original survey, 61% were female, 38% were male and 0.5% indicated 'other' for their gender (0.5% were missing data). The majority of participants were between 25 and 34 years old (24.4%) with 22.2% aged 18-24, 19.2% 35-44, 14.3%45-54, 11.4%55-64, 6.9%65-74, 0.9%75-85 and0.1% over 85 (0.6% were missing values). The majority of participants had a bachelor's degree education (30.1%), 1.5% had primary education, 13.2% secondary, 9.9% vocational, 22.9% higher, 16.5% has a master's degree and 5.3% had a PhD (0.7%weremissing values). 2.2 Conspiracy theorizing The dependent variable was operationalized as a mean score of three items from the widely used conspiracy mentality questionnaire (CMQ;). These were 'Many very important things happen in the world, which the public is never informed about', 'Politicians usually do not tell us the true motives for their decisions' and 'Gov ernment agencies closely monitor all citizens' (0 = certainly not 0% to 5 = undecided 50% to 10 = certainly 100%, overall a = .73; for reliability statistics per country, see Table S1). The CMQ is a measure of an individual's general tendency towards conspiracy theorizing. It does not refer to specific conspiracy theories (which varywidely across countries). Note that random forests do not assume measurement invariance across countries for either the predictors or outcome variable. If mea surement variance is present and causes heterogeneity in effects of other predictors across countries, the trees in themodel accommodate this by first splitting on country and then splitting on the remaining predictors, effectively estimating country-specific models. Since coun try had low variable importance, however, there is little evidence that this is the case in our data. The full codebook for the survey is available here: https:\\/\\/osf.io\\/qhyue\\/. In most cases, brief or abbreviated mea sures (as was the case for the CMQ) were chosen to reduce the length of the survey and improve sample size and retention for subsequent waves. 2.3 Data cleaning Aspreviouslymentioned,we included80variables fromthePsyCorona Survey. To ensure the stability of the modelling procedure and the performance of the model, we needed to exclude countries with very few participants. In particular, we sought to avoid imbalanced sub groups,which can lead to issueswith reliability and robustness. Countries constituting less than 1% of the total sample were therefore excluded. The final sample consisted of N = 56,072 respondents across 28 countries (all countries are listed in Table S1). As our analyses required complete data and could not use multi ple imputation, we used missForest, a single imputation method with comparable performance to multiple imputation, to impute missing data. Prior to imputation, we plot ted the density of missing values by variable and by participant and observed that missingness in variables was mostly below 20%, and missingness in respondents was mostly below 28%. We excluded vari ables and participants with greater missingness than these subjective thresholds, which resulted in the exclusion of a number of variables that had been added or modified after data collection had started, and 1% of respondents. Third, we computed mean scores for multi item scales using the tidySEM R-package. Two scales were excluded because their reliability fell below acceptable stan dards. These were a three-item boredom scale, and an ad hoc 'Corona Reflection Task' where participants were asked to reason about epi demiological and policy dilemmas (Cronbach's alphas.53 and.27, respectively). 2.4 Data enrichment ThePsyCorona datawere enrichedwith country-level data frompublic sources. These sources were selected due to their international rel evance for affording, shaping or guiding individual-level behavioural responses to COVID-19. They measured pandemic severity (as indi cated by the number of cases, deaths and recovered patients), pandemic-related policies (including both pre-existing policies and ongoing governmental response to the COVID-19 pandemic) and pan demic preparedness. Table 1 presents an overview of the included databases. The time range in data collection afforded variability in the degree to which people in a given country were seeing cases and\\/or engaging in different containment policies. When applicable, respondents' country-level data were matched to their date of par ticipation (e.g., confirmed cases, lockdown severity). After enrichment and data cleaning, there were 115 predictors (80 survey factors, 35 country-level factors). 2.5 Data analytic plan Prior to analysis, we used random sampling to construct a training dataset and test dataset consisting of 70% and 30% of the total sam ple, respectively. This ratio of training and test data is arbitrary, but conventional. To avoid any cross-contamination between the training and test sample, this percentage and the random seed used to split the datawere committed to the public GitHub repos itory before gaining access to the data (see the project OSF repository: https:\\/\\/osf.io\\/ev24r\\/). The training set was used for model building, and the test set was used exclusively for unbiased estimation of the final model's predictive performance (generalizability) after all other analyses were complete. Random forest analyses were conducted using the ranger R package. The forest consisted of 1000 trees. Two tuning parameters of random forests are the number of candidate variables to consider at each split of each tree, and the minimum node size resulting fromasplit. Theoptimal tuningparameterswere selected by minimizing the out-of-bag mean squared error (MSE) using model based optimization with the R-package tuneRanger; in large datasets, this approach is equivalent to cross-validation. The best model considered 30 candidate variables at each split, and a min imum of seven cases per terminal node. We report the results of this best model. We report three types of output from the random forests analy sis. The first are predictive performance metrics, which refer to the model's ability to accurately predict new data in the test dataset. As a standardized metric of predictive performance, we examine predic tive R2. It is a measure of explained variance analogous to the regular R2, except that in the machine learning context, it is computed on the test dataset, which was not used to estimate the model. This estimate is unbiased, and always lower than the R2 on the training data. Estimates of R2 on the training sample should be interpreted as a measure of descriptive performance (i.e., how well the model describes the data at hand) and can be (severely) positively biased when used as an estimate of predictive performance in new data. Given that we also recruited an age-gender representative subsample across 20 countries, we were additionally able to compute predic tive performance for the representative subsample of the test sample to better examine the generalizability of our findings to the target population. The second outcome metric is variable importance, which reflects each predictor's relative contribution to prediction accuracy. Variable importance is estimated by randomly permuting (shuffling) the val ues of each predictor variable in turn, thus removing any meaningful association with the outcome. The model's predictive performance is then re-computed with one permuted variable, and the decrease in variable importance relative to the unpermuted model is taken to reflect the (inverse) importance of that variable. Note that, although random forests are robust to multicollinearity, multicollinearity does attenuate estimated variable importance. The third type of output are partial dependence plots. These visu alize the marginal bivariate association between each predictor and the outcome, while averaging over all other predictors. They are derived by computing predictions of the dependent variable across a range of values for each individual predictor, while averaging across all other predictors using Monte Carlo integration. They show the direction and (non)linearity of a specific marginal effect. The partial dependence plots in this article are generated using the metaforest R-package. Note that random forests are not the same as multiple linear regression (MLR). Whereas MLR is easier to interpret, random forests provide a better model. Other than identifying basic marginal associations, random forests are best suited to understand (1) how well an outcome can be pre dicted and (2) which variables are most important in predicting the outcome. 3 RESULTS 3.1 Predictors of conspiracy theorizing The random forest model explained 26% of the variance in conspiracy theorizing in the testing sample, and 29% in the representative sub sample of the testing sample. We tested alternative algorithms and found the predictive performance of the random forest approach to be superior (the results of these additional analyses are provided in Table S2). We report the top 30 predictors here due to space restrictions, but full results are available on the project OSF repository. Figure 1 displays the rank-ordered variable importance, along with an approx imate indication of whether each predictor's effect is positively or negativelymonotonous, or differently shaped (e.g., curvilinear). Table 2 provides a brief legend of the predictors and more detail is available on the project OSF repository. The partial dependence plots show the marginal bivariate association between each predictor and conspiracy theorizing, averaging over all other predictors (Figure 2). Among the top 30 predictors of conspiracy theorizing were 15 individual-level factors and 14 country-level factors. The most impor tant individual-level predictors were higher discontent with the direc tion of society, lower support for extraordinary government economic intervention and higher paranoia. Overall, these individual-level pre dictors are in line with prior theory and research. The most important country-level predictors were higher political stability, higher govern ment effectiveness and lower deaths from COVID-19. Overall, they were objective indices of good life conditions. 4 DISCUSSION Research on the psychology of conspiracy theories has largely been biased towards the individual level of analysis and limited to theoret ical frameworks that specify proximal causal relations between a small number of variables. Using a machine-learning analysis, the present study sought to complement existing knowl edge by providing a more comprehensive empirical overview of the associations between conspiracy theorizing and potentially relevant individual- and country-level predictors. Our discussion of the results focusses on the 30 most impor tant correlates of conspiracy theorizing, as identified by the analy ses. Of these, 15 were individual-level factors. Conspiracy theorizing was correlated with--in descending order of magnitude--discontent with the direction of society, (low) support for extraordinary gov ernment intervention in the economy, paranoia, economic conse quences, future focus, COVID-19 personal safety, (low) economic efficacy, perceptions of migrant threat, (low) online contact with immigrants, present focus, past focus, and (low) COVID-19 restrictive measures. The first four of these predictors were more powerful than any country-level predictors. For the most part, these individual-level predictors are in line with prior theory and research, including the results of a similar machine-learning analysis by Brandenstein (2022). Research has found that conspiracy theorizing is be associated with a negative view of elites, disillusionment with society, paranoia and frustrated psychological needs. How ever, several important predictors have not been previously studied in conjunction with conspiracy theorizing, such as the variables related to temporal focus (future, present and past focus;). Future research might seek to explain why these variables are important predictors. For instance, perhaps temporal focus reflects a thinking style associated with a felt need to explain important events. Furthermore, several of the individual-level predictors that were pos itively associated with conspiracy theorizing were related to negative perceptions of society, especially of how society can meet individual needs. It is worth noting, however, that individuals' subjective per ceptions of the overall welfare and direction of society can often be dissociated from objective reality. Experi encing societal discontent, therefore, does not mean that a society is indeed in decline. We therefore turn now to the associations between conspiracy theorizing and objective indicators of social conditions at the country level. Fourteen of the 30 most important correlates of conspiracy theo rizing were contextual (country-level) factors. These correlates had something clear and striking in common: they were objective indices of good life conditions. These were, in descending order: political sta bility, government effectiveness, (low) deaths, (low) confirmed cases of COVID-19, number of doctors per capita, control of corruption, government response to the pandemic, (low) number of recovered cases (entailing a low number of early cases), accountability, the con tainment health index, tourism expenditure, stringency, and rule of law. Conspiracy theorizing generally appeared to be higher in countries in which social, legal, and health conditions were favourable. Note, how ever, that themarginal associations for political stability, governmental effectiveness and corruption control suggest that the association between governmental effectiveness and conspiracy theorizing is pos itive and increasing for all but themost effectively governed countries. These consistently included Australia, Canada, Germany, Japan, the Netherlands and the United Kingdom. At very high levels of these indicators of governmental effectiveness, lower levels of conspiracy theorizing were observed (see Figure 2 and Figures S1 and S2). Furthermore, as was the case for individual predictors, impor tant country-level variables emerged that have not previously been studied in conjunction with conspiracy theorizing, such as country-level tourism expenditure--arguably another indication of a well-functioning society. Future researchmight seek to further explore these associations. In summary, our findings suggest that conspiracy theorizing may flourish more in effectively governed rather than dysfunctional soci eties, except in the most effectively governed societies. This does not negate the hypothesis that conspiracy theorizing is animated by adverse political developments including corporate and political corruption , and government secrecy and surveillance. Nor does it discount the role of collective or group-based adversities including poverty , discrimination , disadvantage , and victim ization. However, our findings are consistent with the perspective that conspiracy theorizing may play a corrective role in the functioning of effective societies where concerns about societal decline are possible , andwith theperspective that conspiracy theorizing is a privilege enjoyed in effectively governed societies where dissenters are suffi ciently resourced to express and share their thoughts publicly. Our finding that conspiracy theorizing seems to thrive in effectively (but not themost effectively) functioning societiesmight at first glance appear to be at odds with prior literature on conspiracy theories. Specifically, con spiracy theorizing has been shown to predict political disengagement and anomie , endorsement of political vio lence , non-normative political action  and hatred of social and political outgroups , none of which are necessarily hallmarks of effective societal functioning. It is important to note, however, that these findings have largely emerged from studies of the consequences of individuals' expo sure to, or belief in, conspiracy theories. It is possible that the adverse effects of conspiracy theorizing on individual behaviour are counter vailed by positive systemic effects of the sort identified by scholars. Alternatively, conspiracy theorizing may flourish in well-functioning societies, while at the same time undermining their functioning. The present findings thus do not contradict previous findings but do indicate fruitful direc tions for future research. Future research should address whether these links are causal in nature, for example, by examining whether conspiracy theorizing is reduced when governments become more transparent and accountable. 4.1 Limitations and future research One limitation of the present study is that our data are correlational. The exploratory findings resulting from machine learning analyses emphatically do not warrant causal conclusions, although they may give rise to causal hypotheses, which can be tested in future confirma tory research. A related limitation is that the results are conditional on the scope of the included predictors, as is the case in all quantitative research. We cannot exclude the possibility that an important predictor or confounder was omitted (e.g., political orienta tion, socialmedia use), nor canweexclude the possibility that anyof the predictors may be colliders. Another limitation relates to the operationalization of conspiracy theorizing. We used three items from the CMQ, a scale intended to assess generalized political suspicions about authority that are associated with the endorsement of conspiracy theories. The CMQ does not assess the endorsement of specific conspiracy theories (e.g., about the moon landings or the death of Diana, Princess of Wales, COVID-19). The use of a more general measure like the CMQ may have some advantages--for example, it may be better suited for multi national research because it is not affected by cultural differences in familiarity with specific conspiracy theories, or impacted by culturally variant prejudices against specific alleged conspiratorial actors. How ever, some have argued that the CMQ assesses different aspects of conspiracy theorizing than other scales , and some have questioned its validity. Another limitation related to the validity of the CMQ is that its items arguably refer to the effective functioning of a powerful state, one that is sufficiently resourced and organized to keep secrets and monitor its citizens. This may introduce confounding with objec tive between-country differences in functioning of the state, which may help explain some of the country-level effects we found. The CMQ items further reflect a sceptical and questioning attitude to political authority; although these attitudes are related to conspiracy theoriz ing, they can also be beneficial to societal functioning. Another advantageof theCMQis that, compared to other scales, it does not reference implausible conspiracies (e.g., about aliens, secret societies of ethnic outgroups, or scientifically implausible events) to which people who are less educated or informed are more susceptible. In sum, there are some unresolved conceptual issues in the liter ature about these different scales and the constructs they measure. Sometimes it is suggested they may address the same predisposition , sometimes that they are substantively differ ent , and sometimes the CMQ is treated as an antecedent of endorsement of specific conspiracy beliefs. This uncertainty about the relation between conspiracy theorizing as measured by the CMQ and endorsement of specific conspiracy theories has important implications for the present results. Although the CMQ correlates with endorsement of specific conspiracy theories, we cannot conclude that factors predicting CMQ scores will also predict endorsement of specific (e.g., COVID-related) conspiracy theories. Further research is needed, therefore, before we can conclude that endorsement of specific conspiracy theories, as well as conspiracy theorizing generally, is associated with positive societal functioning. Another potential limitation of the present study is that, in our inter pretation of the results of the random forest analysis, we examined only bivariate marginal associations. This gives an impression of how each predictor is associated with conspiracy theorizing while aver aging over all levels of all other predictors. This way of visualizing the results does not reveal potential interactions between predictors. However, several of the predictors did not show a clear association with the outcome on average, as indicated by relatively flat bivariate partial dependence plots, despite ranking high on variable importance. Although it is likely that such variables derive their importance from interaction effects, there is no straightforward way to know what these interactions are. Formulating and testing theoretically driven hypotheses about possible interactionswith these variables could thus be an important avenue for future research. Relatedly, although our results suggest that somepredictors have curvilinear associationswith conspiracy theorizing, the present approach does not allow formal testing of the shapeof this relationship. Future researchought to inves tigate this potential non-linearity using parametric models with more nuancedmethods for cross-national comparisons. We also need to acknowledge that the sample as a whole was not representative; for example, educated adults were overrepresented. Although biased sampling might pose a threat to generalizability, we can estimate generalizability on the representative subsample of the testing data. Our model achieved the highest predictive performance in this representative subsample, which suggests that the results are generalizable to the population. Furthermore, all participants took part voluntarily, so the possibility of self-selection bias should also be con sidered. A final limitation of this study is that the amount of variance in conspiracy theorizing that is explained is relatively small. One poten tial explanation may be that the CMQ has low reliability or validity. Another potential explanation is that important predictors may have been omitted. Although a consortium of scientists sought to include all scales relevant to their fields, our analyses cannot speak to poten tially relevant predictors of conspiracy beliefs that were not included in the data. The same principle applies to the country-level predictors, with one major distinction. Specifically, as the variable 'country' was included as a predictor in the model as well, it should account for the effect of any unmeasured between-country differences. The fact that country did not rank highly among the important predictors indicates that there are no important omitted between-country differences. 5 CONCLUSION Research on the psychology of conspiracy theories has flourished in recent years and much has been learned about the antecedents and consequences of conspiracy theorizing. New technologies and computational power have made conspiracy theories much easier to disseminate during this time, and they also allow researchers to study them in new ways. The present study used a large cross-national sur vey to provide a unique insight into both individual- and country-level predictors of conspiracy theorizing, and usedmachine learning to com plement existing theoretical knowledge of the relevant predictors of conspiracy theorizing. Many of the individual-level predictors iden tified as important in the analysis echo previous findings. However, we also identified a number of country-level predictors suggesting- contrary to existing research--that conspiracy theories may thrive the most in relatively well-functioning democratic countries.\",\"1135747499\":\"1. Introduction The success of an organization depends, in part, on the ability of organization members to work harmoniously together. Beyond efficient delegation of tasks and effective communication surrounding those tasks, interpersonal relationships matter a great deal. With repeated encounters, organization members become acquainted with one another through repeated conversations and reciprocal disclosures, such as talking about one's family and hobbies. Self-disclosures lead to liking , as does eliciting other's self-disclosures. Yet, self-disclosure does not always have positive effects on workplace relationships. People may have concerns about disclosing sensitive personal information to others. In addition, employees, particularly minority members, can have concerns about the effects of disclosing to others in the workplace. Where workplace climates have norms that discourage the informal and intimate social interactions reserved more for friends, managing how much to disclose during conversations is a difficult process. Every day, people speak approximately 16,000 words , and one-third of one's adult life is spent at work. The complexity of conversations is compounded by its rapid turn-taking nature. During this process, whether speaking to colleagues, friends, family, or even one's romantic partner, there may be qualities about oneself that one seeks to not reveal during a conversation. An interaction partner might bring up a topic that one does not wish to discuss, which might prompt feelings of awkwardness. Or, if interaction partners disagree on some issue, a heated argument or conflict can result. The tendency to engage in topic avoidance with other people has been generally linked to lower relationship quality. Much of the research on topic avoidance has been conducted within the context of romantic relationships. For instance, relationship dissatisfaction can prompt topic avoidance with one's partner, which can further increase relationship dissatisfaction. Topic avoidance has also been studied in families. For example, children seek to avoid discussing certain topics with their parents, which has also been linked to reduced relationship quality. In addition to being mostly confined to studying romantic and fa mily relationships, this prior work has been confined to the study of people deciding whether or not to bring up a conversation topic. That is, prior work in this domain has examined the participant as the arbiter of what to bring up in conversation. In contrast, prior work has yet to examine what happens when someone else brings up a topic one wishes to avoid talking about. In sum, conversations have the potential to create awkwardness and even conflict, and thus it makes good sense that people, therefore, will seek to not bring up things they do not wish to talk about. If only it was that easy. The topics of a conversation are not up to one person. Other people will introduce topics into a conversation, including topics one may wish to avoid. What happens when someone else brings up an unwanted conversation topic? The current work presents the first empirical examination of the experience of an unwanted topic being introduced into a conversation by someone else. We ask, how do people think, feel, and behave when an unwanted topic comes up in conversation. 1.1. The topic avoidance process model We first conducted three data-driven studies (Studies 1-3) to understand the experiences people have when someone introduces a topic into conversation, which one seeks to avoid. Study 1 identified the conversation topics people seek to avoid, emotions most commonly reported in response to unwanted topics, and strategies most often deployed in response, collecting data on 20,000 unwanted conversations. Study 2 then measured per each recently avoided conversation (among the 10 most common topics identified in Study 1), the extent to which the most commonly reported emotions (from Study 1) were experienced. A two-factor model emerged from the emotions. In Study 3, we additionally collected another 4000 responses to the question of why people seek to avoid such conversations. We submitted this corpus of text responses to a machine learning algorithm to identify the underlying semantic structure of participants' motivations. The commonly avoided topics, motivations, emotions, and behavioral responses to topic avoidance identified in Studies 1-3 formed the basis of our Topic Avoidance Process Model. Specifically, after these studies provided a picture of what topic avoidance looks like, we then proposed and tested our model. We predicted two independent sets of interrelated processes would fit an indirect effect model--from motivational contexts for topic avoidance to behavioral responses--through distinct emotional reactions to unwanted conversation topics. Our model is based in the vast body of literature that demonstrates there are two core systems for behavioral regulation, one centered on inhibiting behaviors that could result in negative outcomes (the behavioral inhibition system), and one centered on taking action either to bring about positive outcomes or to avoid undesired outcomes (the behavioral activation system;). We predicted an approach-based pathway and also an avoidance-based pathway in the context of topic avoidance. Specifically, our model predicted two distinct processes in response to a conversation partner introducing an unwanted topic into a conversation. If one is concerned for privacy, they would be more likely to experience inhibitive emotions (such as anxiety) and decide to remain silent. On the other hand, if one is concerned for creating a conflict, they would be more likely to experience activating emotions (such as anger) and decide to leave the conversation. Studies 4-6 tested this model. Study 4a found the predicted pathways from motivation to emotion in anticipated conversations with known individuals sitting together in a group. Study 4b also found the same predicted pathways in live instant-message conversations with strangers. Study 5 then examined recalled conversations, and provided evidence that these pathways continued to predict behavioral outcomes. Finally, Studies 6a and 6b found that these pathways had implica tions for individual's feelings of authenticity, both self-awareness and self-expression. The inhibition pathway that begins with seeking to avoid a conversation topic out of concern for privacy predicted focusing on the self (e.g., thinking about what one said, and how one felt), and thereby increased levels of authentic self-awareness. The activation pathway that begins with seeking to avoid a conversation topic out of concern for creating a conflict, in contrast, predicted focusing on the other or others in the conversation (e.g., thinking about what the other person\\/people said, thinking about how they felt), and thereby reduced levels of authentic self-expression. 1.2. Motivations for topic avoidance Prior work has looked at which topics people avoid bringing up in specific settings. For example, a parent may not want to bring up a financial issue with their child, an employee may not want to talk about an ongoing work conflict at home, and a woman who recently had a miscarriage may not want to talk about pregnancy until some time has passed. In contrast to this work, we examined the psychological experience of having an unwanted topic brought up by someone else in a conversation. Choosing to not bring something up in a conversation should be quite different from having someone else bring up an unwanted conversation topic. For example, one may not want to talk about sex at work, and thus not bring it up. Yet, having a coworker bring up the topic of sex is quite a different situation. Now, one must decide how to handle being in a conversation that one does not want to have. We propose that how one feels and acts in response to an unwanted conversation topic will depend on one's motivations for avoiding that conversation topic. Studies 1-3 were deliberately datadriven so that we could let the participants tell us about their motivations for topic avoidance (rather than impose any top-down). As will be demonstrated, two broad motivations emerged (in Study 3), concern for privacy and concern for creating a conflict. 1.2.1. Privacy In everyday life, people seek to establish some degree of privacy by setting a boundary between the self and others. A completely permeable boundary between the self and others means that any internal thought or feeling a person has would be freely shared with others, whereas an impermeable boundary would mean complete secrecy. People's personal preference for privacy falls somewhere between the two extremes of total transparency and total secrecy. For employees, managing privacy is complicated given that people on average spend 90,000 hours at work over 40 years of 40-hour work weeks. The average U.S. full-time worker works 8.56 hours every day , spending one-third of their time with their coworkers. With so much time at work, managing privacy and maintaining work-life balance is not easy. Employees seek privacy at work for good reasons; work home-life separation has emotional benefits  and sharing of private information can harm relationships at work when employees come from different backgrounds and have different value systems. Organizations often have a larger diversity of people than one might typically encounter in their friend groups and families, and hence this might heighten concern for privacy. A large body work has studied how employees try to separate their work identity from their private identity. Not only do people try to avoid bringing work home, which only serves to increase a sense of exhaustion , but people also try to avoid bringing their self to work, for fear that one's true self may not belong in the workplace. In the workplace, there is surely at least one domain where an employee feels to be in the minority (whether it is one's hobbies, preferences, personality, prior experiences, upbringing, social network, or other demographic variables;). In particular, if one's personal life attributes seem inconsistent with what makes for a good or professional employee, self-expression at work can feel fraught with risks and uncertainties. Mothers will avoid talking about their family as a family-orientation may be seen as at odds with work. Similarly, other minority group members will conceal invisible social identities such as sexual orientation and multi-racial backgrounds when people feel that these identities will not fit in, or that the expression of these identities may lead to unwarranted assumptions. Thus, the stakes might feel especially high in the workplace; staying quiet may be the preferred option, rather than taking a risk and saying the wrong thing. From the literature on privacy and work-life separation, we thus predicted that the more one is concerned for privacy, the more one will inhibit and stay quiet in response to an unwanted topic introduced into a conversation. 1.2.2. Conflict concern Understanding the processes of topic avoidance will bring not only new theoretical insights, but also practical benefits. For instance, understanding how to foster more effective communication in the workplace should help increase employee satisfaction. That said, reducing barriers to such free communication must be done carefully. When communicators come to the table with different perspectives and values, especially in diverse environments like the workplace, it is likely that those perspectives and values will conflict with one another. Conflicts between interaction partners often arise when there are differences in values, education, and social backgrounds. Having different values can increase relationship conflict and process conflict, and thereby decrease workgroup performance and worker morale. Different belief structures can create interpersonal friction, and subsequent conflicts can hurt job performance. Furthermore, conflicts hurt personal relationships  and make professional relationships difficult to manage. What happens when a conflict arises? Research seeking an answer to this question has focused on different styles of conflict resolution. Overt and significant conflicts more often prompt active styles of conflict resolution, whereas more covert and subtle conflicts prompt passive styles of conflict resolution. Thus, the more a conflict clearly presents itself, the more people are inclined to take action to resolve that conflict. We thus predicted that to the extent someone is concerned with creating a specific conflict with an interaction partner (e.g., stemming from opposing viewpoints), they will be more inclined to take action in response to an unwanted conversation topic (e.g., leave the conversation). 1.3. Reactions to unwanted conversation topics We propose that concern with the integrity of one's privacy is an inhibiting motivation, prompting inhibiting responses to unwanted conversation topics. If the reason one is worried about saying the wrong thing is out of concern for privacy, then not saying anything at all might be the preferred response. In contrast, we propose that concern with creating conflict is an activating motivation, prompting activating responses to unwanted conversation topics. That is, if the reason one seeks to avoid a conversation topic is the concern for creating a conflict about the conversation topic, staying quiet may not be preferred as it still could lead the interaction partner to ask for a response. Taking action, such as exiting the conversation may be the more effective strategy to prevent a conflict. 1.3.1. Emotional reactions An unwanted conversation topic is likely to lead to some discomfort, yet no prior work has explored the emotional reactions people have to a conversation partner bringing up an unwanted conversation topic. There are some hints in the prior literature that topic avoidance is associated with some level of negative affect. For instance, the more people seek to avoid bringing up topics in a conversation, the lower their relationship satisfaction. Both within the context of parent-child dyads and heterosexual dating couples, topic avoidance has been associated with relationship dissatisfaction. Thus, at least with respect to not wanting to introduce a topic into a conversation oneself, negative affective judgments seem to follow. Yet, rather than looking at global negative responses to unwanted conversation topics, we seek to understand the more nuanced emotional reactions people have to someone else bringing up an unwanted topic, and how these, in turn, relate to motivations and behaviors. Gathering all common emotional reactions participants reported in an initial free-response survey, we created a scale that we introduced to a new set of participants based on 1000 participants' free responses. This scale was found to have two factors, one which aligned with the behavioral inhibition system (anxiety, nervousness, and embarrassment), and one which aligned with the behavioral activation system (annoyance, irritation, frustration). 1.3.2. Behavioral reactions Prior work on topic avoidance has examined which topics partici pants introduce or seek to not introduce in a conversation, rather than the experience of being in a conversation when someone else brings up a topic that one does not want to talk about. Accordingly, prior work has yet to examine how people respond to unwanted conversation topics arising. When someone starts talking about something one does not want to talk about, what happens next? When the conversation has many people, one option is simply staying quiet, waiting for the topic to change. Yet, if the conversation is a dyad only, simply staying quiet will not be a particularly feasible option. Perhaps instead the person can seek to change the subject. Another option might be to exit the conversation. We theorized that these potential reactions would fall under two categories: inhibitive reactions such as staying quiet and more active reactions such as leaving the conversation. 1.3.3. Felt authenticity Finally, if during a conversation, one's conversation partner brings up a topic that one wishes to not talk about, whatever the response, it seems likely that feelings of authenticity will come to mind. While interest in authenticity has recently grown, its roots can be traced back to work on private and public self-consciousness. That is, one can focus on one's private self (who one truly is), or how one appears to others (how one responds to others and is seen in their eyes). We propose that this self vs. other focus distinction underlies a distinction found in the literature on authenticity. Authenticity has been considered by many as a multi-dimensional construct, composed of authentic self-awareness and authentic self-expression. We examined whether these distinctions align with our two pathways for topic avoidance, specifically in the workplace domain. We predicted that finding oneself in unwanted conversations at work would predict feelings of both authentic self-awareness and authentic self-expression. Authentic self-awareness involves understanding oneself and being aware of who one truly is, whereas authentic self-expression is the inverse of accepting others' external influence (i.e., not simply saying what others want to hear, not letting oneself be influenced by others;). We predicted that because a concern with privacy should enhance a self-focus, topic avoidance motivated by privacy should predict increased authentic self-awareness (i.e., an understanding of one's true self and what one feels comfortable discussing). In contrast, we predicted that because a concern with conflict with another should enhance a focus on that other, topic avoidance motivated by conflict should predict accepting others' external influence (i.e., reduced authentic self-expression to avoid creating a conflict). 1.4. The current work The first three studies use a bottom-up approach that examined the topics people seek to avoid in conversation, emotions experienced to unwanted conversation topics, and motivations for avoiding them. The later studies use a top-down approach that tested our dual pathway model (developed based off the results of Studies 1-3). Throughout the studies, we used a range of methods and participant populations. We collected data on multiple conversations per participant, and analyzed thousands of conversations with multilevel modeling. We treated conversation topic as a random factor in our analyses, and estimated fixed effects of interest that were not attributable to any particular unwanted conversation topic, thus allowing our results to generalize to unsampled conversation topics. We also collected a large corpus of texts responses, which we sub mitted to a machine learning algorithm to uncover the latent semantic structure of the motivations people have for topic avoidance. Additionally, we conducted studies with a large diversity of participants, with diverse ages across the U.S. (and the world), both online and in the field, and we examined topic avoidance in general as well as in the workplace. Finally, we examined past experiences with topic avoidance, an ticipated reactions to an upcoming unwanted conversation, and live conversations where an unwanted topic was introduced. Across the studies, in addition to examining motivations for topic avoidance and emotional reactions, we also explored a range of outcomes including behavioral reactions, focus of attention during the conversation, and feelings of authenticity. Implications for topic avoidance, workplace authenticity, and employee and managerial practices are discussed. 2. Study 1: The conversation topics people seek to Avoid, emotional reactions and behavioral responses Missing from prior work is a broad and systematic overview of the common topics people seek to avoid talking about in their daily life, the motivations for such topic avoidance, the emotional reactions experienced when unwanted topics come up in conversations, and the subsequent behavioral responses. In introducing a process model of conversation topic avoidance, the current work sought to fill this research gap. Study 1 first examined the conversation topics people avoid in everyday life, across a range of contexts, through a large online study. 2.1. Participants and design We recruited 1000 participants via Amazon Mechanical Turk (Mage = 36.27 years, SD = 11.64, range = 18-80, 637 women, 360 men, 3 other). 2.1.1. Conversation topics Participants were asked to list five topics they seek to avoid talking about with four different groups (order randomized), friends, family, romantic partner, and coworkers. Participants were allowed to list the same topic across multiple social groups. 2.1.2. Emotions and behavioral strategies After having listed the 20 topics, participants were asked two ad ditional open-ended questions: 1) how they would feel and 2) what they might do when any of the topics (they listed above) come up during a conversation. 2.2. Results and discussion 2.2.1. Unwanted conversation topics Free response data was analyzed via R statistical software (version 1.2.1268). We identified the most frequent topics across the four categories. Using a standard dictionary from the R-package tm , we removed \\\"stop words\\\" (i.e., common function words that do not have content such as \\\"the,\\\" \\\"to,\\\" \\\"a,\\\" etc.) and stemmed the words (i.e., \\\"finance\\\" and \\\"financial\\\" will be counted towards the same stem of \\\"financ\\\"). Subsequently, we combined the synonyms (following a recent paper that identified the most common secrets people keep;) and counted the frequency of all words, presented with frequency tables. We selected the top 10 unwanted conversation topics identified with this approach, and use these for the remainder of the paper (see Table 1). People most commonly seek to avoid talking about politics, money, personal issues\\/problems, work, religion, family, romantic relationships, sex, the past, and friends (the SOM presents the less common responses). 2.2.2. Emotional experiences The goal of identifying the emotions experienced was to create a scale for later studies, and thus it was desirable to not combine synonyms (i.e., emotion scales typically include several adjectives that could be considered synonyms, but have important and meaningful differences). We identified the ten emotion words that were most frequently reported as experienced when unwanted topics came up in conversation (see Table 2). Participants reported (in order of decreasing frequency) commonly feeling uncomfortable, anxious, nervous, annoyed, awkward, irritated, angry, uneasy, embarrassed, and frustrated (see SOM for less common responses). 2.2.3. Behavioral strategies Unlike conversation topics which were commonly reported using single words and short phrases, and emotion adjectives which were single words, the behavioral strategies were full sentences that required a different text analysis. We thus calculated the frequency of bi-grams to reduce this complexity while still maintaining more nuance than a single word could represent. As before, stop words were removed and the words were stemmed. We combined similar bi-grams, arriving only at three common strategies implemented when unwanted topics came up (Table 3; see SOM for the less common response). When an unwanted topic came up in conversation, people com monly reported (in order of decreasing frequency) that they sought to change the subject, leave the conversation, and stay quiet (other words were specific ways to achieve those aims; e.g., making an excuse, listening). 2.2.4. Unwanted conversations questionnaire Study 1 recruited a sample of 1000 participants across the U.S., and found that across a range of contexts, a set of conversation topics are commonly sought to be avoided. People often do not want to talk about politics, money, personal issues\\/problems, work, religion, family, romantic relationships, sex, the past, and friends. From these responses, we created the Unwanted Conversations Questionnaire, presented in Appendix A. In subsequent studies, we provide participants with this questionnaire to examine their specific experiences with seeking to avoid these conversation topics, when introduced by others, and their motivations for not talking about these topics. In Study 1, we also found a set of emotions people frequently ex perience in response to unwanted conversation topics as well as common strategies deployed in response to unwanted conversation topics. In Studies 2-3, we explore the relationships between the emotions and strategies people deploy in encountering these conversation topics as well as the motivations participants have for avoiding these topics. 3. Study 2: Inhibiting and activating emotions, and behavioral responses Study 1 revealed that people commonly sought to avoid talking about politics, money, personal issues\\/problems, work, religion, family, romantic relationships, sex, the past, and friends. When these topics were introduced into conversation, people reported feeling uncomfortable, anxious, nervous, annoyed, awkward, irritated, angry, uneasy, embarrassed, and frustrated. Finally, in response to these unwanted conversation topics being introduced, people reported changing the subject, leaving the conversation, and staying quiet. We created a scale from the most commonly experienced emotions. We predicted that in reporting one's experience with an unwanted conversation topic, participants' responses to the emotion scale would fall into two factors, inhibition- and activation-oriented emotions. Moreover, we predicted that inhibition-oriented emotions would predict staying quiet (an inhibition response), whereas activation-oriented emotions would predict leaving the conversation (an action-oriented response). We were agnostic as to whether trying to change the subject would be more linked to inhibition or activation as this could be in service of trying to stay quiet about a topic, or might be one trying to take action and change the course of the conversation. In other words, this goal may be common to all contexts in which an unwanted topic enters a conversation. The number of people in a conversation might also predict the strategy utilized when an unwanted conversation arises. A conversation with more than two parties differs in numerous ways from a dyadic conversation. Of particular relevance to the current work, in a multiparty conversation, as compared to conversation composed of only a dyad, each individual will have less airtime. Thus, staying quiet should be more difficult in a conversation of two people, whereas this is far more feasible in a conversation composed of more people. Likewise, leaving a conversation of only two people is quite different from leaving a conversation with many people. In a multi-party conversation, when one individual exits, this does not necessarily mean the end of the conversation, whereas in a dyadic conversation, to leave the conversation is to end the conversation. Accordingly, we predicted that when an unwanted topic is introduced, people would be both more likely to stay quiet in a multi-party conversation, and more likely to leave a multi-party conversation than a dyadic one. 3.1. Participants and design We recruited 200 participants on MTurk, and 206 completed the study. Due to a programming error in the survey flow, the demographics block of questions was only displayed to a subset of participants (we thus had demographic data only for 28 males and 21 females, 24% of the participants; Mage = 34.39, SD = 11.93, range = 19-63). 3.1.1. Unwanted conversations questionnaire Participants were presented with the 10 topics identified in Study 1. Specifically, participants completed the Unwanted Conversations Questionnaire that we introduced in the current work (see Table 4 above). Participants were asked to reflect on the past month. Per each topic, they were asked to choose from the following choices: 1) \\\"Yes, this recently came up in a conversation I was in, and I did not want to talk about it\\\"; 2) \\\"Yes, this recently came up in a conversation I was in, and I did not mind talking about it\\\"; 3) \\\"No, this did not recently come up in a conversation I was in.\\\" Per each instance of an unwanted topic coming up in a conversation (i.e., response option 1), participants reported the type of conversation (dyadic vs. multi-party) they were in, the emotions experienced when the topic came up, and the behavioral responses one took when the topic came up. We analyzed each individual conversation with multilevel modeling, treating participant and topic as random factors. 3.1.2. Conversation type We first collected data to capture a dichotomous variable, mea suring whether the conversation was a dyad (only one other person beyond the participant), or whether the participant was involved in a multi-party conversation. 3.1.3. Emotions and behavioral responses Participants were asked, when the unwanted topic came up in conversation, to what extent they felt each of the 10 emotions (most frequently experienced by Study 1 participants): uncomfortable, nervous, irritated, uneasy, annoyed, awkward, embarrassed, frustrated, anxious, and angry (ranging from 1-not at all to 7-very much). 1When participants for whatever reason do not submit their code for payment on Mechanical Turk, it allows additional participants beyond the recruitment allotment to participate. Whenever this happens in the current work, we analyze all participants' data. 2 Participants were asked, \\\"including you, how many people were having the conversation.\\\" The critical distinction was whether the conversation was composed of a dyad (one other person besides the participant), or a multiparty conversation (multiple others in the conversation). Answering how many were in the conversation, occasionally the participant entered \\\"one,\\\" while meaning \\\"two,\\\" and thus this was coded as a dyad. Later studies resolve this ambiguity by providing participants with a dichotomous choice [two people total, including the participant (dyad), or more than two people total, including the participant (multi-party)]. Additionally, participants were asked what they did when the unwanted topic came up. Participants were shown the top three strategies identified in Study 1. Participants were allowed to select any options that fit, \\\"stayed quiet,\\\" \\\"tried to change the subject,\\\" and \\\"left the conversation.\\\" Also, to allow the participants to indicate that they did not actually avoid the conversation topic, we allowed them to indicate \\\"talked about it anyway.\\\" At the end of the study, participants were asked whether they had read all survey questions and responded carefully, or if they did not respond with care and that their data should be dropped. 3.1.4. Analysis strategy As we collected multiple observations per each participant, we analyzed our data via multilevel modeling. We tested our fixed effects of interest while including participant and conversation topic as crossed random factors. Correspondingly, the remaining variance explained in each model corresponds to the general relationships of our measures that are not specific to any particular participant or conversation topic. R-packages lme4  and lmerTest  ran multilevel lmer models through Satterthwaite approximation tests to calculate p-values (scaling model estimates to approximate the F-distribution to estimate degrees of freedom, which are thus non-whole numbers and differ by predictor). Models that examined binary outcomes used glmer to model a binomial distribution (thus not needing to approximate the F-distribution, and hence yielding whole number degrees of freedom to test the significance of Wald's z tests). 3.2. Results and discussion 3.2.1. Frequency of topics Fig. 1 presents the extent to which participants in the past month had sought to avoid each conversation topic. From our sample of 206 participants, there was a total of 510 times when an unwanted conversation came up, which translates to an average of 2-3 unwanted conversations per participant. Of course, not every instance of having politics, religion, family, etc. arise in conversation is unwanted. Indeed, as can be seen in Fig. 1, occasionally our participants did not mind talking about these topics. That said, they also frequently did not want to talk about them either. The frequently unwanted topics identified in the current work can be clearly differentiated from the topics people like to talk about (as demonstrated in Study 4b; those topics include, movies, TV shows, food, and music). Importantly, our study designs ensured we specifically examined psychological processes related to when these unwanted topics were indeed unwanted. 3.2.2. Emotions A Principal Factor Analysis with a varimax rotation identified two factors with an Eigenvalues larger than 1 (Table 4). We label the first factor \\\"anxiety,\\\" which includes anxious, awkward, embarrassed, nervous, uncomfortable, and uneasy, emotions that have been shown to map onto the behavioral inhibition system (i.e., rather than acting, inhibiting to avoid a negative outcome;). We label the second factor \\\"annoyance,\\\" which includes annoyed, frustrated, irritated, and angry, emotions that have been shown to map onto the behavioral activation system (i.e., taking action to avoid a negative outcome, or bring about a positive outcome;). Both factors together accounted for 69% of the variance in the 10 variables. We thus averaged the emotions per each factor to examine how these emotion composites predicted behavioral responses to unwanted topics coming up in conversation. 3.2.3. Behavioral responses Implementing the multilevel modeling strategy described above, per each insistence of topic avoidance, we entered both emotion composites (inhibition- and activation-oriented emotions) as simultaneous predictors of each behavioral response to address the unwanted conversation topic. These models included random intercepts for participant and topic of conversation. The outcome variable was binary, coded as 0 if the participant did not select the strategy, and 1 if the participant selected the strategy. Whether the conversation was a dyadic conversation (coded as 0) or multi-party (coded as 1) would also likely determine the behavioral response, and thus was also included as a predictor (see Table 5). This revealed that, as predicted, the more anxiety participants ex perienced when an unwanted topic was introduced into the conversation, the more likely they were to stay quiet. In contrast, the more annoyance participants experienced when an unwanted topic was introduced into the conversation, the more likely they were to leave the conversation. The results demonstrate that the emotions experienced in response to unwanted conversation topics indeed cohere with the two core systems for behavioral regulation, the behavioral inhibition system and the behavioral activation system. That is, the emotions fell into two factors, with the corresponding emotions per each factor perfectly aligning with the ways in which these emotions have been previously associated with the behavioral inhibition and activation systems. As further evidence for this alignment, emotions that are inhibiting (e.g., anxious, nervousness, embarrassment) predicted staying quiet when someone brought up an unwanted topic, and emotions that are activating (e.g., annoyance, irritation, frustration) predicted taking action, and leaving the conversation. We did not have a clear prediction for which behavioral system would predict trying to change the conversation topic. That said, we found that inhibiting emotions predicted trying to change the conversation topic. Trying to change the subject of a conversation could be more in service of an inhibition goal than a goal of taking action per se. If anything, activating emotions predicted a reduced tendency to change the subject. Conversation type also predicted behavioral responses. In response to an unwanted conversation topic, when participants were in a multiparty (vs. dyadic) conversation, they were 2.36 times more likely to stay quiet (far more feasible in a multi-party than when in a dyad), and 2.59 times more likely to leave the conversation (also easier to do in a multi-party than when in a dyad). In contrast, in response to an unwanted conversation topic, when participants were in a dyadic (vs. multi-party) conversation, they were 1.67 times more likely to try to change the subject (participants may assume in a multi-party conversation, the topic will change course on its own). In sum, in response to common unwanted conversation topics (when these topics were indeed unwanted), the emotions people frequently experience cohere into two factors, each representing a core system for behavioral regulation, 1) the behavioral inhibition system and 2) the behavioral activation system. Emotions that are inhibiting were related to trying to stay quiet and trying to change the subject. Activating emotions were associated with leaving the conversation. To clarify the nature of these relationships, Study 3 next sought to examine the motivations people have for topic avoidance. 4. Study 3: Motivations for conversation topic avoidance Study 2 found that the most common emotional reactions to unwanted topics introduced into a conversation (from Study 1) cohered into two factors that aligned with the behavioral inhibition system and the behavioral activation system, which in turn predicted corresponding behavioral responses to unwanted conversations. Inhibiting emotions predicted inhibiting behavior (e.g., staying quiet) and activated emotions predicted taking action (e.g., leaving the conversation). However, the specific motivations behind these responses remain unknown. In Study 3, we collected free response data from participants on why they might seek to avoid talking about the commonly avoided conversation topics identified in Study 1. We aimed to collect free-text responses per each of 1000 participants, yielding 1000 documents of text responses for which to submit to a machine-learning algorithm. Through this analysis we reveal the latent semantic structure in participants' described motivations for avoiding unwanted conversation topics. 4.1. Participants and design We recruited 1000 participants on MTurk, and 1005 participants completed the study (mean age = 36.41, SD = 12.40, range = 18-77, 621 women, 383 men, and 1 other). We included an honesty check question at the end of the survey asking participants whether they had been honest in their responses in the survey (they were paid regardless of their answer); all participants responded yes. Participants were randomly presented with one of the 10 topics identified by the Unwanted Conversations Questionnaire, introduced in Study 1 and implemented in Study 2. To generate a large dataset of text responses, we provided participants with a free response textbox per each of four social groups. Specifically, for a randomly selected topic, participants were asked to write about the reasons for which people would not want to talk about the topic with family, friends, romantic partners and coworkers. 4.2. Results and discussion Consistent with our proposal of two distinct topic avoidance processes (one based in the behavioral inhibition system, and one based in the behavioral activation system), we predicted two major corresponding machine-learning derived topics (i.e., constellations of co-occurring text in participants' responses) would emerge, one per system. We first combined the four responses from each participant into one document. We removed English stop words and other contextspecific words (topic-related words, i.e., family, friends) and stemmed the words. Then, we implemented a machine learning algorithm to identify the underlying structure of the free responses for why they sought to avoid the conversation topics. Specifically, to identify two topics that emerged from this free re sponse data, we utilized the Latent Dirichlet Allocation topic model using R-package topicmodels , and constructed a frequency table to visualize the words that most differentiated the two underlying clusters of motivations for topic avoidance (see Table 6). The machine learning algorithm identifies constellations of words that tend to uniquely cooccur, but the results (like with a factor analysis) require some interpretation. Table 6 thus provides the per-topic-per-word beta probability of stemmed words that are highly associated with each topic. As can be seen, the first motivation for conversation avoidance that emerged from this analysis deals with privacy, worry, awkwardness, and concern with being judged (Topic 1). We label this as privacy concerns. The second motivation deals with different opinions\\/views, conflict, and argument, which we label as conflict concerns (Topic 2). With privacy concerns, people noted being worried about how they look to others and were afraid of being judged. With conflict concerns, people described wanting to avoid creating arguments among those with different views and opinions. We propose that the privacy concern topic that emerged from the machine learning algorithm is an inhibiting motivation (i.e., wanting to hold back private personal information to avoid feeling embarrassed or being judged). In contrast, we suggest that the conflict concern topic is an activating motivation (i.e., wanting to take action so as to circumvent an argument, a fight or a conflict). Accordingly, in Studies 4a and 4b, we predicted that when an un wanted topic comes up in conversation, privacy concerns would predict inhibiting emotions, whereas conflict concerns would predict activating emotions. 5. Studies 4a and 4b: Conversations in the field and online Study 3 identified two broad motivations for conversation topic avoidance, concern for one's privacy and concern for creating a conflict with another. Studies 4a and 4b built on Study 3 by formally introducing and testing our Topic Avoidance Process Model. Specifically, we tested our prediction that 1) concern for privacy that follows from an unwanted conversation would predict inhibition-oriented emotions, whereas 2) concern for creating a conflict would predict activationoriented emotions (Study 5 then also examines behavioral responses). We examined this hypothesis in two unique settings. In Study 4a, we approached individuals in the field (Central Park in New York City), and randomly assigned them to (believe that they would) talk about one of the unwanted conversation topics (from the prior studies), specifically with a known other\\/others, with them in the park. In Study 4b, we recruited individuals online, and they had an instant message computer-mediated conversation with another participant (a stranger). In both studies, immediately before the conversation, we measured privacy and conflict concerns. In Study 4a, before the conversation started, we measured anticipated emotions in the conversation, and in Study 4b, we measured emotions experienced during the conversation (after it finished). We predicted that increasing levels of privacy concerns would pre dict increased inhibition-oriented emotions (e.g., anxious, embarrassment, nervous), whereas increasing level of conflict concerns would predict increased activation-oriented emotions (e.g., annoyance, irritation, anger). 5.1. Study 4a: Anticipated conversations with known others in the field We sought 200 participants as in the prior studies and recruited as many participants as possible over two Saturdays in the summer. This led to 223 participants approached in Central Park in New York City. After excluding three participants who did not speak English, our final sample size was 220 participants (Mage = 30.40, SD = 8.52, range = 18-68, 129 women and 91 men). These participants included those who lived in the U.S., but also tourists who reported to be visiting from Argentina, Australia, Bangladesh, Belarus, Brazil, Canada, Chile, China (\\/Hong Kong), Colombia, Dominican Republic, Egypt, Germany, India, Israel, Italy, Japan, Korea, Kosovo, Mexico, New Zealand, Pakistan, Philippines, Poland, Puerto Rico, South Africa, Trinidad, U.K., and Ukraine. Experimenters approached groups (which ranged naturally from 2 to 6 individuals) by asking if they would be interested in participating in a very short study. After the participants agreed, the experimenter informed participants that they would have a very short, two-minute conversation with each other on a randomly selected topic. After informing participants of the randomly-selected conversation topic (from the ten unwanted topics from Study 1; politics, money, personal issues\\/problems, work, religion, family, romantic relationships, sex, the past, and friends), but before the conversation, participants were asked to complete a one-page survey. The survey contained two sets of questions, a six-item motivation scale (introduced here), and a ten-item emotion scale (from Study 2). For the former, participants were asked, in having the upcoming conversation, how much they were concerned (from 1-not at all concerned to 7-very concerned) with \\\"privacy,\\\" \\\"being judged,\\\" \\\"how people think of you,\\\" (privacy concerns; a = 0.75), and \\\"causing an argument,\\\" \\\"creating a conflict,\\\" and \\\"having a disagreement\\\" (conflict concerns; a = 0.83). After the participants completed the scales, participants were thanked and debriefed (no conversation about the randomly-assigned topic actually took place). 5.2. Study 4b: Live conversations with strangers online In Study 4b, we connected participants online through ChatPlat, an instant message platform for research. Anticipating some participants would fail to connect with others, we recruited 250 participants on MTurk, seeking a final sample of 200 participants. We received 253 responses and three participants reported that their data should be dropped from the study (not responding with honesty or care), and 73 participants were unable to connect with another participant (i.e., two participants were not online at the same time). This led to a sample size of 177 participants who connected with a conversation partner for the study (Mage = 35.37, SD= 10.72, range = 21-72, 102 women and 75 men). Participants first were informed they would have a conversation with another person (but not until after the study begun), and we randomly displayed a choice between two topics. One option was always \\\"personal stories,\\\" a topic we anticipated would be unwanted relative to a randomly selected topic pre-tested to be desirable (movies, food, TV shows, hobbies and music). As anticipated, most participants did not want to talk about \\\"personal stories,\\\" relative to the other topics (130 participants; 73% of the 177 participants). We did not include in analysis the participants who wanted to talk about personal stories as such participants could not be said to be having an unwanted conversation. To increase the personal nature of what was to be discussed, par ticipants completed a modified version of the Common Secrets Questionnaire. Participants were shown five common categories of secrets, presented in quotes, that shared conceptual overlap with the topics from Study 1 (presented in parentheses), such as \\\"dislike a friend\\\" (topic: friends), \\\"dissatisfied with your situation at work\\\" (topic: work), \\\"personal beliefs\\\" (topics: religion and politics), and \\\"unhappy in a romantic relationship\\\" (topic: romantic relationships). Participants were asked among the common secrets (presented in quotes), which secrets they were currently keeping, and to identify one to two secrets that they could use some advice on. Participants were next informed that they would have a conversa tion where they would specifically ask for advice on one of their secrets. Participants reported, in having the conversation, how much they were concerned with privacy, and creating a conflict with the other participant (as in Study 4a). Participants were then paired with one another for a live con versation via ChatPlat, and asked to have their conversation (i.e., about their secrets and to ask for advice). After the online conversation, which lasted 5 to 10 min, participants reported the emotions they experienced during the conversation (using the same scale items as in Studies 2 and 4a). 5.3. Results and discussion Adopting the multilevel modeling strategy from Study 2, we entered privacy and conflict motivations for topic avoidance as simultaneous predictors of emotions, treating conversation group (Study 4a)\\/dyad (Study 4b) as a random factor. To isolate the unique relationship of each motivation with each emotional response to an unwanted conversation topic, we entered the alternate emotion composite in each analysis. As conversation type varied in Study 4a (i.e., dyadic vs. multi-party, depending on the size of the group that Central Park participants were in), we also included whether the conversation was dyadic (coded as 0) or multi-party (coded as 1) as in the prior studies. As can be seen in Tables 7 and 8, only two positive effects consistently emerged in both studies: privacy concerns predicted inhibition-emotions of anxiety, whereas conflict concerns predicted activation-emotions of annoyance. Whereas there was a positive relationship between conflict concern and anxiety in Study 4a, there was no such relationship in Study 4b; hence this unexpected relationship was not reliable. In both Studies 4a and 4b, there was also a negative relationship, such that while conflict concerns predicted increased annoyance, privacy concerns predicted reduced annoyance. In Study 4a, which had both dyads and multi-party groups, an unwanted conversation topic evoked more anxiety in multiparty groups than in dyads. Study 4a asked participants who were with known others (who were lounging in a park) to have a conversation about a randomly-chosen topic from the 10 most-commonly unwanted conversations. We measured concerns with the upcoming conversation and anticipated emotions (participants believed they were about to have the conversation). Study 4b paired participants with strangers over the internet, and asked participants to actually have the unwanted conversation, thus allowing us to measure emotions experienced in the conversation (immediately after it finished). In both studies, despite different manners of having a conversation (live in person, vs. over the internet), different interaction partners (known others vs. strangers), and very different participant populations (tourists hailing from all across the world vs. MTurk participants), we found evidence for both of our predicted effects in both studies. As predicted, the more participants were concerned with privacy with regard to the unwanted conversation, the more they anticipated and experienced inhibiting emotions of anxiety (e.g., anxious, nervous, embarrassed). In contrast, the more participants were concerned with creating a conflict with their conversation partners, the more they anticipated and experienced activating emotions of annoyance (e.g., annoyed, irritated, frustrated). Although not predicted, this distinction was further reinforced by privacy concerns being linked with reduced activating emotions. 6. Study 5: Motivations, emotions, and behavioral responses The Topic Avoidance Process Model that we introduce in the current work proposed motivational contexts for topic avoidance would predict behavioral responses--through distinct emotional reactions to unwanted conversation topics. Specifically, we predicted an inhibiting pathway and an activating pathway based in the two core systems for behavioral regulation (i.e., inhibiting behaviors that could result in negative outcomes, and taking action to bring about positive outcomes or avoid undesired outcomes;). Study 2 revealed that, in response to unwanted conversations, in hibiting emotions of anxiety (anxious, nervous, embarrassed) predicted an inhibited response (staying quiet), whereas activating emotions of annoyance (annoyed, irritated, frustrated) predicted an activating response (taking action by leaving the conversation). Study 3 then revealed with a bottom-up descriptive approach two broad motivations for conversations avoidance, concern for privacy and concern with creating a conflict. Studies 4a and 4b found that these two motivations align with the two proposed pathways, whereby privacy concerns predicted emotions that were inhibiting, and conflict concerns predicted emotions that were activating. Studies 4a and 4b designs did not allow participants to have the option to remain quiet, change the conversation topic or leave the conversation. Therefore, in Study 5, we measured behavioral responses to unwanted conversations to test our full model (Fig. 2). 6.1. Participants and design We recruited 200 participants on MTurk and received 202 responses. We included the same honesty check question as in the prior studies; excluding the two participants who indicated they did not respond with care and accuracy, which yielded a final sample of 200 participants (Mage = 37.43 years, SD = 12.00, range = 20-76, 105 women, 96 men). Study 5 had a similar design to Study 2. As in Study 2, participants completed the Unwanted Conversations Questionnaire, but we limited participants to reflect on only conversations from the past week (rather than the past month as in Study 2). Per each topic that participants answered \\\"Yes, this recently came up in a conversation I was in, and I did not want to talk about it,\\\" participants completed measures per the specific conversation they were in. Per each recent unwanted conversation, participants completed the scale of motivations for topic avoidance (from Studies 4a and 4b), as well as the scale of inhibiting and activating emotions experienced during the conversation (from Studies 2, 4a, and 4b), and behavioral responses to the unwanted conversation (from Study 2). 6.2. Results and discussion 6.2.1. Frequency of topic avoidance From our sample of 200 participants, there was a total of 440 times when an unwanted conversation came up, which translates to an average of 2-3 conversations each participant was in (in the past week) where they sought to avoid talking about a topic introduced by someone else (Fig. 3). 6.2.2. Motivations for topic avoidance predicting emotional reactions We first examined the motivations for topic avoidance as simulta neous predictors of emotional reactions to unwanted conversation topics, implementing the same multilevel modeling approach from the prior studies. We examined the unique relationship of each motivation for topic avoidance with each emotional response to an unwanted conversation topic (see Table 9). As predicted, the more that participants were concerned for their privacy when an unwanted topic came up in conversation, the more they experienced inhibiting emotions of anxiety (e.g., anxious, nervous, embarrassed). In contrast, the more that participants were concerned with creating conflict, the more they experienced activating emotions of annoyance (e.g., annoyed, irritated, frustrated). The concern with privacy also predicted reduced activating emotions. Each of these effects replicated Studies 4a and 4b's results. We also found that when an unwanted conversation topic came up in a dyadic conversation (vs. multi-party conversation), people were significantly more anxious, and independently, marginally more annoyed. 6.2.3. Emotional reactions predicting behavioral responses We next examined whether emotional reactions to unwanted con versations predicted behavioral responses. In pursuit of testing a mediational model, we also entered the two motivations (as is required by a mediational model to isolate the b paths; see Table 10 below). This revealed that independent of the motivations for topic avoid ance, the more inhibiting emotions of anxiety that participants experienced (e.g., uncomfortable, uneasy, awkward) the more likely they were to stay quiet. In contrast, the more activating emotions of annoyance they ex perienced (e.g., annoyed, frustrated, angry), the significantly more likely they were to leave the conversation. Here, neither anxiety nor annoyance predicted changing the subject, whereas changing the subject was predicted by anxiety in Study 2. Across studies, changing the subject thus was not reliably related to one pathway over the other (this might be an issue with the item wording, an issue we return to in the General Discussion). Consistent with Study 2, when participants were in a multi-party (vs. dyadic) conversation, they were more likely to stay quiet. 6.2.4. Mediation analysis Our mediation analysis is unique in that the models are multilevel and the outcome variable is binary. There is no current consensus on how to conduct multilevel mediation analyses nor on how to examine indirect effects when the units of the two paths differ (i.e., the a path here is an unstandardized regression coefficient from a Gaussian model, whereas the b path is a log-likelihood value from a binomial model). A recent paper suggests a formula for calculating an indirect effect that circumvents both of these issues. The logic of the bootstrapped indirect effect test (which multiplies the two path coefficients per some number of empirical bootstrapped simulations of the dataset) is maintained in this method, while also converting the paths into standardized units so that they can be multiplied. The ZMediation statistic divides the a coefficient by its standard error, and the b coefficient by its standard error, and multiples these resulting z-values, yielding the numerator of the equation, which is divided by the pooled standard error (i.e., the square root of the sum of the two squared zvalues and one). The result is the ZMediation statistic, a standardized representation of the strength of the indirect effect, whereby its significance can be tested via a z-test. We calculated the indirect effect for our two postulated pathways with this formula using the coefficients from Tables 9 and 10 (see Table 11). While it might be logical to consider the behavioral outcome as occurring last, and the motivation occurring first, these indirect effects should be understood as based in correlation, rather than as demonstrating a casual process. We found that when an unwanted conversation topic came up in the past week, the more participants were concerned with privacy, the more likely they were to stay quiet, through increased inhibiting emotions of anxiety. In contrast, the more participants were concerned with creating conflict, the more likely they were to leave the conversation, through increased activating emotions of annoyance. The concern with privacy did not predict staying quiet through activating emotions, nor did concern with conflict predict leaving the conversation through inhibiting emotions. These findings provide support for our Topic Avoidance Process Model, whereby in responses to unwanted topics being brought up in conversation, we predicted 1) a set of two pathways, one based in the behavioral inhibition system, and one based in the behavioral activation system, and 2) that motivational contexts for topic avoidance would predict behavioral responses through corresponding emotional reactions (see Table 10). Indeed, participants' emotional reactions to unwanted conversation topics showed a two-factor structure of inhibiting and activating emotions, which aligned both with corresponding motivations for topic avoidance (concerns for privacy and for conflict), and corresponding responses (staying quiet, and leaving the conversation, respectively). 7. Studies 6a and 6b: Unwanted conversations at work, self\\/otherfocus, and feelings of authenticity Conversations can be fraught with challenge and uncertainty. As discussed in the introduction, the workplace is a particularly challenging place when it comes to managing the boundaries of interpersonal relationships. Our final Study 6a (and its exact replication Study 6b) sought to examine topic avoidance in this consequential context. With diverse perspectives and individuals in the workplace, people are likely to encounter individuals with different value systems and beliefs, which could cause concern for being judged (privacy concern) but also for voicing opinions that could upset another (conflict concern). Moving beyond emotional reactions and behavioral reactions to unwanted conversation topics, Studies 6a and 6b examined what participants attend to during these conversations. When do participants attend more to the self, and when do they attend more to their interaction partners? Our prior studies suggest that there are two broad motivations and corresponding pathways for when people seek to avoid a topic that has been introduced in conversation. We predicted that concern with privacy would prompt a participant to attend more to the self during a conversation. In contrast, we predicted that concern with creating a conflict would prompt a participant to attend more to the other person\\/people during conversation. Finally, we predicted that these processes would have relevance to feelings of authenticity. Prior work on authenticity in the workplace has distinguished between knowing oneself and expressing oneself. Drawing from research on self-consciousness, we propose that these two forms of authenticity align with self-focus and other-focus, respectively. The distinction between attending to oneself during a conversation versus attending to others shares conceptual overlap with research on private and public self-consciousness. The more one attends to the self (e.g., what one has said, how one feels), the more one experiences private self-consciousness. Private selfconsciousness is associated with an enhanced awareness of one's selfconcept, with a focus on being aware of who one truly is, and understanding one's actions. We thus predicted that when an unwanted conversation topic evoked privacy concerns, people would attend more to themselves during the conversation, which would, in turn, predict greater feelings of self-awareness (i.e., increasing the feeling that they know who they truly are and what they feel comfortable discussing). In contrast, the more one attends to others (e.g., what others have said, what others might feel), the more one experiences public selfconsciousness. By focusing on how others might feel and maintaining one's relationship with those others, individuals with public self-consciousness adjust their behavior to others accordingly. We thus predicted that when an unwanted conversation topic evoked conflict concerns, people would attend more to their interaction partners during the conversation. We predicted that this focus on responding appropriately to the other person (to maintain the relationship) would predict feeling that one is accepting external influence (the inverse of authentic self-expression;). To test these hypotheses, we adapted a measure previously used to capture self-awareness and acceptance of external influence in the workplace. The Authenticity Inventory from Knoll et al. (2015) found two factors in self-reported authenticity, Authentic Self-Awareness (knowing one's true self) and Authentic Self-Expression (expressing one's true self; i.e., the inverse of accepting external influence). 7.1. Participants and design In both Studies 6a and 6b, we sought to recruit 200 participants on MTurk to participate in the study. For Study 6a, we received 202 responses (Mage = 33.27 years, SD= 9.79, range = 18-65, 107 men, 94 women, and one other). For Study 6b, we received 199 responses (Mage = 35.67 years, SD = 9.04, range = 20 to 66, 104 men, 94 women, and 1 other). We included the same honesty check question as in previous studies. Two Study 6a participants were excluded, and three Study 6b participants were excluded, yielding final samples of 199 and 196 participants, respectively. Both studies had the exact same methods. Study 6b was an exact replication of Study 6a. Participants first completed the Unwanted Conversations Questionnaire as in Studies 2 and 5. Per each topic that was recently brought up (in the past month) that participants did not want to talk about at work, participants indicated the type of conversation they were in (dyadic vs. multi-party), their motivations for avoiding that topic (concern for privacy and concern with creating a conflict)--each from the earlier studies--as well as what they were attending to (the self vs. the other\\/s, described below). Finally, they reported on their feelings of authenticity during the conversation (selfawareness and self-expression, described below). Drawing upon prior work on conversations , per each conversation, participants reported the extent of focus on the self vs. others during the conversation. Participants were asked during the conversation (from 1-not at all to 7-very much), how much were they \\\"thinking about what I said,\\\" \\\"thinking about how I felt,\\\" and \\\"thinking about my true self\\\" to capture a self-focus, and \\\"thinking about what the other person\\/people said,\\\" \\\"thinking about how they felt,\\\" and \\\"thinking about maintaining a relationship with them\\\" to capture a focus on others. We predicted that privacy concerns would predict focusing on one's self during the conversation, whereas conflict concerns would predict focusing on the other person\\/people in the conversation. Also per each conversation that participants did not want to have at work, participants reported their feelings of authenticity in that conversation. Participants answered a six-item authenticity questionnaire. The six-item scale captures two dimensions of authenticity, and as can be seen in Table 12, we found the same factor structure in the present study as in prior work. As in Studies 2 and 5, at the end of the study, participants were asked whether they had read all survey questions and responded carefully, or if they did not respond with care and that their data should be dropped. 7.2. Results and discussion 7.2.1. Frequency of topic avoidance Figs. 4 and 5 present the extent to which participants recently had sought to avoid each conversation topic while at work. From Study 6a's final sample of 199 participants, there was a total of 432 times when an unwanted conversation came up, and from Study 6b's final sample of 196 participants, there was a total of 392 times when an unwanted conversation came up, each of which translates to an average of 2-3 recent unwanted conversations in the workplace (from the 10 topics). 7.2.2. Predicting self vs. other focus from motivations for topic avoidance We implemented the same multilelevl modelling analysis strategy as in the earlier studies. As predicted (see Tables 13 and 14), in both studies, privacy concerns were significantly positively related to a self focus during conversations in the workplace (thinking about \\\"what I said,\\\" \\\"how I felt\\\", and \\\"my true self\\\") whereas conflict concerns were significantly positively related to an other-focus during conversations in the workplace (thinking about \\\"what the other person\\/people said,\\\" \\\"how they felt,\\\" and \\\"maintaining a relationship with them\\\"). 7.2.3. Predicting feelings of authenticity from self and other focus Next, in pursuit of testing a mediational model on feelings of au thenticity, we examined both foci (self and other) as simultaneous predictors of felt authenticity, including the preceding variables (as would be required by a mediational model to isolate the b path), and also including the alternate authenticity scale to isolate the unique relationship with each. As predicted (see Tables 15 and 16), a self-focus during the con versation most closely aligned with the Authentic Self-Awareness felt during workplace conversations, whereas an other-focus during the conversation most closely aligned with the Authentic Self-Expression felt during workplace conversations. Specifically, in both Studies 6a and 6b, although focusing on others was associated with increased Authentic Self-Awareness, focusing on the self most strongly predicted Authentic Self-Awareness (as indicated by nonoverlapping 95% CIs in both cases). A focus on the self was not consistently associated with Authentic Self-Expression, whereas a focus on others in the conversation was consistently associated with reduced Authentic Self-Expression. It is important to note that these two authenticity scales capture notably distinct aspects of authenticity. Self-awareness does not refer to how people express themselves to others, and self-expression does not refer to awareness of one's true self. The most reliable effects were that--independent of how authentic people felt their expressions were--the more they were focusing on themselves in the workplace conversation, the better they felt they understood themselves. In contrast--independent of how well people felt they understood themselves--the more they were focusing on the other person\\/people in the workplace conversation, the less authentic they felt their expressions were. 7.2.4. Mediation analysis Finally, we implemented the same statistical mediation modeling strategy as in Study 5. We hypothesized that privacy concerns would be related to heightened levels of self-awareness, through greater focus on the self. And we predicted that conflict concerns would be related to lower levels of self-expression authenticity (i.e., increased acceptance of external influence), through more focus on others in the conversation. Indeed, these mediation paths were significant (see Tables 17 and 18). Unexpectedly, in both studies we observed a significant pathway from conflict concern to higher levels of self-awareness through an enhanced other-focus. Accordingly, topic avoidance motivated by privacy concerns and conflict concerns were both associated with increased self-awareness, but through different mechanisms. Topic avoidance motivated by privacy concerns predicted an in creased self-awareness through an increased self-focus. Topic avoidance motivated by conflict concerns also predicted an increased self-awareness, yet here through an increased other-focus. Thus, higher levels of self-awareness seem to follow from both a self-focus and an other-focus (see Tables 17 and 18 above). Self-awareness may require paying attention to both the internal and external world. In sum, motivation to avoid an unwanted conversation topic pre dicts increased authentic self-awareness, but through two different pathways. Topic avoidance motivated by privacy concerns predicts a heightened self-focus, and thereby self-awareness. Topic avoidance motivated by conflict concerns predicts a heightened other-focus, which also feeds forward to increased self-awareness. Yet, by one pathway, topic avoidance predicts reduced authentic self-expression. Topic avoidance motivated by conflict concerns was associated with diminished feelings of self-expression through an increased focus on others. 8. General discussion Much of life is filled with social interaction and conversation. Yet with the diversity of situations and places in which these conversations occur along with the diversity of individuals that we encounter, an interaction partner may sometimes bring up a topic of conversation one would rather not talk about. The current work finds that this experience is common, and examined underlying motivations for topic avoidance as well as emotional reactions, behavioral responses, and foci of attention. We presented the Topic Avoidance Process Model, which predicted two distinct motivational pathways in response to an interaction partner introducing a topic of conversation one would rather not talk about. We tested and found support for this model across diverse contexts, including retrospective recall, live conversations, and with studies online and in the field. Additionally, we found that finding oneself in these unwanted conversations has implications for the workplace as well as personal feelings of authenticity. We briefly summarize our studies below, and then discuss theoretical and practical implications of our findings. In the current work, we implemented a novel approach to the study of conversation topic avoidance. First, we administered a large scale (N = 1000) survey in a sample of participants distributed across the U.S., asking per a series of social relationships, the conversation topics they seek to avoid talking about. This enabled us to obtain a set of frequent topics people seek to avoid talking about in their daily life, across a range of contexts, from a large and generalizable sample. Prior work in this domain, in contrast, has only studied specific contexts, such as parent-children relationships in divorced households , or topics avoided in conversation with one's romantic partner. In Study 1, we identified the top ten topics people sought to avoid talking about: politics, money, personal issues\\/ problems, work, religion, family, romantic relationships, sex, the past and friends. When these topics were brought up in a conversation, participants reported experiencing negative emotions such as anxiety and annoyance. In addition, individuals reacted to such unwanted conversation topics by staying quiet, trying to change the topic, or leaving the conversation. In Study 2, we found that the emotions and reactions identified from Study 1 mapped onto two behavioral systems. Within the behavioral inhibition system, participants experienced inhibiting emotions of being uncomfortable, nervous, uneasy, awkward, embarrassed and anxious. Through such inhibiting emotions, participants were more likely to stay quiet during the conversation. On the other hand, within the behavioral activation system, participants experienced activating emotions of being angry, irritated, annoyed and frustrated. Through such activating emotions, participants were more likely to take action and leave the conversation. In Study 3, by employing a machine learning algorithm, we iden tified two broad motivations that underlie why people seek to avoid unwanted conversation topics, centering on concern for privacy (not wanting people to judge oneself), and concern for creating a conflict (not wanting to have an argument). Studies 4a, 4b, and 5 found that these motivations align with the inhibiting and activating pathways predicted by our Topic Avoidance Process Model. Privacy concerns evoked by unwanted conversation topics predicted staying quiet in the conversation through inhibiting emotions, whereas conflict concerns predicted leaving the conversation through activating emotions. Finally, Studies 6a and 6b found these pathways have implications for personal feelings of authenticity in the workplace as a function of what people attend to during workplace conversations. The inhibiting privacy pathway was associated with focusing on one's self during a conversation in the workplace, which was thereby associated with feeling that one has a good sense of who they are. In other words, while seeking to uphold privacy boundaries evokes inhibiting processes, a corresponding enhanced self-focus was not felt as less authentic. For example, if one is the kind of person to not discuss sex or religion at work, then to stay quiet during such conversation is felt as authentic, as characterized by being aware of who one's self is (and what one feels comfortable discussing). In contrast, the activating conflict pathway was associated with focusing on the other person\\/people in the conversation. A focus on others (during a workplace conversation) was also associated with increased self-awareness. Perhaps if one is the kind of person to not want to create a conflict, then to attend to others can also feel authentic (in the sense of being aware of who one's self is, and one's values). Yet a focus on others also corresponded with accepting those others' external influence (i.e., feeling that one is not authentically expressing oneself). Independent of how aware one is of their true self (i.e., self-awareness), an enhanced focus on others that follows from concern with creating conflict seems to feel inauthentic (presumably as a function of not expressing one's true opinions). 8.1. Theoretical implications The current research advances the understanding of the psychology of conversations. Conversation is a joint action  and a coordinating process. Moreover, the content of dialogue serves a functional purpose; it allows people to understand each other. There are also social benefits to conversation. For instance, asking questions during conversation leads one to be more liked by an interaction partner. Less research, however, has examined the disruptive components of conversations. While asking questions increases liking, asking questions may also elicit topics that conversation partners would prefer not to talk about. The current research adds to the literature on conversation by investigating psychological processes that arise when an unwanted conversation topic is introduced into conversation. Whereas prior work has examined participants as the arbiter of what topics are introduced into a conversation , we examined how individuals react to unwanted conversation topics that were brought up by another party. Through a bottom-up process, we uncovered two broad motivations and emotional reactions. Additionally, whereas past literature has examined topic avoidance in close relationships and family relationships , we examined topic avoidance across all relationship types (Studies 1-5) as well as specifically in the workplace (Studies 6a and 6b). A recent review highlights that as the number of individuals in volved in a conversation increases, so does the complexity of the conversation. The difference between a dyadic conversation and a multi-party one is more than an increase in numbers. In a multi-party conversation, as compared to a dyadic conversation, each participant will have less airtime, each will have a harder time taking turns, and each will have greater difficulty in providing feedback to the person speaking (for the reason that the conversation will not work if multiple people speak at once). As a function of each individual having less air time, people should be more likely to stay quiet (in response to an unwanted topic) if they are in a multi-party conversation, as compared to a dyadic conversation. And given that a conversation which is composed of more than two parties can survive an individual exiting it, whereas a conversation of only two parties cannot, it should be easier to exit a multi-party conversation, relative to a dyadic one. Indeed, the current work provides support for these predictions. The present results are among the first to compare dyadic to multi-party conversation within the same empirical setting. Comparing dyadic to multi-party conversation is an area ripe for future research. Along these lines, with more parties present in a conversation, the risk of any individual disclosure may be greater. A potential strategy for seeking to avoid an unwanted conversation topic (when introduced by an interaction partner) may be to speak to the topic, but without revealing anything personal. Perhaps the depth of disclosure thus varies with conversation group size. Relatedly, while a growing body of work documents the ways in which secrecy impacts well-being (; Slepian, Greenaway, & Masicampo, in press), the different ways in which people keep and maintain their secrets is understudied. Perhaps topic avoidance is one way in which people keep secrets. Additionally, certain personality traits of interaction partners may discourage disclosure. Thus, how different individuals or situations promote topic avoidance is another area for future research. In addition to providing the first picture of what happens when an interaction partner brings up a conversation topic one seeks to avoid, the current work offers novel insights into experiences of felt authenticity. We add to a growing body of work  that recognizes that feelings of authenticity not only vary by person (trait authenticity), but also by context and situation (state authenticity). For instance, we examined how personal feelings of authenticity vary by conversation. Given the central role of social interaction in daily life, it makes a good deal of sense that our conversations with others would evoke varying levels of felt authenticity. That is, we may not always feel that we present our true selves across every conversation. We suggest that examining conversations people seek to avoid will shed greater insight into feelings of state authenticity. There are many aspects about oneself one may wish to not disclose. People believe that omitting information is better than lying. Yet non-disclosure can produce negative outcomes of its own. Concealment can be costly in social interactions. Likewise, dodging a direct question can lead to less trust and liking towards the speaker once detected. The usage of so-called paltering could be seen with a benign intention, but also can be perceived as dishonest. Accordingly, deflecting a question by asking another direct question yields better outcomes than other deceptive methods. But still, it is nonnormative to respond to an inquiry with something like \\\"I would rather not talk about that\\\" . The current work suggests that how someone handles an unwanted topic or attempts to skirt around it  will have important implications for authenticity. 8.2. Practical implications The current research suggests practical implications for employees and managers. First, when an unwanted conversation topic arises in the workplace, focusing on one's self may provide a compass for how to successfully navigate that interaction. Focusing on the self was associated with heightened feelings of self-awareness. Recognizing \\\"I am not the kind of person who feels comfortable talking about X at work\\\" may offset some of the discomforts of the behavioral strategies deployed to avoid an unwanted topic in conversation. A recent model of authenticity suggests that certain contexts are more likely to evoke feelings of reduced authenticity (e.g., when someone feels like they do not fit in;). Managers should thus be sensitive to contexts in which employees might not feel comfortable expressing themselves in the workplace, and how a simple conversation could cause discomfort in such contexts. Future work could explore how different relationship types or differences in status  impact the processes we explored in the current work. Interestingly, we found that within the context of unwanted con versations at work, participants reported higher levels of authentic selfawareness than authentic self-expression. We found that focusing on the self and others were both associated with higher feelings of authentic self-awareness, whereas only focusing on others was associated with (reduced) feelings of authentic self-expression. Integrating research on conversation with that on authenticity we believe will be particularly fruitful in better understanding when people feel they authentically know themselves, and when they feel they are authentically expressing themselves. Indeed, organizations can surely benefit from a better understanding of what fosters feelings of authenticity in the workplace. When employees feel like they can be themselves at work, they have higher well-being. An interesting future direction would examine what happens after people have an unwanted conversation. Perhaps the conversation is not as uncomfortable as individuals anticipate. Moreover, perhaps difficult conversations--under certain conditions--could make individuals feel closer to each other , better understand each other , and improve outcomes for both parties. In seeking to improve communication in the workplace, future work could examine whether people recognize when an interaction partner is uncomfortable with a topic that has been introduced into the conversation, and how they respond in turn. 8.3. Future research directions The present work identified potential strategies for avoiding unwanted conversation topics. But the utility of these strategies and their psychological implications await future research. One strategy, \\\"change the subject,\\\" was not clearly linked to either activating or inhibiting systems. Future work might choose to create two versions of this item, one that could be considered approach-oriented (e.g., explicit calls to change the subject), and the other, avoidance-oriented (e.g., more subtle shifts to redirect the conversation). Future research should more closely explore how people seek to shift the topic of conversation, and how successful these attempts are. Future research could also examine direct rejections of the conversation topic. For example, when one's conversation partner is of lower status, frank disapprovals may follow from them introducing an unwanted conversation topic. More generally, future work could gain better temporal resolution into how the unwanted conversation unfolds (or dissolves). Context is likely to moderate the present results. For example, in creased closeness with one's interaction partner could magnify the effects we find (the stakes are higher), or perhaps mitigate them (we are more at ease). Beyond social closeness, other relevant variables that will likely moderate the present results include where the conversation is taking place or its medium (e.g., in-person, over the phone, instant message). Finally, while we believe that there is more to gain in first understanding processes that generalize across diverse conversation topics, future work could also compare the conversation topics to one another. Last but not least, the unwanted topics identified in this paper are also categories that people often do talk about. Figs. 1, 3, 4, & 5 demonstrate that across the top 10 most common unwanted conversation topics that come up in people's daily life, people also frequently do not mind talking about them. Of course, there are instances in which people enjoy talking about these topics too. Aside from extremes of threat and enjoyment, people's preferences and non-preferences for discussing these topics will be multiply determined. One may seek to avoid a topic in conversation to keep it light and enjoyable. Or, one may want to discuss a certain topic, but at a different time. Future work should explore these and other motivations for wanting to avoid a topic of conversation introduced by one's conversation partner, and how these motivations differentially shape downstream processes and outcomes. In sum, across a diverse set of topics, relationships, and forms of conversation, we found two reliable pathways for topic avoidance (paralleling the two core systems for behavioral regulation). With this first step, future work could begin to compare topic avoidance across different contexts, topics, and relationships, and we believe that future work would benefit from the new methods and instruments introduced in the current work (e.g., the Unwanted Conversations Questionnaire, motivations for topic avoidance, and emotional reactions to unwanted conversations). Social life is rich in interactions, relationships, and different topics for discussion. Inevitably, conversations will veer into territories we wish they would not, and we hope that future work further explores this pervasive phenomenon.\",\"1135747511\":\"We understand our daily lives as involving series of events structured in a meaningful way. Spending a weekend alone except for one long evening out with friends feels different from spending a weekend meeting up 'in small doses' with different people for shorter interactions, including a lunch, a coffee, or a beer. We can imagine these patterns of socializing mapping on to different dimensions of personality. The long, extended period of social behaviour might be preferred by someone who is more extraverted. The busier, shorter bursts of activity might be preferred by someone who is higher in negative emotionality and therefore gets anxious or self-conscious easily. Ambulatory assessment methods make it possible to observe these differences in patterns of socializing, but they are commonly analysed using the average of a behaviour across a designated period of time--a measure that fails to preserve information about the way the behaviour is patterned across time. If we were to characterize our two different weekends using averages, we might conclude that they were very similar: the average amount of time spent socializing is the same whether it all occurs at once or is spread across many interactions. Most analyses of ambulatory assessment data are therefore missing something important: information about the patterning of events in time. Dynamic systems theories and research provide tools for both conceptualizing and quantifying patterns over time. In addition to how often an event occurs, such as a social interaction, dynamic systems theorists would want to consider how long the event lasts and whether there appears to be any consistent patterns in the typical length and frequency of the event. Systems changing over time can be subject to feedback loops, where a local process repeated over and over can quickly lead to non-linear change in the system's state. For example, a person who begins feeling slightly anxious during a social interaction might have a slightly worse interaction--which would then increase anxiety during the next interaction, in turn leading it to go worse. Although the local change in social anxiety in response to interaction quality might be thought of as linear, repeating that process over time can quickly lead a person's anxiety to ratchet up in a non-linear trajectory. In this way, by moving beyond linear associations, the application of dynamic systems modelling has the potential to advance our understanding of the links between daily behaviour and personality. There is no previous literature establishing measures of patterning over time with data from this ambulatory assessment method (the Electronically Activated Recorder or EAR), nor are there standard measures of patterning of socializing over time in the current mobile sensing literature. Some new mobile sensing research includes variable related to patterning over time in the context of a broad, exploratory method that employed over 15 000 different predictors, such as standard deviation of variables related to smartphone use (e.g. SD of call length), but these metrics do not include the non-linear time series metrics described here (Stachl et al., in press). Complementary to this ad hoc feature extraction approach, we opted to take an established analysis technique from the dynamic systems literature that yields a coherent set of indices of patterning over time. Importing established methods from dynamic systems theory provides a readymade and well-validated feature toolbox for understanding ambulatory assessment data that captures patterns in people's social lives over time. This manuscript examines patterns of social behaviour over time, operationalized as whether an individual is talking to someone else, and whether and how personality traits are associated with these patterns. We refer to measurements of when an individual is engaged in a conversation (i.e. talking to someone else) as socializing throughout this manuscript; our use of the term 'socializing' thus refers specifically to 'being in a state of interacting with others' as opposed to uses that imply participating in large social gatherings or uses that define socializing as behaving in a way that is socially acceptable. The patterns of talking with others throughout a person's daily life are therefore referred to as the dynamics of socializing. Socializing is thought to serve a basic need to belong , and needs are commonly conceptualized as fluctuating in strength over time, based on how they have been satisfied. Social behaviour can therefore be conceptualized as a system where the prior state--whether or not the person was socializing or felt that social needs were being met--influences the next state. As people spend more time socializing, they may become more 'saturated' by it and, eventually, may want to pursue another goal. When they have gone for a long period without socializing, they may desire it more and spend more time doing it. These self-regulatory processes suggest that the dynamic systems framework is appropriate for understanding socializing. THE ELECTRONICALLYACTIVATED RECORDER The EAR allows for the unobtrusive observation of daily social life in its naturalistic context, giving an objective (in the sense of observational) sense of personality in situ (; Tackman et al.,in press). Participants in EAR studies typically wear a dedicated mobile device with the EAR app for several days, and it samples whatever ambient sounds are in the participant's environment at intermittent intervals (e.g. 30 seconds every 12 minutes during waking hours). Target behaviours are coded, traditionally by having multiple research assistants listen to each file, from the ambient sound recordings made throughout the day. Electronically Activated Recorder coding typically yields a series of binary values for the presence or absence of a behaviour in a given sound file that can be detected by listening, such as 'talking to another person'. Results of this coding is typically analysed as a proportion: how many of a participant's recordings included the code (e.g. 'talking to another person') out of the total number of valid (i.e. deemed adherent to wearing the EAR) and waking (i.e. deemed not sleeping) recordings made. This approach has yielded valuable insights, such as that people report more life satisfaction when a greater proportion of their daily life is spent socializing. However, this method of analysis removes any information about the timing and patterning of participants' daily socializing from consideration. Here, we propose a method from dynamic systems research to quantify and analyse some of this information about timing. RECURRENCE QUANTIFICATION ANALYSIS Recurrence quantification analysis (RQA) is a tool for analysing time series data that is meant to capture the stability, flexibility, and complexity of dynamic systems. RQA can be used on categorical data, with a relatively short time series, and does not make any assumptions about the underlying distribution of the data. This makes it well suited to analyse behaviourally coded EAR data, which are typically binary in nature (e.g. 'Was the target behaviour present in a given sound file or not?', although coding is not always binary;), which have highly skewed distributions and which involve relatively short time series (e.g. 80-120 data points or sound files per day;+ recommended for fractal analysis, for example). Recurrence quantification analysis and extensions of the technique are increasingly being used to examine psychological processes. These analyses have, for example, quantified the patterning of saccades when looking at different parts of a screen, instead of just examining average length of time looking at a particular region. RQA is used to capture the patterning of word usage in persuasive essays, and analyses found that RQA parameters could account for 43% of the variance in scores made by expert human raters using the SAT grading rubric. Multivariate extensions of RQA examine synchrony in two physiological processes in a group of people , turn-taking in conversations  and mother-child emotional interactions. Despite the growing application of RQA to characterize the dynamics of behaviour in lab tasks, few studies use this method with ambulatory assessment data collected in naturalistic settings. Applying RQA to observational, real-world data collected throughout the day, such as with the EAR, can open new domains of application for RQA and broaden the analytic toolbox of ambulatory assessment researchers, particularly with non-normally distributed, non-continuous data such as behaviour counts. Because several useful tutorials for using RQA are available, we focus here on illustrating and understanding its use in a novel context: binary coded EAR data time series. Recurrence quantification analysis is a technique built around recurrence plots. A recurrence plot is constructed by considering the same time series laid out along both the x and y axes of a graph. Each coordinate on the graph corresponds to two points in the same time series--one represented along the x-axis, another along the y-axis. For each coordinate on the graph, if the values of the time series are the same (i.e. the same behaviour pattern occurred; the system is in the same state again), then a point is placed; if the values are different (i.e. a different behaviour pattern occurred; the system is in a different state), then the coordinate is left empty. All the positions on the graph are filled using this rule. For example, consider a time series that measures 100 equally spaced time points during a person's day. The time series contains two different values: 'was talking to someone' (coded as a 1) and 'was not talking to someone' (coded as a 0). The coordinate (6, 8) corresponds to the sixth and eighth values of the time series. If they are the same, for example, in both measures the participant was talking to someone, then a point would be placed. If not, no point would be placed. This process is repeated for every point on the graph. Note that the coordinate (8, 6) would necessarily have the same value, because the same time series is being examined along each axis. RQA plots are therefore symmetric about the line x = y. Additionally, because the time series is necessarily the same at lag 0 (e.g. points 1, 1; 2, 2; 3, 3 are always the same), RQA plots have a connected set of points along the line x = y. This approach works well in cases with multiple categorical responses when, for example, researchers divide a screen into six distinct regions in an eye tracking study and considered a gaze fixation on the same region as an instance of recurrence. In the case of examining the binary code of socializing (i.e. talking with someone as determined by listening to an EAR sound file), however, there will be extended periods of time when an individual is in the 'not talking with someone' category that tend to fill the recurrence plot. This can obscure dynamics related specifically to recurrence of socialization, while inflating recurrence based on what may be heterogeneous categories ('not talking with someone' encompasses multiple behaviours, not all of which we want to consider instances of being in the same state). In cases like this, the original time series may be recoded so that only instances where the state 'talking with someone' recurs are plotted, not cases when the state 'not talking with someone' recurs (see footnote 3 in Coco & Dale, 2014). To better understand the technique, consider the plots in Figure 1. In Figure 1, the raw time series is plotted on the left. This participant was talking to someone in nine consecutive sound files, as indicated by the values plotted 1. One the right side of Figure 1, the time series is converted to an RQA plot. Only instances where talking matches talking were plotted. We can see from this figure that the extended period of conversation creates a square set of points. Along the central diagonal of the square are the exact contemporaneous relationships. Any time there is talking in a plot, there will be a point along the central diagonal. Off the central diagonal are lagged relationships. Moving one step away from the central diagonal is the lag 1 relationship of talking to itself. The point (at 4, 5) indicates that talking at point 4 was associated with talking at point 5. The average value of this diagonal line across the whole plot can be thought of as the lag 1 correlation of the time series with itself (the autocorrelation). Moving two steps away from the central diagonal is the lag 2 relationship of talking to itself; this holds for all the diagonals in the plot. Ultimately, the recurrence plot represents every possible autocorrelation graphically along the diagonals. Figure 2 illustrates a time series with just two isolated instances of socializing converted to an RQA plot. The RQA plot contains only four points: two along the main diagonal, where the points match with themselves, and two off the diagonal where the points match with each other. In this case, there are no extended diagonal lines, because there were no extended periods of socializing. Diagonal lines are used in the calculation of certain quantitative features from recurrence plots. We should note from this plot that people who have no periods of extended socializing will have no diagonal lines, and so, they will have zero values on some of the RQAmetrics introduced in the following discussion. Figure 3 helps to further strengthen our intuitions about RQA plots. In this time series, there are two cases where the individual was socializing in two consecutive time points. This creates four small square structures. Two are created close to the main diagonal, as the two bouts of socializing match with themselves. Two others are created far off the main diagonal as the brief socializing bouts match with each other. Multiple extended bouts of socializing will create multiple 'squares' with diagonal lines in them. The figures given are essentially qualitative representations of the temporal patterns in the time series they represent. To conduct formal analyses, the recurrence plot is quantified. For example, the proportion of filled points on the plot is called the per cent recurrence or recurrence rate (RR) and can be used as a global index of repetitiveness of a sequence. If there are 100 places where a point could be placed, and points are placed in 10 of them, this would yield a recurrence rate of 0.1 (or 10%). Several different ways of quantifying recurrence plots have been proposed and used in previous research. Many of these focus on diagonal lines in the recurrence plot. Diagonal lines are formed when the system remains in the same state for more than one time point. These extended periods of time in the same state are sometimes referred to as deterministic structures because they suggest that knowing one value in the sequence gives information about others. The proportion of recurrence points in diagonal lines is referred to as 'per cent determinism' (DET). If there were 10 recurrence points and four of them were in diagonal line structures, DET would be 0.4. This measure is associated with how many repetitions are part of connected trajectories; structured signals such as a sine wave have high DET scores, while randomly generated numbers will have very low DET scores. In the context of coded instances of socializing, DET indicates the proportion of recurrence points associated with consecutive measures of talking to others. This indicates an instance where a conversation extended long enough to be captured in more than one sound file, or when the break in socializing between one conversation and the next was smaller than the sampling rate. If we were sampling a person's day every 12 minutes, diagonal lines could occur when an individual has a conversation that lasts more than 12 minutes (e.g. the recording catches the beginning of the conversation, it continues for 12 minutes, and the recording catches more of it at the next measurement occasion). It could also occur if the individual is talking to someone, stops within the 12 minutes between recordings, but then starts talking to someone again by the next measurement occasion. In this case, the deterministic structure is not a single extended conversation but can be conceptualized as an extended period of 'being on' socially --meaning interacting frequently without much down time between interactions. The DET statistic is therefore indicative of the proportion of socializing that takes place in 'extended bouts'. Note also from examining Figure 3 that having two extended bouts of socializing creates four square structures (or sets of structured diagonal lines; see next paragraph). Complementing the DET statistic is a count of the number of diagonal lines in the plot (NumL). Because DET is a ratio, high values can be obtained when a person socializes frequently and much of that socializing is spent in extended bouts, or when a person socializes infrequently but the small amount of time spent with others is typically spent in extended bouts. The NumL statistic simply counts up the number of times a person has extended bouts of socializing, without comparing it to overall rates of socializing. The average length of a diagonal line is used to represent the typical length of an extended bout of socializing. The length of the single longest diagonal line is captured by the maximum line (MaxL) statistic. In contexts where RQA has been applied to continuous data, MaxL has been found to correspond to the strength of the strongest attractor in the system--roughly indicating how likely the system is to stay in the pattern it 'most prefers' once it reaches it. In our context, it simply corresponds to the length of the longest extended bout of socializing. This might be of particular theoretical interest if a researcher believes that one long period of socializing can be particularly draining (e.g. if it is a challenge for introverts to stay in a 'social state' for a long period of time) or fulfilling (e.g. indicating a meaningful conversation) to an individual. Variability in the distribution of diagonal line lengths is measured through entropy (ENTR). Entropy refers to Shannon information entropy , calculated using all diagonal line lengths distributed over integer bins in a histogram. It is taken to measure signal complexity, such that more complex systems (such as a 'chaotic attractor') will have higher scores than more regular systems (such a sine wave). In our context, entropy captures the 'complexity' of socializing by giving an index of how variable bouts of socializing are for this person. Individuals whose extended social interactions tend to be of a more uniform length (e.g. typically lasting 3 points or ~36 minutes) will have lower ENTR scores, while those with interactions of many different lengths (e.g. a wider mixture of interactions lasting 2, 3, 4, and 5 points--or ~24, 36, 48, or 60 minutes) will have higher ENTR scores. Other metrics are available for quantification of recurrence plots, such as trapping time and laminarity, but these have less obvious interpretations in the context of a binary categorical value such as socializing. We therefore restrict ourselves to examining the five metrics described earlier in this context. They are summarized in Table 1. Consider two further examples, now that we have discussed the RQA metrics derived from recurrence plots. Figure 4 can help demonstrate that extended bouts of socializing are prioritized in recurrence plots. Although this person was socializing on seven distinct occasions, the only set of diagonal lines created by the time series are those associated with the period of four consecutive time points when socializing occurred. While the recurrence rate for this person would pick up on the multiple brief periods of socialization, these would not be included when calculating other values such as number of lines, max line, average line length, or entropy. RQA therefore primarily captures information about the patterning of extended socializing bouts. Figure 5 demonstrates the way that extended bouts of socializing lead to the creation of several square structures and many diagonal lines. In this person's time series, the variety of lengths for the extended bouts of socializing will be captured in many of the RQA metrics. This suggests that RQA will be most effective in characterizing differences between time series with multiple extended bouts of socializing, such as the differences between the recurrence plots in Figures 3 and 5. Said differently, aspects of the temporal patterning of single conversational bouts (i.e. that last only one sound file) are not well represented in the proposed RQA metrics. Researchers typically derive RQA metrics from a time series and then use them in traditional statistical analyses. For example, a researcher interested in how well personality predicted repetitiveness of socializing might construct recurrence plots of social behaviour for all participants in her study, calculate RR for each plot, and then use traditional regression analysis to predict RR from a personality measure. Our goal in this manuscript is to demonstrate how RQA metrics are calculated and to demonstrate how they are related to personality. Empirical example Thus far, we have explained how RQA is calculated and what it means in the context of daily social behaviour. Now, we apply this approach to the study of how Big Five personality traits are associated with dynamic features of socializing captured by RQA. Previous analyses of personality and EAR data find that extraversion is associated with socializing; as expected, people higher in extraversion spend more time socializing (; Tackman et al., under review). However, other aspects of personality might influence the ways in which people socialize. For example, people higher in agreeableness might have more extended bouts of socializing because they encourage people to continue engaging with them. People higher in neuroticism, on the other hand, might feel discomfort in social situations and therefore be more likely to cut them short. Additionally, although extraversion is associated with proportion of time spent socializing, it might be even more closely associated with measures of extended socializing, such as having social bouts of many different lengths. This could indicate a willingness to engage in both longer, planned and spontaneous, unplanned socializing. One use of RQA is to quantify these subtle, dynamic aspects of socializing so that we can identify how personality is related to temporal regularities in or 'styles' of socializing. We predict personality from RQA parameters to demonstrate that this technique captures aspects of socializing that are related to personality above-and-beyond what would be predict using the traditional static (i.e. average) measure. If we can demonstrate that personality is predicted by an RQA parameter as well as or better than by the average amount of socializing, this would suggest that these dynamic aspects of socializing are at least as closely related to personality as amount of time spent talking is. We use two analysis strategies: (i) correlations and (ii) predictive algorithms. In both cases, our analyses should be considered exploratory. We present correlations of each of the Big Five traits with the RQA parameters to estimate their independent associations, as well as partial correlations of RQA parameters with personality adjusted for the association between average amount of talking and the trait. Partial correlations illustrate the incremental validity of using RQA parameters to predict personality above-and-beyond average amount of socializing in an analysis framework familiar to most personality psychologists. The second approach we employed was to develop a predictive algorithm in a machine learning framework. Machine learning models are typically evaluated based on their ability to accurately predict new cases, not through significance tests of specific regression coefficients. Instead of calculating the optimal set of regression coefficients for a sample, the machine learning framework emphasizes the idea of a regression as a formula for predicting new data. The formula is developed on one subset of the data, and the coefficients are used to predict an outcome in new data, typically a subset that was held aside. In penalized regression, the type of machine learning algorithm we employed, different values of the regression coefficients are 'tried out' in the training data to find the best ones, which are then used to predict the held aside (test) data. This is performed via cross-validation. When a predictor that does not have a true relationship with the outcome is added to the model, it is common for the model to either have no increase in accuracy (because the effect of the 'bad' predictor is often estimated to be zero) or actively reduce the accuracy of the model. Added predictors can reduce the accuracy of a machine learning model because their success is judged based on performance in new data. In some cases, there appears to be a relationship between a predictor and the outcome in the training data, but that relationship does not hold in the test data. In these cases, idiosyncrasies of the training data were added to the model, and that hurt generalized predictive ability. We call this overfitting. Contemporary replicability concerns have been conceptualized as resulting from a field-wide tendency to over-fit models , so that results hold only in one particular sample--but not for people in general. Cross-validated results are a more stringent test of the reliability of our analysis and potentially indicate that the results are more likely to replicate. Because predictors with no true relationship with the outcome tend to either have no effect or reduce predictive accuracy in new data, whether a predictor adds value to a machine learning model can be assessed by fitting a model that has all predictors but the focal one and comparing it with a model that has all predictors including the focal one. If the model with the focal predictor can predict new data better, this is an evidence that the predictor adds value above-and-beyond the others. See our Supporting information for more details on machine learning and cross-validation. We make use of an archival EAR data set that was collected in the context of a larger study on how individuals cope with divorce. This data set is useful for analysing socializing dynamics because people going through a separation from their partner are likely to have disrupted and highly variable social lives. RQA should be able to characterize unusual patterning that may be going on in these 'lives in transition'. Although this study involves multiple waves, we only analyse the first wave here. METHOD Participants Older adults separated from their marital partners were recruited from the Tucson, Arizona, area. There were 140 participants recruited, but the final sample included only the 93 participants for whom valid EAR data were recorded without any missing values in the time series (e.g. one missing file in a day). The average age was 43 years old (SD = 10.7); 69.9% were women and 30.1% were men; and 62.5% were White, 21.3% were Hispanic or Latino, 5.1% were Black or African American, 2.2% were Asian, and 2.2% were Native American or Alaskan Natives. For more details on the sample, please see Bourassa et al. (2019), Hasselmo et al. (2018), and O'Hara, Grinberg, Tackman, Mehl, and Sbarra (in press). Procedures Participants first completed a series of questionnaires, then wore the EAR over the course of a weekend (Friday evening to Monday morning). The average number of valid sound files per participant was 176 files (SD = 60). The EAR recorded periodically for 30 seconds every 12 minutes with a blackout period overnight. This study involved follow-up assessments, but only the data from Wave 1 were included here because this was the wave closest to the personality measure. Measures Participants completed all items on the Big Five inventory at the study baseline. Reliability for all scales was good: extraversion (a = .85), agreeableness (a = .77), conscientiousness (a = .79), neuroticism (a = .80), and openness (a = .78). RQA parameters were calculated as described previously for each participant day separately. Then RQA parameters for the three days measured for each participant were averaged to give a single score for that participant. All cases where a sound file was missing from the time series were not analysed, as missing data have the potential to alter dynamic analyses like these in ways that conventional missing data techniques have not been tested on. Note that the average amount of time spent socializing is calculated by dividing number of sound files in which the person is talking by the total number of (valid and waking) sound files available for that person. At a conceptual level, recurrence rate is a transformation of this value, but in our data set, it is not a perfect transformation. This is because recurrence rate is calculated separately for each of the three days the individual was being recorded and then averaged. Because there was no averaging performed across days for overall time spent socializing, the variables are not perfectly correlated. RESULTS Intercorrelation of recurrence quantification analysis parameters The correlations of the RQA parameters with the average amount of time spent talking, and with each other, are presented in the upper diagonal of Figure 6. Results indicate that all of the measures have a moderate correlation with the average amount of talking (r = .47 to r = .69). However, several of the RQA parameters are highly correlated with each other. Entropy and average line length are correlated at r = .89, average line length and maximum line length at r = .85, and entropy and % determinism at r = .82. These high correlations suggest that these variables should not be included in a simultaneous regression, due to variance inflation. On the other hand, several RQA parameters are correlated at low levels, suggesting there is less overlap between them. For example, entropy and number of lines are correlated at r = .42, % determinism and the number of lines are correlated at r = .44, and average number of lines and maximum number of lines are correlated at r = .47. When creating models with multiple RQA parameters (see in the next discussion), we elected to use RR, DET, NumL, and MaxL to exclude predictors with the highest intercorrelations. Note that recurrence rate and average amount of time spent talking are not perfect transformations because, for the purposes of calculating RQA parameters, each day of data was treated as a separate time series and the parameters from the three time series were averaged. Correlations with Big Five measures The raw and partial correlations of each Big Five trait with each of the measures of socializing dynamics are given in Table 2. Partial correlations are the associations of each RQA variable with personality, adjusted for the association of average amount of time spent talking. These partial correlations address the association of RQA parameters with personality 'above-and-beyond' average amount of time spent talking. Note that, more often than not, adjusting for average amount of time spent talking increased the associations of RQA parameters with personality. Extraversion was associated with average amount of time spent talking, as expected from prior research. Without adjusting for this association, extraversion was also associated with two measures capturing extended socializing bouts: per cent determinism and number of lines. These correlations indicate that people higher in extraversion spend a greater proportion of their social lives in extended socializing bouts and that they have more of these bouts overall. Prior research has found an association between socializing and extraversion (; Tackman et al.,in press, but this study suggests that extraversion is also related to extended periods of socialization. Additionally, extraversion was associated with entropy, which indicates how regular and patterned extended bouts of socializing are. High entropy scores mean more 'disordered' distributions of line lengths, so this indicates that people who are more extraverted have more variable bouts of socializing. This indicates that extraverted people have less regularity in their social interaction schedules. Note that these associations became non-significant after adjusting for average amount of time spent talking, but the magnitude of the partial correlation coefficients are all in between small and moderate effect sizes for the personality literature (|.10| < r < |.20|;). After adjusting for average amount of time spent talking, many of the associations between RQA parameters and conscientiousness increased in magnitude, moving from non-significant to significant associations. Overall, the partial associations indicate that people who are more conscientious tend to have longer social interactions. This can be seen in the association with MaxL, which indicates that more conscientious individuals tend to have their longest extended interaction bout last longer, and in the association with L, which indicates that more conscientious individuals tend to have their extended interactions last longer, on average. Further, conscientious individuals tend to have more variable conversation lengths (higher entropy), after adjusting for overall amount of socializing. Neuroticism had several associations with RQA parameters that increased in magnitude once we adjusted for average amount of time spent talking. Recurrence rate is a transformation of average amount of time spent socializing that emphasizes differences at the high end of socializing. Neuroticism had a significant association with this parameter but not with average amount of time spent talking--in fact, the association increased in size when adjusting for average amount of time spent talking. This suggests that being neurotic primarily reduces amount of time spent talking at the high end: people highest in neuroticism tend to spend less time talking to others. People with low to medium levels of neuroticism do not spend more or less time talking to others. People higher in neuroticism also tended to have fewer extended socializing bouts. This can be seen in the associations between neuroticism and determinism, which indicates that neurotic people spend a smaller proportion of their time in extended interactions; neuroticism and number of lines, which indicates that neurotic people have fewer overall extended interaction bouts; and neuroticism and maximum line length, which indicates that the longest interaction a neurotic person has tends to be shorter. Further, neurotic individuals tended to have less variability in their interaction lengths. There were no significant zero-order or partial correlations of RQA variables with agreeableness or openness. Predictive models We estimated the accuracy of models including RQA parameters to predict Big Five personality traits, above-and-beyond our static (average) measure of socializing. For each trait, we estimated models using each RQA variable and average amount of talking (two predictor models) and a model including four of the RQA parameters (RR, %DET, NumL, and MaxL) in addition to the average amount of talking (five-predictor model). These were elastic net models using nested, 10-fold cross-validation. Elastic net models are a form of penalized regression; as noted earlier, they allow for the regression coefficients to be 'tuned' to improve predictive accuracy before being tested in new, held-out (or test) data. More information on elastic net is available in our Supporting information. We compared these to the accuracy obtained using a single predictor regression with average amount of time spent talking. In Figure 7, we plot the root mean squared error (RMSE) across the 10 outof-sample folds for each model. Lower RMSE values indicate less error, and the dotted line in each panel represents the RMSE for the single predictor regression model. Error of the models varied based on which data were included in each fold. Note that (as mentioned earlier), when estimating accuracy using cross-validation, adding variables that have no real association (e.g. overfitting the model) leads to reduced accuracy. This can be seen in the model results for agreeableness in Figure 7. Even though all of the models being tested include both average amount of time spent talking and another variable (the different RQA parameters) as predictors, they have lower predictive accuracy than models with just average amount of time spent talking. Unlike the R2 value for a traditional multiple regression model, the accuracy for a cross-validated machine learning model has a penalty for fitting noise built in. In that sense, it is more comparable with an adjusted R2 value, which penalizes complexity--and therefore makes models with extra terms that do not add predictive accuracy look worse. Results indicate that including information about socializing dynamics improves the ability to predict extraversion, conscientiousness, neuroticism, and openness, but not agreeableness. The five-predictor model, which included multiple RQA parameters, was most accurate at predicting conscientiousness (median RMSE = 0.88) and neuroticism (median RMSE = 0.84), but not extraversion (median RMSE = 0.90) and openness (median RMSE = 0.88). The best model for predicting extraversion in sample included just average amount of time spent talking and DET (median RMSE = 0.89). The best model for predicting openness in sample included just average amount of time spent talking and entropy (median RMSE = 0.87). The prediction error from the analyses given can be converted to an R2 metric. This value is calculated using nested cross-validation, which provides a rigorous approach to 1Recurrence rate is the square of average amount of talking each day. Squaring this value can be thought of as a transformation (akin to taking the log). The transformed value makes high values spread apart (but does not do this for low values), emphasizing high values. This can be seen in the uppermost scatterplot in Figure 7, where low values of mTalk correspond only to low values of RR, but high values of mTalk correspond to a range of values of RR. Note that this is not an exact transformation, as the RR variable was calculated for each day and then averaged. estimating how accurate the model would be if it were used to predict scores from new data (see Supporting information). This version of R2 is calculated using sum of squared errors, which allows R2 to take negative values when guessing the mean of the out-of-sample data would give more accurate estimates than the model. The most accurate model for predicting extraversion is the two-predictor model including average amount of time talking and number of lines (R2 = ~.00), the most accurate model for agreeableness is the two-predictor model including average amount of time talking and entropy (R2 = .01), the most accurate model for predicting conscientiousness is the two-predictor model including average amount of talking and entropy (R2 = .01), the most accurate model for neuroticism is the five-predictor model that includes average amount of talking and four RQA parameters (RR, DET, NumL, andMaxL; R2 = .09), and the most accurate model for predicting openness is the two-predictor model including average amount of talking and DET (R2 = ~.00). The instances where the R2 is approximately equal to zero indicates that the value is so low that it rounds to zero at two decimals. The results presented in Figure 7 indicate that RQA parameters improve prediction given and beyond using a regression with just average amount of talking; however, the R2 values are very low. This is because Figure 7 compares the prediction accuracy of the five-predictor and two-predictor models to a one-predictor model that includes just the average amount of time spent talking. Our comparison point is how well we can predict new data from different models estimated using the same training data. The R2 values, on the other hand, are comparing the accuracy of different models that only had access to training data to predictions made from the mean of the test data. We can think of this as a situation where we are given a new data set and allowed to use one of two prediction rules: (i) make predictions using a fixed model that was developed on previous data or (ii) make predictions just using the mean of the new data. We only get positive R2 values when the fixed models (the five-predictor and two-predictor elastic FIGURE 7. Prediction error for elastic net models using socializing dynamics as predictors. Note: RMSE is root mean squared error. The density plot represents the variability in accuracy across 10 folds of nested cross-validation for the five-predictor model (the top row, labelled 'RQA') and all the two-predictor models (labelled based on the specific recurrence quantification analysis (RQA) parameter that is being tested 'above and beyond' average amount of talking). The line on the density distribution indicates the median of the distribution. The dotted lines in each panel represent the RMSE of a model using just average amount of time spent talking as a predictor. [Colour figure can be viewed at wileyonlinelibrary.com] net regressions) are better than guessing the mean of the new data. It might surprise many psychologists who do not routinely use machine learning in their research to learn that this is a very high bar to pass. The 'baseline models' in Figure 7 that use just average amount of time spent talking also do not pass this threshold (except in the case of agreeableness). We can therefore interpret our results as indicating that RQA parameters improve predictions of personality above-andbeyond using our static measure of socializing for all traits but agreeableness but that the models we estimated should not be expected to give very accurate results on new data (except in the case of neuroticism). The regression coefficients estimated in the cross validation are depicted in Figure 8. Figure 8A plots the coefficients in the models where only one RQAvariable at a time is used. Figure 8B plots the coefficients in the models where multiple RQA variables were used simultaneously. The narrow width of these box plots indicates that there was little variability in the estimate of the regression coefficients when different random subsets of the data were used. DISCUSSION We used RQA to quantify temporal aspects of socializing over the course of a single weekend for adults shortly following a marital separation. Prior research has examined dynamics in self-report data in the context of romantic relationships ; this research extends it into the domain of behaviour coded from observational ambulatory assessments. Comparisons of zero-order and partial correlations, and results of a machine learning model, support several general conclusions. First, as established in the literature, extraversion is most robustly associated with average amount of time spent talking with someone. Several RQA parameters were also significantly associated with extraversion, including measures of extended socializing bouts (per cent determinism and number of lines) and variability in length of extended socializing bouts (entropy). When adjusting for average amount of time spent talking, however, none of these associations remained significant at the p < .05 level. However, the adjusted association between extraversion and per cent determinism was close to significance (p < .10), and machine learning results indicate that adding number of lines to average time spent talking led to the best predictive accuracy in new, held-out data. Because these results were not as strong, further follow-up research is needed to establish this association. However, we would tentatively conclude that being extraverted is expressed not just in talking more often but in spending a greater proportion of socializing time in extended interactions. Second, we found that conscientiousness was associated to patterns of socializing throughout the day. Examining partial correlations that controlled for average amount of time spent talking, conscientiousness was significantly related to three indicators of socializing dynamics: max line, average line length, and entropy. The first two indicators suggest that more conscientious people have longer extended bouts of socializing. This may be because conscientiousness can be expressed in a conversation as a willingness to listen to another person fully, or to follow extended discussions to their full conclusions. The third indicator suggests that more conscientious people have extended interactions that vary more in length. This conclusion was also supported by the machine learning analysis, which found that a model with average amount of time spent talking combined with entropy was best at predicting conscientiousness. This association is less intuitive, but it may reflect a pattern of allowing conversations to last as long as it takes to express an idea--and not to fit them to proscribed lengths (like an hour-long meeting). Given the convergence between the partial correlation and machine learning results, the association between conscientiousness and entropy is most robust and therefore should be subject to further theorizing and research. The most robust associations were found between RQA parameters and neuroticism. Partial correlations, adjusted for average amount of time spent talking, found that neuroticism was associated with recurrence rate, per cent determinism, number of lines, maximum line length, and entropy. Further, the most accurate elastic net model used average amount of time spent talking and these same four RQA parameters. This predictive model had a large effect, by modern standards of personality (equivalent to r = .30). This association is particularly likely to be robust, given that it was estimating using cross-validation to avoid overfitting. Had these data been analysed using the traditional average amount of time spent talking, no significant association between socializing and neuroticism would have been found. When the dynamic variables were calculated using RQA, a large effect was found. Variable associations indicate that people who were more neurotic tend to spend a smaller proportion of their socializing time in extended interactions, to have fewer overall extended interactions, and to have their longest interactions in a day be shorter. In sum, these effects suggest that neurotic people have shorter conversations. The association of recurrence rate and neuroticism is analogous to (although not identical to, because of the daily averaging of RQA parameters) a quadratic effect. People who are more neurotic tend to be less likely to spend a lot of time socializing. Additionally, the association of neuroticism with entropy indicates that more neurotic people tend to have less variability in the lengths of their extended interactions. Together, these results suggest that neuroticism, not extraversion, is the personality trait most closely associated with the patterning of social interactions throughout the day. More neurotic people tend to have shorter interactions, perhaps because they become socially anxious during interactions, and so neurotic individuals end them more quickly. Alternately, it could be that people interacting with neurotic individuals feel less at ease, and so other people end conversations with neurotic individuals more quickly. Anxiety or sadness might similarly lead neurotic individuals to avoid interactions more generally, leading to fewer interactions (and explaining the negative association between neuroticism and recurrence rate). Less variability in conversation length might be related to this effect: by not letting conversations go on for very long, neurotic individuals might be constraining the variability in conversation lengths to short and medium length. This would explain the negative association between neuroticism and entropy. Given that our sample was of individuals who were undergoing marital separation, it may be that these effects are specific to the way neurotic individuals respond to this life stressor. These results suggest an important future direction for personality research: better explaining how and why neuroticism alters socializing dynamics in daily life, including whether this effect generalizes to new populations. Overall, our results suggest that just analysing the average amount of time a person spends talking can miss important relationships between personality and style of socializing. Our work adds to the growing literature on personality dynamics  and the dynamics of socializing. We present the first instance (to the best of our knowledge) of relating non-linear socializing dynamics assessed through ambulatory assessment to personality traits (; and Wang et al., 2018, for related analysis strategies, and Stachl et al., 2020, for some linear dynamic measures). Theoretically, a dynamic systems perspective suggests that personality can be conceptualized as a self-regulating network of components (;,;). Person and situation are in continual interaction, forming a feedback loop. These dynamics allow for self-regulation and the pursuit of key goals. For example, recent research has found that personality states 'enacted' at a given moment were related to the goals individuals were pursuing at that moment. This research found that people feeling extraverted in the moment tended to be more likely to report trying to 'be the centre of attention' and 'fit in'. Over time, a person's need to meet a goal--such as a need for belongingness--might increase as it has been longer because they have had a social interaction. This might drive their personality state, making them more extraverted, and their behaviour, making them more likely to interact with others. Once the person does have an interaction, that need might be reduced, altering personality state (less extraverted) and behaviour (less time talking to others). We might similarly suggest that socializing meets certain goals, and the patterning of socializing throughout the day might relate to how those goals are met. If trait level personality is taken as an indicator of the relative importance of goals for an individual (e.g. high extraversion indicates that an individual values social contact more highly), then we can explain our results in terms of need satisfaction. Extraverts have longer interactions because they have a stronger need for social interaction, and so it takes longer for their need for socializing to be sated. Avoidance goals--like a desire to avoid being judged negatively by others--might similarly drive conversation length for neurotic individuals. The need to avoid being judged might increase more quickly across the course of an interaction, leading more neurotic people to end the interaction more quickly. Although we did not assess need fulfilment in this study, examining the goal of specific interactions is an important future direction that can help develop the theoretical bases of personality dynamics. Limitations Applying RQA to EAR data involves assumptions that can guide the design of future studies. Sampling rate of sound files becomes much more important. The theoretical interpretations advanced here assume that when two successive time points include socializing, then there was enough socializing going on in between these sampled time points to consider the individual to be in a continuous state of socializing; this assumption is more likely the shorter the interval between time points. Future studies using the EAR or similar ambulatory assessment tools to measure socializing should test whether a sampling rate of 12 minutes yields similar RQA estimates of socializing dynamics as a study using a more frequent rate. Sampling rate also has implications for the time scale of the process being studied. Processes occurring faster than the sampling rate cannot be adequately captured because entire cycles could occur between measurement points. One concern related to this work is that RQA is more complex than it needs to be. If we are interested in studying extended socializing bouts, why do not we just calculate that number? As mentioned earlier, some machine learning researchers have used a 'kitchen sink' approach to studying socializing indicators of mobile sensing, using over 15 000 different variables in an analysis pipeline. One response is that adopting a well-established approach with a limited set of parameters, like RQA, allows us to focus more of our attention on the meaning of these parameters and how to interpret them in the context of mobile sensing. Indeed, we devoted a great deal of space to explaining how we calculate RQA and what its parameters mean theoretically for mobile sensing and personality research. Another answer is that linking mobile sensing dynamics to RQA connects the specific results we present here with a well-established empirical literature using RQA, starting in statistical physics. A physicist doing personality research might independently arrive at the idea of an eigenvalue\\/ eigenvector decomposition as a strategy for identifying major sources of variability in a personality inventory, but if they did, it would be incumbent on them to use the wellestablished vocabulary of exploratory factor analysis to describe their results to personality psychologists. Similarly, in adopting the RQA method from physics, psychologists should not reinvent or relabel the resulting metrics but instead should retain the names while interpreting their significance in this specific context. More importantly, simply calculating measures of extended socializing bouts without acknowledging the fact that these measures were developed based on years of learning about RQA would hide the intellectual history that led to these metrics. Further, RQA is an approach that is being actively developed, with new extensions being suggested recently. By adopting RQA as a standardized tool for the exploration of dynamics, we can more readily add to our analytic toolbox as other dynamics researchers build on the technique. Extensions and future directions Recurrence quantification analysis is a promising technique for application to observational ambulatory assessment and the EAR in particular, with many natural extensions. This manuscript focuses on one behavioural code, whether a sound file includes talking or not, because it is relatively frequent behaviour and has important psychological implications. However, it could readily be applied to other types of measurements. For example, periods of extended television watching--as quantified by RQA parameters--might be more highly associated with well-being than just overall amount of television watching. There are also several multivariate extensions of RQA that could be used to characterize the association between two or more EAR codes. For example, cross RQA, an analysis that quantifies recurrence patterns across two variables, could be used to identify how patterns of socializing relate to patterns of video game use in college students (e.g. bouts of social vs. solitary video game playing) or how patterns of socializing with friends are related to patterns of socializing with family members. Multivariate RQAmight be creatively used to study the relationship of three or more different features observed in EAR files and related ambulatory assessments. Applying RQA to high density observational data presents a large research community with a new method for capturing the patterning of daily life over time. Additionally, recent research has explored how instances of behavioural expression and trait personality are associated over time. Multivariate extensions of RQA would be able to capture non-linear associations between time series of these variables. As a new approach to analysing ambulatory assessment data, several questions remain to be addressed. These include the following: (i) establishing norms for RQA parameters in a larger sample; (ii) establishing the stability of RQA parameters across different lengths of time; (iii) examining the effects of sampling rate on RQA parameters; (iv) associating RQA parameters with more well-established constructs associated with socializing; and (v) determining whether experimental manipulations can alter RQA parameters and whether this can have further downstream effects. Planned research projects will address several of these issues, and we encourage researchers to consider using RQA to analyse their categorical mobile sensing data. CONCLUSION Using RQA on patterns of daily socialization reveals new associations between personality and socializing. By providing ways to quantify extended periods of socialization, it demonstrates how neuroticism is strongly associated to having shorter periods of socializing. These associations would have been missed if our only measure of socializing had been average amount of time spent talking. More broadly, these results suggest that personality does not just influence how often people engage in specific behaviours but the patterning of behaviours in their daily lives.\"}"